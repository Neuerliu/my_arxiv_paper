<!DOCTYPE html>
<html lang="en">

<head>
    <title>MyArxiv</title>
    <meta charset="utf-8"/>
    <meta http-equiv="X-UA-Compatible" content="IE=edge"/>
    <meta name="robots" content="noindex, nofollow"/>
    <meta name="viewport" content="width=device-width, initial-scale=1"/>
    <link rel="shortcut icon" type="image/x-icon" href="favicon.ico"/>
    <link href="index.css" rel="stylesheet"/>
    <link href="https://cdn.jsdelivr.net/npm/remixicon@2.5.0/fonts/remixicon.css" rel="stylesheet">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
          integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
            integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
            crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
            integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
            crossorigin="anonymous"></script>
    <script>
        document.addEventListener("DOMContentLoaded", function () {
            renderMathInElement(document.body, {
                // customised options
                // • auto-render specific keys, e.g.:
                delimiters: [
                    {left: '$$', right: '$$', display: true},
                    {left: '$', right: '$', display: false},
                    {left: '\\(', right: '\\)', display: false},
                    {left: '\\[', right: '\\]', display: true},
                    {left: "\\begin{equation}", right: "\\end{equation}", display: true},
                    {left: "\\begin{align}", right: "\\end{align}", display: true},
                    {left: "\\begin{alignat}", right: "\\end{alignat}", display: true},
                    {left: "\\begin{gather}", right: "\\end{gather}", display: true},
                    {left: "\\begin{CD}", right: "\\end{CD}", display: true},
                ],
                // • rendering keys, e.g.:
                throwOnError: false
            });
        });
    </script>
</head>

<body>
<section class="header-container">
    <div style="display:flex; justify-content:space-between; align-items:flex-end;">
        <div>
            <div class="header-title">
                MyArxiv
            </div>
        </div>

        <div class=icons>
            <label class="theme-switch" for="checkbox">
                <input type="checkbox" id="checkbox"/>
                <i id="theme-icon" class="ri-moon-line" style="font-size: 32px" rel="noopener noreferrer"></i>
            </label>
        </div>
    </div>
</section>

    <section class="day-container">
        <div class="date">
            <time datetime="2023-06-09T00:00:00Z">2023-06-09</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">49</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Models for Scalable Vector Graphics-Driven
  Image Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mu Cai, Zeyi Huang, Yuheng Li, Haohan Wang, Yong Jae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, large language models (LLMs) have made significant advancements in
natural language understanding and generation. However, their potential in
computer vision remains largely unexplored. In this paper, we introduce a new,
exploratory approach that enables LLMs to process images using the Scalable
Vector Graphics (SVG) format. By leveraging the XML-based textual descriptions
of SVG representations instead of raster images, we aim to bridge the gap
between the visual and textual modalities, allowing LLMs to directly understand
and manipulate images without the need for parameterized visual components. Our
method facilitates simple image classification, generation, and in-context
learning using only LLM capabilities. We demonstrate the promise of our
approach across discriminative and generative tasks, highlighting its (i)
robustness against distribution shift, (ii) substantial improvements achieved
by tapping into the in-context learning abilities of LLMs, and (iii) image
understanding and generation capabilities with human guidance. Our code, data,
and models can be found here https://github.com/mu-cai/svg-llm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Developing Speech Processing Pipelines for Police Accountability <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06086v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06086v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anjalie Field, Prateek Verma, Nay San, Jennifer L. Eberhardt, Dan Jurafsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Police body-worn cameras have the potential to improve accountability and
transparency in policing. Yet in practice, they result in millions of hours of
footage that is never reviewed. We investigate the potential of large
pre-trained speech models for facilitating reviews, focusing on ASR and officer
speech detection in footage from traffic stops. Our proposed pipeline includes
training data alignment and filtering, fine-tuning with resource constraints,
and combining officer speech detection with ASR for a fully automated approach.
We find that (1) fine-tuning strongly improves ASR performance on officer
speech (WER=12-13%), (2) ASR on officer speech is much more accurate than on
community member speech (WER=43.55-49.07%), (3) domain-specific tasks like
officer speech detection and diarization remain challenging. Our work offers
practical applications for reviewing body camera footage and general guidance
for adapting pre-trained speech models to noisy multi-speaker domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Trapping LLM Hallucinations Using Tagged Context <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Philip Feldman, James R. Foulds, Shimei Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language models (LLMs), such as ChatGPT, have led to
highly sophisticated conversation agents. However, these models suffer from
"hallucinations," where the model generates false or fabricated information.
Addressing this challenge is crucial, particularly with AI-driven platforms
being adopted across various sectors. In this paper, we propose a novel method
to recognize and flag instances when LLMs perform outside their domain
knowledge, and ensuring users receive accurate information.
  We find that the use of context combined with embedded tags can successfully
combat hallucinations within generative language models. To do this, we
baseline hallucination frequency in no-context prompt-response pairs using
generated URLs as easily-tested indicators of fabricated data. We observed a
significant reduction in overall hallucination when context was supplied along
with question prompts for tested generative engines. Lastly, we evaluated how
placing tags within contexts impacted model responses and were able to
eliminate hallucinations in responses with 98.88% effectiveness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 3 Figures, 2 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mind2Web: Towards a Generalist Agent for the Web 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06070v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06070v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samuel Stevens, Boshi Wang, Huan Sun, Yu Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Mind2Web, the first dataset for developing and evaluating
generalist agents for the web that can follow language instructions to complete
complex tasks on any website. Existing datasets for web agents either use
simulated websites or only cover a limited set of websites and tasks, thus not
suitable for generalist web agents. With over 2,000 open-ended tasks collected
from 137 websites spanning 31 domains and crowdsourced action sequences for the
tasks, Mind2Web provides three necessary ingredients for building generalist
web agents: 1) diverse domains, websites, and tasks, 2) use of real-world
websites instead of simulated and simplified ones, and 3) a broad spectrum of
user interaction patterns. Based on Mind2Web, we conduct an initial exploration
of using large language models (LLMs) for building generalist web agents. While
the raw HTML of real-world websites are often too large to be fed to LLMs, we
show that first filtering it with a small LM significantly improves the
effectiveness and efficiency of LLMs. Our solution demonstrates a decent level
of performance, even on websites or entire domains the model has never seen
before, but there is still a substantial room to improve towards truly
generalizable agents. We open-source our dataset, model implementation, and
trained models (https://osu-nlp-group.github.io/Mind2Web) to facilitate further
research on building a generalist agent for the web.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>website: https://osu-nlp-group.github.io/Mind2Web</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assisting Language Learners: Automated Trans-Lingual Definition
  Generation via Contrastive <span class="highlight-title">Prompt</span> Learning <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hengyuan Zhang, Dawei Li, Yanran Li, Chenming Shang, Chufan Shi, Yong Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The standard definition generation task requires to automatically produce
mono-lingual definitions (e.g., English definitions for English words), but
ignores that the generated definitions may also consist of unfamiliar words for
language learners. In this work, we propose a novel task of Trans-Lingual
Definition Generation (TLDG), which aims to generate definitions in another
language, i.e., the native speaker's language. Initially, we explore the
unsupervised manner of this task and build up a simple implementation of
fine-tuning the multi-lingual machine translation model. Then, we develop two
novel methods, Prompt Combination and Contrastive Prompt Learning, for further
enhancing the quality of the generation. Our methods are evaluated against the
baseline Pipeline method in both rich- and low-resource settings, and we
empirically establish its superiority in generating higher-quality
trans-lingual definitions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL-BEA workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fin<span class="highlight-title">GPT</span>: Open-Source Financial Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyang Yang, Xiao-Yang Liu, Christina Dan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown the potential of revolutionizing
natural language processing tasks in diverse domains, sparking great interest
in finance. Accessing high-quality financial data is the first challenge for
financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken
advantage of their unique data accumulation, such privileged access calls for
an open-source alternative to democratize Internet-scale financial data.
  In this paper, we present an open-source large language model, FinGPT, for
the finance sector. Unlike proprietary models, FinGPT takes a data-centric
approach, providing researchers and practitioners with accessible and
transparent resources to develop their FinLLMs. We highlight the importance of
an automatic data curation pipeline and the lightweight low-rank adaptation
technique in building FinGPT. Furthermore, we showcase several potential
applications as stepping stones for users, such as robo-advising, algorithmic
trading, and low-code development. Through collaborative efforts within the
open-source AI4Finance community, FinGPT aims to stimulate innovation,
democratize FinLLMs, and unlock new opportunities in open finance. Two
associated code repos are \url{https://github.com/AI4Finance-Foundation/FinGPT}
and \url{https://github.com/AI4Finance-Foundation/FinNLP}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HiTZ@Antidote: Argumentation-driven Explainable Artificial Intelligence
  for Digital Medicine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Agerri, Iñigo Alonso, Aitziber Atutxa, Ander Berrondo, Ainara Estarrona, Iker Garcia-Ferrero, Iakes Goenaga, Koldo Gojenola, Maite Oronoz, Igor Perez-Tejedor, German Rigau, Anar Yeginbergenova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Providing high quality explanations for AI predictions based on machine
learning is a challenging and complex task. To work well it requires, among
other factors: selecting a proper level of generality/specificity of the
explanation; considering assumptions about the familiarity of the explanation
beneficiary with the AI task under consideration; referring to specific
elements that have contributed to the decision; making use of additional
knowledge (e.g. expert evidence) which might not be part of the prediction
process; and providing evidence supporting negative hypothesis. Finally, the
system needs to formulate the explanation in a clearly interpretable, and
possibly convincing, way. Given these considerations, ANTIDOTE fosters an
integrated vision of explainable AI, where low-level characteristics of the
deep learning process are combined with higher level schemes proper of the
human argumentation capacity. ANTIDOTE will exploit cross-disciplinary
competences in deep learning and argumentation to support a broader and
innovative view of explainable AI, where the need for high-quality explanations
for clinical cases deliberation is critical. As a first result of the project,
we publish the Antidote CasiMedicos dataset to facilitate research on
explainable AI in general, and argumentation in the medical domain in
particular.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear: In SEPLN 2023: 39th International Conference of the
  Spanish Society for Natural Language Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Labeling of German Chest X-Ray Radiology Reports using Deep
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Wollek, Philip Haitzer, Thomas Sedlmeyr, Sardi Hyska, Johannes Rueckel, Bastian Sabel, Michael Ingrisch, Tobias Lasser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Radiologists are in short supply globally, and deep learning models offer a
promising solution to address this shortage as part of clinical
decision-support systems. However, training such models often requires
expensive and time-consuming manual labeling of large datasets. Automatic label
extraction from radiology reports can reduce the time required to obtain
labeled datasets, but this task is challenging due to semantically similar
words and missing annotated data. In this work, we explore the potential of
weak supervision of a deep learning-based label prediction model, using a
rule-based labeler. We propose a deep learning-based CheXpert label prediction
model, pre-trained on reports labeled by a rule-based German CheXpert model and
fine-tuned on a small dataset of manually labeled reports. Our results
demonstrate the effectiveness of our approach, which significantly outperformed
the rule-based model on all three tasks. Our findings highlight the benefits of
employing deep learning-based models even in scenarios with sparse data and the
use of the rule-based labeler as a tool for weak supervision.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models Can Learn Exceptions to Syntactic Rules <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cara Su-Yi Leong, Tal Linzen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial neural networks can generalize productively to novel contexts. Can
they also learn exceptions to those productive rules? We explore this question
using the case of restrictions on English passivization (e.g., the fact that
"The vacation lasted five days" is grammatical, but "*Five days was lasted by
the vacation" is not). We collect human acceptability judgments for passive
sentences with a range of verbs, and show that the probability distribution
defined by GPT-2, a language model, matches the human judgments with high
correlation. We also show that the relative acceptability of a verb in the
active vs. passive voice is positively correlated with the relative frequency
of its occurrence in those voices. These results provide preliminary support
for the entrenchment hypothesis, according to which learners track and uses the
distributional properties of their input to learn negative exceptions to rules.
At the same time, this hypothesis fails to explain the magnitude of
unpassivizability demonstrated by certain individual verbs, suggesting that
other cues to exceptionality are available in the linguistic input.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to SCiL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Efficient Speech Separation Network Based on Recurrent Fusion Dilated
  Convolution and Channel Attention 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an efficient speech separation neural network, ARFDCN, which
combines dilated convolutions, multi-scale fusion (MSF), and channel attention
to overcome the limited receptive field of convolution-based networks and the
high computational cost of transformer-based networks. The suggested network
architecture is encoder-decoder based. By using dilated convolutions with
gradually increasing dilation value to learn local and global features and
fusing them at adjacent stages, the model can learn rich feature content.
Meanwhile, by adding channel attention modules to the network, the model can
extract channel weights, learn more important features, and thus improve its
expressive power and robustness. Experimental results indicate that the model
achieves a decent balance between performance and computational efficiency,
making it a promising alternative to current mainstream models for practical
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Good, but not always Fair: An Evaluation of Gender Bias for three
  commercial Machine Translation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Silvia Alma Piazzolla, Beatrice Savoldi, Luisa Bentivogli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Translation (MT) continues to make significant strides in quality and
is increasingly adopted on a larger scale. Consequently, analyses have been
redirected to more nuanced aspects, intricate phenomena, as well as potential
risks that may arise from the widespread use of MT tools. Along this line, this
paper offers a meticulous assessment of three commercial MT systems - Google
Translate, DeepL, and Modern MT - with a specific focus on gender translation
and bias. For three language pairs (English/Spanish, English/Italian, and
English/French), we scrutinize the behavior of such systems at several levels
of granularity and on a variety of naturally occurring gender phenomena in
translation. Our study takes stock of the current state of online MT tools, by
revealing significant discrepancies in the gender translation of the three
systems, with each system displaying varying degrees of bias despite their
overall translation quality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review at HERMES Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards a Robust Detection of Language Model Generated Text: Is Chat<span class="highlight-title">GPT</span>
  that Easy to Detect? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wissam Antoun, Virginie Mouilleron, Benoît Sagot, Djamé Seddah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in natural language processing (NLP) have led to the
development of large language models (LLMs) such as ChatGPT. This paper
proposes a methodology for developing and evaluating ChatGPT detectors for
French text, with a focus on investigating their robustness on out-of-domain
data and against common attack schemes. The proposed method involves
translating an English dataset into French and training a classifier on the
translated data. Results show that the detectors can effectively detect
ChatGPT-generated text, with a degree of robustness against basic attack
techniques in in-domain settings. However, vulnerabilities are evident in
out-of-domain contexts, highlighting the challenge of detecting adversarial
text. The study emphasizes caution when applying in-domain testing results to a
wider variety of content. We provide our translated datasets and models as
open-source resources. https://gitlab.inria.fr/wantoun/robust-chatgpt-detection
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TALN 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Encoder-Decoder and Dual-Path Conformer for Comprehensive
  Feature Learning in Speech Enhancement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05861v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05861v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current speech enhancement (SE) research has largely neglected channel
attention and spatial attention, and encoder-decoder architecture-based
networks have not adequately considered how to provide efficient inputs to the
intermediate enhancement layer. To address these issues, this paper proposes a
time-frequency (T-F) domain SE network (DPCFCS-Net) that incorporates improved
densely connected blocks, dual-path modules, convolution-augmented transformers
(conformers), channel attention, and spatial attention. Compared with previous
models, our proposed model has a more efficient encoder-decoder and can learn
comprehensive features. Experimental results on the VCTK+DEMAND dataset
demonstrate that our method outperforms existing techniques in SE performance.
Furthermore, the improved densely connected block and two dimensions attention
module developed in this work are highly adaptable and easily integrated into
existing networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models Infer Causation from Correlation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona Diab, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal inference is one of the hallmarks of human intelligence. While the
field of CausalNLP has attracted much interest in the recent years, existing
causal inference datasets in NLP primarily rely on discovering causality from
empirical knowledge (e.g., commonsense knowledge). In this work, we propose the
first benchmark dataset to test the pure causal inference skills of large
language models (LLMs). Specifically, we formulate a novel task Corr2Cause,
which takes a set of correlational statements and determines the causal
relationship between the variables. We curate a large-scale dataset of more
than 400K samples, on which we evaluate seventeen existing LLMs. Through our
experiments, we identify a key shortcoming of LLMs in terms of their causal
inference skills, and show that these models achieve almost close to random
performance on the task. This shortcoming is somewhat mitigated when we try to
re-purpose LLMs for this skill via finetuning, but we find that these models
still fail to generalize -- they can only perform causal inference in
in-distribution settings when variable names and textual expressions used in
the queries are similar to those in the training set, but fail in
out-of-distribution settings generated by perturbing these queries. Corr2Cause
is a challenging task for LLMs, and would be helpful in guiding future research
on improving LLMs' pure reasoning skills and generalizability. Our data is at
https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at
https://github.com/causalNLP/corr2cause.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards the Exploitation of LLM-based Chatbot for Providing Legal
  Support to Palestinian Cooperatives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rabee Qasem, Banan Tantour, Mohammed Maree
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the ever-increasing utilization of natural language processing (NLP), we
started to witness over the past few years a significant transformation in our
interaction with legal texts. This technology has advanced the analysis and
enhanced the understanding of complex legal terminology and contexts. The
development of recent large language models (LLMs), particularly ChatGPT, has
also introduced a revolutionary contribution to the way that legal texts can be
processed and comprehended. In this paper, we present our work on a
cooperative-legal question-answering LLM-based chatbot, where we developed a
set of legal questions about Palestinian cooperatives, associated with their
regulations and compared the auto-generated answers by the chatbot to their
correspondences that are designed by a legal expert. To evaluate the proposed
chatbot, we have used 50 queries generated by the legal expert and compared the
answers produced by the chart to their relevance judgments. Finding
demonstrated that an overall accuracy rate of 82% has been achieved when
answering the queries, while exhibiting an F1 score equivalent to 79%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causality between Sentiment and Cryptocurrency Prices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lubdhak Mondal, Udeshya Raj, Abinandhan S, Began Gowsik S, Sarwesh P, Abhijeet Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the relationship between narratives conveyed through
microblogging platforms, namely Twitter, and the value of crypto assets. Our
study provides a unique technique to build narratives about cryptocurrency by
combining topic modelling of short texts with sentiment analysis. First, we
used an unsupervised machine learning algorithm to discover the latent topics
within the massive and noisy textual data from Twitter, and then we revealed
4-5 cryptocurrency-related narratives, including financial investment,
technological advancement related to crypto, financial and political
regulations, crypto assets, and media coverage. In a number of situations, we
noticed a strong link between our narratives and crypto prices. Our work
connects the most recent innovation in economics, Narrative Economics, to a new
area of study that combines topic modelling and sentiment analysis to relate
consumer behaviour to narratives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Xiezhi: An Ever-Updating Benchmark for Holistic Domain Knowledge
  Evaluation <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang, Sihang Jiang, Zhuozhi Xiong, Zihan Li, Qianyu He, Rui Xu, Wenhao Huang, Weiguo Zheng, Hongwei Feng, Yanghua Xiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  New Natural Langauge Process~(NLP) benchmarks are urgently needed to align
with the rapid development of large language models (LLMs). We present Xiezhi,
the most comprehensive evaluation suite designed to assess holistic domain
knowledge. Xiezhi comprises multiple-choice questions across 516 diverse
disciplines ranging from 13 different subjects with 220,000 questions and
accompanied by Xiezhi-Specialty and Xiezhi-Interdiscipline, both with 15k
questions. We conduct evaluation of the 47 cutting-edge LLMs on Xiezhi. Results
indicate that LLMs exceed average performance of humans in science,
engineering, agronomy, medicine, and art, but fall short in economics,
jurisprudence, pedagogy, literature, history, and management. We anticipate
Xiezhi will help analyze important strengths and shortcomings of LLMs, and the
benchmark is released in https://github.com/MikeGu721/XiezhiBenchmark .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review of NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>-based Time-to-Event Prediction for Chronic Kidney Disease
  Deterioration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moshe Zisser, Dvir Aran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep-learning techniques, particularly the transformer model, have shown
great potential in enhancing the prediction performance of longitudinal health
records. While previous methods have mainly focused on fixed-time risk
prediction, time-to-event prediction (also known as survival analysis) is often
more appropriate for clinical scenarios. Here, we present a novel deep-learning
architecture we named STRAFE, a generalizable survival analysis
transformer-based architecture for electronic health records. The performance
of STRAFE was evaluated using a real-world claim dataset of over 130,000
individuals with stage 3 chronic kidney disease (CKD) and was found to
outperform other time-to-event prediction algorithms in predicting the exact
time of deterioration to stage 5. Additionally, STRAFE was found to outperform
binary outcome algorithms in predicting fixed-time risk, possibly due to its
ability to train on censored data. We show that STRAFE predictions can improve
the positive predictive value of high-risk patients by 3-fold, demonstrating
possible usage to improve targeting for intervention programs. Finally, we
suggest a novel visualization approach to predictions on a per-patient basis.
In conclusion, STRAFE is a cutting-edge time-to-event prediction algorithm that
has the potential to enhance risk predictions in large claims datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Challenges and Opportunities for the Design of Smart Speakers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05741v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05741v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Long, Lydia B. Chilton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advances in voice technology and voice user interfaces (VUIs) -- such as
Alexa, Siri, and Google Home -- have opened up the potential for many new types
of interaction. However, despite the potential of these devices reflected by
the growing market and body of VUI research, there is a lingering sense that
the technology is still underused. In this paper, we conducted a systematic
literature review of 35 papers to identify and synthesize 127 VUI design
guidelines into five themes. Additionally, we conducted semi-structured
interviews with 15 smart speaker users to understand their use and non-use of
the technology. From the interviews, we distill four design challenges that
contribute the most to non-use. Based on their (non-)use, we identify four
opportunity spaces for designers to explore such as focusing on information
support while multitasking (cooking, driving, childcare, etc), incorporating
users' mental models for smart speakers, and integrating calm design
principles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Responses of Large Language Models to Beginner
  Programmers' Help Requests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arto Hellas, Juho Leinonen, Sami Sarsa, Charles Koutcheme, Lilja Kujanpää, Juha Sorva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background and Context: Over the past year, large language models (LLMs) have
taken the world by storm. In computing education, like in other walks of life,
many opportunities and threats have emerged as a consequence.
  Objectives: In this article, we explore such opportunities and threats in a
specific area: responding to student programmers' help requests. More
specifically, we assess how good LLMs are at identifying issues in problematic
code that students request help on.
  Method: We collected a sample of help requests and code from an online
programming course. We then prompted two different LLMs (OpenAI Codex and
GPT-3.5) to identify and explain the issues in the students' code and assessed
the LLM-generated answers both quantitatively and qualitatively.
  Findings: GPT-3.5 outperforms Codex in most respects. Both LLMs frequently
find at least one actual issue in each student program (GPT-3.5 in 90% of the
cases). Neither LLM excels at finding all the issues (GPT-3.5 finding them 57%
of the time). False positives are common (40% chance for GPT-3.5). The advice
that the LLMs provide on the issues is often sensible. The LLMs perform better
on issues involving program logic rather than on output formatting. Model
solutions are frequently provided even when the LLM is prompted not to. LLM
responses to prompts in a non-English language are only slightly worse than
responses to English prompts.
  Implications: Our results continue to highlight the utility of LLMs in
programming education. At the same time, the results highlight the
unreliability of LLMs: LLMs make some of the same mistakes that students do,
perhaps especially when formatting output as required by automated assessment
systems. Our study informs teachers interested in using LLMs as well as future
efforts to customize LLMs for the needs of programming education.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 1 figure. To be published in Proceedings of the 2023 ACM
  Conference on International Computing Education Research V.1 (ICER '23 V1)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Emotional Representations from Imbalanced Speech Data for
  Speech Emotion Recognition and Emotional Text-to-Speech <span class="chip">INTERSPEECH2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shijun Wang, Jón Guðnason, Damian Borth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective speech emotional representations play a key role in Speech Emotion
Recognition (SER) and Emotional Text-To-Speech (TTS) tasks. However, emotional
speech samples are more difficult and expensive to acquire compared with
Neutral style speech, which causes one issue that most related works
unfortunately neglect: imbalanced datasets. Models might overfit to the
majority Neutral class and fail to produce robust and effective emotional
representations. In this paper, we propose an Emotion Extractor to address this
issue. We use augmentation approaches to train the model and enable it to
extract effective and generalizable emotional representations from imbalanced
datasets. Our empirical results show that (1) for the SER task, the proposed
Emotion Extractor surpasses the state-of-the-art baseline on three imbalanced
datasets; (2) the produced representations from our Emotion Extractor benefit
the TTS model, and enable it to synthesize more expressive speech.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by INTERSPEECH2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Judging LLM-as-a-judge with MT-Bench and Chatbot Arena 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. P Xing, Hao Zhang, Joseph E. Gonzalez, Ion Stoica
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Evaluating large language model (LLM) based chat assistants is challenging
due to their broad capabilities and the inadequacy of existing benchmarks in
measuring human preferences. To address this, we explore using strong LLMs as
judges to evaluate these models on more open-ended questions. We examine the
usage and limitations of LLM-as-a-judge, such as position and verbosity biases
and limited reasoning ability, and propose solutions to migrate some of them.
We then verify the agreement between LLM judges and human preferences by
introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot
Arena, a crowdsourced battle platform. Our results reveal that strong LLM
judges like GPT-4 can match both controlled and crowdsourced human preferences
well, achieving over 80\% agreement, the same level of agreement between
humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate
human preferences, which are otherwise very expensive to obtain. Additionally,
we show our benchmark and traditional benchmarks complement each other by
evaluating several variants of LLaMA/Vicuna. We will publicly release 80
MT-bench questions, 3K expert votes, and 30K conversations with human
preferences from Chatbot Arena.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ I run as fast as a rabbit, can you? A Multilingual Simile Dialogue
  <span class="highlight-title">Dataset</span> <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05672v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05672v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Longxuan Ma, Weinan Zhang, Shuhan Zhou, Churui Sun, Changxin Ke, Ting Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A simile is a figure of speech that compares two different things (called the
tenor and the vehicle) via shared properties. The tenor and the vehicle are
usually connected with comparator words such as "like" or "as". The simile
phenomena are unique and complex in a real-life dialogue scene where the tenor
and the vehicle can be verbal phrases or sentences, mentioned by different
speakers, exist in different sentences, or occur in reversed order. However,
the current simile research usually focuses on similes in a triplet tuple
(tenor, property, vehicle) or a single sentence where the tenor and vehicle are
usually entities or noun phrases, which could not reflect complex simile
phenomena in real scenarios. In this paper, we propose a novel and high-quality
multilingual simile dialogue (MSD) dataset to facilitate the study of complex
simile phenomena. The MSD is the largest manually annotated simile data
($\sim$20K) and it contains both English and Chinese data. Meanwhile, the MSD
data can also be used on dialogue tasks to test the ability of dialogue systems
when using similes. We design 3 simile tasks (recognition, interpretation, and
generation) and 2 dialogue tasks (retrieval and generation) with MSD. For each
task, we provide experimental results from strong pre-trained or
state-of-the-art models. The experiments demonstrate the challenge of MSD and
we have released the data/code on GitHub.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 Pages, 1 Figure, 12 Tables, ACL 2023 findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COVER: A Heuristic Greedy Adversarial Attack on <span class="highlight-title">Prompt</span>-based Learning in
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihao Tan, Qingliang Chen, Wenbin Zhu, Yongjian Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-based learning has been proved to be an effective way in pre-trained
language models (PLMs), especially in low-resource scenarios like few-shot
settings. However, the trustworthiness of PLMs is of paramount significance and
potential vulnerabilities have been shown in prompt-based templates that could
mislead the predictions of language models, causing serious security concerns.
In this paper, we will shed light on some vulnerabilities of PLMs, by proposing
a prompt-based adversarial attack on manual templates in black box scenarios.
First of all, we design character-level and word-level heuristic approaches to
break manual templates separately. Then we present a greedy algorithm for the
attack based on the above heuristic destructive approaches. Finally, we
evaluate our approach with the classification tasks on three variants of BERT
series models and eight datasets. And comprehensive experimental results
justify the effectiveness of our approach in terms of attack success rate and
attack speed. Further experimental studies indicate that our proposed method
also displays good capabilities in scenarios with varying shot counts, template
lengths and query counts, exhibiting good generalizability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy Aware Question-Answering System for Online Mental Health Risk
  Assessment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prateek Chhikara, Ujjwal Pasupulety, John Marshall, Dhiraj Chaurasia, Shweta Kumari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media platforms have enabled individuals suffering from mental
illnesses to share their lived experiences and find the online support
necessary to cope. However, many users fail to receive genuine clinical
support, thus exacerbating their symptoms. Screening users based on what they
post online can aid providers in administering targeted healthcare and minimize
false positives. Pre-trained Language Models (LMs) can assess users' social
media data and classify them in terms of their mental health risk. We propose a
Question-Answering (QA) approach to assess mental health risk using the
Unified-QA model on two large mental health datasets. To protect user data, we
extend Unified-QA by anonymizing the model training process using differential
privacy. Our results demonstrate the effectiveness of modeling risk assessment
as a QA task, specifically for mental health use cases. Furthermore, the
model's performance decreases by less than 1% with the inclusion of
differential privacy. The proposed system's performance is indicative of a
promising research direction that will lead to the development of privacy-aware
diagnostic systems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ WSPAlign: Word Alignment <span class="highlight-title">Pre-train</span>ing via Large-Scale Weakly Supervised
  Span Prediction <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05644v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05644v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiyu Wu, Masaaki Nagata, Yoshimasa Tsuruoka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing word alignment methods rely on manual alignment datasets or
parallel corpora, which limits their usefulness. Here, to mitigate the
dependence on manual data, we broaden the source of supervision by relaxing the
requirement for correct, fully-aligned, and parallel sentences. Specifically,
we make noisy, partially aligned, and non-parallel paragraphs. We then use such
a large-scale weakly-supervised dataset for word alignment pre-training via
span prediction. Extensive experiments with various settings empirically
demonstrate that our approach, which is named WSPAlign, is an effective and
scalable way to pre-train word aligners without manual data. When fine-tuned on
standard benchmarks, WSPAlign has set a new state-of-the-art by improving upon
the best-supervised baseline by 3.3~6.1 points in F1 and 1.5~6.1 points in AER.
Furthermore, WSPAlign also achieves competitive performance compared with the
corresponding baselines in few-shot, zero-shot and cross-lingual tests, which
demonstrates that WSPAlign is potentially more practical for low-resource
languages than existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Customizing General-Purpose Foundation Models for Medical Report
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05642v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05642v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bang Yang, Asif Raza, Yuexian Zou, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical caption prediction which can be regarded as a task of medical report
generation (MRG), requires the automatic generation of coherent and accurate
captions for the given medical images. However, the scarcity of labelled
medical image-report pairs presents great challenges in the development of deep
and large-scale neural networks capable of harnessing the potential artificial
general intelligence power like large language models (LLMs). In this work, we
propose customizing off-the-shelf general-purpose large-scale pre-trained
models, i.e., foundation models (FMs), in computer vision and natural language
processing with a specific focus on medical report generation. Specifically,
following BLIP-2, a state-of-the-art vision-language pre-training approach, we
introduce our encoder-decoder-based MRG model. This model utilizes a
lightweight query Transformer to connect two FMs: the giant vision Transformer
EVA-ViT-g and a bilingual LLM trained to align with human intentions (referred
to as ChatGLM-6B). Furthermore, we conduct ablative experiments on the
trainable components of the model to identify the crucial factors for effective
transfer learning. Our findings demonstrate that unfreezing EVA-ViT-g to learn
medical image representations, followed by parameter-efficient training of
ChatGLM-6B to capture the writing styles of medical reports, is essential for
achieving optimal results. Our best attempt (PCLmed Team) achieved the 4th and
the 2nd, respectively, out of 13 participating teams, based on the BERTScore
and ROUGE-1 metrics, in the ImageCLEFmedical Caption 2023 Caption Prediction
Task competition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Low-rank Adaptation Method for Wav2vec2-based Fake Audio Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05617v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05617v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenglong Wang, Jiangyan Yi, Xiaohui Zhang, Jianhua Tao, Le Xu, Ruibo Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised speech models are a rapidly developing research topic in fake
audio detection. Many pre-trained models can serve as feature extractors,
learning richer and higher-level speech features. However,when fine-tuning
pre-trained models, there is often a challenge of excessively long training
times and high memory consumption, and complete fine-tuning is also very
expensive. To alleviate this problem, we apply low-rank adaptation(LoRA) to the
wav2vec2 model, freezing the pre-trained model weights and injecting a
trainable rank-decomposition matrix into each layer of the transformer
architecture, greatly reducing the number of trainable parameters for
downstream tasks. Compared with fine-tuning with Adam on the wav2vec2 model
containing 317M training parameters, LoRA achieved similar performance by
reducing the number of trainable parameters by 198 times.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Word sense extension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05609v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05609v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Yu, Yang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans often make creative use of words to express novel senses. A
long-standing effort in natural language processing has been focusing on word
sense disambiguation (WSD), but little has been explored about how the sense
inventory of a word may be extended toward novel meanings. We present a
paradigm of word sense extension (WSE) that enables words to spawn new senses
toward novel context. We develop a framework that simulates novel word sense
extension by first partitioning a polysemous word type into two pseudo-tokens
that mark its different senses, and then inferring whether the meaning of a
pseudo-token can be extended to convey the sense denoted by the token
partitioned from the same word type. Our framework combines cognitive models of
chaining with a learning scheme that transforms a language model embedding
space to support various types of word sense extension. We evaluate our
framework against several competitive baselines and show that it is superior in
predicting plausible novel senses for over 7,500 English words. Furthermore, we
show that our WSE framework improves performance over a range of
transformer-based WSD models in predicting rare word senses with few or zero
mentions in the training data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Unified Generative Approach to Product Attribute-Value Identification <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05605v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05605v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keiji Shinzato, Naoki Yoshinaga, Yandi Xia, Wei-Te Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Product attribute-value identification (PAVI) has been studied to link
products on e-commerce sites with their attribute values (e.g., <Material,
Cotton>) using product text as clues. Technical demands from real-world
e-commerce platforms require PAVI methods to handle unseen values,
multi-attribute values, and canonicalized values, which are only partly
addressed in existing extraction- and classification-based approaches.
Motivated by this, we explore a generative approach to the PAVI task. We
finetune a pre-trained generative model, T5, to decode a set of attribute-value
pairs as a target sequence from the given product text. Since the attribute
value pairs are unordered set elements, how to linearize them will matter; we,
thus, explore methods of composing an attribute-value pair and ordering the
pairs for the task. Experimental results confirm that our generation-based
approach outperforms the existing extraction and classification-based methods
on large-scale real-world datasets meant for those methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the Findings of ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Reliability of Watermarks for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, Tom Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As LLMs become commonplace, machine-generated text has the potential to flood
the internet with spam, social media bots, and valueless content. Watermarking
is a simple and effective strategy for mitigating such harms by enabling the
detection and documentation of LLM-generated text. Yet a crucial question
remains: How reliable is watermarking in realistic settings in the wild? There,
watermarked text may be modified to suit a user's needs, or entirely rewritten
to avoid detection.
  We study the robustness of watermarked text after it is re-written by humans,
paraphrased by a non-watermarked LLM, or mixed into a longer hand-written
document. We find that watermarks remain detectable even after human and
machine paraphrasing. While these attacks dilute the strength of the watermark,
paraphrases are statistically likely to leak n-grams or even longer fragments
of the original text, resulting in high-confidence detections when enough
tokens are observed. For example, after strong human paraphrasing the watermark
is detectable after observing 800 tokens on average, when setting a 1e-5 false
positive rate. We also consider a range of new detection schemes that are
sensitive to short spans of watermarked text embedded inside a large document,
and we compare the robustness of watermarking to other kinds of detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages in the main body. Code is available at
  https://github.com/jwkirchenbauer/lm-watermarking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AUTODIAL: Efficient Asynchronous Task-Oriented Dialogue Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06245v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06245v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prajjwal Bhargava, Pooyan Amini, Shahin Shayandeh, Chinnadhurai Sankar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As large dialogue models become commonplace in practice, the problems
surrounding high compute requirements for training, inference and larger memory
footprint still persists. In this work, we present AUTODIAL, a multi-task
dialogue model that addresses the challenges of deploying dialogue model.
AUTODIAL utilizes parallel decoders to perform tasks such as dialogue act
prediction, domain prediction, intent prediction, and dialogue state tracking.
Using classification decoders over generative decoders allows AUTODIAL to
significantly reduce memory footprint and achieve faster inference times
compared to existing generative approach namely SimpleTOD. We demonstrate that
AUTODIAL provides 3-6x speedups during inference while having 11x fewer
parameters on three dialogue tasks compared to SimpleTOD. Our results show that
extending current dialogue models to have parallel decoders can be a viable
alternative for deploying them in resource-constrained environments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Unleash the Power of Large Language Models for Few-shot Relation
  Extraction? <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.01555v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.01555v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Xu, Yuqi Zhu, Xiaohan Wang, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling language models have revolutionized widespread NLP tasks, yet little
comprehensively explored few-shot relation extraction with large language
models. In this paper, we investigate principal methodologies, in-context
learning and data generation, for few-shot relation extraction via GPT-3.5
through exhaustive experiments. To enhance few-shot performance, we further
propose task-related instructions and schema-constrained data generation. We
observe that in-context learning can achieve performance on par with previous
prompt learning approaches, and data generation with the large language model
can boost previous solutions to obtain new state-of-the-art few-shot results on
four widely-studied relation extraction datasets. We hope our work can inspire
future research for the capabilities of large language models in few-shot
relation extraction. Code is available in
https://github.com/zjunlp/DeepKE/tree/main/example/llm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SustaiNLP Workshop@ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Discontinuous Constituency Parsing with Mildly
  Context-Sensitive Grammars <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09140v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09140v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Songlin Yang, Roger P. Levy, Yoon Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study grammar induction with mildly context-sensitive grammars for
unsupervised discontinuous parsing. Using the probabilistic linear context-free
rewriting system (LCFRS) formalism, our approach fixes the rule structure in
advance and focuses on parameter learning with maximum likelihood. To reduce
the computational complexity of both parsing and parameter estimation, we
restrict the grammar formalism to LCFRS-2 (i.e., binary LCFRS with fan-out two)
and further discard rules that require O(n^6) time to parse, reducing inference
to O(n^5). We find that using a large number of nonterminals is beneficial and
thus make use of tensor decomposition-based rank-space dynamic programming with
an embedding-based parameterization of rule probabilities to scale up the
number of nonterminals. Experiments on German and Dutch show that our approach
is able to induce linguistically meaningful trees with continuous and
discontinuous structures
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Abstraction and Reasoning through Language <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04091v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04091v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Camposampiero, Loic Houmard, Benjamin Estermann, Joël Mathys, Roger Wattenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Artificial Intelligence (AI) models have achieved human or even
superhuman performance in narrowly defined applications, they still struggle to
show signs of broader and more flexible intelligence. The Abstraction and
Reasoning Corpus (ARC), introduced by Fran\c{c}ois Chollet, aims to assess how
close AI systems are to human-like cognitive abilities. Most current approaches
rely on carefully handcrafted domain-specific languages (DSLs), which are used
to brute-force solutions to the tasks present in ARC. In this work, we propose
a general framework for solving ARC based on natural language descriptions of
the tasks. While not yet beating state-of-the-art DSL models on ARC, we
demonstrate the immense potential of our approach hinted at by the ability to
solve previously unsolved tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors have contributed equally to this work. Accepted
  as regular paper at CVPR 2023 Workshop and Challenges for New Frontiers in
  Visual Language Reasoning: Compositionality, Prompts and Causality (NFVLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BridgeTower: Building Bridges Between Encoders in Vision-Language
  Representation Learning <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08657v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08657v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language (VL) models with the Two-Tower architecture have dominated
visual-language representation learning in recent years. Current VL models
either use lightweight uni-modal encoders and learn to extract, align and fuse
both modalities simultaneously in a deep cross-modal encoder, or feed the
last-layer uni-modal representations from the deep pre-trained uni-modal
encoders into the top cross-modal encoder. Both approaches potentially restrict
vision-language representation learning and limit model performance. In this
paper, we propose BridgeTower, which introduces multiple bridge layers that
build a connection between the top layers of uni-modal encoders and each layer
of the cross-modal encoder. This enables effective bottom-up cross-modal
alignment and fusion between visual and textual representations of different
semantic levels of pre-trained uni-modal encoders in the cross-modal encoder.
Pre-trained with only 4M images, BridgeTower achieves state-of-the-art
performance on various downstream vision-language tasks. In particular, on the
VQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming
the previous state-of-the-art model METER by 1.09% with the same pre-training
data and almost negligible additional parameters and computational costs.
Notably, when further scaling the model, BridgeTower achieves an accuracy of
81.15%, surpassing models that are pre-trained on orders-of-magnitude larger
datasets. Code and checkpoints are available at
https://github.com/microsoft/BridgeTower.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023, Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HiTIN: Hierarchy-aware Tree Isomorphism Network for Hierarchical Text
  Classification <span class="chip">ACL'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15182v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15182v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        He Zhu, Chong Zhang, Junjie Huang, Junran Wu, Ke Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hierarchical text classification (HTC) is a challenging subtask of
multi-label classification as the labels form a complex hierarchical structure.
Existing dual-encoder methods in HTC achieve weak performance gains with huge
memory overheads and their structure encoders heavily rely on domain knowledge.
Under such observation, we tend to investigate the feasibility of a
memory-friendly model with strong generalization capability that could boost
the performance of HTC without prior statistics or label semantics. In this
paper, we propose Hierarchy-aware Tree Isomorphism Network (HiTIN) to enhance
the text representations with only syntactic information of the label
hierarchy. Specifically, we convert the label hierarchy into an unweighted tree
structure, termed coding tree, with the guidance of structural entropy. Then we
design a structure encoder to incorporate hierarchy-aware information in the
coding tree into text representations. Besides the text encoder, HiTIN only
contains a few multi-layer perceptions and linear transformations, which
greatly saves memory. We conduct experiments on three commonly used datasets
and the results demonstrate that HiTIN could achieve better test performance
and less memory consumption than state-of-the-art (SOTA) methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Semiotics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2008.10522v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2008.10522v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peter beim Graben, Markus Huber-Liebl, Peter Klimczak, Günther Wirsching
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recognizing a basic difference between the semiotics of humans and machines
presents a possibility to overcome the shortcomings of current speech assistive
devices. For the machine, the meaning of a (human) utterance is defined by its
own scope of actions. Machines, thus, do not need to understand the
conventional meaning of an utterance. Rather, they draw conversational
implicatures in the sense of (neo-)Gricean pragmatics. For speech assistive
devices, the learning of machine-specific meanings of human utterances, i.e.
the fossilization of conversational implicatures into conventionalized ones by
trial and error through lexicalization appears to be sufficient. Using the
quite trivial example of a cognitive heating device, we show that - based on
dynamic semantics - this process can be formalized as the reinforcement
learning of utterance-meaning pairs (UMP).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Arithmetic-Based <span class="highlight-title">Pretrain</span>ing -- Improving Numeracy of <span class="highlight-title">Pretrain</span>ed
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.06733v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.06733v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dominic Petrak, Nafise Sadat Moosavi, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  State-of-the-art pretrained language models tend to perform below their
capabilities when applied out-of-the-box on tasks that require understanding
and working with numbers. Recent work suggests two main reasons for this: (1)
popular tokenisation algorithms have limited expressiveness for numbers, and
(2) common pretraining objectives do not target numeracy. Approaches that
address these shortcomings usually require architectural changes or pretraining
from scratch. In this paper, we propose a new extended pretraining approach
called Arithmetic-Based Pretraining that jointly addresses both in one extended
pretraining step without requiring architectural changes or pretraining from
scratch. Arithmetic-Based Pretraining combines contrastive learning to improve
the number representation, and a novel extended pretraining objective called
Inferable Number Prediction Task to improve numeracy. Our experiments show the
effectiveness of Arithmetic-Based Pretraining in three different tasks that
require improved numeracy, i.e., reading comprehension in the DROP dataset,
inference-on-tables in the InfoTabs dataset, and table-to-text generation in
the WikiBio and SciGen datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at StarSEM2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Multi-bit Natural Language Watermarking through Invariant
  Features <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.01904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.01904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        KiYoon Yoo, Wonhyuk Ahn, Jiho Jang, Nojun Kwak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have witnessed a proliferation of valuable original natural
language contents found in subscription-based media outlets, web novel
platforms, and outputs of large language models. However, these contents are
susceptible to illegal piracy and potential misuse without proper security
measures. This calls for a secure watermarking system to guarantee copyright
protection through leakage tracing or ownership identification. To effectively
combat piracy and protect copyrights, a multi-bit watermarking framework should
be able to embed adequate bits of information and extract the watermarks in a
robust manner despite possible corruption. In this work, we explore ways to
advance both payload and robustness by following a well-known proposition from
image watermarking and identify features in natural language that are invariant
to minor corruption. Through a systematic analysis of the possible sources of
errors, we further propose a corruption-resistant infill model. Our full method
improves upon the previous work on robustness by +16.8% point on average on
four datasets, three corruption types, and two corruption ratios. Code
available at https://github.com/bangawayoo/nlp-watermarking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 long</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CELDA: Leveraging Black-box Language Model as Enhanced Classifier
  without Labels <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02693v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02693v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunsoo Cho, Youna Kim, Sang-goo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing language models (LMs) without internal access is becoming an
attractive paradigm in the field of NLP as many cutting-edge LMs are released
through APIs and boast a massive scale. The de-facto method in this type of
black-box scenario is known as prompting, which has shown progressive
performance enhancements in situations where data labels are scarce or
unavailable. Despite their efficacy, they still fall short in comparison to
fully supervised counterparts and are generally brittle to slight
modifications. In this paper, we propose Clustering-enhanced Linear
Discriminative Analysis, a novel approach that improves the text classification
accuracy with a very weak-supervision signal (i.e., name of the labels). Our
framework draws a precise decision boundary without accessing weights or
gradients of the LM model or data labels. The core ideas of CELDA are twofold:
(1) extracting a refined pseudo-labeled dataset from an unlabeled dataset, and
(2) training a lightweight and robust model on the top of LM, which learns an
accurate decision boundary from an extracted noisy dataset. Throughout in-depth
investigations on various datasets, we demonstrated that CELDA reaches new
state-of-the-art in weakly-supervised text classification and narrows the gap
with a fully-supervised model. Additionally, our proposed methodology can be
applied universally to any LM and has the potential to scale to larger models,
making it a more viable option for utilizing large LMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Probing Out-of-Distribution Robustness of Language Models with
  Parameter-Efficient Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11660v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11660v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunsoo Cho, Choonghyun Park, Junyeop Kim, Hyuhng Joon Kim, Kang Min Yoo, Sang-goo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the size of the pre-trained language model (PLM) continues to increase,
numerous parameter-efficient transfer learning methods have been proposed
recently to compensate for the tremendous cost of fine-tuning. Despite the
impressive results achieved by large pre-trained language models (PLMs) and
various parameter-efficient transfer learning (PETL) methods on sundry
benchmarks, it remains unclear if they can handle inputs that have been
distributionally shifted effectively. In this study, we systematically explore
how the ability to detect out-of-distribution (OOD) changes as the size of the
PLM grows or the transfer methods are altered. Specifically, we evaluated
various PETL techniques, including fine-tuning, Adapter, LoRA, and
prefix-tuning, on three different intention classification tasks, each
utilizing various language models with different scales.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>*SEM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Training Data in Few-Shot <span class="highlight-title">Prompt</span>ing for Numerical Reasoning <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18170v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18170v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanming Jie, Wei Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought (CoT) prompting with large language models has proven
effective in numerous natural language processing tasks, but designing prompts
that generalize well to diverse problem types can be challenging, especially in
the context of math word problem (MWP) solving. Additionally, it is common to
have a large amount of training data that have a better diversity coverage but
CoT annotations are not available, which limits the use of supervised learning
techniques. To address these issues, we investigate two approaches to leverage
the training data in a few-shot prompting scenario: dynamic program prompting
and program distillation. Our approach is largely inspired by Gao et al.,
(2022), where they proposed to replace the CoT with the programs as the
intermediate reasoning step. Such a prompting strategy allows us to accurately
verify the answer correctness through program execution in MWP solving. Our
dynamic program prompting involves annotating the training data by sampling
correct programs from a large language model, while program distillation
involves adapting a smaller model to the program-annotated training data. Our
experiments on three standard MWP datasets demonstrate the effectiveness of
these approaches, yielding significant improvements over previous baselines for
prompting and fine-tuning. Our results suggest that leveraging a large amount
of training data can improve the generalization ability of prompts and boost
the performance of fine-tuned small models in MWP solving.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Speech-Text Dialog <span class="highlight-title">Pre-train</span>ing for Spoken Dialog Understanding with
  Explicit Cross-Modal Alignment <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11579v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11579v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianshu Yu, Haoyu Gao, Ting-En Lin, Min Yang, Yuchuan Wu, Wentao Ma, Chao Wang, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, speech-text pre-training methods have shown remarkable success in
many speech and natural language processing tasks. However, most previous
pre-trained models are usually tailored for one or two specific tasks, but fail
to conquer a wide range of speech-text tasks. In addition, existing speech-text
pre-training methods fail to explore the contextual information within a
dialogue to enrich utterance representations. In this paper, we propose
Speech-text dialog Pre-training for spoken dialog understanding with ExpliCiT
cRoss-Modal Alignment (SPECTRA), which is the first-ever speech-text dialog
pre-training model. Concretely, to consider the temporality of speech modality,
we design a novel temporal position prediction task to capture the speech-text
alignment. This pre-training task aims to predict the start and end time of
each textual word in the corresponding speech waveform. In addition, to learn
the characteristics of spoken dialogs, we generalize a response selection task
from textual dialog pre-training to speech-text dialog pre-training scenarios.
Experimental results on four different downstream speech-text tasks demonstrate
the superiority of SPECTRA in learning speech-text alignment and multi-turn
dialog context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2023 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RRWKV: Capturing Long-range Dependencies in RWKV 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05176v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05176v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leilei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Owing to the impressive dot-product attention, the Transformers have been the
dominant architectures in various natural language processing (NLP) tasks.
Recently, the Receptance Weighted Key Value (RWKV) architecture follows a
non-transformer architecture to eliminate the drawbacks of dot-product
attention, where memory and computational complexity exhibits quadratic scaling
with sequence length. Although RWKV has exploited a linearly tensor-product
attention mechanism and achieved parallelized computations by deploying the
time-sequential mode, it fails to capture long-range dependencies because of
its limitation on looking back at previous information, compared with full
information obtained by direct interactions in the standard transformer.
Therefore, the paper devises the Retrospected Receptance Weighted Key Value
(RRWKV) architecture via incorporating the retrospecting ability into the RWKV
to effectively absorb information, which maintains memory and computational
efficiency as well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How Do In-Context Examples Affect Compositional Generalization? <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04835v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04835v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shengnan An, Zeqi Lin, Qiang Fu, Bei Chen, Nanning Zheng, Jian-Guang Lou, Dongmei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositional generalization--understanding unseen combinations of seen
primitives--is an essential reasoning capability in human intelligence. The AI
community mainly studies this capability by fine-tuning neural networks on lots
of training samples, while it is still unclear whether and how in-context
learning--the prevailing few-shot paradigm based on large language
models--exhibits compositional generalization. In this paper, we present CoFe,
a test suite to investigate in-context compositional generalization. We find
that the compositional generalization performance can be easily affected by the
selection of in-context examples, thus raising the research question what the
key factors are to make good in-context examples for compositional
generalization. We study three potential factors: similarity, diversity and
complexity. Our systematic experiments indicate that in-context examples should
be structurally similar to the test case, diverse from each other, and
individually simple. Furthermore, two strong limitations are observed:
in-context compositional generalization on fictional words is much weaker than
that on commonly used ones; it is still critical that the in-context examples
should cover required linguistic structures, even though the backbone model has
been pre-trained on large corpus. We hope our analysis would facilitate the
understanding and utilization of in-context learning paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 main conference, long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive
  Statements <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01985v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01985v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuhui Zhou, Hao Zhu, Akhila Yerukola, Thomas Davidson, Jena D. Hwang, Swabha Swayamdipta, Maarten Sap
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Warning: This paper contains content that may be offensive or upsetting.
Understanding the harms and offensiveness of statements requires reasoning
about the social and situational context in which statements are made. For
example, the utterance "your English is very good" may implicitly signal an
insult when uttered by a white man to a non-white colleague, but uttered by an
ESL teacher to their student would be interpreted as a genuine compliment. Such
contextual factors have been largely ignored by previous approaches to toxic
language detection. We introduce COBRA frames, the first context-aware
formalism for explaining the intents, reactions, and harms of offensive or
biased statements grounded in their social and situational context. We create
COBRACORPUS, a dataset of 33k potentially offensive statements paired with
machine-generated contexts and free-text explanations of offensiveness, implied
biases, speaker intents, and listener reactions. To study the contextual
dynamics of offensiveness, we train models to generate COBRA explanations, with
and without access to the context. We find that explanations by
context-agnostic models are significantly worse than by context-aware ones,
especially in situations where the context inverts the statement's
offensiveness (29% accuracy drop). Our work highlights the importance and
feasibility of contextualized NLP by modeling social factors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reasoning Implicit Sentiment with Chain-of-Thought <span class="highlight-title">Prompt</span>ing <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11255v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11255v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Fei, Bobo Li, Qian Liu, Lidong Bing, Fei Li, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While sentiment analysis systems try to determine the sentiment polarities of
given targets based on the key opinion expressions in input texts, in implicit
sentiment analysis (ISA) the opinion cues come in an implicit and obscure
manner. Thus detecting implicit sentiment requires the common-sense and
multi-hop reasoning ability to infer the latent intent of opinion. Inspired by
the recent chain-of-thought (CoT) idea, in this work we introduce a Three-hop
Reasoning (THOR) CoT framework to mimic the human-like reasoning process for
ISA. We design a three-step prompting principle for THOR to step-by-step induce
the implicit aspect, opinion, and finally the sentiment polarity. Our
THOR+Flan-T5 (11B) pushes the state-of-the-art (SoTA) by over 6% F1 on
supervised setup. More strikingly, THOR+GPT3 (175B) boosts the SoTA by over 50%
F1 on zero-shot setting. Our code is open at
https://github.com/scofield7419/THOR-ISA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL2023 Short Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward More Accurate and Generalizable Evaluation Metrics for
  Task-Oriented Dialogs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abishek Komma, Nagesh Panyam Chandrasekarasastry, Timothy Leffel, Anuj Goyal, Angeliki Metallinou, Spyros Matsoukas, Aram Galstyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measurement of interaction quality is a critical task for the improvement of
spoken dialog systems. Existing approaches to dialog quality estimation either
focus on evaluating the quality of individual turns, or collect dialog-level
quality measurements from end users immediately following an interaction. In
contrast to these approaches, we introduce a new dialog-level annotation
workflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate
the quality of dialogs as a whole, and also label dialogs for attributes such
as goal completion and user sentiment. In this contribution, we show that: (i)
while dialog quality cannot be completely decomposed into dialog-level
attributes, there is a strong relationship between some objective dialog
attributes and judgments of dialog quality; (ii) for the task of dialog-level
quality estimation, a supervised model trained on dialog-level annotations
outperforms methods based purely on aggregating turn-level features; and (iii)
the proposed evaluation model shows better domain generalization ability
compared to the baselines. On the basis of these results, we argue that having
high-quality human-annotated data is an important component of evaluating
interaction quality for large industrial-scale voice assistant platforms.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Optimization and Control <span class="chip" style="font-size: 60%">40</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prodigy: An Expeditiously Adaptive Parameter-Free Learner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Mishchenko, Aaron Defazio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of estimating the learning rate in adaptive methods,
such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to
provably estimate the distance to the solution $D$, which is needed to set the
learning rate optimally. Our techniques are modifications of the D-Adaptation
method for learning-rate-free learning. Our methods improve upon the
convergence rate of D-Adaptation by a factor of $O(\sqrt{\log(D/d_0)})$, where
$d_0$ is the initial estimate of $D$. We test our methods on 12 common
logistic-regression benchmark datasets, VGG11 and ResNet-50 training on
CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on
Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT
transformer training on BookWiki. Our experimental results show that our
approaches consistently outperform D-Adaptation and reach test accuracy values
close to that of hand-tuned Adam.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Error Feedback Can Accurately Compress Preconditioners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ionut-Vlad Modoranu, Aleksei Kalinov, Eldar Kurtic, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging second-order information at the scale of deep networks is one of
the main lines of approach for improving the performance of current optimizers
for deep learning. Yet, existing approaches for accurate full-matrix
preconditioning, such as Full-Matrix Adagrad (GGT) or Matrix-Free Approximate
Curvature (M-FAC) suffer from massive storage costs when applied even to
medium-scale models, as they must store a sliding window of gradients, whose
memory requirements are multiplicative in the model dimension. In this paper,
we address this issue via an efficient and simple-to-implement error-feedback
technique that can be applied to compress preconditioners by up to two orders
of magnitude in practice, without loss of convergence. Specifically, our
approach compresses the gradient information via sparsification or low-rank
compression \emph{before} it is fed into the preconditioner, feeding the
compression error back into future iterations. Extensive experiments on deep
neural networks for vision show that this approach can compress full-matrix
preconditioners by up to two orders of magnitude without impact on accuracy,
effectively removing the memory overhead of full-matrix preconditioning for
implementations of full-matrix Adagrad (GGT) and natural gradient (M-FAC). Our
code is available at https://github.com/IST-DASLab/EFCP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Branching via Cutting Plane Selection: Improving Hybrid Branching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06050v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06050v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Turner, Timo Berthold, Mathieu Besançon, Thorsten Koch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cutting planes and branching are two of the most important algorithms for
solving mixed-integer linear programs. For both algorithms, disjunctions play
an important role, being used both as branching candidates and as the
foundation for some cutting planes. We relate branching decisions and cutting
planes to each other through the underlying disjunctions that they are based
on, with a focus on Gomory mixed-integer cuts and their corresponding split
disjunctions. We show that selecting branching decisions based on quality
measures of Gomory mixed-integer cuts leads to relatively small
branch-and-bound trees, and that the result improves when using cuts that more
accurately represent the branching decisions. Finally, we show how the history
of previously computed Gomory mixed-integer cuts can be used to improve the
performance of the state-of-the-art hybrid branching rule of SCIP. Our results
show a 4\% decrease in solve time, and an 8\% decrease in number of nodes over
affected instances of MIPLIB 2017.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lifting partial smoothing to solve HJB equations and stochastic control
  problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fausto Gozzi, Federica Masiero
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a family of stochastic control problems arising in typical
applications (such as boundary control and control of delay equations with
delay in the control) with the ultimate aim of finding solutions of the
associated HJB equations, regular enough to find optimal feedback controls.
These problems are difficult to treat since the underlying transition
semigroups do not possess good smoothing properties nor the so-called
"structure condition" which typically allows to apply the backward equations
approach. In the papers [14], [15], and, more recently, [16] we studied such
problems developing new partial smoothing techniques which allowed us to obtain
the required regularity in the case when the cost functional is independent of
the state variable. This is a somehow strong restriction which is not verified
in most applications. In this paper (which can be considered a continuation of
the research of the above papers) we develop a new approach to overcome this
restriction. We extend the partial smoothing result to a wider class of
functions which depend on the whole trajectory of the underlying semigroup and
we use this as a key tool to improve our regularity result for the HJB
equation. The fact that such class depends on trajectories requires a
nontrivial technical work as we have to lift the original transition semigroup
to a space of trajectories, defining a new "high-level" environment where our
problems can be solved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2107.04305</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Data-driven Prescriptiveness Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehran Poursoltani, Erick Delage, Angelos Georghiou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The abundance of data has led to the emergence of a variety of optimization
techniques that attempt to leverage available side information to provide more
anticipative decisions. The wide range of methods and contexts of application
have motivated the design of a universal unitless measure of performance known
as the coefficient of prescriptiveness. This coefficient was designed to
quantify both the quality of contextual decisions compared to a reference one
and the prescriptive power of side information. To identify policies that
maximize the former in a data-driven context, this paper introduces a
distributionally robust contextual optimization model where the coefficient of
prescriptiveness substitutes for the classical empirical risk minimization
objective. We present a bisection algorithm to solve this model, which relies
on solving a series of linear programs when the distributional ambiguity set
has an appropriate nested form and polyhedral structure. Studying a contextual
shortest path problem, we evaluate the robustness of the resulting policies
against alternative methods when the out-of-sample dataset is subject to
varying amounts of distribution shift.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TreeDQN: Learning to minimize Branch-and-Bound tree <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitry Sorokin, Alexander Kostin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combinatorial optimization problems require an exhaustive search to find the
optimal solution. A convenient approach to solving combinatorial optimization
tasks in the form of Mixed Integer Linear Programs is Branch-and-Bound.
Branch-and-Bound solver splits a task into two parts dividing the domain of an
integer variable, then it solves them recursively, producing a tree of nested
sub-tasks. The efficiency of the solver depends on the branchning heuristic
used to select a variable for splitting. In the present work, we propose a
reinforcement learning method that can efficiently learn the branching
heuristic. We view the variable selection task as a tree Markov Decision
Process, prove that the Bellman operator adapted for the tree Markov Decision
Process is contracting in mean, and propose a modified learning objective for
the reinforcement learning agent. Our agent requires less training data and
produces smaller trees compared to previous reinforcement learning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Accelerated Stochastic ADMM for Nonconvex and Nonsmooth Finite-Sum
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Zeng, Zhiguo Wang, Jianchao Bai, Xiaojing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The nonconvex and nonsmooth finite-sum optimization problem with linear
constraint has attracted much attention in the fields of artificial
intelligence, computer, and mathematics, due to its wide applications in
machine learning and the lack of efficient algorithms with convincing
convergence theories. A popular approach to solve it is the stochastic
Alternating Direction Method of Multipliers (ADMM), but most stochastic
ADMM-type methods focus on convex models. In addition, the variance reduction
(VR) and acceleration techniques are useful tools in the development of
stochastic methods due to their simplicity and practicability in providing
acceleration characteristics of various machine learning models. However, it
remains unclear whether accelerated SVRG-ADMM algorithm (ASVRG-ADMM), which
extends SVRG-ADMM by incorporating momentum techniques, exhibits a comparable
acceleration characteristic or convergence rate in the nonconvex setting. To
fill this gap, we consider a general nonconvex nonsmooth optimization problem
and study the convergence of ASVRG-ADMM. By utilizing a well-defined potential
energy function, we establish its sublinear convergence rate $O(1/T)$, where
$T$ denotes the iteration number. Furthermore, under the additional
Kurdyka-Lojasiewicz (KL) property which is less stringent than the frequently
used conditions for showcasing linear convergence rates, such as strong
convexity, we show that the ASVRG-ADMM sequence has a finite length and
converges to a stationary solution with a linear convergence rate. Several
experiments on solving the graph-guided fused lasso problem and regularized
logistic regression problem validate that the proposed ASVRG-ADMM performs
better than the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 Pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extremal properties of the first eigenvalue and the fundamental gap of a
  sub-elliptic operator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05825v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05825v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongli Sun, Weijia Wu, Donghui Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problems of extreming the first eigenvalue and the
fundamental gap of a sub-elliptic operator with Dirichlet boundary condition,
when the potential $V$ is subjected to a $p$-norm constraint. The existence
results for weak solutions, compact embedding theorem and spectral theory for
sub-elliptic equation are given. Moreover, we provide the specific
characteristics of the corresponding optimal potential function.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sums of squares certificates for polynomial moment inequalities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05761v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05761v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Igor Klep, Victor Magron, Jurij Volčič
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces and develops the algebraic framework of moment
polynomials, which are polynomial expressions in commuting variables and their
formal mixed moments. Their positivity and optimization over probability
measures supported on semialgebraic sets and subject to moment polynomial
constraints is investigated. A positive solution to Hilbert's 17th problem for
pseudo-moments is given. On the other hand, moment polynomials positive on
actual measures are shown to be sums of squares and formal moments of squares
up to arbitrarily small perturbation of their coefficients. When only measures
supported on a bounded semialgebraic set are considered, a stronger algebraic
certificate for moment polynomial positivity is derived. This result gives rise
to a converging hierarchy of semidefinite programs for moment polynomial
optimization. Finally, as an application, two nonlinear Bell inequalities from
quantum physics are settled.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computing Algorithm for an Equilibrium of the Generalized Stackelberg
  Game 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeyeon Jo, Jihwan Yu, Jinkyoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The $1-N$ generalized Stackelberg game (single-leader multi-follower game) is
intricately intertwined with the interaction between a leader and followers
(hierarchical interaction) and the interaction among followers (simultaneous
interaction). However, obtaining the optimal strategy of the leader is
generally challenging due to the complex interactions among the leader and
followers. Here, we propose a general methodology to find a generalized
Stackelberg equilibrium of a $1-N$ generalized Stackelberg game. Specifically,
we first provide the conditions where a generalized Stackelberg equilibrium
always exists using the variational equilibrium concept. Next, to find an
equilibrium in polynomial time, we transformed the $1-N$ generalized
Stackelberg game into a $1-1$ Stackelberg game whose Stackelberg equilibrium is
identical to that of the original. Finally, we propose an effective computation
procedure based on the projected implicit gradient descent algorithm to find a
Stackelberg equilibrium of the transformed $1-1$ Stackelberg game. We validate
the proposed approaches using the two problems of deriving operating strategies
for EV charging stations: (1) the first problem is optimizing the one-time
charging price for EV users, in which a platform operator determines the price
of electricity and EV users determine the optimal amount of charging for their
satisfaction; and (2) the second problem is to determine the spatially varying
charging price to optimally balance the demand and supply over every charging
station.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>37 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding How Consistency Works in Federated Learning via Stage-wise
  Relaxed Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Sun, Li Shen, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a distributed paradigm that coordinates massive
local clients to collaboratively train a global model via stage-wise local
training processes on the heterogeneous dataset. Previous works have implicitly
studied that FL suffers from the ``client-drift'' problem, which is caused by
the inconsistent optimum across local clients. However, till now it still lacks
solid theoretical analysis to explain the impact of this local inconsistency.
To alleviate the negative impact of the ``client drift'' and explore its
substance in FL, in this paper, we first design an efficient FL algorithm
\textit{FedInit}, which allows employing the personalized relaxed
initialization state at the beginning of each local training stage.
Specifically, \textit{FedInit} initializes the local state by moving away from
the current global state towards the reverse direction of the latest local
state. This relaxed initialization helps to revise the local divergence and
enhance the local consistency level. Moreover, to further understand how
inconsistency disrupts performance in FL, we introduce the excess risk analysis
and study the divergence term to investigate the test error of the proposed
\textit{FedInit} method. Our studies show that optimization error is not
sensitive to this local inconsistency, while it mainly affects the
generalization error bound in \textit{FedInit}. Extensive experiments are
conducted to validate this conclusion. Our proposed \textit{FedInit} could
achieve state-of-the-art~(SOTA) results compared to several advanced benchmarks
without any additional costs. Meanwhile, stage-wise relaxed initialization
could also be incorporated into the current advanced algorithms to achieve
higher performance in the FL paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Consensus ALADIN: A Framework for Distributed Optimization and Its
  Application in Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05662v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05662v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xu Du, Jingzhe Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates algorithms for solving distributed consensus
optimization problems that are non-convex. Since Typical ALADIN (Typical
Augmented Lagrangian based Alternating Direction Inexact Newton Method,
T-ALADIN for short) [1] is a well-performed algorithm treating distributed
optimization problems that are non-convex, directly adopting T-ALADIN to those
of consensus is a natural approach. However, T-ALADIN typically results in high
communication and computation overhead, which makes such an approach far from
efficient. In this paper, we propose a new variant of the ALADIN family, coined
consensus ALADIN (C-ALADIN for short). C-ALADIN inherits all the good
properties of T-ALADIN, such as the local linear or super-linear convergence
rate and the local convergence guarantees for non-convex optimization problems;
besides, C-ALADIN offers unique improvements in terms of communication
efficiency and computational efficiency. Moreover, C-ALADIN involves a reduced
version, in comparison with Consensus ADMM (Alternating Direction Method of
Multipliers) [3], showing significant convergence performance, even without the
help of second-order information. We also propose a practical version of
C-ALADIN, named FedALADIN, that seamlessly serves the emerging federated
learning applications, which expands the reach of our proposed C-ALADIN. We
provide numerical experiments to demonstrate the effectiveness of C-ALADIN. The
results show that C-ALADIN has significant improvements in convergence
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Communication-Efficient Zeroth-Order Distributed Online Optimization:
  Algorithm, Theory, and Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ege C. Kaya, M. Berk Sahin, Abolfazl Hashemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on a multi-agent zeroth-order online optimization problem
in a federated learning setting for target tracking. The agents only sense
their current distances to their targets and aim to maintain a minimum safe
distance from each other to prevent collisions. The coordination among the
agents and dissemination of collision-prevention information is managed by a
central server using the federated learning paradigm. The proposed formulation
leads to an instance of distributed online nonconvex optimization problem that
is solved via a group of communication-constrained agents. To deal with the
communication limitations of the agents, an error feedback-based compression
scheme is utilized for agent-to-server communication. The proposed algorithm is
analyzed theoretically for the general class of distributed online nonconvex
optimization problems. We provide non-asymptotic convergence rates that show
the dominant term is independent of the characteristics of the compression
scheme. Our theoretical results feature a new approach that employs
significantly more relaxed assumptions in comparison to standard literature.
The performance of the proposed solution is further analyzed numerically in
terms of tracking errors and collisions between agents in two relevant
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures, and this paper has been accepted by IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Specifying and Solving Robust Empirical Risk Minimization Problems Using
  CVXPY 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Luxenberg, Dhruv Malik, Yuanzhi Li, Aarti Singh, Stephen Boyd
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider robust empirical risk minimization (ERM), where model parameters
are chosen to minimize the worst-case empirical loss when each data point
varies over a given convex uncertainty set. In some simple cases, such problems
can be expressed in an analytical form. In general the problem can be made
tractable via dualization, which turns a min-max problem into a min-min
problem. Dualization requires expertise and is tedious and error-prone. We
demonstrate how CVXPY can be used to automate this dualization procedure in a
user-friendly manner. Our framework allows practitioners to specify and solve
robust ERM problems with a general class of convex losses, capturing many
standard regression and classification problems. Users can easily specify any
complex uncertainty set that is representable via disciplined convex
programming (DCP) constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Macro-Micro Approach to Reconstructing Vehicle Trajectories on
  Multi-Lane Freeways with Lane Changing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05627v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05627v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuejian Chen, Guoyang Qin, Toru Seo, Ye Tian, Jian Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vehicle trajectories can offer the most precise and detailed depiction of
traffic flow and serve as a critical component in traffic management and
control applications. Various technologies have been applied to reconstruct
vehicle trajectories from sparse fixed and mobile detection data. However,
existing methods predominantly concentrate on single-lane scenarios and neglect
lane-changing (LC) behaviors that occur across multiple lanes, which limit
their applicability in practical traffic systems. To address this research gap,
we propose a macro-micro approach for reconstructing complete vehicle
trajectories on multi-lane freeways, wherein the macro traffic state
information and micro driving models are integrated to overcome the
restrictions imposed by lane boundary. Particularly, the macroscopic velocity
contour maps are established for each lane to regulate the movement of vehicle
platoons, meanwhile the velocity difference between adjacent lanes provide
valuable criteria for guiding LC behaviors. Simultaneously, the car-following
models are extended from micro perspective to supply lane-based candidate
trajectories and define the plausible range for LC positions. Later, a
two-stage trajectory fusion algorithm is proposed to jointly infer both the
car-following and LC behaviors, in which the optimal LC positions is identified
and candidate trajectories are adjusted according to their weights. The
proposed framework was evaluated using NGSIM dataset, and the results indicated
a remarkable enhancement in both the accuracy and smoothness of reconstructed
trajectories, with performance indicators reduced by over 30% compared to two
representative reconstruction methods. Furthermore, the reconstruction process
effectively reproduced LC behaviors across contiguous lanes, adding to the
framework's comprehensiveness and realism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixed-Integer Programming for a Class of Robust Submodular Maximization
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hsin-Yi Huang, Hao-Hsiang Wu, Simge Kucukyavuz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider robust submodular maximization problems (RSMs), where given a set
of $m$ monotone submodular objective functions, the robustness is with respect
to the worst-case (scaled) objective function. The model we consider
generalizes two variants of robust submodular maximization problems in the
literature, depending on the choice of the scaling vector. On one hand, by
using unit scaling, we obtain a usual robust submodular maximization problem.
On the other hand, by letting the scaling vector be the optimal objective
function of each individual (NP-hard) submodular maximization problem, we
obtain a second variant. While the robust version of the objective is no longer
submodular, we reformulate the problem by exploiting the submodularity of each
function. We conduct a polyhedral study of the resulting formulation and
provide conditions under which the submodular inequalities are facet-defining
for a key mixed-integer set. We investigate several strategies for
incorporating these inequalities within a delayed cut generation framework to
solve the problem exactly. For the second variant, we provide an algorithm to
obtain a feasible solution along with its optimality gap. We apply the proposed
methods to a sensor placement optimization problem in water distribution
networks using real-world datasets to demonstrate the effectiveness of the
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Iteration Complexity and Finite-Time Efficiency of Adaptive Sampling
  Trust-Region Methods for Stochastic Derivative-Free Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10650v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10650v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yunsoo Ha, Sara Shashaani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive sampling with interpolation-based trust regions or ASTRO-DF is a
successful algorithm for stochastic derivative-free optimization with an
easy-to-understand-and-implement concept that guarantees almost sure
convergence to a first-order critical point. To reduce its dependence on the
problem dimension, we present local models with diagonal Hessians constructed
on interpolation points based on a coordinate basis. We also leverage the
interpolation points in a direct search manner whenever possible to boost
ASTRO-DF's performance in a finite time. We prove that the algorithm has a
canonical iteration complexity of $\mathcal{O}(\epsilon^{-2})$ almost surely,
which is the first guarantee of its kind without placing assumptions on the
quality of function estimates or model quality or independence between them.
Numerical experimentation reveals the computational advantage of ASTRO-DF with
coordinate direct search due to saving and better steps in the early iterations
of the search.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Learning for Selecting Top-m Context-Dependent Designs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04086v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04086v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gongbo Zhang, Sihua Chen, Kuihua Huang, Yijie Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a simulation optimization problem for a context-dependent
decision-making, which aims to determine the top-m designs for all contexts.
Under a Bayesian framework, we formulate the optimal dynamic sampling decision
as a stochastic dynamic programming problem, and develop a sequential sampling
policy to efficiently learn the performance of each design under each context.
The asymptotically optimal sampling ratios are derived to attain the optimal
large deviations rate of the worst-case of probability of false selection. The
proposed sampling policy is proved to be consistent and its asymptotic sampling
ratios are asymptotically optimal. Numerical experiments demonstrate that the
proposed method improves the efficiency for selection of top-m
context-dependent designs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributionally Robust Learning with Weakly Convex Losses: Convergence
  Rates and Finite-Sample Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06619v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06619v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Landi Zhu, Mert Gürbüzbalaban, Andrzej Ruszczyński
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a distributionally robust stochastic optimization problem and
formulate it as a stochastic two-level composition optimization problem with
the use of the mean--semideviation risk measure. In this setting, we consider a
single time-scale algorithm, involving two versions of the inner function value
tracking: linearized tracking of a continuously differentiable loss function,
and SPIDER tracking of a weakly convex loss function. We adopt the norm of the
gradient of the Moreau envelope as our measure of stationarity and show that
the sample complexity of $\mathcal{O}(\varepsilon^{-3})$ is possible in both
cases, with only the constant larger in the second case. Finally, we
demonstrate the performance of our algorithm with a robust learning example and
a weakly convex, non-smooth regression example.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Converse Robust-Safety Theorem for Differential Inclusions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.11364v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.11364v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Maghenem, Masoumeh Ghanbarpour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper establishes the equivalence between robust safety and the
existence of a barrier function certificate for differential inclusions. More
precisely, for a robustly-safe system, a barrier function is constructed as the
time-to-impact function with respect to a specifically-constructed reachable
set. Using techniques from set-valued and nonsmooth analysis, we show that such
a function, although being possibly discontinuous, certifies robust safety by
verifying a condition involving the system's solutions. Furthermore, we refine
this construction, using integral techniques from the literature, to provide a
smooth barrier certificate that certifies robust safety by verifying a
condition involving only the barrier function and the system's right-hand side.
In comparison with existing converse robust-safety theorems, our result is more
general as it allows the safety region to be unbounded, the right-hand side to
be a general continuous set-valued map, and the solutions to be non-unique.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Stochastic Operator Framework for Optimization and Learning with
  Sub-Weibull Errors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2105.09884v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2105.09884v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicola Bastianello, Liam Madden, Ruggero Carli, Emiliano Dall'Anese
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a framework to study the convergence of stochastic
optimization and learning algorithms. The framework is modeled over the
different challenges that these algorithms pose, such as (i) the presence of
random additive errors (e.g. due to stochastic gradients), and (ii) random
coordinate updates (e.g. due to asynchrony in distributed set-ups). The paper
covers both convex and strongly convex problems, and it also analyzes online
scenarios, involving changes in the data and costs. The paper relies on
interpreting stochastic algorithms as the iterated application of stochastic
operators, thus allowing us to use the powerful tools of operator theory. In
particular, we consider operators characterized by additive errors with
sub-Weibull distribution (which parameterize a broad class of errors by their
tail probability), and random updates. In this framework we derive convergence
results in mean and in high probability, by providing bounds to the distance of
the current iteration from a solution of the optimization or learning problem.
The contributions are discussed in light of federated learning applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLSTRA: Federated Learning in Stratosphere 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00163v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00163v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Farajzadeh, Animesh Yadav, Omid Abbasi, Wael Jaafar, Halim Yanikomeroglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a federated learning (FL) in stratosphere (FLSTRA) system, where a
high altitude platform station (HAPS) facilitates a large number of terrestrial
clients to collaboratively learn a global model without sharing the training
data. FLSTRA overcomes the challenges faced by FL in terrestrial networks, such
as slow convergence and high communication delay due to limited client
participation and multi-hop communications. HAPS leverages its altitude and
size to allow the participation of more clients with line-of-sight (LOS) links
and the placement of a powerful server. However, handling many clients at once
introduces computing and transmission delays. Thus, we aim to obtain a
delay-accuracy trade-off for FLSTRA. Specifically, we first develop a joint
client selection and resource allocation algorithm for uplink and downlink to
minimize the FL delay subject to the energy and quality-of-service (QoS)
constraints. Second, we propose a communication and computation resource-aware
(CCRA-FL) algorithm to achieve the target FL accuracy while deriving an upper
bound for its convergence rate. The formulated problem is non-convex; thus, we
propose an iterative algorithm to solve it. Simulation results demonstrate the
effectiveness of the proposed FLSTRA system, compared to terrestrial
benchmarks, in terms of FL delay and accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Wireless Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Novel Correlation-optimized Deep Learning Method for Wind Speed
  Forecast 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yang, Jin Lang, Jian Wu, Yanyan Zhang, Xiang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing installation rate of wind power poses great challenges to the
global power system. In order to ensure the reliable operation of the power
system, it is necessary to accurately forecast the wind speed and power of the
wind turbines. At present, deep learning is progressively applied to the wind
speed prediction. Nevertheless, the recent deep learning methods still reflect
the embarrassment for practical applications due to model interpretability and
hardware limitation. To this end, a novel deep knowledge-based learning method
is proposed in this paper. The proposed method hybridizes pre-training method
and auto-encoder structure to improve data representation and modeling of the
deep knowledge-based learning framework. In order to form knowledge and
corresponding absorbers, the original data is preprocessed by an optimization
model based on correlation to construct multi-layer networks (knowledge) which
are absorbed by sequence to sequence (Seq2Seq) models. Specifically, new
cognition and memory units (CMU) are designed to reinforce traditional deep
learning framework. Finally, the effectiveness of the proposed method is
verified by three wind prediction cases from a wind farm in Liaoning, China.
Experimental results show that the proposed method increases the stability and
training efficiency compared to the traditional LSTM method and LSTM/GRU-based
Seq2Seq method for applications of wind speed forecasting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving the Model Consistency of Decentralized Federated Learning <span class="chip">ICML2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04083v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04083v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Shi, Li Shen, Kang Wei, Yan Sun, Bo Yuan, Xueqian Wang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To mitigate the privacy leakages and communication burdens of Federated
Learning (FL), decentralized FL (DFL) discards the central server and each
client only communicates with its neighbors in a decentralized communication
network. However, existing DFL suffers from high inconsistency among local
clients, which results in severe distribution shift and inferior performance
compared with centralized FL (CFL), especially on heterogeneous data or sparse
communication topology. To alleviate this issue, we propose two DFL algorithms
named DFedSAM and DFedSAM-MGS to improve the performance of DFL. Specifically,
DFedSAM leverages gradient perturbation to generate local flat models via
Sharpness Aware Minimization (SAM), which searches for models with uniformly
low loss values. DFedSAM-MGS further boosts DFedSAM by adopting Multiple Gossip
Steps (MGS) for better model consistency, which accelerates the aggregation of
local flat models and better balances communication complexity and
generalization. Theoretically, we present improved convergence rates $\small
\mathcal{O}\big(\frac{1}{\sqrt{KT}}+\frac{1}{T}+\frac{1}{K^{1/2}T^{3/2}(1-\lambda)^2}\big)$
and $\small
\mathcal{O}\big(\frac{1}{\sqrt{KT}}+\frac{1}{T}+\frac{\lambda^Q+1}{K^{1/2}T^{3/2}(1-\lambda^Q)^2}\big)$
in non-convex setting for DFedSAM and DFedSAM-MGS, respectively, where
$1-\lambda$ is the spectral gap of gossip matrix and $Q$ is the number of MGS.
Empirically, our methods can achieve competitive performance compared with CFL
methods and outperform existing DFL methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linearly convergent adjoint free solution of least squares problems by
  random descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01946v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01946v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dirk A. Lorenz, Felix Schneppe, Lionel Tondji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of solving linear least squares problems in a
framework where only evaluations of the linear map are possible. We derive
randomized methods that do not need any other matrix operations than forward
evaluations, especially no evaluation of the adjoint map is needed. Our method
is motivated by the simple observation that one can get an unbiased estimate of
the application of the adjoint. We show convergence of the method and then
derive a more efficient method that uses an exact linesearch. This method,
called random descent, resembles known methods in other context and has the
randomized coordinate descent method as special case. We provide convergence
analysis of the random descent method emphasizing the dependence on the
underlying distribution of the random vectors. Furthermore we investigate the
applicability of the method in the context of ill-posed inverse problems and
show that the method can have beneficial properties when the unknown solution
is rough. We illustrate the theoretical findings in numerical examples. One
particular result is that the random descent method actually outperforms
established transposed-free methods (TFQMR and CGS) in examples.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Policy Mirror Ascent for Efficient and Independent Learning in Mean
  Field Games <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Batuhan Yardim, Semih Cayci, Matthieu Geist, Niao He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mean-field games have been used as a theoretical tool to obtain an
approximate Nash equilibrium for symmetric and anonymous $N$-player games.
However, limiting applicability, existing theoretical results assume variations
of a "population generative model", which allows arbitrary modifications of the
population distribution by the learning algorithm. Moreover, learning
algorithms typically work on abstract simulators with population instead of the
$N$-player game. Instead, we show that $N$ agents running policy mirror ascent
converge to the Nash equilibrium of the regularized game within
$\widetilde{\mathcal{O}}(\varepsilon^{-2})$ samples from a single sample
trajectory without a population generative model, up to a standard
$\mathcal{O}(\frac{1}{\sqrt{N}})$ error due to the mean field. Taking a
divergent approach from the literature, instead of working with the
best-response map we first show that a policy mirror ascent map can be used to
construct a contractive operator having the Nash equilibrium as its fixed
point. We analyze single-path TD learning for $N$-agent games, proving sample
complexity guarantees by only using a sample path from the $N$-agent simulator
without a population generative model. Furthermore, we demonstrate that our
methodology allows for independent learning by $N$ agents with finite sample
guarantees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The First Proven Performance Guarantees for the Non-Dominated Sorting
  Genetic Algorithm II (NSGA-II) on a Combinatorial Optimization Problem <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13459v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13459v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sacha Cerf, Benjamin Doerr, Benjamin Hebras, Yakob Kahane, Simon Wietheger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Non-dominated Sorting Genetic Algorithm-II (NSGA-II) is one of the most
prominent algorithms to solve multi-objective optimization problems. Recently,
the first mathematical runtime guarantees have been obtained for this
algorithm, however only for synthetic benchmark problems.
  In this work, we give the first proven performance guarantees for a classic
optimization problem, the NP-complete bi-objective minimum spanning tree
problem. More specifically, we show that the NSGA-II with population size $N
\ge 4((n-1) w_{\max} + 1)$ computes all extremal points of the Pareto front in
an expected number of $O(m^2 n w_{\max} \log(n w_{\max}))$ iterations, where
$n$ is the number of vertices, $m$ the number of edges, and $w_{\max}$ is the
maximum edge weight in the problem instance. This result confirms, via
mathematical means, the good performance of the NSGA-II observed empirically.
It also shows that mathematical analyses of this algorithm are not only
possible for synthetic benchmark problems, but also for more complex
combinatorial optimization problems.
  As a side result, we also obtain a new analysis of the performance of the
global SEMO algorithm on the bi-objective minimum spanning tree problem, which
improves the previous best result by a factor of $|F|$, the number of extremal
points of the Pareto front, a set that can be as large as $n w_{\max}$. The
main reason for this improvement is our observation that both multi-objective
evolutionary algorithms find the different extremal points in parallel rather
than sequentially, as assumed in the previous proofs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Author-generated version of a paper appearing in the proceedings of
  IJCAI 2023, with appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Locating the source of forced oscillations in transmission power grids 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16064v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16064v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin Delabays, Andrey Y. Lokhov, Melvyn Tyloo, Marc Vuffray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forced oscillation event in power grids refers to a state where
malfunctioning or abnormally operating equipment causes persisting periodic
disturbances in the system. While power grids are designed to damp most of
perturbations during standard operations, some of them can excite normal modes
of the system and cause significant energy transfers across the system,
creating large oscillations thousands of miles away from the source.
Localization of the source of such disturbances remains an outstanding
challenge due to a limited knowledge of the system parameters outside of the
zone of responsibility of system operators. Here, we propose a new method for
locating the source of forced oscillations which addresses this challenge by
performing a simultaneous dynamic model identification using a principled
maximum likelihood approach. We illustrate the validity of the algorithm on a
variety of examples where forcing leads to resonance conditions in the system
dynamics. Our results establish that an accurate knowledge of system parameters
is not required for a successful inference of the source and frequency of a
forced oscillation. We anticipate that our method will find a broader
application in general dynamical systems that can be well-described by their
linearized dynamics over short periods of time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main text: 9 pages, 4 figures. Supplementary material: 10 pages, 9
  figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A short simple proof of closedness of convex cones and Farkas' lemma 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.11678v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.11678v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wouter Kager
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Proving that a finitely generated convex cone is closed is often considered
the most difficult part of geometric proofs of Farkas' lemma. We provide a
short simple proof of this fact and (for completeness) derive Farkas' lemma
from it using well-known arguments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2 pages; v2: note largely rewritten, provided more context, improved
  presentation, added 5 references</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeyoung Cha, Jaewook Lee, Chulhee Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study convergence lower bounds of without-replacement stochastic gradient
descent (SGD) for solving smooth (strongly-)convex finite-sum minimization
problems. Unlike most existing results focusing on final iterate lower bounds
in terms of the number of components $n$ and the number of epochs $K$, we seek
bounds for arbitrary weighted average iterates that are tight in all factors
including the condition number $\kappa$. For SGD with Random Reshuffling, we
present lower bounds that have tighter $\kappa$ dependencies than existing
bounds. Our results are the first to perfectly close the gap between lower and
upper bounds for weighted average iterates in both strongly-convex and convex
cases. We also prove weighted average iterate lower bounds for arbitrary
permutation-based SGD, which apply to all variants that carefully choose the
best permutation. Our bounds improve the existing bounds in factors of $n$ and
$\kappa$ and thereby match the upper bounds shown for a recently proposed
algorithm called GraB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>58 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Non-uniform Observability for Moving Horizon Estimation and stability
  with respect to additive perturbation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.12328v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.12328v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emilien Flayac, Iman Shames
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper formalises the concepts of weakly and weakly regularly persistent
input trajectory as well as their link to the Observability Grammian and the
existence and uniqueness of solutions of Moving Horizon Estimation (MHE)
problems. Additionally, thanks to a new time-uniform Implicit Function Theorem,
these notions are proved to imply the stability of MHE solutions with respect
to small additive perturbation in the measurements and in the dynamics, both
uniformly and non-uniformly in time. Finally, examples and counter-examples of
weakly persistent and weakly regularly persistent input trajectories are given
in the case of 2D bearing-only navigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting the central limit theorems for the SGD-type methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.11755v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.11755v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiejun Li, Tiannan Xiao, Guoguo Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We revisited the central limit theorem (CLT) for stochastic gradient descent
(SGD) type methods, including the vanilla SGD, momentum SGD and Nesterov
accelerated SGD methods with constant or vanishing damping parameters. By
taking advantage of Lyapunov function technique and $L^p$ bound estimates, we
established the CLT under more general conditions on learning rates for broader
classes of SGD methods compared with previous results. The CLT for the time
average was also investigated, and we found that it held in the linear case,
while it was not generally true in nonlinear situation. Numerical tests were
also carried out to verify our theoretical analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variance Reduced Random Relaxed Projection Method for Constrained
  Finite-sum Minimization Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.13090v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.13090v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichun Yang, Fu-quan Xia, Kai Tu, Man-Chung Yue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For many applications in signal processing and machine learning, we are
tasked with minimizing a large sum of convex functions subject to a large
number of convex constraints. In this paper, we devise a new random projection
method (RPM) to efficiently solve this problem. Compared with existing RPMs,
our proposed algorithm features two useful algorithmic ideas. First, at each
iteration, instead of projecting onto the subset defined by one of the
constraints, our algorithm only requires projecting onto a half-space
approximation of the subset, which significantly reduces the computational cost
as it admits a closed-form formula. Second, to exploit the structure that the
objective is a sum, variance reduction is incorporated into our algorithm to
further improve the performance. As theoretical contributions, under an error
bound condition and other standard assumptions, we prove that the proposed RPM
converges to an optimal solution and that both optimality and feasibility gaps
vanish at a sublinear rate. We also provide sufficient conditions for the error
bound condition to hold. Experiments on a beamforming problem and a robust
classification problem are also presented to demonstrate the superiority of our
RPM over existing ones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Doubly Smoothed GDA for Constrained Nonconvex-Nonconcave Minimax
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12978v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12978v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taoli Zheng, Linglingzhi Zhu, Anthony Man-Cho So, Jose Blanchet, Jiajin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonconvex-nonconcave minimax optimization has received intense attention over
the last decade due to its broad applications in machine learning.
Unfortunately, most existing algorithms cannot be guaranteed to converge
globally and even suffer from limit cycles. To address this issue, we propose a
novel single-loop algorithm called doubly smoothed gradient descent ascent
method (DSGDA), which naturally balances the primal and dual updates. The
proposed DSGDA can get rid of limit cycles in various challenging
nonconvex-nonconcave examples in the literature, including Forsaken,
Bilinearly-coupled minimax, Sixth-order polynomial, and PolarGame. We further
show that under an one-sided Kurdyka-\L{}ojasiewicz condition with exponent
$\theta\in(0,1)$ (resp. convex primal/concave dual function), DSGDA can find a
game-stationary point with an iteration complexity of
$\mathcal{O}(\epsilon^{-2\max\{2\theta,1\}})$ (resp.
$\mathcal{O}(\epsilon^{-4})$). These match the best results for single-loop
algorithms that solve nonconvex-concave or convex-nonconcave minimax problems,
or problems satisfying the rather restrictive one-sided Polyak-\L{}ojasiewicz
condition. Our work demonstrates, for the first time, the possibility of having
a simple and unified single-loop algorithm for solving nonconvex-nonconcave,
nonconvex-concave, and convex-nonconcave minimax problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A convergence study of SGD-type methods for stochastic optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.06197v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.06197v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tiannan Xiao, Guoguo Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we first reinvestigate the convergence of vanilla SGD method
in the sense of $L^2$ under more general learning rates conditions and a more
general convex assumption, which relieves the conditions on learning rates and
do not need the problem to be strongly convex. Then, by taking advantage of the
Lyapunov function technique, we present the convergence of the momentum SGD and
Nesterov accelerated SGD methods for the convex and non-convex problem under
$L$-smooth assumption that extends the bounded gradient limitation to a certain
extent. The convergence of time averaged SGD was also analyzed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Optimization for Smooth Nonconvex ERM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changyu Gao, Stephen J. Wright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop simple differentially private optimization algorithms that move
along directions of (expected) descent to find an approximate second-order
solution for nonconvex ERM. We use line search, mini-batching, and a two-phase
strategy to improve the speed and practicality of the algorithm. Numerical
experiments demonstrate the effectiveness of these approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Almost Surely $\sqrt{T}$ Regret Bound for Adaptive LQR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Lu, Yilin Mo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Linear-Quadratic Regulation (LQR) problem with unknown system parameters
has been widely studied, but it has remained unclear whether $\tilde{
\mathcal{O}}(\sqrt{T})$ regret, which is the best known dependence on time, can
be achieved almost surely. In this paper, we propose an adaptive LQR controller
with almost surely $\tilde{ \mathcal{O}}(\sqrt{T})$ regret upper bound. The
controller features a circuit-breaking mechanism, which circumvents potential
safety breach and guarantees the convergence of the system parameter estimate,
but is shown to be triggered only finitely often and hence has negligible
effect on the asymptotic performance of the controller. The proposed controller
is also validated via simulation on Tennessee Eastman Process~(TEP), a commonly
used industrial process example.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Approach to Synchronization Problems over Subgroups of the
  Orthogonal Group 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2009.07514v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2009.07514v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huikang Liu, Man-Chung Yue, Anthony Man-Cho So
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of synchronization over a group $\mathcal{G}$ aims to estimate a
collection of group elements $G^*_1, \dots, G^*_n \in \mathcal{G}$ based on
noisy observations of a subset of all pairwise ratios of the form $G^*_i
{G^*_j}^{-1}$. Such a problem has gained much attention recently and finds many
applications across a wide range of scientific and engineering areas. In this
paper, we consider the class of synchronization problems in which the group is
a closed subgroup of the orthogonal group. This class covers many group
synchronization problems that arise in practice. Our contribution is fivefold.
First, we propose a unified approach for solving this class of group
synchronization problems, which consists of a suitable initialization step and
an iterative refinement step based on the generalized power method, and show
that it enjoys a strong theoretical guarantee on the estimation error under
certain assumptions on the group, measurement graph, noise, and initialization.
Second, we formulate two geometric conditions that are required by our approach
and show that they hold for various practically relevant subgroups of the
orthogonal group. The conditions are closely related to the error-bound
geometry of the subgroup -- an important notion in optimization. Third, we
verify the assumptions on the measurement graph and noise for standard random
graph and random matrix models. Fourth, based on the classic notion of metric
entropy, we develop and analyze a novel spectral-type estimator. Finally, we
show via extensive numerical experiments that our proposed non-convex approach
outperforms existing approaches in terms of computational speed, scalability,
and/or estimation error.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regularized Nonsmooth Newton Algorithms for Best Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.13182v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.13182v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yair Censor, Walaa M. Moursi, Tyler Weames, Henry Wolkowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of finding the best approximation point from a
polyhedral set, and its applications, in particular to solving large-scale
linear programs. The classical projection problem has many various and many
applications. We study a regularized nonsmooth Newton type solution method
where the Jacobian is singular; and we compare the computational performance to
that of the classical projection method of Halperin-Lions-Wittmann-Bauschke
(HLWB).
  We observe empirically that the regularized nonsmooth method significantly
outperforms the HLWB method. However, the HLWB has a convergence guarantee
while the nonsmooth method is not monotonic and does not guarantee convergence
due in part to singularity of the generalized Jacobian.
  Our application to solving large-scale linear programs uses a parametrized
projection problem. This leads to a \emph{stepping stone external path
following} algorithm. Other applications are finding triangles from branch and
bound methods, and generalized constrained linear least squares. We include
scaling methods that improve the efficiency and robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 7 tables, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Control Barrier Functions under High Relative Degree and Input
  Constraints for Satellite Trajectories 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2107.04094v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2107.04094v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Breeden, Dimitra Panagou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents methodologies for constructing Control Barrier Functions
(CBFs) for nonlinear, control-affine systems, in the presence of input
constraints and bounded disturbances. More specifically, given a constraint
function with high-relative-degree with respect to the system dynamics, the
paper considers three methodologies, two for relative-degree 2 and one for
higher relative-degrees, for creating CBFs whose zero sublevel sets are subsets
of the constraint function's zero sublevel set. Three special forms of Robust
CBFs (RCBFs) are developed as functions of the input constraints, system
dynamics, and disturbance bounds, such that the resultant RCBF condition on the
control input is always feasible for states in the RCBF zero sublevel set. The
RCBF condition is then enforced in a switched fashion, which allows the system
to operate safely without enforcing the RCBF condition when far from the safe
set boundary and allows tuning of how closely trajectories approach the safe
set boundary. The proposed methods are verified in simulations demonstrating
the developed RCBFs in an asteroid flyby scenario for a satellite with
low-thrust actuators, and in asteroid proximity operations for a satellite with
high-thrust actuators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, extended version contains additional simulations and
  proofs. Accepted to Automatica</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prodigy: An Expeditiously Adaptive Parameter-Free Learner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Mishchenko, Aaron Defazio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of estimating the learning rate in adaptive methods,
such as Adagrad and Adam. We describe two techniques, Prodigy and Resetting, to
provably estimate the distance to the solution $D$, which is needed to set the
learning rate optimally. Our techniques are modifications of the D-Adaptation
method for learning-rate-free learning. Our methods improve upon the
convergence rate of D-Adaptation by a factor of $O(\sqrt{\log(D/d_0)})$, where
$d_0$ is the initial estimate of $D$. We test our methods on 12 common
logistic-regression benchmark datasets, VGG11 and ResNet-50 training on
CIFAR10, ViT training on Imagenet, LSTM training on IWSLT14, DLRM training on
Criteo dataset, VarNet on Knee MRI dataset, as well as RoBERTa and GPT
transformer training on BookWiki. Our experimental results show that our
approaches consistently outperform D-Adaptation and reach test accuracy values
close to that of hand-tuned Adam.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NuCLR: Nuclear Co-Learned Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06099v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06099v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ouail Kitouni, Niklas Nolte, Sokratis Trifinopoulos, Subhash Kantamneni, Mike Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Nuclear Co-Learned Representations (NuCLR), a deep learning
model that predicts various nuclear observables, including binding and decay
energies, and nuclear charge radii. The model is trained using a multi-task
approach with shared representations and obtains state-of-the-art performance,
achieving levels of precision that are crucial for understanding fundamental
phenomena in nuclear (astro)physics. We also report an intriguing finding that
the learned representation of NuCLR exhibits the prominent emergence of crucial
aspects of the nuclear shell model, namely the shell structure, including the
well-known magic numbers, and the Pauli Exclusion Principle. This suggests that
the model is capable of capturing the underlying physical principles and that
our approach has the potential to offer valuable insights into nuclear theory.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Error Feedback Can Accurately Compress Preconditioners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06098v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06098v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ionut-Vlad Modoranu, Aleksei Kalinov, Eldar Kurtic, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Leveraging second-order information at the scale of deep networks is one of
the main lines of approach for improving the performance of current optimizers
for deep learning. Yet, existing approaches for accurate full-matrix
preconditioning, such as Full-Matrix Adagrad (GGT) or Matrix-Free Approximate
Curvature (M-FAC) suffer from massive storage costs when applied even to
medium-scale models, as they must store a sliding window of gradients, whose
memory requirements are multiplicative in the model dimension. In this paper,
we address this issue via an efficient and simple-to-implement error-feedback
technique that can be applied to compress preconditioners by up to two orders
of magnitude in practice, without loss of convergence. Specifically, our
approach compresses the gradient information via sparsification or low-rank
compression \emph{before} it is fed into the preconditioner, feeding the
compression error back into future iterations. Extensive experiments on deep
neural networks for vision show that this approach can compress full-matrix
preconditioners by up to two orders of magnitude without impact on accuracy,
effectively removing the memory overhead of full-matrix preconditioning for
implementations of full-matrix Adagrad (GGT) and natural gradient (M-FAC). Our
code is available at https://github.com/IST-DASLab/EFCP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Models for Scalable Vector Graphics-Driven
  Image Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06094v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06094v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mu Cai, Zeyi Huang, Yuheng Li, Haohan Wang, Yong Jae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, large language models (LLMs) have made significant advancements in
natural language understanding and generation. However, their potential in
computer vision remains largely unexplored. In this paper, we introduce a new,
exploratory approach that enables LLMs to process images using the Scalable
Vector Graphics (SVG) format. By leveraging the XML-based textual descriptions
of SVG representations instead of raster images, we aim to bridge the gap
between the visual and textual modalities, allowing LLMs to directly understand
and manipulate images without the need for parameterized visual components. Our
method facilitates simple image classification, generation, and in-context
learning using only LLM capabilities. We demonstrate the promise of our
approach across discriminative and generative tasks, highlighting its (i)
robustness against distribution shift, (ii) substantial improvements achieved
by tapping into the in-context learning abilities of LLMs, and (iii) image
understanding and generation capabilities with human guidance. Our code, data,
and models can be found here https://github.com/mu-cai/svg-llm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SENS: Sketch-based Implicit Neural Shape Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Binninger, Amir Hertz, Olga Sorkine-Hornung, Daniel Cohen-Or, Raja Giryes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present SENS, a novel method for generating and editing 3D models from
hand-drawn sketches, including those of an abstract nature. Our method allows
users to quickly and easily sketch a shape, and then maps the sketch into the
latent space of a part-aware neural implicit shape architecture. SENS analyzes
the sketch and encodes its parts into ViT patch encoding, then feeds them into
a transformer decoder that converts them to shape embeddings, suitable for
editing 3D neural implicit shapes. SENS not only provides intuitive
sketch-based generation and editing, but also excels in capturing the intent of
the user's sketch to generate a variety of novel and expressive 3D shapes, even
from abstract sketches. We demonstrate the effectiveness of our model compared
to the state-of-the-art using objective metric evaluation criteria and a
decisive user study, both indicating strong performance on sketches with a
medium level of abstraction. Furthermore, we showcase its intuitive
sketch-based shape editing capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 18 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Not to Spoof 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Byrd
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As intelligent trading agents based on reinforcement learning (RL) gain
prevalence, it becomes more important to ensure that RL agents obey laws,
regulations, and human behavioral expectations. There is substantial literature
concerning the aversion of obvious catastrophes like crashing a helicopter or
bankrupting a trading account, but little around the avoidance of subtle
non-normative behavior for which there are examples, but no programmable
definition. Such behavior may violate legal or regulatory, rather than physical
or monetary, constraints.
  In this article, I consider a series of experiments in which an intelligent
stock trading agent maximizes profit but may also inadvertently learn to spoof
the market in which it participates. I first inject a hand-coded spoofing agent
to a multi-agent market simulation and learn to recognize spoofing activity
sequences. Then I replace the hand-coded spoofing trader with a simple
profit-maximizing RL agent and observe that it independently discovers spoofing
as the optimal strategy. Finally, I introduce a method to incorporate the
recognizer as normative guide, shaping the agent's perceived rewards and
altering its selected actions. The agent remains profitable while avoiding
spoofing behaviors that would result in even higher profit. After presenting
the empirical results, I conclude with some recommendations. The method should
generalize to the reduction of any unwanted behavior for which a recognizer can
be learned.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Virtual Node Tuning for Few-shot Node Classification <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06063v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06063v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Tan, Ruocheng Guo, Kaize Ding, Huan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot Node Classification (FSNC) is a challenge in graph representation
learning where only a few labeled nodes per class are available for training.
To tackle this issue, meta-learning has been proposed to transfer structural
knowledge from base classes with abundant labels to target novel classes.
However, existing solutions become ineffective or inapplicable when base
classes have no or limited labeled nodes. To address this challenge, we propose
an innovative method dubbed Virtual Node Tuning (VNT). Our approach utilizes a
pretrained graph transformer as the encoder and injects virtual nodes as soft
prompts in the embedding space, which can be optimized with few-shot labels in
novel classes to modulate node embeddings for each specific FSNC task. A unique
feature of VNT is that, by incorporating a Graph-based Pseudo Prompt Evolution
(GPPE) module, VNT-GPPE can handle scenarios with sparse labels in base
classes. Experimental results on four datasets demonstrate the superiority of
the proposed approach in addressing FSNC with unlabeled or sparsely labeled
base classes, outperforming existing state-of-the-art methods and even fully
supervised baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to KDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Does Fine-Tuning Impact Out-of-Distribution Detection for
  Vision-Language Models? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06048v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06048v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifei Ming, Yixuan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent large vision-language models such as CLIP have shown remarkable
out-of-distribution (OOD) detection and generalization performance. However,
their zero-shot in-distribution (ID) accuracy is often limited for downstream
datasets. Recent CLIP-based fine-tuning methods such as prompt learning have
demonstrated significant improvements in ID classification and OOD
generalization where OOD labels are available. Nonetheless, it remains unclear
whether the model is reliable to semantic shifts without OOD labels. In this
paper, we aim to bridge the gap and present a comprehensive study to understand
how fine-tuning impact OOD detection for few-shot downstream tasks. By framing
OOD detection as multi-modal concept matching, we establish a connection
between fine-tuning methods and various OOD scores. Our results suggest that a
proper choice of OOD scores is essential for CLIP-based fine-tuning. In
particular, the maximum concept matching (MCM) score provides a promising
solution consistently. We also show that prompt learning demonstrates the
state-of-the-art OOD detection performance over the zero-shot counterpart.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Dynamical Graph Prior for Relational Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liming Pan, Cheng Shi, Ivan Dokmanić
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relational inference aims to identify interactions between parts of a
dynamical system from the observed dynamics. Current state-of-the-art methods
fit a graph neural network (GNN) on a learnable graph to the dynamics. They use
one-step message-passing GNNs -- intuitively the right choice since
non-locality of multi-step or spectral GNNs may confuse direct and indirect
interactions. But the \textit{effective} interaction graph depends on the
sampling rate and it is rarely localized to direct neighbors, leading to local
minima for the one-step model. In this work, we propose a \textit{dynamical
graph prior} (DYGR) for relational inference. The reason we call it a prior is
that, contrary to established practice, it constructively uses error
amplification in high-degree non-local polynomial filters to generate good
gradients for graph learning. To deal with non-uniqueness, DYGR simultaneously
fits a ``shallow'' one-step model with shared graph topology. Experiments show
that DYGR reconstructs graphs far more accurately than earlier methods, with
remarkable robustness to under-sampling. Since appropriate sampling rates for
unknown dynamical systems are not known a priori, this robustness makes DYGR
suitable for real applications in scientific machine learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reconstructing Human Expressiveness in Piano Performances with a
  <span class="highlight-title">Transformer</span> Network 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingjing Tang, Geraint Wiggins, George Fazekas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capturing intricate and subtle variations in human expressiveness in music
performance using computational approaches is challenging. In this paper, we
propose a novel approach for reconstructing human expressiveness in piano
performance with a multi-layer bi-directional Transformer encoder. To address
the needs for large amounts of accurately captured and score-aligned
performance data in training neural networks, we use transcribed scores
obtained from an existing transcription model to train our model. We integrate
pianist identities to control the sampling process and explore the ability of
our system to model variations in expressiveness for different pianists. The
system is evaluated through statistical analysis of generated expressive
performances and a listening test. Overall, the results suggest that our method
achieves state-of-the-art in generating human-like piano performances from
transcribed scores, while fully and consistently reconstructing human
expressiveness poses further challenges.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, submitted to CMMR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RANS-PINN based Simulation Surrogates for Predicting Turbulent Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06034v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06034v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shinjan Ghosh, Amit Chakraborty, Georgia Olympia Brikis, Biswadip Dey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed neural networks (PINNs) provide a framework to build
surrogate models for dynamical systems governed by differential equations.
During the learning process, PINNs incorporate a physics-based regularization
term within the loss function to enhance generalization performance. Since
simulating dynamics controlled by partial differential equations (PDEs) can be
computationally expensive, PINNs have gained popularity in learning parametric
surrogates for fluid flow problems governed by Navier-Stokes equations. In this
work, we introduce RANS-PINN, a modified PINN framework, to predict flow fields
(i.e., velocity and pressure) in high Reynolds number turbulent flow regime. To
account for the additional complexity introduced by turbulence, RANS-PINN
employs a 2-equation eddy viscosity model based on a Reynolds-averaged
Navier-Stokes (RANS) formulation. Furthermore, we adopt a novel training
approach that ensures effective initialization and balance among the various
components of the loss function. The effectiveness of RANS-PINN framework is
then demonstrated using a parametric PINN.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fin<span class="highlight-title">GPT</span>: Open-Source Financial Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongyang Yang, Xiao-Yang Liu, Christina Dan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have shown the potential of revolutionizing
natural language processing tasks in diverse domains, sparking great interest
in finance. Accessing high-quality financial data is the first challenge for
financial LLMs (FinLLMs). While proprietary models like BloombergGPT have taken
advantage of their unique data accumulation, such privileged access calls for
an open-source alternative to democratize Internet-scale financial data.
  In this paper, we present an open-source large language model, FinGPT, for
the finance sector. Unlike proprietary models, FinGPT takes a data-centric
approach, providing researchers and practitioners with accessible and
transparent resources to develop their FinLLMs. We highlight the importance of
an automatic data curation pipeline and the lightweight low-rank adaptation
technique in building FinGPT. Furthermore, we showcase several potential
applications as stepping stones for users, such as robo-advising, algorithmic
trading, and low-code development. Through collaborative efforts within the
open-source AI4Finance community, FinGPT aims to stimulate innovation,
democratize FinLLMs, and unlock new opportunities in open finance. Two
associated code repos are \url{https://github.com/AI4Finance-Foundation/FinGPT}
and \url{https://github.com/AI4Finance-Foundation/FinNLP}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Interpretable Time Series Prediction with Counterfactual
  Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.06024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.06024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingquan Yan, Hao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interpretable time series prediction is crucial for safety-critical areas
such as healthcare and autonomous driving. Most existing methods focus on
interpreting predictions by assigning important scores to segments of time
series. In this paper, we take a different and more challenging route and aim
at developing a self-interpretable model, dubbed Counterfactual Time Series
(CounTS), which generates counterfactual and actionable explanations for time
series predictions. Specifically, we formalize the problem of time series
counterfactual explanations, establish associated evaluation protocols, and
propose a variational Bayesian deep learning model equipped with counterfactual
inference capability of time series abduction, action, and prediction. Compared
with state-of-the-art baselines, our self-interpretable model can generate
better counterfactual explanations while maintaining comparable prediction
accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed Consensus Algorithm for Decision-Making in Multi-agent
  Multi-armed Bandit 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05998v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05998v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaotong Cheng, Setareh Maghsudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a structured multi-agent multi-armed bandit (MAMAB) problem in a
dynamic environment. A graph reflects the information-sharing structure among
agents, and the arms' reward distributions are piecewise-stationary with
several unknown change points. The agents face the identical
piecewise-stationary MAB problem. The goal is to develop a decision-making
policy for the agents that minimizes the regret, which is the expected total
loss of not playing the optimal arm at each time step. Our proposed solution,
Restarted Bayesian Online Change Point Detection in Cooperative Upper
Confidence Bound Algorithm (RBO-Coop-UCB), involves an efficient multi-agent
UCB algorithm as its core enhanced with a Bayesian change point detector. We
also develop a simple restart decision cooperation that improves
decision-making. Theoretically, we establish that the expected group regret of
RBO-Coop-UCB is upper bounded by $\mathcal{O}(KNM\log T + K\sqrt{MT\log T})$,
where K is the number of agents, M is the number of arms, and T is the number
of time steps. Numerical experiments on synthetic and real-world datasets
demonstrate that our proposed method outperforms the state-of-the-art
algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Approximate information state based convergence analysis of recurrent
  Q-learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05991v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05991v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erfan Seyedsalehi, Nima Akbarzadeh, Amit Sinha, Aditya Mahajan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In spite of the large literature on reinforcement learning (RL) algorithms
for partially observable Markov decision processes (POMDPs), a complete
theoretical understanding is still lacking. In a partially observable setting,
the history of data available to the agent increases over time so most
practical algorithms either truncate the history to a finite window or compress
it using a recurrent neural network leading to an agent state that is
non-Markovian. In this paper, it is shown that in spite of the lack of the
Markov property, recurrent Q-learning (RQL) converges in the tabular setting.
Moreover, it is shown that the quality of the converged limit depends on the
quality of the representation which is quantified in terms of what is known as
an approximate information state (AIS). Based on this characterization of the
approximation error, a variant of RQL with AIS losses is presented. This
variant performs better than a strong baseline for RQL that does not use AIS
losses. It is demonstrated that there is a strong correlation between the
performance of RQL over time and the loss associated with the AIS
representation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quartile-Based Seasonality Decomposition for Time Series Forecasting and
  Anomaly Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05989v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05989v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ebenezer RHP Isaac, Bulbul Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The timely detection of anomalies is essential in the telecom domain as it
facilitates the identification and characterization of irregular patterns,
abnormal behaviors, and network anomalies, contributing to enhanced service
quality and operational efficiency. Precisely forecasting and eliminating
predictable time series patterns constitutes a vital component of time series
anomaly detection. While the state-of-the-art methods aim to maximize
forecasting accuracy, the computational performance takes a hit. In a system
composed of a large number of time series variables, e.g., cell Key Performance
Indicators (KPIs), the time and space complexity of the forecasting employed is
of crucial importance. Quartile-Based Seasonality Decomposition (QBSD) is a
live forecasting method proposed in this paper to make an optimal trade-off
between computational complexity and forecasting accuracy. This paper compares
the performance of QBSD to the state-of-the-art forecasting methods and their
applicability to practical anomaly detection. To demonstrate the efficacy of
the proposed solution, experimental evaluation was conducted using publicly
available datasets as well as a telecom KPI dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Agent market orders representation through a contrastive learning
  approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruihua Ruan, Emmanuel Bacry, Jean-François Muzy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the access to the labeled orders on the CAC40 data from Euronext, we
are able to analyse agents' behaviours in the market based on their placed
orders. In this study, we construct a self-supervised learning model using
triplet loss to effectively learn the representation of agent market orders. By
acquiring this learned representation, various downstream tasks become
feasible. In this work, we utilise the K-means clustering algorithm on the
learned representation vectors of agent orders to identify distinct behaviour
types within each cluster.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automating Model Comparison in Factor Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bart van Erp, Wouter W. L. Nuijten, Thijs van de Laar, Bert de Vries
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian state and parameter estimation have been automated effectively in
the literature, however, this has not yet been the case for model comparison,
which therefore still requires error-prone and time-consuming manual
derivations. As a result, model comparison is often overlooked and ignored,
despite its importance. This paper efficiently automates Bayesian model
averaging, selection, and combination by message passing on a Forney-style
factor graph with a custom mixture node. Parameter and state inference, and
model comparison can then be executed simultaneously using message passing with
scale factors. This approach shortens the model design cycle and allows for the
straightforward extension to hierarchical and temporal model priors to
accommodate for modeling complicated time-varying processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Contextual Perception: How to Generalize to New Backgrounds and
  Ambiguous Objects 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05963v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05963v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuofan Ying, Peter Hase, Mohit Bansal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Biological vision systems make adaptive use of context to recognize objects
in new settings with novel contexts as well as occluded or blurry objects in
familiar settings. In this paper, we investigate how vision models adaptively
use context for out-of-distribution (OOD) generalization and leverage our
analysis results to improve model OOD generalization. First, we formulate two
distinct OOD settings where the contexts are either irrelevant
(Background-Invariance) or beneficial (Object-Disambiguation), reflecting the
diverse contextual challenges faced in biological vision. We then analyze model
performance in these two different OOD settings and demonstrate that models
that excel in one setting tend to struggle in the other. Notably, prior works
on learning causal features improve on one setting but hurt in the other. This
underscores the importance of generalizing across both OOD settings, as this
ability is crucial for both human cognition and robust AI systems. Next, to
better understand the model properties contributing to OOD generalization, we
use representational geometry analysis and our own probing methods to examine a
population of models, and we discover that those with more factorized
representations and appropriate feature weighting are more successful in
handling Background-Invariance and Object-Disambiguation tests. We further
validate these findings through causal intervention on representation
factorization and feature weighting to demonstrate their causal effect on
performance. Lastly, we propose new augmentation methods to enhance model
generalization. These methods outperform strong baselines, yielding
improvements in both in-distribution and OOD tests. In conclusion, to replicate
the generalization abilities of biological vision, computer vision models must
have factorized object vs. background representations and appropriately weight
both kinds of features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 12 figures. Our code is available at
  https://github.com/zfying/AdaptiveContext</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DDLP: Unsupervised Object-Centric Video Prediction with Deep Dynamic
  Latent Particles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tal Daniel, Aviv Tamar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new object-centric video prediction algorithm based on the deep
latent particle (DLP) representation. In comparison to existing slot- or
patch-based representations, DLPs model the scene using a set of keypoints with
learned parameters for properties such as position and size, and are both
efficient and interpretable. Our method, deep dynamic latent particles (DDLP),
yields state-of-the-art object-centric video prediction results on several
challenging datasets. The interpretable nature of DDLP allows us to perform
``what-if'' generation -- predict the consequence of changing properties of
objects in the initial frames, and DLP's compact structure enables efficient
diffusion-based unconditional video generation. Videos, code and pre-trained
models are available: https://taldatech.github.io/ddlp-web
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project site: https://taldatech.github.io/ddlp-web</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Path Neural Networks: Expressive and Accurate Graph Neural Networks <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gaspard Michel, Giannis Nikolentzos, Johannes Lutzeyer, Michalis Vazirgiannis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs) have recently become the standard approach for
learning with graph-structured data. Prior work has shed light into their
potential, but also their limitations. Unfortunately, it was shown that
standard GNNs are limited in their expressive power. These models are no more
powerful than the 1-dimensional Weisfeiler-Leman (1-WL) algorithm in terms of
distinguishing non-isomorphic graphs. In this paper, we propose Path Neural
Networks (PathNNs), a model that updates node representations by aggregating
paths emanating from nodes. We derive three different variants of the PathNN
model that aggregate single shortest paths, all shortest paths and all simple
paths of length up to K. We prove that two of these variants are strictly more
powerful than the 1-WL algorithm, and we experimentally validate our
theoretical results. We find that PathNNs can distinguish pairs of
non-isomorphic graphs that are indistinguishable by 1-WL, while our most
expressive PathNN variant can even distinguish between 3-WL indistinguishable
graphs. The different PathNN variants are also evaluated on graph
classification and graph regression datasets, where in most cases, they
outperform the baseline methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Overcoming Adversarial Attacks for Human-in-the-Loop Applications <span class="chip">ICML 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan McCoppin, Marla Kennedy, Platon Lukyanenko, Sean Kennedy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Including human analysis has the potential to positively affect the
robustness of Deep Neural Networks and is relatively unexplored in the
Adversarial Machine Learning literature. Neural network visual explanation maps
have been shown to be prone to adversarial attacks. Further research is needed
in order to select robust visualizations of explanations for the image analyst
to evaluate a given model. These factors greatly impact Human-In-The-Loop
(HITL) evaluation tools due to their reliance on adversarial images, including
explanation maps and measurements of robustness. We believe models of human
visual attention may improve interpretability and robustness of human-machine
imagery analysis systems. Our challenge remains, how can HITL evaluation be
robust in this adversarial landscape?
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>New Frontiers in Adversarial Machine Learning, ICML 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prediction of Transportation Index for Urban Patterns in Small and
  Medium-sized Indian Cities using Hybrid RidgeGAN Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05951v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05951v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahisha Thottolil, Uttam Kumar, Tanujit Chakraborty
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid urbanization trend in most developing countries including India is
creating a plethora of civic concerns such as loss of green space, degradation
of environmental health, clean water availability, air pollution, traffic
congestion leading to delays in vehicular transportation, etc. Transportation
and network modeling through transportation indices have been widely used to
understand transportation problems in the recent past. This necessitates
predicting transportation indices to facilitate sustainable urban planning and
traffic management. Recent advancements in deep learning research, in
particular, Generative Adversarial Networks (GANs), and their modifications in
spatial data analysis such as CityGAN, Conditional GAN, and MetroGAN have
enabled urban planners to simulate hyper-realistic urban patterns. These
synthetic urban universes mimic global urban patterns and evaluating their
landscape structures through spatial pattern analysis can aid in comprehending
landscape dynamics, thereby enhancing sustainable urban planning. This research
addresses several challenges in predicting the urban transportation index for
small and medium-sized Indian cities. A hybrid framework based on Kernel Ridge
Regression (KRR) and CityGAN is introduced to predict transportation index
using spatial indicators of human settlement patterns. This paper establishes a
relationship between the transportation index and human settlement indicators
and models it using KRR for the selected 503 Indian cities. The proposed hybrid
pipeline, we call it RidgeGAN model, can evaluate the sustainability of urban
sprawl associated with infrastructure development and transportation systems in
sprawling cities. Experimental results show that the two-step pipeline approach
outperforms existing benchmarks based on spatial and statistical measures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Data-driven Prescriptiveness Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05937v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05937v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehran Poursoltani, Erick Delage, Angelos Georghiou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The abundance of data has led to the emergence of a variety of optimization
techniques that attempt to leverage available side information to provide more
anticipative decisions. The wide range of methods and contexts of application
have motivated the design of a universal unitless measure of performance known
as the coefficient of prescriptiveness. This coefficient was designed to
quantify both the quality of contextual decisions compared to a reference one
and the prescriptive power of side information. To identify policies that
maximize the former in a data-driven context, this paper introduces a
distributionally robust contextual optimization model where the coefficient of
prescriptiveness substitutes for the classical empirical risk minimization
objective. We present a bisection algorithm to solve this model, which relies
on solving a series of linear programs when the distributional ambiguity set
has an appropriate nested form and polyhedral structure. Studying a contextual
shortest path problem, we evaluate the robustness of the resulting policies
against alternative methods when the out-of-sample dataset is subject to
varying amounts of distribution shift.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Speaker Embeddings as Individuality Proxy for Voice Stress Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05915v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05915v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Wu, Neil Scheidwasser-Clow, Karl El Hajal, Milos Cernak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the mental states of the speaker modulate speech, stress introduced by
cognitive or physical loads could be detected in the voice. The existing voice
stress detection benchmark has shown that the audio embeddings extracted from
the Hybrid BYOL-S self-supervised model perform well. However, the benchmark
only evaluates performance separately on each dataset, but does not evaluate
performance across the different types of stress and different languages.
Moreover, previous studies found strong individual differences in stress
susceptibility. This paper presents the design and development of voice stress
detection, trained on more than 100 speakers from 9 language groups and five
different types of stress. We address individual variabilities in voice stress
analysis by adding speaker embeddings to the hybrid BYOL-S features. The
proposed method significantly improves voice stress detection performance with
an input audio length of only 3-5 seconds.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures. Accepted at Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ 2DeteCT -- A large 2D expandable, trainable, experimental Computed
  Tomography <span class="highlight-title">dataset</span> for machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian B. Kiss, Sophia B. Coban, K. Joost Batenburg, Tristan van Leeuwen, Felix Lucka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research in computational imaging largely focuses on developing
machine learning (ML) techniques for image reconstruction, which requires
large-scale training datasets consisting of measurement data and ground-truth
images. However, suitable experimental datasets for X-ray Computed Tomography
(CT) are scarce, and methods are often developed and evaluated only on
simulated data. We fill this gap by providing the community with a versatile,
open 2D fan-beam CT dataset suitable for developing ML techniques for a range
of image reconstruction tasks. To acquire it, we designed a sophisticated,
semi-automatic scan procedure that utilizes a highly-flexible laboratory X-ray
CT setup. A diverse mix of samples with high natural variability in shape and
density was scanned slice-by-slice (5000 slices in total) with high angular and
spatial resolution and three different beam characteristics: A high-fidelity, a
low-dose and a beam-hardening-inflicted mode. In addition, 750
out-of-distribution slices were scanned with sample and beam variations to
accommodate robustness and segmentation tasks. We provide raw projection data,
reference reconstructions and segmentations based on an open-source data
processing pipeline.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TreeDQN: Learning to minimize Branch-and-Bound tree <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05905v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05905v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dmitry Sorokin, Alexander Kostin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combinatorial optimization problems require an exhaustive search to find the
optimal solution. A convenient approach to solving combinatorial optimization
tasks in the form of Mixed Integer Linear Programs is Branch-and-Bound.
Branch-and-Bound solver splits a task into two parts dividing the domain of an
integer variable, then it solves them recursively, producing a tree of nested
sub-tasks. The efficiency of the solver depends on the branchning heuristic
used to select a variable for splitting. In the present work, we propose a
reinforcement learning method that can efficiently learn the branching
heuristic. We view the variable selection task as a tree Markov Decision
Process, prove that the Bellman operator adapted for the tree Markov Decision
Process is contracting in mean, and propose a modified learning objective for
the reinforcement learning agent. Our agent requires less training data and
produces smaller trees compared to previous reinforcement learning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ C(NN)FD -- a deep learning framework for turbomachinery CFD analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05889v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05889v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giuseppe Bruni, Sepehr Maleki, Senthil K. Krishnababu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning methods have seen a wide range of successful applications
across different industries. Up until now, applications to physical simulations
such as CFD (Computational Fluid Dynamics), have been limited to simple
test-cases of minor industrial relevance. This paper demonstrates the
development of a novel deep learning framework for real-time predictions of the
impact of manufacturing and build variations on the overall performance of
axial compressors in gas turbines, with a focus on tip clearance variations.
The associated scatter in efficiency can significantly increase the $CO_2$
emissions, thus being of great industrial and environmental relevance. The
proposed \textit{C(NN)FD} architecture achieves in real-time accuracy
comparable to the CFD benchmark. Predicting the flow field and using it to
calculate the corresponding overall performance renders the methodology
generalisable, while filtering only relevant parts of the CFD solution makes
the methodology scalable to industrial applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time Series Continuous Modeling for Imputation and Forecasting with
  Implicit Neural Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05880v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05880v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Etienne Le Naour, Louis Serrano, Léon Migus, Yuan Yin, patrick gallinari, Ghislain Agoua, Nicolas Baskiotis, Vincent Guigue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although widely explored, time series modeling continues to encounter
significant challenges when confronted with real-world data. We propose a novel
modeling approach leveraging Implicit Neural Representations (INR). This
approach enables us to effectively capture the continuous aspect of time series
and provides a natural solution to recurring modeling issues such as handling
missing data, dealing with irregular sampling, or unaligned observations from
multiple sensors. By introducing conditional modulation of INR parameters and
leveraging meta-learning techniques, we address the issue of generalization to
both unseen samples and time window shifts. Through extensive experimentation,
our model demonstrates state-of-the-art performance in forecasting and
imputation tasks, while exhibiting flexibility in handling a wide range of
challenging scenarios that competing models cannot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Normalization Indispensable for Multi-domain Federated Learning? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weiming Zhuang, Lingjuan Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) enhances data privacy with collaborative in-situ
training on decentralized clients. Nevertheless, FL encounters challenges due
to non-independent and identically distributed (non-i.i.d) data, leading to
potential performance degradation and hindered convergence. While prior studies
predominantly addressed the issue of skewed label distribution, our research
addresses a crucial yet frequently overlooked problem known as multi-domain FL.
In this scenario, clients' data originate from diverse domains with distinct
feature distributions, as opposed to label distributions. To address the
multi-domain problem in FL, we propose a novel method called Federated learning
Without normalizations (FedWon). FedWon draws inspiration from the observation
that batch normalization (BN) faces challenges in effectively modeling the
statistics of multiple domains, while alternative normalization techniques
possess their own limitations. In order to address these issues, FedWon
eliminates all normalizations in FL and reparameterizes convolution layers with
scaled weight standardization. Through comprehensive experimentation on four
datasets and four models, our results demonstrate that FedWon surpasses both
FedAvg and the current state-of-the-art method (FedBN) across all experimental
setups, achieving notable improvements of over 10% in certain domains.
Furthermore, FedWon is versatile for both cross-silo and cross-device FL,
exhibiting strong performance even with a batch size as small as 1, thereby
catering to resource-constrained devices. Additionally, FedWon effectively
tackles the challenge of skewed label distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Detecting Adversarial Directions in Deep Reinforcement Learning to Make
  Robust Decisions <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ezgi Korkmaz, Jonah Brown-Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning in MDPs with highly complex state representations is currently
possible due to multiple advancements in reinforcement learning algorithm
design. However, this incline in complexity, and furthermore the increase in
the dimensions of the observation came at the cost of volatility that can be
taken advantage of via adversarial attacks (i.e. moving along worst-case
directions in the observation space). To solve this policy instability problem
we propose a novel method to detect the presence of these non-robust directions
via local quadratic approximation of the deep neural policy loss. Our method
provides a theoretical basis for the fundamental cut-off between safe
observations and adversarial observations. Furthermore, our technique is
computationally efficient, and does not depend on the methods used to produce
the worst-case directions. We conduct extensive experiments in the Arcade
Learning Environment with several different adversarial attack techniques. Most
significantly, we demonstrate the effectiveness of our approach even in the
setting where non-robust directions are explicitly optimized to circumvent our
proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faster Discrete Convex Function Minimization with Predictions: The
  M-Convex Case 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taihei Oki, Shinsaku Sakaue
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent years have seen a growing interest in accelerating optimization
algorithms with machine-learned predictions. Sakaue and Oki (NeurIPS 2022) have
developed a general framework that warm-starts the L-convex function
minimization method with predictions, revealing the idea's usefulness for
various discrete optimization problems. In this paper, we present a framework
for using predictions to accelerate M-convex function minimization, thus
complementing previous research and extending the range of discrete
optimization algorithms that can benefit from predictions. Our framework is
particularly effective for an important subclass called laminar convex
minimization, which appears in many operations research applications. Our
methods can improve time complexity bounds upon the best worst-case results by
using predictions and even have potential to go beyond a lower-bound result.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning You May Communicate Less Often! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05862v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05862v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milad Sefidgaran, Romain Chor, Abdellatif Zaidi, Yijun Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate the generalization error of statistical learning models in a
Federated Learning (FL) setting. Specifically, we study the evolution of the
generalization error with the number of communication rounds between the
clients and the parameter server, i.e., the effect on the generalization error
of how often the local models as computed by the clients are aggregated at the
parameter server. We establish PAC-Bayes and rate-distortion theoretic bounds
on the generalization error that account explicitly for the effect of the
number of rounds, say $ R \in \mathbb{N}$, in addition to the number of
participating devices $K$ and individual datasets size $n$. The bounds, which
apply in their generality for a large class of loss functions and learning
algorithms, appear to be the first of their kind for the FL setting.
Furthermore, we apply our bounds to FL-type Support Vector Machines (FSVM); and
we derive (more) explicit bounds on the generalization error in this case. In
particular, we show that the generalization error of FSVM increases with $R$,
suggesting that more frequent communication with the parameter server
diminishes the generalization power of such learning algorithms. Combined with
that the empirical risk generally decreases for larger values of $R$, this
indicates that $R$ might be a parameter to optimize in order to minimize the
population risk of FL algorithms. Moreover, specialized to the case $R=1$
(sometimes referred to as "one-shot" FL or distributed learning) our bounds
suggest that the generalization error of the FL setting decreases faster than
that of centralized learning by a factor of $\mathcal{O}(\sqrt{\log(K)/K})$,
thereby generalizing recent findings in this direction to arbitrary loss
functions and algorithms. The results of this paper are also validated on some
experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Reinforcement Learning via Adversarial Kernel Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixin Wang, Uri Gadot, Navdeep Kumar, Kfir Levy, Shie Mannor
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust Markov Decision Processes (RMDPs) provide a framework for sequential
decision-making that is robust to perturbations on the transition kernel.
However, robust reinforcement learning (RL) approaches in RMDPs do not scale
well to realistic online settings with high-dimensional domains. By
characterizing the adversarial kernel in RMDPs, we propose a novel approach for
online robust RL that approximates the adversarial kernel and uses a standard
(non-robust) RL algorithm to learn a robust policy. Notably, our approach can
be applied on top of any underlying RL algorithm, enabling easy scaling to
high-dimensional domains. Experiments in classic control tasks, MinAtar and
DeepMind Control Suite demonstrate the effectiveness and the applicability of
our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Sparse Can We Prune A Deep Network: A Geometric Viewpoint 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05857v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05857v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaozhe Zhang, Ruijie Zhang, Jun Sun, Yingzhuang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Overparameterization constitutes one of the most significant hallmarks of
deep neural networks. Though it can offer the advantage of outstanding
generalization performance, it meanwhile imposes substantial storage burden,
thus necessitating the study of network pruning. A natural and fundamental
question is: How sparse can we prune a deep network (with almost no hurt on the
performance)? To address this problem, in this work we take a first principles
approach, specifically, by merely enforcing the sparsity constraint on the
original loss function, we're able to characterize the sharp phase transition
point of pruning ratio, which corresponds to the boundary between the feasible
and the infeasible, from the perspective of high-dimensional geometry. It turns
out that the phase transition point of pruning ratio equals the squared
Gaussian width of some convex body resulting from the $l_1$-regularized loss
function, normalized by the original dimension of parameters. As a byproduct,
we provide a novel network pruning algorithm which is essentially a global
one-shot pruning one. Furthermore, we provide efficient countermeasures to
address the challenges in computing the involved Gaussian width, including the
spectrum estimation of a large-scale Hessian matrix and dealing with the
non-definite positiveness of a Hessian matrix. It is demonstrated that the
predicted pruning ratio threshold coincides very well with the actual value
obtained from the experiments and our proposed pruning algorithm can achieve
competitive or even better performance than the existing pruning algorithms.
All codes are available at:
https://github.com/QiaozheZhang/Global-One-shot-Pruning
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Object Information Improves Skeleton-based Human Action Recognition
  in Assembly Tasks <span class="chip">IJCNN</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05844v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05844v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dustin Aganian, Mona Köhler, Sebastian Baake, Markus Eisenbach, Horst-Michael Gross
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the use of collaborative robots (cobots) in industrial manufacturing
continues to grow, human action recognition for effective human-robot
collaboration becomes increasingly important. This ability is crucial for
cobots to act autonomously and assist in assembly tasks. Recently,
skeleton-based approaches are often used as they tend to generalize better to
different people and environments. However, when processing skeletons alone,
information about the objects a human interacts with is lost. Therefore, we
present a novel approach of integrating object information into skeleton-based
action recognition. We enhance two state-of-the-art methods by treating object
centers as further skeleton joints. Our experiments on the assembly dataset
IKEA ASM show that our approach improves the performance of these
state-of-the-art methods to a large extent when combining skeleton joints with
objects predicted by a state-of-the-art instance segmentation model. Our
research sheds light on the benefits of combining skeleton joints with object
information for human action recognition in assembly tasks. We analyze the
effect of the object detector on the combination for action classification and
discuss the important factors that must be taken into account.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IEEE International Joint Conference on Neural Networks (IJCNN) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Domain-Agnostic Batch Bayesian Optimization with Diverse Constraints via
  Bayesian Quadrature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masaki Adachi, Satoshi Hayakawa, Xingchen Wan, Martin Jørgensen, Harald Oberhauser, Michael A. Osborne
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Real-world optimisation problems often feature complex combinations of (1)
diverse constraints, (2) discrete and mixed spaces, and are (3) highly
parallelisable. (4) There are also cases where the objective function cannot be
queried if unknown constraints are not satisfied, e.g. in drug discovery,
safety on animal experiments (unknown constraints) must be established before
human clinical trials (querying objective function) may proceed. However, most
existing works target each of the above three problems in isolation and do not
consider (4) unknown constraints with query rejection. For problems with
diverse constraints and/or unconventional input spaces, it is difficult to
apply these techniques as they are often mutually incompatible. We propose
cSOBER, a domain-agnostic prudent parallel active sampler for Bayesian
optimisation, based on SOBER of Adachi et al. (2023). We consider infeasibility
under unknown constraints as a type of integration error that we can estimate.
We propose a theoretically-driven approach that propagates such error as a
tolerance in the quadrature precision that automatically balances exploitation
and exploration with the expected rejection rate. Moreover, our method flexibly
accommodates diverse constraints and/or discrete and mixed spaces via adaptive
tolerance, including conventional zero-risk cases. We show that cSOBER
outperforms competitive baselines on diverse real-world blackbox-constrained
problems, including safety-constrained drug discovery, and
human-relationship-aware team optimisation over graph-structured space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expectation-Complete Graph Representations with Homomorphisms <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pascal Welke, Maximilian Thiessen, Fabian Jogl, Thomas Gärtner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate novel random graph embeddings that can be computed in expected
polynomial time and that are able to distinguish all non-isomorphic graphs in
expectation. Previous graph embeddings have limited expressiveness and either
cannot distinguish all graphs or cannot be computed efficiently for every
graph. To be able to approximate arbitrary functions on graphs, we are
interested in efficient alternatives that become arbitrarily expressive with
increasing resources. Our approach is based on Lov\'asz' characterisation of
graph isomorphism through an infinite dimensional vector of homomorphism
counts. Our empirical evaluation shows competitive results on several benchmark
graph learning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted for publication at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can Large Language Models Infer Causation from Correlation? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona Diab, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal inference is one of the hallmarks of human intelligence. While the
field of CausalNLP has attracted much interest in the recent years, existing
causal inference datasets in NLP primarily rely on discovering causality from
empirical knowledge (e.g., commonsense knowledge). In this work, we propose the
first benchmark dataset to test the pure causal inference skills of large
language models (LLMs). Specifically, we formulate a novel task Corr2Cause,
which takes a set of correlational statements and determines the causal
relationship between the variables. We curate a large-scale dataset of more
than 400K samples, on which we evaluate seventeen existing LLMs. Through our
experiments, we identify a key shortcoming of LLMs in terms of their causal
inference skills, and show that these models achieve almost close to random
performance on the task. This shortcoming is somewhat mitigated when we try to
re-purpose LLMs for this skill via finetuning, but we find that these models
still fail to generalize -- they can only perform causal inference in
in-distribution settings when variable names and textual expressions used in
the queries are similar to those in the training set, but fail in
out-of-distribution settings generated by perturbing these queries. Corr2Cause
is a challenging task for LLMs, and would be helpful in guiding future research
on improving LLMs' pure reasoning skills and generalizability. Our data is at
https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at
https://github.com/causalNLP/corr2cause.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extending Kernel PCA through Dualization: Sparsity, Robustness and Fast
  Algorithms <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Tonin, Alex Lambert, Panagiotis Patrinos, Johan A. K. Suykens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of this paper is to revisit Kernel Principal Component Analysis
(KPCA) through dualization of a difference of convex functions. This allows to
naturally extend KPCA to multiple objective functions and leads to efficient
gradient-based algorithms avoiding the expensive SVD of the Gram matrix.
Particularly, we consider objective functions that can be written as Moreau
envelopes, demonstrating how to promote robustness and sparsity within the same
framework. The proposed method is evaluated on synthetic and real-world
benchmarks, showing significant speedup in KPCA training time as well as
highlighting the benefits in terms of robustness and sparsity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporating Prior Knowledge in Deep Learning Models via Pathway
  Activity Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05813v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05813v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro Henrique da Costa Avelar, Min Wu, Sophia Tsoka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Motivation: Despite advances in the computational analysis of high-throughput
molecular profiling assays (e.g. transcriptomics), a dichotomy exists between
methods that are simple and interpretable, and ones that are complex but with
lower degree of interpretability. Furthermore, very few methods deal with
trying to translate interpretability in biologically relevant terms, such as
known pathway cascades. Biological pathways reflecting signalling events or
metabolic conversions are Small improvements or modifications of existing
algorithms will generally not be suitable, unless novel biological results have
been predicted and verified. Determining which pathways are implicated in
disease and incorporating such pathway data as prior knowledge may enhance
predictive modelling and personalised strategies for diagnosis, treatment and
prevention of disease.
  Results: We propose a novel prior-knowledge-based deep auto-encoding
framework, PAAE, together with its accompanying generative variant, PAVAE, for
RNA-seq data in cancer. Through comprehensive comparisons among various
learning models, we show that, despite having access to a smaller set of
features, our PAAE and PAVAE models achieve better out-of-set reconstruction
results compared to common methodologies. Furthermore, we compare our model
with equivalent baselines on a classification task and show that they achieve
better results than models which have access to the full input gene set.
Another result is that using vanilla variational frameworks might negatively
impact both reconstruction outputs as well as classification performance.
Finally, our work directly contributes by providing comprehensive
interpretability analyses on our models on top of improving prognostication for
translational medicine.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HRTF upsampling with a generative adversarial network using a gnomonic
  equiangular projection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aidan O. T. Hogg, Mads Jenkins, He Liu, Isaac Squires, Samuel J. Cooper, Lorenzo Picinali
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An individualised head-related transfer function (HRTF) is essential for
creating realistic virtual reality (VR) and augmented reality (AR)
environments. However, acoustically measuring high-quality HRTFs requires
expensive equipment and an acoustic lab setting. To overcome these limitations
and to make this measurement more efficient HRTF upsampling has been exploited
in the past where a high-resolution HRTF is created from a low-resolution one.
This paper demonstrates how generative adversarial networks (GANs) can be
applied to HRTF upsampling. We propose a novel approach that transforms the
HRTF data for convenient use with a convolutional super-resolution generative
adversarial network (SRGAN). This new approach is benchmarked against two
baselines: barycentric upsampling and a HRTF selection approach. Experimental
results show that the proposed method outperforms both baselines in terms of
log-spectral distortion (LSD) and localisation performance using perceptual
models when the input HRTF is sparse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 9 figures, Preprint (Submitted to Transactions on Audio,
  Speech and Language Processing on the 24 Feb 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining Reinforcement Learning with Shapley Values <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05810v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05810v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Beechey, Thomas M. S. Smith, Özgür Şimşek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  For reinforcement learning systems to be widely adopted, their users must
understand and trust them. We present a theoretical analysis of explaining
reinforcement learning using Shapley values, following a principled approach
from game theory for identifying the contribution of individual players to the
outcome of a cooperative game. We call this general framework Shapley Values
for Explaining Reinforcement Learning (SVERL). Our analysis exposes the
limitations of earlier uses of Shapley values in reinforcement learning. We
then develop an approach that uses Shapley values to explain agent performance.
In a variety of domains, SVERL produces meaningful explanations that match and
supplement human intuition.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 9 figures. Accepted at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RankFormer: Listwise Learning-to-Rank Using Listwide Labels <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maarten Buyl, Paul Missault, Pierre-Antoine Sondag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Web applications where users are presented with a limited selection of items
have long employed ranking models to put the most relevant results first. Any
feedback received from users is typically assumed to reflect a relative
judgement on the utility of items, e.g. a user clicking on an item only implies
it is better than items not clicked in the same ranked list. Hence, the
objectives optimized in Learning-to-Rank (LTR) tend to be pairwise or listwise.
  Yet, by only viewing feedback as relative, we neglect the user's absolute
feedback on the list's overall quality, e.g. when no items in the selection are
clicked. We thus reconsider the standard LTR paradigm and argue the benefits of
learning from this listwide signal. To this end, we propose the RankFormer as
an architecture that, with a Transformer at its core, can jointly optimize a
novel listwide assessment objective and a traditional listwise LTR objective.
  We simulate implicit feedback on public datasets and observe that the
RankFormer succeeds in benefitting from listwide signals. Additionally, we
conduct experiments in e-commerce on Amazon Search data and find the RankFormer
to be superior to all baselines offline. An online experiment shows that
knowledge distillation can be used to find immediate practical use for the
RankFormer.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at KDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DynaBench: A benchmark <span class="highlight-title">dataset</span> for learning dynamical systems from
  low-resolution data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrzej Dulny, Andreas Hotho, Anna Krause
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous work on learning physical systems from data has focused on
high-resolution grid-structured measurements. However, real-world knowledge of
such systems (e.g. weather data) relies on sparsely scattered measuring
stations. In this paper, we introduce a novel simulated benchmark dataset,
DynaBench, for learning dynamical systems directly from sparsely scattered data
without prior knowledge of the equations. The dataset focuses on predicting the
evolution of a dynamical system from low-resolution, unstructured measurements.
We simulate six different partial differential equations covering a variety of
physical systems commonly used in the literature and evaluate several machine
learning models, including traditional graph neural networks and point cloud
processing models, with the task of predicting the evolution of the system. The
proposed benchmark dataset is expected to advance the state of art as an
out-of-the-box easy-to-use tool for evaluating models in a setting where only
unstructured low-resolution observations are available. The benchmark is
available at https://anonymous.4open.science/r/code-2022-dynabench/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causality between Sentiment and Cryptocurrency Prices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lubdhak Mondal, Udeshya Raj, Abinandhan S, Began Gowsik S, Sarwesh P, Abhijeet Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the relationship between narratives conveyed through
microblogging platforms, namely Twitter, and the value of crypto assets. Our
study provides a unique technique to build narratives about cryptocurrency by
combining topic modelling of short texts with sentiment analysis. First, we
used an unsupervised machine learning algorithm to discover the latent topics
within the massive and noisy textual data from Twitter, and then we revealed
4-5 cryptocurrency-related narratives, including financial investment,
technological advancement related to crypto, financial and political
regulations, crypto assets, and media coverage. In a number of situations, we
noticed a strong link between our narratives and crypto prices. Our work
connects the most recent innovation in economics, Narrative Economics, to a new
area of study that combines topic modelling and sentiment analysis to relate
consumer behaviour to narratives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-level histograms for dealing with outliers and heavy tail
  distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marc Boullé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Histograms are among the most popular methods used in exploratory analysis to
summarize univariate distributions. In particular, irregular histograms are
good non-parametric density estimators that require very few parameters: the
number of bins with their lengths and frequencies. Many approaches have been
proposed in the literature to infer these parameters, either assuming
hypotheses about the underlying data distributions or exploiting a model
selection approach. In this paper, we focus on the G-Enum histogram method,
which exploits the Minimum Description Length (MDL) principle to build
histograms without any user parameter and achieves state-of-the art performance
w.r.t accuracy; parsimony and computation time. We investigate on the limits of
this method in the case of outliers or heavy-tailed distributions. We suggest a
two-level heuristic to deal with such cases. The first level exploits a
logarithmic transformation of the data to split the data set into a list of
data subsets with a controlled range of values. The second level builds a
sub-histogram for each data subset and aggregates them to obtain a complete
histogram. Extensive experiments show the benefits of the approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 47 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Neural Network Compression via $\frac{\ell_1}{\ell_2}$
  Regularized Latency Surrogates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05785v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05785v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anshul Nasery, Hardik Shah, Arun Sai Suggala, Prateek Jain
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural network (NN) compression via techniques such as pruning, quantization
requires setting compression hyperparameters (e.g., number of channels to be
pruned, bitwidths for quantization) for each layer either manually or via
neural architecture search (NAS) which can be computationally expensive. We
address this problem by providing an end-to-end technique that optimizes for
model's Floating Point Operations (FLOPs) or for on-device latency via a novel
$\frac{\ell_1}{\ell_2}$ latency surrogate. Our algorithm is versatile and can
be used with many popular compression methods including pruning, low-rank
factorization, and quantization. Crucially, it is fast and runs in almost the
same amount of time as single model training; which is a significant training
speed-up over standard NAS methods. For BERT compression on GLUE fine-tuning
tasks, we achieve $50\%$ reduction in FLOPs with only $1\%$ drop in
performance. For compressing MobileNetV3 on ImageNet-1K, we achieve $15\%$
reduction in FLOPs, and $11\%$ reduction in on-device latency without drop in
accuracy, while still requiring $3\times$ less training compute than SOTA
compression techniques. Finally, for transfer learning on smaller datasets, our
technique identifies $1.2\times$-$1.4\times$ cheaper architectures than
standard MobileNetV3, EfficientNet suite of architectures at almost the same
training cost and accuracy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantitative Ink Analysis: Estimating the Number of Inks in Documents
  through Hyperspectral Imaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aneeqa Abrar, Hamza Iqbal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of document forensics, ink analysis plays a crucial role in
determining the authenticity of legal and historic documents and detecting
forgery. Visual examination alone is insufficient for distinguishing visually
similar inks, necessitating the use of advanced scientific techniques. This
paper proposes an ink analysis technique based on hyperspectral imaging, which
enables the examination of documents in hundreds of narrowly spaced spectral
bands, revealing hidden details. The main objective of this study is to
identify the number of distinct inks used in a document. Three clustering
algorithms, namely k-means, Agglomerative, and c-means, are employed to
estimate the number of inks present. The methodology involves data extraction,
ink pixel segmentation, and ink number determination. The results demonstrate
the effectiveness of the proposed technique in identifying ink clusters and
distinguishing between different inks. The analysis of a hyperspectral cube
dataset reveals variations in spectral reflectance across different bands and
distinct spectral responses among the 12 lines, indicating the presence of
multiple inks. The clustering algorithms successfully identify ink clusters,
with k-means clustering showing superior classification performance. These
findings contribute to the development of reliable methodologies for ink
analysis using hyperspectral imaging, enhancing the
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptivity Complexity for Causal Graph Discovery <span class="chip">UAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davin Choo, Kirankumar Shiragur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Causal discovery from interventional data is an important problem, where the
task is to design an interventional strategy that learns the hidden ground
truth causal graph $G(V,E)$ on $|V| = n$ nodes while minimizing the number of
performed interventions. Most prior interventional strategies broadly fall into
two categories: non-adaptive and adaptive. Non-adaptive strategies decide on a
single fixed set of interventions to be performed while adaptive strategies can
decide on which nodes to intervene on sequentially based on past interventions.
While adaptive algorithms may use exponentially fewer interventions than their
non-adaptive counterparts, there are practical concerns that constrain the
amount of adaptivity allowed. Motivated by this trade-off, we study the problem
of $r$-adaptivity, where the algorithm designer recovers the causal graph under
a total of $r$ sequential rounds whilst trying to minimize the total number of
interventions. For this problem, we provide a $r$-adaptive algorithm that
achieves $O(\min\{r,\log n\} \cdot n^{1/\min\{r,\log n\}})$ approximation with
respect to the verification number, a well-known lower bound for adaptive
algorithms. Furthermore, for every $r$, we show that our approximation is
tight. Our definition of $r$-adaptivity interpolates nicely between the
non-adaptive ($r=1$) and fully adaptive ($r=n$) settings where our
approximation simplifies to $O(n)$ and $O(\log n)$ respectively, matching the
best-known approximation guarantees for both extremes. Our results also extend
naturally to the bounded size interventions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted into UAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>-based Time-to-Event Prediction for Chronic Kidney Disease
  Deterioration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05779v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05779v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moshe Zisser, Dvir Aran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep-learning techniques, particularly the transformer model, have shown
great potential in enhancing the prediction performance of longitudinal health
records. While previous methods have mainly focused on fixed-time risk
prediction, time-to-event prediction (also known as survival analysis) is often
more appropriate for clinical scenarios. Here, we present a novel deep-learning
architecture we named STRAFE, a generalizable survival analysis
transformer-based architecture for electronic health records. The performance
of STRAFE was evaluated using a real-world claim dataset of over 130,000
individuals with stage 3 chronic kidney disease (CKD) and was found to
outperform other time-to-event prediction algorithms in predicting the exact
time of deterioration to stage 5. Additionally, STRAFE was found to outperform
binary outcome algorithms in predicting fixed-time risk, possibly due to its
ability to train on censored data. We show that STRAFE predictions can improve
the positive predictive value of high-risk patients by 3-fold, demonstrating
possible usage to improve targeting for intervention programs. Finally, we
suggest a novel visualization approach to predictions on a per-patient basis.
In conclusion, STRAFE is a cutting-edge time-to-event prediction algorithm that
has the potential to enhance risk predictions in large claims datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weight Re-Mapping for Variational Quantum Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05776v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05776v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Kölle, Alessandro Giovagnoli, Jonas Stein, Maximilian Balthasar Mansky, Julian Hager, Tobias Rohe, Robert Müller, Claudia Linnhoff-Popien
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Inspired by the remarkable success of artificial neural networks across a
broad spectrum of AI tasks, variational quantum circuits (VQCs) have recently
seen an upsurge in quantum machine learning applications. The promising
outcomes shown by VQCs, such as improved generalization and reduced parameter
training requirements, are attributed to the robust algorithmic capabilities of
quantum computing. However, the current gradient-based training approaches for
VQCs do not adequately accommodate the fact that trainable parameters (or
weights) are typically used as angles in rotational gates. To address this, we
extend the concept of weight re-mapping for VQCs, as introduced by K\"olle et
al. (2023). This approach unambiguously maps the weights to an interval of
length $2\pi$, mirroring data rescaling techniques in conventional machine
learning that have proven to be highly beneficial in numerous scenarios. In our
study, we employ seven distinct weight re-mapping functions to assess their
impact on eight classification datasets, using variational classifiers as a
representative example. Our results indicate that weight re-mapping can enhance
the convergence speed of the VQC. We assess the efficacy of various re-mapping
functions across all datasets and measure their influence on the VQC's average
performance. Our findings indicate that weight re-mapping not only consistently
accelerates the convergence of VQCs, regardless of the specific re-mapping
function employed, but also significantly increases accuracy in certain cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weight Freezing: A Regularization Approach for Fully Connected Layers
  with an Application in EEG Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhengqing Miao, Meirong Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the realm of EEG decoding, enhancing the performance of artificial neural
networks (ANNs) carries significant potential. This study introduces a novel
approach, termed "weight freezing", that is anchored on the principles of ANN
regularization and neuroscience prior knowledge. The concept of weight freezing
revolves around the idea of reducing certain neurons' influence on the
decision-making process for a specific EEG task by freezing specific weights in
the fully connected layer during the backpropagation process. This is
actualized through the use of a mask matrix and a threshold to determine the
proportion of weights to be frozen during backpropagation. Moreover, by setting
the masked weights to zero, weight freezing can not only realize sparse
connections in networks with a fully connected layer as the classifier but also
function as an efficacious regularization method for fully connected layers.
Through experiments involving three distinct ANN architectures and three widely
recognized EEG datasets, we validate the potency of weight freezing. Our method
significantly surpasses previous peak performances in classification accuracy
across all examined datasets. Supplementary control experiments offer insights
into performance differences pre and post weight freezing implementation and
scrutinize the influence of the threshold in the weight freezing process. Our
study underscores the superior efficacy of weight freezing compared to
traditional fully connected networks for EEG feature classification tasks. With
its proven effectiveness, this innovative approach holds substantial promise
for contributing to future strides in EEG decoding research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Paced Absolute Learning Progress as a Regularized Approach to
  Curriculum Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Niehues, Ulla Scheler, Pascal Klink
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The usability of Reinforcement Learning is restricted by the large
computation times it requires. Curriculum Reinforcement Learning speeds up
learning by defining a helpful order in which an agent encounters tasks, i.e.
from simple to hard. Curricula based on Absolute Learning Progress (ALP) have
proven successful in different environments, but waste computation on repeating
already learned behaviour in new tasks. We solve this problem by introducing a
new regularization method based on Self-Paced (Deep) Learning, called
Self-Paced Absolute Learning Progress (SPALP). We evaluate our method in three
different environments. Our method achieves performance comparable to original
ALP in all cases, and reaches it quicker than ALP in two of them. We illustrate
possibilities to further improve the efficiency and performance of SPALP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 8 figures. The paper was a result from an Integrated
  Project at TU Darmstadt for which we received course credit (9 ECTS) and is
  not meant to be published elsewhere</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair yet Asymptotically Equal Collaborative Learning <span class="chip">ICML
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05764v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05764v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoqiang Lin, Xinyi Xu, See-Kiong Ng, Chuan-Sheng Foo, Bryan Kian Hsiang Low
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In collaborative learning with streaming data, nodes (e.g., organizations)
jointly and continuously learn a machine learning (ML) model by sharing the
latest model updates computed from their latest streaming data. For the more
resourceful nodes to be willing to share their model updates, they need to be
fairly incentivized. This paper explores an incentive design that guarantees
fairness so that nodes receive rewards commensurate to their contributions. Our
approach leverages an explore-then-exploit formulation to estimate the nodes'
contributions (i.e., exploration) for realizing our theoretically guaranteed
fair incentives (i.e., exploitation). However, we observe a "rich get richer"
phenomenon arising from the existing approaches to guarantee fairness and it
discourages the participation of the less resourceful nodes. To remedy this, we
additionally preserve asymptotic equality, i.e., less resourceful nodes achieve
equal performance eventually to the more resourceful/"rich" nodes. We
empirically demonstrate in two settings with real-world streaming data:
federated online incremental learning and federated reinforcement learning,
that our proposed approach outperforms existing baselines in fairness and
learning performance while remaining competitive in preserving equality.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 40th International Conference on Machine Learning (ICML
  2023), 37 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient GNN Explanation via Learning Removal-based Attribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05760v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05760v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Rong, Guanchu Wang, Qizhang Feng, Ninghao Liu, Zirui Liu, Enkelejda Kasneci, Xia Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Graph Neural Networks (GNNs) have been widely used in real-world
applications, model explanations are required not only by users but also by
legal regulations. However, simultaneously achieving high fidelity and low
computational costs in generating explanations has been a challenge for current
methods. In this work, we propose a framework of GNN explanation named LeArn
Removal-based Attribution (LARA) to address this problem. Specifically, we
introduce removal-based attribution and demonstrate its substantiated link to
interpretability fidelity theoretically and experimentally. The explainer in
LARA learns to generate removal-based attribution which enables providing
explanations with high fidelity. A strategy of subgraph sampling is designed in
LARA to improve the scalability of the training process. In the deployment,
LARA can efficiently generate the explanation through a feed-forward pass. We
benchmark our approach with other state-of-the-art GNN explanation methods on
six datasets. Results highlight the effectiveness of our framework regarding
both efficiency and fidelity. In particular, LARA is 3.5 times faster and
achieves higher fidelity than the state-of-the-art method on the large dataset
ogbn-arxiv (more than 160K nodes and 1M edges), showing its great potential in
real-world applications. Our source code is available at
https://anonymous.4open.science/r/LARA-10D8/README.md.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Counterfactual Inference through Quantile Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shaoan Xie, Biwei Huang, Bin Gu, Tongliang Liu, Kun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The capacity to address counterfactual "what if" inquiries is crucial for
understanding and making use of causal influences. Traditional counterfactual
inference usually assumes a structural causal model is available. However, in
practice, such a causal model is often unknown and may not be identifiable.
This paper aims to perform reliable counterfactual inference based on the
(learned) qualitative causal structure and observational data, without a given
causal model or even directly estimating conditional distributions. We re-cast
counterfactual reasoning as an extended quantile regression problem using
neural networks. The approach is statistically more efficient than existing
ones, and further makes it possible to develop the generalization ability of
the estimated counterfactual outcome to unseen data and provide an upper bound
on the generalization error. Experiment results on multiple datasets strongly
support our theoretical claims.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An End-to-End Reinforcement Learning Approach for Job-Shop Scheduling
  Problems Based on Constraint Programming <span class="chip">ICAPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierre Tassel, Martin Gebser, Konstantin Schekotihin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constraint Programming (CP) is a declarative programming paradigm that allows
for modeling and solving combinatorial optimization problems, such as the
Job-Shop Scheduling Problem (JSSP). While CP solvers manage to find optimal or
near-optimal solutions for small instances, they do not scale well to large
ones, i.e., they require long computation times or yield low-quality solutions.
Therefore, real-world scheduling applications often resort to fast,
handcrafted, priority-based dispatching heuristics to find a good initial
solution and then refine it using optimization methods.
  This paper proposes a novel end-to-end approach to solving scheduling
problems by means of CP and Reinforcement Learning (RL). In contrast to
previous RL methods, tailored for a given problem by including procedural
simulation algorithms, complex feature engineering, or handcrafted reward
functions, our neural-network architecture and training algorithm merely
require a generic CP encoding of some scheduling problem along with a set of
small instances. Our approach leverages existing CP solvers to train an agent
learning a Priority Dispatching Rule (PDR) that generalizes well to large
instances, even from separate datasets. We evaluate our method on seven JSSP
datasets from the literature, showing its ability to find higher-quality
solutions for very large instances than obtained by static PDRs and by a CP
solver within the same time limit.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published at ICAPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two Independent Teachers are Better Role Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afifa Khaled, Ahmed A. Mubarak, Kun He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent deep learning models have attracted substantial attention in infant
brain analysis. These models have performed state-of-the-art performance, such
as semi-supervised techniques (e.g., Temporal Ensembling, mean teacher).
However, these models depend on an encoder-decoder structure with stacked local
operators to gather long-range information, and the local operators limit the
efficiency and effectiveness. Besides, the $MRI$ data contain different tissue
properties ($TPs$) such as $T1$ and $T2$. One major limitation of these models
is that they use both data as inputs to the segment process, i.e., the models
are trained on the dataset once, and it requires much computational and memory
requirements during inference. In this work, we address the above limitations
by designing a new deep-learning model, called 3D-DenseUNet, which works as
adaptable global aggregation blocks in down-sampling to solve the issue of
spatial information loss. The self-attention module connects the down-sampling
blocks to up-sampling blocks, and integrates the feature maps in three
dimensions of spatial and channel, effectively improving the representation
potential and discriminating ability of the model. Additionally, we propose a
new method called Two Independent Teachers ($2IT$), that summarizes the model
weights instead of label predictions. Each teacher model is trained on
different types of brain data, $T1$ and $T2$, respectively. Then, a fuse model
is added to improve test accuracy and enable training with fewer parameters and
labels compared to the Temporal Ensembling method without modifying the network
architecture. Empirical results demonstrate the effectiveness of the proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This manuscript contains 14 pages, 7 figures. We have submitted the
  manuscript to Journal of IEEE Transactions on Medical Imaging (TMI) in June
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leaping through tree space: continuous phylogenetic inference for rooted
  and unrooted trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew J Penn, Neil Scheidwasser, Joseph Penn, Christl A Donnelly, David A Duchêne, Samir Bhatt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phylogenetics is now fundamental in life sciences, providing insights into
the earliest branches of life and the origins and spread of epidemics. However,
finding suitable phylogenies from the vast space of possible trees remains
challenging. To address this problem, for the first time, we perform both tree
exploration and inference in a continuous space where the computation of
gradients is possible. This continuous relaxation allows for major leaps across
tree space in both rooted and unrooted trees, and is less susceptible to
convergence to local minima. Our approach outperforms the current best methods
for inference on unrooted trees and, in simulation, accurately infers the tree
and root in ultrametric cases. The approach is effective in cases of empirical
data with negligible amounts of data, which we demonstrate on the phylogeny of
jawed vertebrates. Indeed, only a few genes with an ultrametric signal were
generally sufficient for resolving the major lineages of vertebrate. With
cubic-time complexity and efficient optimisation via automatic differentiation,
our method presents an effective way forwards for exploring the most difficult,
data-deficient phylogenetic questions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, 14 supplementary pages, 2 supplementary figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DP-HyPO: An Adaptive Private Hyperparameter Optimization Framework 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hua Wang, Sheng Gao, Huanyu Zhang, Weijie J. Su, Milan Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperparameter optimization, also known as hyperparameter tuning, is a widely
recognized technique for improving model performance. Regrettably, when
training private ML models, many practitioners often overlook the privacy risks
associated with hyperparameter optimization, which could potentially expose
sensitive information about the underlying dataset. Currently, the sole
existing approach to allow privacy-preserving hyperparameter optimization is to
uniformly and randomly select hyperparameters for a number of runs,
subsequently reporting the best-performing hyperparameter. In contrast, in
non-private settings, practitioners commonly utilize "adaptive" hyperparameter
optimization methods such as Gaussian process-based optimization, which select
the next candidate based on information gathered from previous outputs. This
substantial contrast between private and non-private hyperparameter
optimization underscores a critical concern. In our paper, we introduce
DP-HyPO, a pioneering framework for "adaptive" private hyperparameter
optimization, aiming to bridge the gap between private and non-private
hyperparameter optimization. To accomplish this, we provide a comprehensive
differential privacy analysis of our framework. Furthermore, we empirically
demonstrate the effectiveness of DP-HyPO on a diverse set of real-world and
synthetic datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Role of Diverse Replay for Generalisation in Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Weltevrede, Matthijs T. J. Spaan, Wendelin Böhmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reinforcement learning (RL), key components of many algorithms are the
exploration strategy and replay buffer. These strategies regulate what
environment data is collected and trained on and have been extensively studied
in the RL literature. In this paper, we investigate the impact of these
components in the context of generalisation in multi-task RL. We investigate
the hypothesis that collecting and training on more diverse data from the
training environment will improve zero-shot generalisation to new
environments/tasks. We motivate mathematically and show empirically that
generalisation to states that are "reachable" during training is improved by
increasing the diversity of transitions in the replay buffer. Furthermore, we
show empirically that this same strategy also shows improvement for
generalisation to similar but "unreachable" states and could be due to improved
generalisation of latent representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Sample Policy Iteration for Offline Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Hu, Yi Ma, Chenjun Xiao, Yan Zheng, Zhaopeng Meng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL) seeks to derive an effective control
policy from previously collected data. To circumvent errors due to inadequate
data coverage, behavior-regularized methods optimize the control policy while
concurrently minimizing deviation from the data collection policy.
Nevertheless, these methods often exhibit subpar practical performance,
particularly when the offline dataset is collected by sub-optimal policies. In
this paper, we propose a novel algorithm employing in-sample policy iteration
that substantially enhances behavior-regularized methods in offline RL. The
core insight is that by continuously refining the policy used for behavior
regularization, in-sample policy iteration gradually improves itself while
implicitly avoids querying out-of-sample actions to avert catastrophic learning
failures. Our theoretical analysis verifies its ability to learn the in-sample
optimal policy, exclusively utilizing actions well-covered by the dataset.
Moreover, we propose competitive policy improvement, a technique applying two
competitive policies, both of which are trained by iteratively improving over
the best competitor. We show that this simple yet potent technique
significantly enhances learning efficiency when function approximation is
applied. Lastly, experimental results on the D4RL benchmark indicate that our
algorithm outperforms previous state-of-the-art methods in most tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explaining Predictive Uncertainty with Information Theoretic Shapley
  Values 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David S. Watson, Joshua O'Hara, Niek Tax, Richard Mudd, Ido Guy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Researchers in explainable artificial intelligence have developed numerous
methods for helping users understand the predictions of complex supervised
learning models. By contrast, explaining the $\textit{uncertainty}$ of model
outputs has received relatively little attention. We adapt the popular Shapley
value framework to explain various types of predictive uncertainty, quantifying
each feature's contribution to the conditional entropy of individual model
outputs. We consider games with modified characteristic functions and find deep
connections between the resulting Shapley values and fundamental quantities
from information theory and conditional independence testing. We outline
inference procedures for finite sample error rate control with provable
guarantees, and implement an efficient algorithm that performs well in a range
of experiments on real and simulated data. Our method has applications to
covariate shift detection, active learning, feature selection, and active
feature-value acquisition.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimation of Ridge Using Nonlinear Transformation on Density Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Zhai, Hengchao Chen, Zhigang Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ridges play a vital role in accurately approximating the underlying structure
of manifolds. In this paper, we explore the ridge's variation by applying a
concave nonlinear transformation to the density function. Through the
derivation of the Hessian matrix, we observe that nonlinear transformations
yield a rank-one modification of the Hessian matrix. Leveraging the variational
properties of eigenvalue problems, we establish a partial order inclusion
relationship among the corresponding ridges. We intuitively discover that the
transformation can lead to improved estimation of the tangent space via
rank-one modification of the Hessian matrix. To validate our theories, we
conduct extensive numerical experiments on synthetic and real-world datasets
that demonstrate the superiority of the ridges obtained from our transformed
approach in approximating the underlying truth manifold compared to other
manifold fitting algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Surface Statistics: Scene Representations in a Latent Diffusion
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05720v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05720v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yida Chen, Fernanda Viégas, Martin Wattenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent diffusion models (LDMs) exhibit an impressive ability to produce
realistic images, yet the inner workings of these models remain mysterious.
Even when trained purely on images without explicit depth information, they
typically output coherent pictures of 3D scenes. In this work, we investigate a
basic interpretability question: does an LDM create and use an internal
representation of simple scene geometry? Using linear probes, we find evidence
that the internal activations of the LDM encode linear representations of both
3D depth data and a salient-object / background distinction. These
representations appear surprisingly early in the denoising process$-$well
before a human can easily make sense of the noisy images. Intervention
experiments further indicate these representations play a causal role in image
synthesis, and may be used for simple high-level editing of an LDM's output.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Fast and High-Quality Speech Synthesis with Linear Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05708v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05708v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haogeng Liu, Tao Wang, Jie Cao, Ran He, Jianhua Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising Diffusion Probabilistic Models have shown extraordinary ability on
various generative tasks. However, their slow inference speed renders them
impractical in speech synthesis. This paper proposes a linear diffusion model
(LinDiff) based on an ordinary differential equation to simultaneously reach
fast inference and high sample quality. Firstly, we employ linear interpolation
between the target and noise to design a diffusion sequence for training, while
previously the diffusion path that links the noise and target is a curved
segment. When decreasing the number of sampling steps (i.e., the number of line
segments used to fit the path), the ease of fitting straight lines compared to
curves allows us to generate higher quality samples from a random noise with
fewer iterations. Secondly, to reduce computational complexity and achieve
effective global modeling of noisy speech, LinDiff employs a patch-based
processing approach that partitions the input signal into small patches. The
patch-wise token leverages Transformer architecture for effective modeling of
global information. Adversarial training is used to further improve the sample
quality with decreased sampling steps. We test proposed method with speech
synthesis conditioned on acoustic feature (Mel-spectrograms). Experimental
results verify that our model can synthesize high-quality speech even with only
one diffusion step. Both subjective and objective evaluations demonstrate that
our model can synthesize speech of a quality comparable to that of
autoregressive models with faster synthesis speed (3 diffusion steps).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding How Consistency Works in Federated Learning via Stage-wise
  Relaxed Initialization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05706v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05706v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Sun, Li Shen, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a distributed paradigm that coordinates massive
local clients to collaboratively train a global model via stage-wise local
training processes on the heterogeneous dataset. Previous works have implicitly
studied that FL suffers from the ``client-drift'' problem, which is caused by
the inconsistent optimum across local clients. However, till now it still lacks
solid theoretical analysis to explain the impact of this local inconsistency.
To alleviate the negative impact of the ``client drift'' and explore its
substance in FL, in this paper, we first design an efficient FL algorithm
\textit{FedInit}, which allows employing the personalized relaxed
initialization state at the beginning of each local training stage.
Specifically, \textit{FedInit} initializes the local state by moving away from
the current global state towards the reverse direction of the latest local
state. This relaxed initialization helps to revise the local divergence and
enhance the local consistency level. Moreover, to further understand how
inconsistency disrupts performance in FL, we introduce the excess risk analysis
and study the divergence term to investigate the test error of the proposed
\textit{FedInit} method. Our studies show that optimization error is not
sensitive to this local inconsistency, while it mainly affects the
generalization error bound in \textit{FedInit}. Extensive experiments are
conducted to validate this conclusion. Our proposed \textit{FedInit} could
achieve state-of-the-art~(SOTA) results compared to several advanced benchmarks
without any additional costs. Meanwhile, stage-wise relaxed initialization
could also be incorporated into the current advanced algorithms to achieve
higher performance in the FL paradigm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finite-Time Analysis of Minimax Q-Learning for Two-Player Zero-Sum
  Markov Games: Switching System Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05700v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05700v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Donghwan Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The objective of this paper is to investigate the finite-time analysis of a
Q-learning algorithm applied to two-player zero-sum Markov games. Specifically,
we establish a finite-time analysis of both the minimax Q-learning algorithm
and the corresponding value iteration method. To enhance the analysis of both
value iteration and Q-learning, we employ the switching system model of minimax
Q-learning and the associated value iteration. This approach provides further
insights into minimax Q-learning and facilitates a more straightforward and
insightful convergence analysis. We anticipate that the introduction of these
additional insights has the potential to uncover novel connections and foster
collaboration between concepts in the fields of control theory and
reinforcement learning communities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2205.05455</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ JABBERWOCK: A Tool for WebAssembly <span class="highlight-title">Dataset</span> Generation and Its
  Application to Malicious Website Detection <span class="chip">DSN 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chika Komiya, Naoto Yanai, Kyosuke Yamashita, Shingo Okamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning is often used for malicious website detection, but an
approach incorporating WebAssembly as a feature has not been explored due to a
limited number of samples, to the best of our knowledge. In this paper, we
propose JABBERWOCK (JAvascript-Based Binary EncodeR by WebAssembly Optimization
paCKer), a tool to generate WebAssembly datasets in a pseudo fashion via
JavaScript. Loosely speaking, JABBERWOCK automatically gathers JavaScript code
in the real world, convert them into WebAssembly, and then outputs vectors of
the WebAssembly as samples for malicious website detection. We also conduct
experimental evaluations of JABBERWOCK in terms of the processing time for
dataset generation, comparison of the generated samples with actual WebAssembly
samples gathered from the Internet, and an application for malicious website
detection. Regarding the processing time, we show that JABBERWOCK can construct
a dataset in 4.5 seconds per sample for any number of samples. Next, comparing
10,000 samples output by JABBERWOCK with 168 gathered WebAssembly samples, we
believe that the generated samples by JABBERWOCK are similar to those in the
real world. We then show that JABBERWOCK can provide malicious website
detection with 99\% F1-score because JABBERWOCK makes a gap between benign and
malicious samples as the reason for the above high score. We also confirm that
JABBERWOCK can be combined with an existing malicious website detection tool to
improve F1-scores. JABBERWOCK is publicly available via GitHub
(https://github.com/c-chocolate/Jabberwock).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in DCDS 2023 (co-located in DSN 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Group Equivariant Fourier Neural Operators for Partial Differential
  Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05697v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05697v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob Helwig, Xuan Zhang, Cong Fu, Jerry Kurtin, Stephan Wojtowytsch, Shuiwang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider solving partial differential equations (PDEs) with Fourier neural
operators (FNOs), which operate in the frequency domain. Since the laws of
physics do not depend on the coordinate system used to describe them, it is
desirable to encode such symmetries in the neural operator architecture for
better performance and easier learning. While encoding symmetries in the
physical domain using group theory has been studied extensively, how to capture
symmetries in the frequency domain is under-explored. In this work, we extend
group convolutions to the frequency domain and design Fourier layers that are
equivariant to rotations, translations, and reflections by leveraging the
equivariance property of the Fourier transform. The resulting $G$-FNO
architecture generalizes well across input resolutions and performs well in
settings with varying levels of symmetry. Our code is publicly available as
part of the AIRS library (https://github.com/divelab/AIRS).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 40th International Conference on Machine Learning
  https://icml.cc/virtual/2023/poster/23875</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable Representation Learning of Small Quantum States 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05694v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05694v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Frohnert, Evert van Nieuwenburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised machine learning models build an internal representation of
their training data without the need for explicit human guidance or feature
engineering. This learned representation provides insights into which features
of the data are relevant for the task at hand. In the context of quantum
physics, training models to describe quantum states without human intervention
offers a promising approach to gaining insight into how machines represent
complex quantum states. The ability to interpret the learned representation may
offer a new perspective on non-trivial features of quantum systems and their
efficient representation. We train a generative model on two-qubit density
matrices generated by a parameterized quantum circuit. In a series of
computational experiments, we investigate the learned representation of the
model and its internal understanding of the data. We observe that the model
learns an interpretable representation which relates the quantum states to
their underlying entanglement characteristics. In particular, our results
demonstrate that the latent representation of the model is directly correlated
with the entanglement measure concurrence. The insights from this study
represent proof of concept towards interpretable machine learning of quantum
states. Our approach offers insight into how machines learn to represent
small-scale quantum systems autonomously.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lightweight Monocular Depth Estimation via Token-Sharing <span class="highlight-title">Transformer</span> <span class="chip">ICRA 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dong-Jae Lee, Jae Young Lee, Hyounguk Shon, Eojindl Yi, Yeong-Hun Park, Sung-Sik Cho, Junmo Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Depth estimation is an important task in various robotics systems and
applications. In mobile robotics systems, monocular depth estimation is
desirable since a single RGB camera can be deployable at a low cost and compact
size. Due to its significant and growing needs, many lightweight monocular
depth estimation networks have been proposed for mobile robotics systems. While
most lightweight monocular depth estimation methods have been developed using
convolution neural networks, the Transformer has been gradually utilized in
monocular depth estimation recently. However, massive parameters and large
computational costs in the Transformer disturb the deployment to embedded
devices. In this paper, we present a Token-Sharing Transformer (TST), an
architecture using the Transformer for monocular depth estimation, optimized
especially in embedded devices. The proposed TST utilizes global token sharing,
which enables the model to obtain an accurate depth prediction with high
throughput in embedded devices. Experimental results show that TST outperforms
the existing lightweight monocular depth estimation methods. On the NYU Depth
v2 dataset, TST can deliver depth maps up to 63.4 FPS in NVIDIA Jetson nano and
142.6 FPS in NVIDIA Jetson TX2, with lower errors than the existing methods.
Furthermore, TST achieves real-time depth estimation of high-resolution images
on Jetson TX2 with competitive results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICRA 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion Detection from EEG using Transfer Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sidharth Sidharth, Ashish Abraham Samuel, Ranjana H, Jerrin Thomas Panachakel, Sana Parveen K
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The detection of emotions using an Electroencephalogram (EEG) is a crucial
area in brain-computer interfaces and has valuable applications in fields such
as rehabilitation and medicine. In this study, we employed transfer learning to
overcome the challenge of limited data availability in EEG-based emotion
detection. The base model used in this study was Resnet50. Additionally, we
employed a novel feature combination in EEG-based emotion detection. The input
to the model was in the form of an image matrix, which comprised Mean Phase
Coherence (MPC) and Magnitude Squared Coherence (MSC) in the upper-triangular
and lower-triangular matrices, respectively. We further improved the technique
by incorporating features obtained from the Differential Entropy (DE) into the
diagonal, which previously held little to no useful information for classifying
emotions. The dataset used in this study, SEED EEG (62 channel EEG), comprises
three classes (Positive, Neutral, and Negative). We calculated both
subject-independent and subject-dependent accuracy. The subject-dependent
accuracy was obtained using a 10-fold cross-validation method and was 93.1%,
while the subject-independent classification was performed by employing the
leave-one-subject-out (LOSO) strategy. The accuracy obtained in
subject-independent classification was 71.6%. Both of these accuracies are at
least twice better than the chance accuracy of classifying 3 classes. The study
found the use of MSC and MPC in EEG-based emotion detection promising for
emotion classification. The future scope of this work includes the use of data
augmentation techniques, enhanced classifiers, and better features for emotion
classification.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint of the manuscript accepted for presentation in 45th Annual
  International Conference of the IEEE Engineering in Medicine and Biology
  Society. DOI will be updated soon</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Uncertainty Quantification and Reduction for
  Over-Parameterized Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05674v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05674v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Huang, Henry Lam, Haofeng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Uncertainty quantification (UQ) is important for reliability assessment and
enhancement of machine learning models. In deep learning, uncertainties arise
not only from data, but also from the training procedure that often injects
substantial noises and biases. These hinder the attainment of statistical
guarantees and, moreover, impose computational challenges on UQ due to the need
for repeated network retraining. Building upon the recent neural tangent kernel
theory, we create statistically guaranteed schemes to principally
\emph{quantify}, and \emph{remove}, the procedural uncertainty of
over-parameterized neural networks with very low computation effort. In
particular, our approach, based on what we call a procedural-noise-correcting
(PNC) predictor, removes the procedural uncertainty by using only \emph{one}
auxiliary network that is trained on a suitably labeled data set, instead of
many retrained networks employed in deep ensembles. Moreover, by combining our
PNC predictor with suitable light-computation resampling methods, we build
several approaches to construct asymptotically exact-coverage confidence
intervals using as low as four trained networks without additional overheads.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ One-Shot Machine Unlearning with Mnemonic Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05670v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05670v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomoya Yamashita, Masanori Yamada, Takashi Shibata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has achieved significant improvements in accuracy and has been
applied to various fields. With the spread of deep learning, a new problem has
also emerged; deep learning models can sometimes have undesirable information
from an ethical standpoint. This problem must be resolved if deep learning is
to make sensitive decisions such as hiring and prison sentencing. Machine
unlearning (MU) is the research area that responds to such demands. MU aims at
forgetting about undesirable training data from a trained deep learning model.
A naive MU approach is to re-train the whole model with the training data from
which the undesirable data has been removed. However, re-training the whole
model can take a huge amount of time and consumes significant computer
resources. To make MU even more practical, a simple-yet-effective MU method is
required. In this paper, we propose a one-shot MU method, which does not need
additional training. To design one-shot MU, we add noise to the model
parameters that are sensitive to undesirable information. In our proposed
method, we use the Fisher information matrix (FIM) to estimate the sensitive
model parameters. Training data were usually used to evaluate the FIM in
existing methods. In contrast, we avoid the need to retain the training data
for calculating the FIM by using class-specific synthetic signals called
mnemonic code. Extensive experiments using artificial and natural datasets
demonstrate that our method outperforms the existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, welcome coments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ QuestEnvSim: Environment-Aware Simulated Motion Tracking from Sparse
  Sensors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunmin Lee, Sebastian Starke, Yuting Ye, Jungdam Won, Alexander Winkler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Replicating a user's pose from only wearable sensors is important for many
AR/VR applications. Most existing methods for motion tracking avoid environment
interaction apart from foot-floor contact due to their complex dynamics and
hard constraints. However, in daily life people regularly interact with their
environment, e.g. by sitting on a couch or leaning on a desk. Using
Reinforcement Learning, we show that headset and controller pose, if combined
with physics simulation and environment observations can generate realistic
full-body poses even in highly constrained environments. The physics simulation
automatically enforces the various constraints necessary for realistic poses,
instead of manually specifying them as in many kinematic approaches. These hard
constraints allow us to achieve high-quality interaction motions without
typical artifacts such as penetration or contact sliding. We discuss three
features, the environment representation, the contact reward and scene
randomization, crucial to the performance of the method. We demonstrate the
generality of the approach through various examples, such as sitting on chairs,
a couch and boxes, stepping over boxes, rocking a chair and turning an office
chair. We believe these are some of the highest-quality results achieved for
motion tracking from sparse sensor with scene interaction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Communication-Efficient Zeroth-Order Distributed Online Optimization:
  Algorithm, Theory, and Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ege C. Kaya, M. Berk Sahin, Abolfazl Hashemi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper focuses on a multi-agent zeroth-order online optimization problem
in a federated learning setting for target tracking. The agents only sense
their current distances to their targets and aim to maintain a minimum safe
distance from each other to prevent collisions. The coordination among the
agents and dissemination of collision-prevention information is managed by a
central server using the federated learning paradigm. The proposed formulation
leads to an instance of distributed online nonconvex optimization problem that
is solved via a group of communication-constrained agents. To deal with the
communication limitations of the agents, an error feedback-based compression
scheme is utilized for agent-to-server communication. The proposed algorithm is
analyzed theoretically for the general class of distributed online nonconvex
optimization problems. We provide non-asymptotic convergence rates that show
the dominant term is independent of the characteristics of the compression
scheme. Our theoretical results feature a new approach that employs
significantly more relaxed assumptions in comparison to standard literature.
The performance of the proposed solution is further analyzed numerically in
terms of tracking errors and collisions between agents in two relevant
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 5 figures, and this paper has been accepted by IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Differentially Private Sharpness-Aware Training <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05651v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05651v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinseong Park, Hoki Kim, Yujin Choi, Jaewook Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training deep learning models with differential privacy (DP) results in a
degradation of performance. The training dynamics of models with DP show a
significant difference from standard training, whereas understanding the
geometric properties of private learning remains largely unexplored. In this
paper, we investigate sharpness, a key factor in achieving better
generalization, in private learning. We show that flat minima can help reduce
the negative effects of per-example gradient clipping and the addition of
Gaussian noise. We then verify the effectiveness of Sharpness-Aware
Minimization (SAM) for seeking flat minima in private learning. However, we
also discover that SAM is detrimental to the privacy budget and computational
time due to its two-step optimization. Thus, we propose a new sharpness-aware
training method that mitigates the privacy-optimization trade-off. Our
experimental results demonstrate that the proposed method improves the
performance of deep learning models with DP from both scratch and fine-tuning.
Code is available at https://github.com/jinseongP/DPSAT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Specifying and Solving Robust Empirical Risk Minimization Problems Using
  CVXPY 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05649v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05649v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eric Luxenberg, Dhruv Malik, Yuanzhi Li, Aarti Singh, Stephen Boyd
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider robust empirical risk minimization (ERM), where model parameters
are chosen to minimize the worst-case empirical loss when each data point
varies over a given convex uncertainty set. In some simple cases, such problems
can be expressed in an analytical form. In general the problem can be made
tractable via dualization, which turns a min-max problem into a min-min
problem. Dualization requires expertise and is tedious and error-prone. We
demonstrate how CVXPY can be used to automate this dualization procedure in a
user-friendly manner. Our framework allows practitioners to specify and solve
robust ERM problems with a general class of convex losses, capturing many
standard regression and classification problems. Users can easily specify any
complex uncertainty set that is representable via disciplined convex
programming (DCP) constraints.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Permutation Symmetry for Merging Models between Different
  <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05641v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05641v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Masanori Yamada, Tomoya Yamashita, Shin'ya Yamaguchi, Daiki Chijiwa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model merging is a new approach to creating a new model by combining the
weights of different trained models. Previous studies report that model merging
works well for models trained on a single dataset with different random seeds,
while model merging between different datasets is difficult. Merging knowledge
from different datasets has practical significance, but it has not been well
investigated. In this paper, we investigate the properties of merging models
between different datasets. Through theoretical and empirical analyses, we find
that the accuracy of the merged model decreases more significantly as the
datasets diverge more and that the different loss landscapes for each dataset
make model merging between different datasets difficult. We also show that
merged models require datasets for merging in order to achieve a high accuracy.
Furthermore, we show that condensed datasets created by dataset condensation
can be used as substitutes for the original datasets when merging models. We
conduct experiments for model merging between different datasets. When merging
between MNIST and Fashion- MNIST models, the accuracy significantly improves by
28% using the dataset and 25% using the condensed dataset compared with not
using the dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages; comments are welcome</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Importance of Feature Decorrelation for Unsupervised
  Representation Learning in Reinforcement Learning <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hojoon Lee, Koanho Lee, Dongyoon Hwang, Hyunho Lee, Byungkun Lee, Jaegul Choo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, unsupervised representation learning (URL) has improved the sample
efficiency of Reinforcement Learning (RL) by pretraining a model from a large
unlabeled dataset. The underlying principle of these methods is to learn
temporally predictive representations by predicting future states in the latent
space. However, an important challenge of this approach is the representational
collapse, where the subspace of the latent representations collapses into a
low-dimensional manifold. To address this issue, we propose a novel URL
framework that causally predicts future states while increasing the dimension
of the latent manifold by decorrelating the features in the latent space.
Through extensive empirical studies, we demonstrate that our framework
effectively learns predictive representations without collapse, which
significantly improves the sample efficiency of state-of-the-art URL methods on
the Atari 100k benchmark. The code is available at
https://github.com/dojeon-ai/SimTPR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Reliability of Watermarks for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04634v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04634v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, Tom Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As LLMs become commonplace, machine-generated text has the potential to flood
the internet with spam, social media bots, and valueless content. Watermarking
is a simple and effective strategy for mitigating such harms by enabling the
detection and documentation of LLM-generated text. Yet a crucial question
remains: How reliable is watermarking in realistic settings in the wild? There,
watermarked text may be modified to suit a user's needs, or entirely rewritten
to avoid detection.
  We study the robustness of watermarked text after it is re-written by humans,
paraphrased by a non-watermarked LLM, or mixed into a longer hand-written
document. We find that watermarks remain detectable even after human and
machine paraphrasing. While these attacks dilute the strength of the watermark,
paraphrases are statistically likely to leak n-grams or even longer fragments
of the original text, resulting in high-confidence detections when enough
tokens are observed. For example, after strong human paraphrasing the watermark
is detectable after observing 800 tokens on average, when setting a 1e-5 false
positive rate. We also consider a range of new detection schemes that are
sensitive to short spans of watermarked text embedded inside a large document,
and we compare the robustness of watermarking to other kinds of detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages in the main body. Code is available at
  https://github.com/jwkirchenbauer/lm-watermarking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ L-GreCo: Layerwise-Adaptive Gradient Compression for Efficient and
  Accurate Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.17357v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.17357v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammadreza Alimohammadi, Ilia Markov, Elias Frantar, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data-parallel distributed training of deep neural networks (DNN) has gained
very widespread adoption, but can still experience communication bottlenecks.
To address this issue, entire families of compression mechanisms have been
developed, including quantization, sparsification, and low-rank approximation,
some of which are seeing significant practical adoption. Despite this progress,
almost all known compression schemes apply compression uniformly across DNN
layers, although layers are heterogeneous in terms of parameter count and their
impact on model accuracy. In this work, we provide a general framework for
adapting the degree of compression across the model's layers dynamically during
training, improving the overall compression, while leading to substantial
speedups, without sacrificing accuracy. Our framework, called L-GreCo, is based
on an adaptive algorithm, which automatically picks the optimal compression
parameters for model layers guaranteeing the best compression ratio while
satisfying an error constraint. Extensive experiments over image classification
and language modeling tasks shows that L-GreCo is effective across all existing
families of compression methods, and achieves up to 2.5$\times$ training
speedup and up to 5$\times$ compression improvement over efficient
implementations of existing approaches, while recovering full accuracy.
Moreover, L-GreCo is complementary to existing adaptive algorithms, improving
their compression ratio by 50% and practical throughput by 66%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised hierarchical clustering using the learning dynamics of RBMs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01851v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01851v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aurélien Decelle, Lorenzo Rosset, Beatriz Seoane
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Datasets in the real world are often complex and to some degree hierarchical,
with groups and sub-groups of data sharing common characteristics at different
levels of abstraction. Understanding and uncovering the hidden structure of
these datasets is an important task that has many practical applications. To
address this challenge, we present a new and general method for building
relational data trees by exploiting the learning dynamics of the Restricted
Boltzmann Machine (RBM). Our method is based on the mean-field approach,
derived from the Plefka expansion, and developed in the context of disordered
systems. It is designed to be easily interpretable. We tested our method in an
artificially created hierarchical dataset and on three different real-world
datasets (images of digits, mutations in the human genome, and a homologous
family of proteins). The method is able to automatically identify the
hierarchical structure of the data. This could be useful in the study of
homologous protein sequences, where the relationships between proteins are
critical for understanding their function and evolution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version accepted in Physical Review E</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Deep Reinforcement Learning Using Observational Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Zhu, Chao Yu, Qiang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep reinforcement learning (DRL) requires the collection of interventional
data, which is sometimes expensive and even unethical in the real world, such
as in the autonomous driving and the medical field. Offline reinforcement
learning promises to alleviate this issue by exploiting the vast amount of
observational data available in the real world. However, observational data may
mislead the learning agent to undesirable outcomes if the behavior policy that
generates the data depends on unobserved random variables (i.e., confounders).
In this paper, we propose two deconfounding methods in DRL to address this
problem. The methods first calculate the importance degree of different samples
based on the causal inference technique, and then adjust the impact of
different samples on the loss function by reweighting or resampling the offline
dataset to ensure its unbiasedness. These deconfounding methods can be flexibly
combined with existing model-free DRL algorithms such as soft actor-critic and
deep Q-learning, provided that a weak condition can be satisfied by the loss
functions of these algorithms. We prove the effectiveness of our deconfounding
methods and validate them experimentally.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Local Explanations of Nonlinear Models Using Animated Linear
  Projections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.05359v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.05359v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Spyrison, Dianne Cook, Przemyslaw Biecek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increased predictive power of machine learning models comes at the cost
of increased complexity and loss of interpretability, particularly in
comparison to parametric statistical models. This trade-off has led to the
emergence of eXplainable AI (XAI) which provides methods, such as local
explanations (LEs) and local variable attributions (LVAs), to shed light on how
a model use predictors to arrive at a prediction. These provide a point
estimate of the linear variable importance in the vicinity of a single
observation. However, LVAs tend not to effectively handle association between
predictors. To understand how the interaction between predictors affects the
variable importance estimate, we can convert LVAs into linear projections and
use the radial tour. This is also useful for learning how a model has made a
mistake, or the effect of outliers, or the clustering of observations. The
approach is illustrated with examples from categorical (penguin species,
chocolate types) and quantitative (soccer/football salaries, house prices)
response models. The methods are implemented in the R package cheem, available
on CRAN.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 10 figures, 0 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Laplacian-based Options for Temporally-Extended Exploration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11181v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11181v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Klissarov, Marlos C. Machado
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Selecting exploratory actions that generate a rich stream of experience for
better learning is a fundamental challenge in reinforcement learning (RL). An
approach to tackle this problem consists in selecting actions according to
specific policies for an extended period of time, also known as options. A
recent line of work to derive such exploratory options builds upon the
eigenfunctions of the graph Laplacian. Importantly, until now these methods
have been mostly limited to tabular domains where (1) the graph Laplacian
matrix was either given or could be fully estimated, (2) performing
eigendecomposition on this matrix was computationally tractable, and (3) value
functions could be learned exactly. Additionally, these methods required a
separate option discovery phase. These assumptions are fundamentally not
scalable. In this paper we address these limitations and show how recent
results for directly approximating the eigenfunctions of the Laplacian can be
leveraged to truly scale up options-based exploration. To do so, we introduce a
fully online deep RL algorithm for discovering Laplacian-based options and
evaluate our approach on a variety of pixel-based tasks. We compare to several
state-of-the-art exploration methods and show that our approach is effective,
general, and especially promising in non-stationary settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ L0Learn: A Scalable Package for Sparse Learning using L0 Regularization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.04820v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.04820v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hussein Hazimeh, Rahul Mazumder, Tim Nonet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present L0Learn: an open-source package for sparse linear regression and
classification using $\ell_0$ regularization. L0Learn implements scalable,
approximate algorithms, based on coordinate descent and local combinatorial
optimization. The package is built using C++ and has user-friendly R and Python
interfaces. L0Learn can address problems with millions of features, achieving
competitive run times and statistical performance with state-of-the-art sparse
learning packages. L0Learn is available on both CRAN and GitHub
(https://cran.r-project.org/package=L0Learn and
https://github.com/hazimehh/L0Learn).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to JMLR (MLOSS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ adSformers: Personalization from Short-Term Sequences and Diversity of
  Representations in Etsy Ads 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01255v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01255v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alaa Awad, Denisa Roberts, Eden Dolev, Andrea Heyman, Zahra Ebrahimzadeh, Zoe Weil, Marcin Mejran, Vaibhav Malpani, Mahir Yavuz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we present a general approach to personalizing ads through
encoding and learning from variable-length sequences of recent user actions and
diverse representations. To this end we introduce a three-component module
called the adSformer diversifiable personalization module (ADPM) that learns a
dynamic user representation. We illustrate the module's effectiveness and
flexibility by personalizing the Click-Through Rate (CTR) and Post-Click
Conversion Rate (PCCVR) models used in sponsored search. The first component of
the ADPM, the adSformer encoder, includes a novel adSformer block which learns
the most salient sequence signals. ADPM's second component enriches the learned
signal through visual, multimodal, and other pretrained representations.
Lastly, the third ADPM "learned on the fly" component further diversifies the
signal encoded in the dynamic user representation. The ADPM-personalized CTR
and PCCVR models, henceforth referred to as adSformer CTR and adSformer PCCVR,
outperform the CTR and PCCVR production baselines by $+2.66\%$ and $+2.42\%$,
respectively, in offline Area Under the Receiver Operating Characteristic Curve
(ROC-AUC). Following the robust online gains in A/B tests, Etsy Ads deployed
the ADPM-personalized sponsored search system to $100\%$ of traffic as of
February 2023.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How to Unleash the Power of Large Language Models for Few-shot Relation
  Extraction? <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.01555v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.01555v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Xu, Yuqi Zhu, Xiaohan Wang, Ningyu Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scaling language models have revolutionized widespread NLP tasks, yet little
comprehensively explored few-shot relation extraction with large language
models. In this paper, we investigate principal methodologies, in-context
learning and data generation, for few-shot relation extraction via GPT-3.5
through exhaustive experiments. To enhance few-shot performance, we further
propose task-related instructions and schema-constrained data generation. We
observe that in-context learning can achieve performance on par with previous
prompt learning approaches, and data generation with the large language model
can boost previous solutions to obtain new state-of-the-art few-shot results on
four widely-studied relation extraction datasets. We hope our work can inspire
future research for the capabilities of large language models in few-shot
relation extraction. Code is available in
https://github.com/zjunlp/DeepKE/tree/main/example/llm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SustaiNLP Workshop@ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Change-Point Detection in Time Series via Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.03860v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.03860v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Li, Paul Fearnhead, Piotr Fryzlewicz, Tengyao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Detecting change-points in data is challenging because of the range of
possible types of change and types of behaviour of data when there is no
change. Statistically efficient methods for detecting a change will depend on
both of these features, and it can be difficult for a practitioner to develop
an appropriate detection method for their application of interest. We show how
to automatically generate new offline detection methods based on training a
neural network. Our approach is motivated by many existing tests for the
presence of a change-point being representable by a simple neural network, and
thus a neural network trained with sufficient data should have performance at
least as good as these methods. We present theory that quantifies the error
rate for such an approach, and how it depends on the amount of training data.
Empirical results show that, even with limited training data, its performance
is competitive with the standard CUSUM-based classifier for detecting a change
in mean when the noise is independent and Gaussian, and can substantially
outperform it in the presence of auto-correlated or heavy-tailed noise. Our
method also shows strong results in detecting and localising changes in
activity based on accelerometer data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 15 figures and 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ 10 Security and Privacy Problems in Large Foundation Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.15444v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.15444v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyuan Jia, Hongbin Liu, Neil Zhenqiang Gong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models--such as GPT, CLIP, and DINO--have achieved revolutionary
progress in the past several years and are commonly believed to be a promising
approach for general-purpose AI. In particular, self-supervised learning is
adopted to pre-train a foundation model using a large amount of unlabeled data.
A pre-trained foundation model is like an ``operating system'' of the AI
ecosystem. Specifically, a foundation model can be used as a feature extractor
for many downstream tasks with little or no labeled training data. Existing
studies on foundation models mainly focused on pre-training a better foundation
model to improve its performance on downstream tasks in non-adversarial
settings, leaving its security and privacy in adversarial settings largely
unexplored. A security or privacy issue of a pre-trained foundation model leads
to a single point of failure for the AI ecosystem. In this book chapter, we
discuss 10 basic security and privacy problems for the pre-trained foundation
models, including six confidentiality problems, three integrity problems, and
one availability problem. For each problem, we discuss potential opportunities
and challenges. We hope our book chapter will inspire future research on the
security and privacy of foundation models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A book chapter</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A memory-efficient neural ODE framework based on high-level adjoint
  differentiation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.01298v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.01298v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hong Zhang, Wenjun Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural ordinary differential equations (neural ODEs) have emerged as a novel
network architecture that bridges dynamical systems and deep learning. However,
the gradient obtained with the continuous adjoint method in the vanilla neural
ODE is not reverse-accurate. Other approaches suffer either from an excessive
memory requirement due to deep computational graphs or from limited choices for
the time integration scheme, hampering their application to large-scale complex
dynamical systems. To achieve accurate gradients without compromising memory
efficiency and flexibility, we present a new neural ODE framework, PNODE, based
on high-level discrete adjoint algorithmic differentiation. By leveraging
discrete adjoint time integrators and advanced checkpointing strategies
tailored for these integrators, PNODE can provide a balance between memory and
computational costs, while computing the gradients consistently and accurately.
We provide an open-source implementation based on PyTorch and PETSc, one of the
most commonly used portable, scalable scientific computing libraries. We
demonstrate the performance through extensive numerical experiments on image
classification and continuous normalizing flow problems. We show that PNODE
achieves the highest memory efficiency when compared with other
reverse-accurate methods. On the image classification problems, PNODE is up to
two times faster than the vanilla neural ODE and up to 2.3 times faster than
the best existing reverse-accurate method. We also show that PNODE enables the
use of the implicit time integration methods that are needed for stiff
dynamical systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distributed Task Management in Fog Computing: A Socially Concave Bandit
  Game 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.14572v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.14572v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaotong Cheng, Setareh Maghsudi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fog computing leverages the task offloading capabilities at the network's
edge to improve efficiency and enable swift responses to application demands.
However, the design of task allocation strategies in a fog computing network is
still challenging because of the heterogeneity of fog nodes and uncertainties
in system dynamics. We formulate the distributed task allocation problem as a
social-concave game with bandit feedback and show that the game has a unique
Nash equilibrium, which is implementable using no-regret learning strategies
(regret with sublinear growth). We then develop two no-regret online
decision-making strategies. One strategy, namely bandit gradient ascent with
momentum, is an online convex optimization algorithm with bandit feedback. The
other strategy, Lipschitz bandit with initialization, is an EXP3 multi-armed
bandit algorithm. We establish regret bounds for both strategies and analyze
their convergence characteristics. Moreover, we compare the proposed strategies
with an allocation strategy named learning with linear rewards. Theoretical-
and numerical analysis shows the superior performance of the proposed
strategies for efficient task allocation compared to the state-of-the-art
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unsupervised Video Domain Adaptation for Action Recognition: A
  Disentanglement Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.07365v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.07365v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengfei Wei, Lingdong Kong, Xinghua Qu, Yi Ren, Zhiqiang Xu, Jing Jiang, Xiang Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised video domain adaptation is a practical yet challenging task. In
this work, for the first time, we tackle it from a disentanglement view. Our
key idea is to handle the spatial and temporal domain divergence separately
through disentanglement. Specifically, we consider the generation of
cross-domain videos from two sets of latent factors, one encoding the static
information and another encoding the dynamic information. A Transfer Sequential
VAE (TranSVAE) framework is then developed to model such generation. To better
serve for adaptation, we propose several objectives to constrain the latent
factors. With these constraints, the spatial divergence can be readily removed
by disentangling the static domain-specific information out, and the temporal
divergence is further reduced from both frame- and video-levels through
adversarial learning. Extensive experiments on the UCF-HMDB, Jester, and
Epic-Kitchens datasets verify the effectiveness and superiority of TranSVAE
compared with several state-of-the-art methods. The code with reproducible
results is publicly accessible.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 9 figures, 7 tables. Code at
  https://github.com/ldkong1205/TranSVAE</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reading Between the Lines: Modeling User Behavior and Costs in
  AI-Assisted Programming 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.14306v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.14306v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hussein Mozannar, Gagan Bansal, Adam Fourney, Eric Horvitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code-recommendation systems, such as Copilot and CodeWhisperer, have the
potential to improve programmer productivity by suggesting and auto-completing
code. However, to fully realize their potential, we must understand how
programmers interact with these systems and identify ways to improve that
interaction. To make progress, we studied GitHub Copilot, a code-recommendation
system used by millions of programmers daily. We developed CUPS, a taxonomy of
common programmer activities when interacting with Copilot. Our study of 21
programmers, who completed coding tasks and retrospectively labeled their
sessions with CUPS, showed that CUPS can help us understand how programmers
interact with code-recommendation systems, revealing inefficiencies and time
costs. Our insights reveal how programmers interact with Copilot and motivate
new interface designs and metrics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Weighted Aggregation in Federated Learning with Neural
  Networks <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10911v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10911v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zexi Li, Tao Lin, Xinyi Shang, Chao Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In federated learning (FL), weighted aggregation of local models is conducted
to generate a global model, and the aggregation weights are normalized (the sum
of weights is 1) and proportional to the local data sizes. In this paper, we
revisit the weighted aggregation process and gain new insights into the
training dynamics of FL. First, we find that the sum of weights can be smaller
than 1, causing global weight shrinking effect (analogous to weight decay) and
improving generalization. We explore how the optimal shrinking factor is
affected by clients' data heterogeneity and local epochs. Second, we dive into
the relative aggregation weights among clients to depict the clients'
importance. We develop client coherence to study the learning dynamics and find
a critical point that exists. Before entering the critical point, more coherent
clients play more essential roles in generalization. Based on the above
insights, we propose an effective method for Federated Learning with Learnable
Aggregation Weights, named as FedLAW. Extensive experiments verify that our
method can improve the generalization of the global model by a large margin on
different datasets and models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EmotionNAS: Two-stream Neural Architecture Search for Speech Emotion
  Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.13617v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.13617v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Sun, Zheng Lian, Bin Liu, Ying Li, Licai Sun, Cong Cai, Jianhua Tao, Meng Wang, Yuan Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech emotion recognition (SER) is an important research topic in
human-computer interaction. Existing works mainly rely on human expertise to
design models. Despite their success, different datasets often require distinct
structures and hyperparameters. Searching for an optimal model for each dataset
is time-consuming and labor-intensive. To address this problem, we propose a
two-stream neural architecture search (NAS) based framework, called
\enquote{EmotionNAS}. Specifically, we take two-stream features (i.e.,
handcrafted and deep features) as the inputs, followed by NAS to search for the
optimal structure for each stream. Furthermore, we incorporate complementary
information in different streams through an efficient information supplement
module. Experimental results demonstrate that our method outperforms existing
manually-designed and NAS-based models, setting the new state-of-the-art
record.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FLSTRA: Federated Learning in Stratosphere 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00163v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00163v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Farajzadeh, Animesh Yadav, Omid Abbasi, Wael Jaafar, Halim Yanikomeroglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a federated learning (FL) in stratosphere (FLSTRA) system, where a
high altitude platform station (HAPS) facilitates a large number of terrestrial
clients to collaboratively learn a global model without sharing the training
data. FLSTRA overcomes the challenges faced by FL in terrestrial networks, such
as slow convergence and high communication delay due to limited client
participation and multi-hop communications. HAPS leverages its altitude and
size to allow the participation of more clients with line-of-sight (LOS) links
and the placement of a powerful server. However, handling many clients at once
introduces computing and transmission delays. Thus, we aim to obtain a
delay-accuracy trade-off for FLSTRA. Specifically, we first develop a joint
client selection and resource allocation algorithm for uplink and downlink to
minimize the FL delay subject to the energy and quality-of-service (QoS)
constraints. Second, we propose a communication and computation resource-aware
(CCRA-FL) algorithm to achieve the target FL accuracy while deriving an upper
bound for its convergence rate. The formulated problem is non-convex; thus, we
propose an iterative algorithm to solve it. Simulation results demonstrate the
effectiveness of the proposed FLSTRA system, compared to terrestrial
benchmarks, in terms of FL delay and accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Wireless Communications</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guillotine Regularization: Why removing layers is needed to improve
  generalization in <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.13378v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.13378v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Bordes, Randall Balestriero, Quentin Garrido, Adrien Bardes, Pascal Vincent
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One unexpected technique that emerged in recent years consists in training a
Deep Network (DN) with a Self-Supervised Learning (SSL) method, and using this
network on downstream tasks but with its last few projector layers entirely
removed. This trick of throwing away the projector is actually critical for SSL
methods to display competitive performances on ImageNet for which more than 30
percentage points can be gained that way. This is a little vexing, as one would
hope that the network layer at which invariance is explicitly enforced by the
SSL criterion during training (the last projector layer) should be the one to
use for best generalization performance downstream. But it seems not to be, and
this study sheds some light on why. This trick, which we name Guillotine
Regularization (GR), is in fact a generically applicable method that has been
used to improve generalization performance in transfer learning scenarios. In
this work, we identify the underlying reasons behind its success and show that
the optimal layer to use might change significantly depending on the training
setup, the data or the downstream task. Lastly, we give some insights on how to
reduce the need for a projector in SSL by aligning the pretext SSL task and the
downstream task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at TMLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Active Learning with Weak Supervision for Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.08335v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.08335v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amanda Olmin, Jakob Lindqvist, Lennart Svensson, Fredrik Lindsten
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Annotating data for supervised learning can be costly. When the annotation
budget is limited, active learning can be used to select and annotate those
observations that are likely to give the most gain in model performance. We
propose an active learning algorithm that, in addition to selecting which
observation to annotate, selects the precision of the annotation that is
acquired. Assuming that annotations with low precision are cheaper to obtain,
this allows the model to explore a larger part of the input space, with the
same annotation budget. We build our acquisition function on the previously
proposed BALD objective for Gaussian Processes, and empirically demonstrate the
gains of being able to adjust the annotation precision in the active learning
loop.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Novel Correlation-optimized Deep Learning Method for Wind Speed
  Forecast 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yang, Jin Lang, Jian Wu, Yanyan Zhang, Xiang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing installation rate of wind power poses great challenges to the
global power system. In order to ensure the reliable operation of the power
system, it is necessary to accurately forecast the wind speed and power of the
wind turbines. At present, deep learning is progressively applied to the wind
speed prediction. Nevertheless, the recent deep learning methods still reflect
the embarrassment for practical applications due to model interpretability and
hardware limitation. To this end, a novel deep knowledge-based learning method
is proposed in this paper. The proposed method hybridizes pre-training method
and auto-encoder structure to improve data representation and modeling of the
deep knowledge-based learning framework. In order to form knowledge and
corresponding absorbers, the original data is preprocessed by an optimization
model based on correlation to construct multi-layer networks (knowledge) which
are absorbed by sequence to sequence (Seq2Seq) models. Specifically, new
cognition and memory units (CMU) are designed to reinforce traditional deep
learning framework. Finally, the effectiveness of the proposed method is
verified by three wind prediction cases from a wind farm in Liaoning, China.
Experimental results show that the proposed method increases the stability and
training efficiency compared to the traditional LSTM method and LSTM/GRU-based
Seq2Seq method for applications of wind speed forecasting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CI-GNN: A Granger Causality-Inspired Graph Neural Network for
  Interpretable Brain Network-Based Psychiatric Diagnosis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.01642v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.01642v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaizhong Zheng, Shujian Yu, Badong Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is a recent trend to leverage the power of graph neural networks (GNNs)
for brain-network based psychiatric diagnosis, which,in turn, also motivates an
urgent need for psychiatrists to fully understand the decision behavior of the
used GNNs. However, most of the existing GNN explainers are either post-hoc in
which another interpretive model needs to be created to explain a well-trained
GNN, or do not consider the causal relationship between the extracted
explanation and the decision, such that the explanation itself contains
spurious correlations and suffers from weak faithfulness. In this work, we
propose a granger causality-inspired graph neural network (CI-GNN), a built-in
interpretable model that is able to identify the most influential subgraph
(i.e., functional connectivity within brain regions) that is causally related
to the decision (e.g., major depressive disorder patients or healthy controls),
without the training of an auxillary interpretive network. CI-GNN learns
disentangled subgraph-level representations {\alpha} and \b{eta} that encode,
respectively, the causal and noncausal aspects of original graph under a graph
variational autoencoder framework, regularized by a conditional mutual
information (CMI) constraint. We theoretically justify the validity of the CMI
regulation in capturing the causal relationship. We also empirically evaluate
the performance of CI-GNN against three baseline GNNs and four state-of-the-art
GNN explainers on synthetic data and three large-scale brain disease datasets.
We observe that CI-GNN achieves the best performance in a wide range of metrics
and provides more reliable and concise explanations which have clinical
evidence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Conformal Credal <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15239v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15239v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian Lienen, Caglar Demir, Eyke Hüllermeier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In semi-supervised learning, the paradigm of self-training refers to the idea
of learning from pseudo-labels suggested by the learner itself. Across various
domains, corresponding methods have proven effective and achieve
state-of-the-art performance. However, pseudo-labels typically stem from ad-hoc
heuristics, relying on the quality of the predictions though without
guaranteeing their validity. One such method, so-called credal self-supervised
learning, maintains pseudo-supervision in the form of sets of (instead of
single) probability distributions over labels, thereby allowing for a flexible
yet uncertainty-aware labeling. Again, however, there is no justification
beyond empirical effectiveness. To address this deficiency, we make use of
conformal prediction, an approach that comes with guarantees on the validity of
set-valued predictions. As a result, the construction of credal sets of labels
is supported by a rigorous theoretical foundation, leading to better calibrated
and less error-prone supervision for unlabeled data. Along with this, we
present effective algorithms for learning from credal self-supervision. An
empirical study demonstrates excellent calibration properties of the
pseudo-supervision, as well as the competitiveness of our method on several
benchmark datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 5 figures, 10 tables, to be published at the 12th Symposium
  on Conformal and Probabilistic Prediction with Applications (COPA 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Strategic Classification: A Tale of Two Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06280v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06280v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Horowitz, Nir Rosenfeld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When users can benefit from certain predictive outcomes, they may be prone to
act to achieve those outcome, e.g., by strategically modifying their features.
The goal in strategic classification is therefore to train predictive models
that are robust to such behavior. However, the conventional framework assumes
that changing features does not change actual outcomes, which depicts users as
"gaming" the system. Here we remove this assumption, and study learning in a
causal strategic setting where true outcomes do change. Focusing on accuracy as
our primary objective, we show how strategic behavior and causal effects
underlie two complementing forms of distribution shift. We characterize these
shifts, and propose a learning algorithm that balances between these two forces
and over time, and permits end-to-end training. Experiments on synthetic and
semi-synthetic data demonstrate the utility of our approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PFNs4BO: In-Context Learning for Bayesian Optimization <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17535v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17535v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Müller, Matthias Feurer, Noah Hollmann, Frank Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we use Prior-data Fitted Networks (PFNs) as a flexible
surrogate for Bayesian Optimization (BO). PFNs are neural processes that are
trained to approximate the posterior predictive distribution (PPD) through
in-context learning on any prior distribution that can be efficiently sampled
from. We describe how this flexibility can be exploited for surrogate modeling
in BO. We use PFNs to mimic a naive Gaussian process (GP), an advanced GP, and
a Bayesian Neural Network (BNN). In addition, we show how to incorporate
further information into the prior, such as allowing hints about the position
of optima (user priors), ignoring irrelevant dimensions, and performing
non-myopic BO by learning the acquisition function. The flexibility underlying
these extensions opens up vast possibilities for using PFNs for BO. We
demonstrate the usefulness of PFNs for BO in a large-scale evaluation on
artificial GP samples and three different hyperparameter optimization testbeds:
HPO-B, Bayesmark, and PD1. We publish code alongside trained models at
https://github.com/automl/PFNs4BO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Visual Abstraction and Reasoning through Language <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04091v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04091v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Camposampiero, Loic Houmard, Benjamin Estermann, Joël Mathys, Roger Wattenhofer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Artificial Intelligence (AI) models have achieved human or even
superhuman performance in narrowly defined applications, they still struggle to
show signs of broader and more flexible intelligence. The Abstraction and
Reasoning Corpus (ARC), introduced by Fran\c{c}ois Chollet, aims to assess how
close AI systems are to human-like cognitive abilities. Most current approaches
rely on carefully handcrafted domain-specific languages (DSLs), which are used
to brute-force solutions to the tasks present in ARC. In this work, we propose
a general framework for solving ARC based on natural language descriptions of
the tasks. While not yet beating state-of-the-art DSL models on ARC, we
demonstrate the immense potential of our approach hinted at by the ability to
solve previously unsolved tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors have contributed equally to this work. Accepted
  as regular paper at CVPR 2023 Workshop and Challenges for New Frontiers in
  Visual Language Reasoning: Compositionality, Prompts and Causality (NFVLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BridgeTower: Building Bridges Between Encoders in Vision-Language
  Representation Learning <span class="chip">AAAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08657v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08657v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Xu, Chenfei Wu, Shachar Rosenman, Vasudev Lal, Wanxiang Che, Nan Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vision-Language (VL) models with the Two-Tower architecture have dominated
visual-language representation learning in recent years. Current VL models
either use lightweight uni-modal encoders and learn to extract, align and fuse
both modalities simultaneously in a deep cross-modal encoder, or feed the
last-layer uni-modal representations from the deep pre-trained uni-modal
encoders into the top cross-modal encoder. Both approaches potentially restrict
vision-language representation learning and limit model performance. In this
paper, we propose BridgeTower, which introduces multiple bridge layers that
build a connection between the top layers of uni-modal encoders and each layer
of the cross-modal encoder. This enables effective bottom-up cross-modal
alignment and fusion between visual and textual representations of different
semantic levels of pre-trained uni-modal encoders in the cross-modal encoder.
Pre-trained with only 4M images, BridgeTower achieves state-of-the-art
performance on various downstream vision-language tasks. In particular, on the
VQAv2 test-std set, BridgeTower achieves an accuracy of 78.73%, outperforming
the previous state-of-the-art model METER by 1.09% with the same pre-training
data and almost negligible additional parameters and computational costs.
Notably, when further scaling the model, BridgeTower achieves an accuracy of
81.15%, surpassing models that are pre-trained on orders-of-magnitude larger
datasets. Code and checkpoints are available at
https://github.com/microsoft/BridgeTower.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by AAAI 2023, Oral</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving the Model Consistency of Decentralized Federated Learning <span class="chip">ICML2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04083v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04083v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Shi, Li Shen, Kang Wei, Yan Sun, Bo Yuan, Xueqian Wang, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To mitigate the privacy leakages and communication burdens of Federated
Learning (FL), decentralized FL (DFL) discards the central server and each
client only communicates with its neighbors in a decentralized communication
network. However, existing DFL suffers from high inconsistency among local
clients, which results in severe distribution shift and inferior performance
compared with centralized FL (CFL), especially on heterogeneous data or sparse
communication topology. To alleviate this issue, we propose two DFL algorithms
named DFedSAM and DFedSAM-MGS to improve the performance of DFL. Specifically,
DFedSAM leverages gradient perturbation to generate local flat models via
Sharpness Aware Minimization (SAM), which searches for models with uniformly
low loss values. DFedSAM-MGS further boosts DFedSAM by adopting Multiple Gossip
Steps (MGS) for better model consistency, which accelerates the aggregation of
local flat models and better balances communication complexity and
generalization. Theoretically, we present improved convergence rates $\small
\mathcal{O}\big(\frac{1}{\sqrt{KT}}+\frac{1}{T}+\frac{1}{K^{1/2}T^{3/2}(1-\lambda)^2}\big)$
and $\small
\mathcal{O}\big(\frac{1}{\sqrt{KT}}+\frac{1}{T}+\frac{\lambda^Q+1}{K^{1/2}T^{3/2}(1-\lambda^Q)^2}\big)$
in non-convex setting for DFedSAM and DFedSAM-MGS, respectively, where
$1-\lambda$ is the spectral gap of gossip matrix and $Q$ is the number of MGS.
Empirically, our methods can achieve competitive performance compared with CFL
methods and outperform existing DFL methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Policy Mirror Ascent for Efficient and Independent Learning in Mean
  Field Games <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14449v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14449v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Batuhan Yardim, Semih Cayci, Matthieu Geist, Niao He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mean-field games have been used as a theoretical tool to obtain an
approximate Nash equilibrium for symmetric and anonymous $N$-player games.
However, limiting applicability, existing theoretical results assume variations
of a "population generative model", which allows arbitrary modifications of the
population distribution by the learning algorithm. Moreover, learning
algorithms typically work on abstract simulators with population instead of the
$N$-player game. Instead, we show that $N$ agents running policy mirror ascent
converge to the Nash equilibrium of the regularized game within
$\widetilde{\mathcal{O}}(\varepsilon^{-2})$ samples from a single sample
trajectory without a population generative model, up to a standard
$\mathcal{O}(\frac{1}{\sqrt{N}})$ error due to the mean field. Taking a
divergent approach from the literature, instead of working with the
best-response map we first show that a policy mirror ascent map can be used to
construct a contractive operator having the Nash equilibrium as its fixed
point. We analyze single-path TD learning for $N$-agent games, proving sample
complexity guarantees by only using a sample path from the $N$-agent simulator
without a population generative model. Furthermore, we demonstrate that our
methodology allows for independent learning by $N$ agents with finite sample
guarantees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Generative Model for Benchmarking Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.04396v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.04396v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minji Yoon, Yue Wu, John Palowitch, Bryan Perozzi, Ruslan Salakhutdinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the field of Graph Neural Networks (GNN) continues to grow, it experiences
a corresponding increase in the need for large, real-world datasets to train
and test new GNN models on challenging, realistic problems. Unfortunately, such
graph datasets are often generated from online, highly privacy-restricted
ecosystems, which makes research and development on these datasets hard, if not
impossible. This greatly reduces the amount of benchmark graphs available to
researchers, causing the field to rely only on a handful of publicly-available
datasets. To address this problem, we introduce a novel graph generative model,
Computation Graph Transformer (CGT) that learns and reproduces the distribution
of real-world graphs in a privacy-controlled way. More specifically, CGT (1)
generates effective benchmark graphs on which GNNs show similar task
performance as on the source graphs, (2) scales to process large-scale graphs,
(3) incorporates off-the-shelf privacy modules to guarantee end-user privacy of
the generated graph. Extensive experiments across a vast body of graph
generative models show that only our model can successfully generate
privacy-controlled, synthetic substitutes of large-scale real-world graphs that
can be effectively used to benchmark GNN models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Correlative Information Maximization: A Biologically Plausible Approach
  to Supervised Deep Neural Networks without Weight Symmetry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04810v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04810v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bariscan Bozkurt, Cengiz Pehlevan, Alper T Erdogan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The backpropagation algorithm has experienced remarkable success in training
large-scale artificial neural networks, however, its biological-plausibility is
disputed, and it remains an open question whether the brain employs supervised
learning mechanisms akin to it. Here, we propose correlative information
maximization between layer activations as an alternative normative approach to
describe the signal propagation in biological neural networks in both forward
and backward directions. This new framework addresses many concerns about the
biological-plausibility of conventional artificial neural networks and the
backpropagation algorithm. The coordinate descent-based optimization of the
corresponding objective, combined with the mean square error loss function for
fitting labeled supervision data, gives rise to a neural network structure that
emulates a more biologically realistic network of multi-compartment pyramidal
neurons with dendritic processing and lateral inhibitory neurons. Furthermore,
our approach provides a natural resolution to the weight symmetry problem
between forward and backward signal propagation paths, a significant critique
against the plausibility of the conventional backpropagation algorithm. This is
achieved by leveraging two alternative, yet equivalent forms of the correlative
mutual information objective. These alternatives intrinsically lead to forward
and backward prediction networks without weight symmetry issues, providing a
compelling solution to this long-standing challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint, 31 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Linear Contextual Bandits with User-level Differential Privacy <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05275v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05275v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiquan Huang, Huanyu Zhang, Luca Melis, Milan Shen, Meisam Hajzinia, Jing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies federated linear contextual bandits under the notion of
user-level differential privacy (DP). We first introduce a unified federated
bandits framework that can accommodate various definitions of DP in the
sequential decision-making setting. We then formally introduce user-level
central DP (CDP) and local DP (LDP) in the federated bandits framework, and
investigate the fundamental trade-offs between the learning regrets and the
corresponding DP guarantees in a federated linear contextual bandits model. For
CDP, we propose a federated algorithm termed as $\texttt{ROBIN}$ and show that
it is near-optimal in terms of the number of clients $M$ and the privacy budget
$\varepsilon$ by deriving nearly-matching upper and lower regret bounds when
user-level DP is satisfied. For LDP, we obtain several lower bounds, indicating
that learning under user-level $(\varepsilon,\delta)$-LDP must suffer a regret
blow-up factor at least $\min\{1/\varepsilon,M\}$ or
$\min\{1/\sqrt{\varepsilon},\sqrt{M}\}$ under different conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the effectiveness of partial variance reduction in federated learning
  with heterogeneous data <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02191v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02191v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Li, Mikkel N. Schmidt, Tommy S. Alstrøm, Sebastian U. Stich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data heterogeneity across clients is a key challenge in federated learning.
Prior works address this by either aligning client and server models or using
control variates to correct client model drift. Although these methods achieve
fast convergence in convex or simple non-convex problems, the performance in
over-parameterized models such as deep neural networks is lacking. In this
paper, we first revisit the widely used FedAvg algorithm in a deep neural
network to understand how data heterogeneity influences the gradient updates
across the neural network layers. We observe that while the feature extraction
layers are learned efficiently by FedAvg, the substantial diversity of the
final classification layers across clients impedes the performance. Motivated
by this, we propose to correct model drift by variance reduction only on the
final layers. We demonstrate that this significantly outperforms existing
benchmarks at a similar or lower communication cost. We furthermore provide
proof for the convergence rate of our algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Double-Weighting for Covariate Shift Adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08637v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08637v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        José I. Segovia-Martín, Santiago Mazuelas, Anqi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Supervised learning is often affected by a covariate shift in which the
marginal distributions of instances (covariates $x$) of training and testing
samples $\mathrm{p}_\text{tr}(x)$ and $\mathrm{p}_\text{te}(x)$ are different
but the label conditionals coincide. Existing approaches address such covariate
shift by either using the ratio
$\mathrm{p}_\text{te}(x)/\mathrm{p}_\text{tr}(x)$ to weight training samples
(reweighted methods) or using the ratio
$\mathrm{p}_\text{tr}(x)/\mathrm{p}_\text{te}(x)$ to weight testing samples
(robust methods). However, the performance of such approaches can be poor under
support mismatch or when the above ratios take large values. We propose a
minimax risk classification (MRC) approach for covariate shift adaptation that
avoids such limitations by weighting both training and testing samples. In
addition, we develop effective techniques that obtain both sets of weights and
generalize the conventional kernel mean matching method. We provide novel
generalization bounds for our method that show a significant increase in the
effective sample size compared with reweighted methods. The proposed method
also achieves enhanced classification performance in both synthetic and
empirical experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MonoFlow: Rethinking Divergence GANs via the Perspective of Wasserstein
  Gradient Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01075v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01075v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingxuan Yi, Zhanxing Zhu, Song Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The conventional understanding of adversarial training in generative
adversarial networks (GANs) is that the discriminator is trained to estimate a
divergence, and the generator learns to minimize this divergence. We argue that
despite the fact that many variants of GANs were developed following this
paradigm, the current theoretical understanding of GANs and their practical
algorithms are inconsistent. In this paper, we leverage Wasserstein gradient
flows which characterize the evolution of particles in the sample space, to
gain theoretical insights and algorithmic inspiration of GANs. We introduce a
unified generative modeling framework - MonoFlow: the particle evolution is
rescaled via a monotonically increasing mapping of the log density ratio. Under
our framework, adversarial training can be viewed as a procedure first
obtaining MonoFlow's vector field via training the discriminator and the
generator learns to draw the particle flow defined by the corresponding vector
field. We also reveal the fundamental difference between variational divergence
minimization and adversarial training. This analysis helps us to identify what
types of generator loss functions can lead to the successful training of GANs
and suggest that GANs may have more loss designs beyond the literature (e.g.,
non-saturated loss), as long as they realize MonoFlow. Consistent empirical
studies are included to validate the effectiveness of our framework.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Out-of-Variable Generalization for Discriminative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07896v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07896v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siyuan Guo, Jonas Wildberger, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability of an agent to do well in new environments is a critical aspect
of intelligence. In machine learning, this ability is known as
$\textit{strong}$ or $\textit{out-of-distribution}$ generalization. However,
merely considering differences in data distributions is inadequate for fully
capturing differences between learning environments. In the present paper, we
investigate $\textit{out-of-variable}$ generalization, which pertains to an
agent's generalization capabilities concerning environments with variables that
were never jointly observed before. This skill closely reflects the process of
animate learning: we, too, explore Nature by probing, observing, and measuring
$\textit{subsets}$ of variables at any given time. Mathematically,
$\textit{out-of-variable}$ generalization requires the efficient re-use of past
marginal information, i.e., information over subsets of previously observed
variables. We study this problem, focusing on prediction tasks across
environments that contain overlapping, yet distinct, sets of causes. We show
that after fitting a classifier, the residual distribution in one environment
reveals the partial derivative of the true generating function with respect to
the unobserved causal parent in that environment. We leverage this information
and propose a method that exhibits non-trivial out-of-variable generalization
performance when facing an overlapping, yet distinct, set of causal predictors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Debiasing Conditional Stochastic Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10613v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10613v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lie He, Shiva Prasad Kasiviswanathan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the conditional stochastic optimization (CSO) problem
which covers a variety of applications including portfolio selection,
reinforcement learning, robust learning, causal inference, etc. The
sample-averaged gradient of the CSO objective is biased due to its nested
structure, and therefore requires a high sample complexity to reach
convergence. We introduce a general stochastic extrapolation technique that
effectively reduces the bias. We show that for nonconvex smooth objectives,
combining this extrapolation with variance reduction techniques can achieve a
significantly better sample complexity than existing bounds. Additionally, we
develop new algorithms for the finite-sum variant of the CSO problem that also
significantly improve upon existing results. Finally, we believe that our
debiasing technique has the potential to be a useful tool for addressing
similar challenges in other stochastic optimization problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No-Reference Point Cloud Quality Assessment via Weighted Patch Quality
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07829v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07829v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Cheng, Honglei Su, Jari Korhonen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the rapid development of 3D vision applications based on point clouds,
point cloud quality assessment(PCQA) is becoming an important research topic.
However, the prior PCQA methods ignore the effect of local quality variance
across different areas of the point cloud. To take an advantage of the quality
distribution imbalance, we propose a no-reference point cloud quality
assessment (NR-PCQA) method with local area correlation analysis capability,
denoted as COPP-Net. More specifically, we split a point cloud into patches,
generate texture and structure features for each patch, and fuse them into
patch features to predict patch quality. Then, we gather the features of all
the patches of a point cloud for correlation analysis, to obtain the
correlation weights. Finally, the predicted qualities and correlation weights
for all the patches are used to derive the final quality score. Experimental
results show that our method outperforms the state-of-the-art benchmark NR-PCQA
methods. The source code for the proposed COPP-Net can be found at
https://github.com/philox12358/COPP-Net.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 5 figures, Accepted by International Conference on Software
  Engineering and Knowledge Engineering(SEKE2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supervised learning with probabilistic morphisms and kernel mean
  embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06348v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06348v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hông Vân Lê
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper I propose a concept of a correct loss function in a generative
model of supervised learning for an input space $\mathcal{X}$ and a label space
$\mathcal{Y}$, both of which are measurable spaces. A correct loss function in
a generative model of supervised learning must accurately measure the
discrepancy between elements of a hypothesis space $\mathcal{H}$ of possible
predictors and the supervisor operator, even when the supervisor operator does
not belong to $\mathcal{H}$. To define correct loss functions, I propose a
characterization of a regular conditional probability measure
$\mu_{\mathcal{Y}|\mathcal{X}}$ for a probability measure $\mu$ on $\mathcal{X}
\times \mathcal{Y}$ relative to the projection $\Pi_{\mathcal{X}}:
\mathcal{X}\times\mathcal{Y}\to \mathcal{X}$ as a solution of a linear operator
equation. If $\mathcal{Y}$ is a separable metrizable topological space with the
Borel $\sigma$-algebra $ \mathcal{B} (\mathcal{Y})$, I propose an additional
characterization of a regular conditional probability measure
$\mu_{\mathcal{Y}|\mathcal{X}}$ as a minimizer of mean square error on the
space of Markov kernels, referred to as probabilistic morphisms, from
$\mathcal{X}$ to $\mathcal{Y}$. This characterization utilizes kernel mean
embeddings. Building upon these results and employing inner measure to quantify
the generalizability of a learning algorithm, I extend a result due to
Cucker-Smale, which addresses the learnability of a regression model, to the
setting of a conditional probability estimation problem. Additionally, I
present a variant of Vapnik's regularization method for solving stochastic
ill-posed problems, incorporating inner measure, and showcase its applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>V4: 50 p., minor corrections and presentation improvement, in
  particular in Lemma 6.8, Corollary 6.13, Example 6.14</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SEGA: Structural Entropy Guided Anchor View for Graph Contrastive
  Learning <span class="chip">ICML'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04501v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04501v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junran Wu, Xueyuan Chen, Bowen Shi, Shangzhe Li, Ke Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contrastive learning, the choice of ``view'' controls the information that
the representation captures and influences the performance of the model.
However, leading graph contrastive learning methods generally produce views via
random corruption or learning, which could lead to the loss of essential
information and alteration of semantic information. An anchor view that
maintains the essential information of input graphs for contrastive learning
has been hardly investigated. In this paper, based on the theory of graph
information bottleneck, we deduce the definition of this anchor view; put
differently, \textit{the anchor view with essential information of input graph
is supposed to have the minimal structural uncertainty}. Furthermore, guided by
structural entropy, we implement the anchor view, termed \textbf{SEGA}, for
graph contrastive learning. We extensively validate the proposed anchor view on
various benchmarks regarding graph classification under unsupervised,
semi-supervised, and transfer learning and achieve significant performance
boosts compared to the state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Distillation for Further <span class="highlight-title">Pre-train</span>ing of <span class="highlight-title">Transformer</span>s <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.02871v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.02871v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seanie Lee, Minki Kang, Juho Lee, Sung Ju Hwang, Kenji Kawaguchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-training a large transformer model on a massive amount of unlabeled data
and fine-tuning it on labeled datasets for diverse downstream tasks has proven
to be a successful strategy, for a variety of vision and natural language
processing tasks. However, direct fine-tuning of the pre-trained model may be
suboptimal if there exist large discrepancies across data domains for
pre-training and fine-tuning. To tackle this issue, several previous studies
have proposed further pre-training strategies, where we continue to pre-train
the model on the target unlabeled dataset before fine-tuning. However, all of
them solely focus on language models and we empirically find that a Vision
Transformer is vulnerable to overfitting as we continue to pretrain the model
on target unlabeled data. In order to tackle this limitation, we propose
self-distillation as a regularization for a further pre-training stage.
Specifically, we first further pre-train the initial pre-trained model on the
target unlabeled data and then consider it as a teacher for self-distillation.
Then we take the same initial pre-trained model as a student and enforce its
hidden representations to be close to those of the teacher while optimizing the
student with a masked auto-encoding objective. We empirically validate the
efficacy of self-distillation on a variety of benchmark datasets for image and
text classification tasks. Experimentally, we show that our proposed method
outperforms all the relevant baselines. Theoretically, we analyze the proposed
method with a simplified model to understand how self-distillation for further
pre-training can potentially help improve the performance of the downstream
tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Huber-energy measure quantization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08162v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08162v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Turinici
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe a measure quantization procedure i.e., an algorithm which finds
the best approximation of a target probability law (and more generally signed
finite variation measure) by a sum of $Q$ Dirac masses ($Q$ being the
quantization parameter). The procedure is implemented by minimizing the
statistical distance between the original measure and its quantized version;
the distance is built from a negative definite kernel and, if necessary, can be
computed on the fly and feed to a stochastic optimization algorithm (such as
SGD, Adam, ...). We investigate theoretically the fundamental questions of
existence of the optimal measure quantizer and identify what are the required
kernel properties that guarantee suitable behavior. We propose two best linear
unbiased (BLUE) estimators for the squared statistical distance and use them in
an unbiased procedure, called HEMQ, to find the optimal quantization. We test
HEMQ on several databases: multi-dimensional Gaussian mixtures, Wiener space
cubature, Italian wine cultivars and the MNIST image database. The results
indicate that the HEMQ algorithm is robust and versatile and, for the class of
Huber-energy kernels, matches the expected intuitive behavior.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-Warping Invariant Quantum Recurrent Neural Networks via
  Quantum-Classical Adaptive Gating 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08173v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08173v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ivana Nikoloska, Osvaldo Simeone, Leonardo Banchi, Petar Veličković
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive gating plays a key role in temporal data processing via classical
recurrent neural networks (RNN), as it facilitates retention of past
information necessary to predict the future, providing a mechanism that
preserves invariance to time warping transformations. This paper builds on
quantum recurrent neural networks (QRNNs), a dynamic model with quantum memory,
to introduce a novel class of temporal data processing quantum models that
preserve invariance to time-warping transformations of the (classical)
input-output sequences. The model, referred to as time warping-invariant QRNN
(TWI-QRNN), augments a QRNN with a quantum-classical adaptive gating mechanism
that chooses whether to apply a parameterized unitary transformation at each
time step as a function of the past samples of the input sequence via a
classical recurrent model. The TWI-QRNN model class is derived from first
principles, and its capacity to successfully implement time-warping
transformations is experimentally demonstrated on examples with classical or
quantum dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted for publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Overcoming the Long Horizon Barrier for Sample-Efficient Reinforcement
  Learning with Latent Low-Rank Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.03569v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.03569v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Sam, Yudong Chen, Christina Lee Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The practicality of reinforcement learning algorithms has been limited due to
poor scaling with respect to the problem size, as the sample complexity of
learning an $\epsilon$-optimal policy is $\tilde{\Omega}\left(|S||A|H^3 /
\epsilon^2\right)$ over worst case instances of an MDP with state space $S$,
action space $A$, and horizon $H$. We consider a class of MDPs for which the
associated optimal $Q^*$ function is low rank, where the latent features are
unknown. While one would hope to achieve linear sample complexity in $|S|$ and
$|A|$ due to the low rank structure, we show that without imposing further
assumptions beyond low rank of $Q^*$, if one is constrained to estimate the $Q$
function using only observations from a subset of entries, there is a worst
case instance in which one must incur a sample complexity exponential in the
horizon $H$ to learn a near optimal policy. We subsequently show that under
stronger low rank structural assumptions, given access to a generative model,
Low Rank Monte Carlo Policy Iteration (LR-MCPI) and Low Rank Empirical Value
Iteration (LR-EVI) achieve the desired sample complexity of
$\tilde{O}\left((|S|+|A|)\mathrm{poly}(d,H)/\epsilon^2\right)$ for a rank $d$
setting, which is minimax optimal with respect to the scaling of $|S|, |A|$,
and $\epsilon$. In contrast to literature on linear and low-rank MDPs, we do
not require a known feature mapping, our algorithm is computationally simple,
and our results hold for long time horizons. Our results provide insights on
the minimal low-rank structural assumptions required on the MDP with respect to
the transition kernel versus the optimal action-value function.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tighter Lower Bounds for Shuffling SGD: Random Permutations and Beyond 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07160v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07160v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeyoung Cha, Jaewook Lee, Chulhee Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study convergence lower bounds of without-replacement stochastic gradient
descent (SGD) for solving smooth (strongly-)convex finite-sum minimization
problems. Unlike most existing results focusing on final iterate lower bounds
in terms of the number of components $n$ and the number of epochs $K$, we seek
bounds for arbitrary weighted average iterates that are tight in all factors
including the condition number $\kappa$. For SGD with Random Reshuffling, we
present lower bounds that have tighter $\kappa$ dependencies than existing
bounds. Our results are the first to perfectly close the gap between lower and
upper bounds for weighted average iterates in both strongly-convex and convex
cases. We also prove weighted average iterate lower bounds for arbitrary
permutation-based SGD, which apply to all variants that carefully choose the
best permutation. Our bounds improve the existing bounds in factors of $n$ and
$\kappa$ and thereby match the upper bounds shown for a recently proposed
algorithm called GraB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>58 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Crystal-Specific <span class="highlight-title">Pre-Train</span>ing Framework for Crystal Material Property
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05344v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05344v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haomin Yu, Yanru Song, Jilin Hu, Chenjuan Guo, Bin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crystal property prediction is a crucial aspect of developing novel
materials. However, there are two technical challenges to be addressed for
speeding up the investigation of crystals. First, labeling crystal properties
is intrinsically difficult due to the high cost and time involved in physical
simulations or lab experiments. Second, crystals adhere to a specific quantum
chemical principle known as periodic invariance, which is often not captured by
existing machine learning methods. To overcome these challenges, we propose the
crystal-specific pre-training framework for learning crystal representations
with self-supervision. The framework designs a mutex mask strategy for
enhancing representation learning so as to alleviate the limited labels
available for crystal property prediction. Moreover, we take into account the
specific periodic invariance in crystal structures by developing a periodic
invariance multi-graph module and periodic attribute learning within our
framework. This framework has been tested on eight different tasks. The
experimental results on these tasks show that the framework achieves promising
prediction performance and is able to outperform recent strong baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SERT: A Transfomer Based Model for Spatio-Temporal Sensor Data with
  Missing Values for Environmental Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03042v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03042v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Shoari Nejad, Rocío Alaiz-Rodríguez, Gerard D. McCarthy, Brian Kelleher, Anthony Grey, Andrew Parnell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environmental monitoring is crucial to our understanding of climate change,
biodiversity loss and pollution. The availability of large-scale
spatio-temporal data from sources such as sensors and satellites allows us to
develop sophisticated models for forecasting and understanding key drivers.
However, the data collected from sensors often contain missing values due to
faulty equipment or maintenance issues. The missing values rarely occur
simultaneously leading to data that are multivariate misaligned sparse time
series. We propose two models that are capable of performing multivariate
spatio-temporal forecasting while handling missing data naturally without the
need for imputation. The first model is a transformer-based model, which we
name SERT (Spatio-temporal Encoder Representations from Transformers). The
second is a simpler model named SST-ANN (Sparse Spatio-Temporal Artificial
Neural Network) which is capable of providing interpretable results. We conduct
extensive experiments on two different datasets for multivariate
spatio-temporal forecasting and show that our models have competitive or
superior performance to those at the state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Robust Statistics for Simulation-based Inference under Model
  Misspecification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15871v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15871v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daolang Huang, Ayush Bharti, Amauri Souza, Luigi Acerbi, Samuel Kaski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simulation-based inference (SBI) methods such as approximate Bayesian
computation (ABC), synthetic likelihood, and neural posterior estimation (NPE)
rely on simulating statistics to infer parameters of intractable likelihood
models. However, such methods are known to yield untrustworthy and misleading
inference outcomes under model misspecification, thus hindering their
widespread applicability. In this work, we propose the first general approach
to handle model misspecification that works across different classes of SBI
methods. Leveraging the fact that the choice of statistics determines the
degree of misspecification in SBI, we introduce a regularized loss function
that penalises those statistics that increase the mismatch between the data and
the model. Taking NPE and ABC as use cases, we demonstrate the superior
performance of our method on high-dimensional time-series models that are
artificially misspecified. We also apply our method to real data from the field
of radio propagation where the model is known to be misspecified. We show
empirically that the method yields robust inference in misspecified scenarios,
whilst still being accurate when the model is well-specified.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sharpness-Aware Minimization Revisited: Weighted Sharpness as a
  Regularization Term <span class="chip">KDD '23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15817v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15817v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun Yue, Jiadi Jiang, Zhiling Ye, Ning Gao, Yongchao Liu, Ke Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Neural Networks (DNNs) generalization is known to be closely related to
the flatness of minima, leading to the development of Sharpness-Aware
Minimization (SAM) for seeking flatter minima and better generalization. In
this paper, we revisit the loss of SAM and propose a more general method,
called WSAM, by incorporating sharpness as a regularization term. We prove its
generalization bound through the combination of PAC and Bayes-PAC techniques,
and evaluate its performance on various public datasets. The results
demonstrate that WSAM achieves improved generalization, or is at least highly
competitive, compared to the vanilla optimizer, SAM and its variants. The code
is available at
https://github.com/intelligent-machine-learning/dlrover/tree/master/atorch/atorch/optimizers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages. Accepted as a conference paper at KDD '23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Reward: Offline Preference-guided Policy Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16217v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16217v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yachen Kang, Diyuan Shi, Jinxin Liu, Li He, Donglin Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study focuses on the topic of offline preference-based reinforcement
learning (PbRL), a variant of conventional reinforcement learning that
dispenses with the need for online interaction or specification of reward
functions. Instead, the agent is provided with fixed offline trajectories and
human preferences between pairs of trajectories to extract the dynamics and
task information, respectively. Since the dynamics and task information are
orthogonal, a naive approach would involve using preference-based reward
learning followed by an off-the-shelf offline RL algorithm. However, this
requires the separate learning of a scalar reward function, which is assumed to
be an information bottleneck of the learning process. To address this issue, we
propose the offline preference-guided policy optimization (OPPO) paradigm,
which models offline trajectories and preferences in a one-step process,
eliminating the need for separately learning a reward function. OPPO achieves
this by introducing an offline hindsight information matching objective for
optimizing a contextual policy and a preference modeling objective for finding
the optimal context. OPPO further integrates a well-performing decision policy
by optimizing the two objectives iteratively. Our empirical results demonstrate
that OPPO effectively models offline preferences and outperforms prior
competing baselines, including offline RL algorithms performed over either true
or pseudo reward function specifications. Our code is available on the project
website: https://sites.google.com/view/oppo-icml-2023 .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient Personalized Federated Learning via Sparse Model-Adaptation <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02776v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02776v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daoyuan Chen, Liuyi Yao, Dawei Gao, Bolin Ding, Yaliang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) aims to train machine learning models for multiple
clients without sharing their own private data. Due to the heterogeneity of
clients' local data distribution, recent studies explore the personalized FL
that learns and deploys distinct local models with the help of auxiliary global
models. However, the clients can be heterogeneous in terms of not only local
data distribution, but also their computation and communication resources. The
capacity and efficiency of personalized models are restricted by the
lowest-resource clients, leading to sub-optimal performance and limited
practicality of personalized FL. To overcome these challenges, we propose a
novel approach named pFedGate for efficient personalized FL by adaptively and
efficiently learning sparse local models. With a lightweight trainable gating
layer, pFedGate enables clients to reach their full potential in model capacity
by generating different sparse models accounting for both the heterogeneous
data distributions and resource constraints. Meanwhile, the computation and
communication efficiency are both improved thanks to the adaptability between
the model sparsity and clients' resources. Further, we theoretically show that
the proposed pFedGate has superior complexity with guaranteed convergence and
generalization error. Extensive experiments show that pFedGate achieves
superior global accuracy, individual accuracy and efficiency simultaneously
over state-of-the-art methods. We also demonstrate that pFedGate performs
better than competitors in the novel clients participation and partial clients
participation scenarios, and can learn meaningful sparse local models adapted
to different data distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A One-shot Framework for Distributed Clustered Learning in Heterogeneous
  Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.10866v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.10866v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksandar Armacki, Dragana Bajovic, Dusan Jakovetic, Soummya Kar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper proposes a family of communication efficient methods for
distributed learning in heterogeneous environments in which users obtain data
from one of $K$ different distributions. In the proposed setup, the grouping of
users (based on the data distributions they sample), as well as the underlying
statistical properties of the distributions, are apriori unknown. A family of
One-shot Distributed Clustered Learning methods (ODCL-$\mathcal{C}$) is
proposed, parametrized by the set of admissible clustering algorithms
$\mathcal{C}$, with the objective of learning the true model at each user. The
admissible clustering methods include $K$-means (KM) and convex clustering
(CC), giving rise to various one-shot methods within the proposed family, such
as ODCL-KM and ODCL-CC. The proposed one-shot approach, based on local
computations at the users and a clustering based aggregation step at the server
is shown to provide strong learning guarantees. In particular, for strongly
convex problems it is shown that, as long as the number of data points per user
is above a threshold, the proposed approach achieves order-optimal mean-squared
error (MSE) rates in terms of the sample size. An explicit characterization of
the threshold is provided in terms of problem parameters. The trade-offs with
respect to selecting various clustering methods (ODCL-CC, ODCL-KM) are
discussed and significant improvements over state-of-the-art are demonstrated.
Numerical experiments illustrate the findings and corroborate the performance
of the proposed methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MGTBench: Benchmarking Machine-Generated Text Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.14822v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.14822v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinlei He, Xinyue Shen, Zeyuan Chen, Michael Backes, Yang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nowadays large language models (LLMs) have shown revolutionary power in a
variety of natural language processing (NLP) tasks such as text classification,
sentiment analysis, language translation, and question-answering. In this way,
detecting machine-generated texts (MGTs) is becoming increasingly important as
LLMs become more advanced and prevalent. These models can generate human-like
language that can be difficult to distinguish from text written by a human,
which raises concerns about authenticity, accountability, and potential bias.
However, existing detection methods against MGTs are evaluated under different
model architectures, datasets, and experimental settings, resulting in a lack
of a comprehensive evaluation framework across different methodologies
  In this paper, we fill this gap by proposing the first benchmark framework
for MGT detection, named MGTBench. Extensive evaluations on public datasets
with curated answers generated by ChatGPT (the most representative and powerful
LLMs thus far) show that most of the current detection methods perform less
satisfactorily against MGTs. An exceptional case is ChatGPT Detector, which is
trained with ChatGPT-generated texts and shows great performance in detecting
MGTs. Nonetheless, we note that only a small fraction of adversarial-crafted
perturbations on MGTs can evade the ChatGPT Detector, thus highlighting the
need for more robust MGT detection methods. We envision that MGTBench will
serve as a benchmark tool to accelerate future investigations involving the
evaluation of state-of-the-art MGT detection methods on their respective
datasets and the development of more advanced MGT detection methods. Our source
code and datasets are available at https://github.com/xinleihe/MGTBench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Doubly Smoothed GDA for Constrained Nonconvex-Nonconcave Minimax
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12978v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12978v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taoli Zheng, Linglingzhi Zhu, Anthony Man-Cho So, Jose Blanchet, Jiajin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonconvex-nonconcave minimax optimization has received intense attention over
the last decade due to its broad applications in machine learning.
Unfortunately, most existing algorithms cannot be guaranteed to converge
globally and even suffer from limit cycles. To address this issue, we propose a
novel single-loop algorithm called doubly smoothed gradient descent ascent
method (DSGDA), which naturally balances the primal and dual updates. The
proposed DSGDA can get rid of limit cycles in various challenging
nonconvex-nonconcave examples in the literature, including Forsaken,
Bilinearly-coupled minimax, Sixth-order polynomial, and PolarGame. We further
show that under an one-sided Kurdyka-\L{}ojasiewicz condition with exponent
$\theta\in(0,1)$ (resp. convex primal/concave dual function), DSGDA can find a
game-stationary point with an iteration complexity of
$\mathcal{O}(\epsilon^{-2\max\{2\theta,1\}})$ (resp.
$\mathcal{O}(\epsilon^{-4})$). These match the best results for single-loop
algorithms that solve nonconvex-concave or convex-nonconcave minimax problems,
or problems satisfying the rather restrictive one-sided Polyak-\L{}ojasiewicz
condition. Our work demonstrates, for the first time, the possibility of having
a simple and unified single-loop algorithm for solving nonconvex-nonconcave,
nonconvex-concave, and convex-nonconcave minimax problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Voxel-wise classification for porosity investigation of additive
  manufactured parts with 3D unsupervised and (deeply) supervised neural
  networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07894v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07894v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Domenico Iuso, Soumick Chatterjee, Sven Cornelissen, Dries Verhees, Jan De Beenhouwer, Jan Sijbers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Additive Manufacturing (AM) has emerged as a manufacturing process that
allows the direct production of samples from digital models. To ensure that
quality standards are met in all manufactured samples of a batch, X-ray
computed tomography (X-CT) is often used combined with automated anomaly
detection. For the latter, deep learning (DL) anomaly detection techniques are
increasingly, as they can be trained to be robust to the material being
analysed and resilient towards poor image quality. Unfortunately, most recent
and popular DL models have been developed for 2D image processing, thereby
disregarding valuable volumetric information.
  This study revisits recent supervised (UNet, UNet++, UNet 3+, MSS-UNet) and
unsupervised (VAE, ceVAE, gmVAE, vqVAE) DL models for porosity analysis of AM
samples from X-CT images and extends them to accept 3D input data with a
3D-patch pipeline for lower computational requirements, improved efficiency and
generalisability. The supervised models were trained using the Focal Tversky
loss to address class imbalance that arises from the low porosity in the
training datasets. The output of the unsupervised models is post-processed to
reduce misclassifications caused by their inability to adequately represent the
object surface. The findings were cross-validated in a 5-fold fashion and
include: a performance benchmark of the DL models, an evaluation of the
post-processing algorithm, an evaluation of the effect of training supervised
models with the output of unsupervised models. In a final performance benchmark
on a test set with poor image quality, the best performing supervised model was
UNet++ with an average precision of 0.751 $\pm$ 0.030, while the best
unsupervised model was the post-processed ceVAE with 0.830 $\pm$ 0.003. The
VAE/ceVAE models demonstrated superior capabilities, particularly when
leveraging post-processing techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Image Clustering via the Principle of Rate Reduction in the Age of
  <span class="highlight-title">Pretrain</span>ed Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05272v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05272v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianzhe Chu, Shengbang Tong, Tianjiao Ding, Xili Dai, Benjamin David Haeffele, René Vidal, Yi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large pre-trained models has brought about a paradigm shift in
both visual representation learning and natural language processing. However,
clustering unlabeled images, as a fundamental and classic machine learning
problem, still lacks effective solution, particularly for large-scale datasets.
In this paper, we propose a novel image clustering pipeline that leverages the
powerful feature representation of large pre-trained models such as CLIP and
cluster images effectively and efficiently at scale. We show that the
pre-trained features are significantly more structured by further optimizing
the rate reduction objective. The resulting features may significantly improve
the clustering accuracy, e.g., from 57\% to 66\% on ImageNet-1k. Furthermore,
by leveraging CLIP's image-text binding, we show how the new clustering method
leads to a simple yet effective self-labeling algorithm that successfully works
on unlabeled large datasets such as MS-COCO and LAION-Aesthetics. We will
release the code in https://github.com/LeslieTrue/CPP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bigger, Better, Faster: Human-level Atari with human-level efficiency <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19452v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19452v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Schwarzer, Johan Obando-Ceron, Aaron Courville, Marc Bellemare, Rishabh Agarwal, Pablo Samuel Castro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a value-based RL agent, which we call BBF, that achieves
super-human performance in the Atari 100K benchmark. BBF relies on scaling the
neural networks used for value estimation, as well as a number of other design
choices that enable this scaling in a sample-efficient manner. We conduct
extensive analyses of these design choices and provide insights for future
work. We end with a discussion about updating the goalposts for
sample-efficient RL research on the ALE. We make our code and data publicly
available at
https://github.com/google-research/google-research/tree/master/bigger_better_faster.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Achieving the Pareto Frontier of Regret Minimization and Best Arm
  Identification in Multi-Armed Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.08627v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.08627v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixin Zhong, Wang Chi Cheung, Vincent Y. F. Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the Pareto frontier of two archetypal objectives in multi-armed
bandits, namely, regret minimization (RM) and best arm identification (BAI)
with a fixed horizon. It is folklore that the balance between exploitation and
exploration is crucial for both RM and BAI, but exploration is more critical in
achieving the optimal performance for the latter objective. To this end, we
design and analyze the BoBW-lil'UCB$(\gamma)$ algorithm. Complementarily, by
establishing lower bounds on the regret achievable by any algorithm with a
given BAI failure probability, we show that (i) no algorithm can simultaneously
perform optimally for both the RM and BAI objectives, and (ii)
BoBW-lil'UCB$(\gamma)$ achieves order-wise optimal performance for RM or BAI
under different values of $\gamma$. Our work elucidates the trade-off more
precisely by showing how the constants in previous works depend on certain
hardness parameters. Finally, we show that BoBW-lil'UCB outperforms a close
competitor UCB$_\alpha$ (Degenne et al., 2019) in terms of the time complexity
and the regret on diverse datasets such as MovieLens and Published Kinase
Inhibitor Set.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OCD: Learning to Overfit with Conditional Diffusion Models <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00471v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00471v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahar Lutati, Lior Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a dynamic model in which the weights are conditioned on an input
sample x and are learned to match those that would be obtained by finetuning a
base model on x and its label y. This mapping between an input sample and
network weights is approximated by a denoising diffusion model. The diffusion
model we employ focuses on modifying a single layer of the base model and is
conditioned on the input, activations, and output of this layer. Since the
diffusion model is stochastic in nature, multiple initializations generate
different networks, forming an ensemble, which leads to further improvements.
Our experiments demonstrate the wide applicability of the method for image
classification, 3D reconstruction, tabular data, speech separation, and natural
language processing. Our code is available at
https://github.com/ShaharLutatiPersonal/OCD
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2023 (Oral & Poster)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Optimization for Smooth Nonconvex ERM 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changyu Gao, Stephen J. Wright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop simple differentially private optimization algorithms that move
along directions of (expected) descent to find an approximate second-order
solution for nonconvex ERM. We use line search, mini-batching, and a two-phase
strategy to improve the speed and practicality of the algorithm. Numerical
experiments demonstrate the effectiveness of these approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Connection Between MPNN and Graph <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11956v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11956v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Cai, Truong Son Hy, Rose Yu, Yusu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Transformer (GT) recently has emerged as a new paradigm of graph
learning algorithms, outperforming the previously popular Message Passing
Neural Network (MPNN) on multiple benchmarks. Previous work (Kim et al., 2022)
shows that with proper position embedding, GT can approximate MPNN arbitrarily
well, implying that GT is at least as powerful as MPNN. In this paper, we study
the inverse connection and show that MPNN with virtual node (VN), a commonly
used heuristic with little theoretical understanding, is powerful enough to
arbitrarily approximate the self-attention layer of GT.
  In particular, we first show that if we consider one type of linear
transformer, the so-called Performer/Linear Transformer (Choromanski et al.,
2020; Katharopoulos et al., 2020), then MPNN + VN with only O(1) depth and O(1)
width can approximate a self-attention layer in Performer/Linear Transformer.
Next, via a connection between MPNN + VN and DeepSets, we prove the MPNN + VN
with O(n^d) width and O(1) depth can approximate the self-attention layer
arbitrarily well, where d is the input feature dimension. Lastly, under some
assumptions, we provide an explicit construction of MPNN + VN with O(1) width
and O(n) depth approximating the self-attention layer in GT arbitrarily well.
On the empirical side, we demonstrate that 1) MPNN + VN is a surprisingly
strong baseline, outperforming GT on the recently proposed Long Range Graph
Benchmark (LRGB) dataset, 2) our MPNN + VN improves over early implementation
on a wide range of OGB datasets and 3) MPNN + VN outperforms Linear Transformer
and MPNN on the climate modeling task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sparse Linear Centroid-Encoder: A Convex Method for Feature Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04824v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04824v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomojit Ghosh, Michael Kirby, Karim Karimov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel feature selection technique, Sparse Linear
Centroid-Encoder (SLCE). The algorithm uses a linear transformation to
reconstruct a point as its class centroid and, at the same time, uses the
$\ell_1$-norm penalty to filter out unnecessary features from the input data.
The original formulation of the optimization problem is nonconvex, but we
propose a two-step approach, where each step is convex. In the first step, we
solve the linear Centroid-Encoder, a convex optimization problem over a matrix
$A$. In the second step, we only search for a sparse solution over a diagonal
matrix $B$ while keeping $A$ fixed. Unlike other linear methods, e.g., Sparse
Support Vector Machines and Lasso, Sparse Linear Centroid-Encoder uses a single
model for multi-class data. We present an in-depth empirical analysis of the
proposed model and show that it promotes sparsity on various data sets,
including high-dimensional biological data. Our experimental results show that
SLCE has a performance advantage over some state-of-the-art neural
network-based feature selection techniques.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A novel linear feature selection technique using convex optimization.
  Total 13 pages including references, 7 figures. The article is under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Feature Selection using Sparse Adaptive Bottleneck Centroid-Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04795v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04795v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomojit Ghosh, Michael Kirby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel nonlinear model, Sparse Adaptive Bottleneck
Centroid-Encoder (SABCE), for determining the features that discriminate
between two or more classes. The algorithm aims to extract discriminatory
features in groups while reconstructing the class centroids in the ambient
space and simultaneously use additional penalty terms in the bottleneck layer
to decrease within-class scatter and increase the separation of different class
centroids. The model has a sparsity-promoting layer (SPL) with a one-to-one
connection to the input layer. Along with the primary objective, we minimize
the $l_{2,1}$-norm of the sparse layer, which filters out unnecessary features
from input data. During training, we update class centroids by taking the
Hadamard product of the centroids and weights of the sparse layer, thus
ignoring the irrelevant features from the target. Therefore the proposed method
learns to reconstruct the critical components of class centroids rather than
the whole centroids. The algorithm is applied to various real-world data sets,
including high-dimensional biological, image, speech, and accelerometer sensor
data. We compared our method to different state-of-the-art feature selection
techniques, including supervised Concrete Autoencoders (SCAE), Feature
Selection Networks (FsNet), Stochastic Gates (STG), and LassoNet. We
empirically showed that SABCE features often produced better classification
accuracy than other methods on the sequester test sets, setting new
state-of-the-art results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A novel nonlinear feature selection technique with new state of the
  art result. 22 pages (including references), 13 figures. The article is in
  review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Almost Surely $\sqrt{T}$ Regret Bound for Adaptive LQR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05537v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05537v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiwen Lu, Yilin Mo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Linear-Quadratic Regulation (LQR) problem with unknown system parameters
has been widely studied, but it has remained unclear whether $\tilde{
\mathcal{O}}(\sqrt{T})$ regret, which is the best known dependence on time, can
be achieved almost surely. In this paper, we propose an adaptive LQR controller
with almost surely $\tilde{ \mathcal{O}}(\sqrt{T})$ regret upper bound. The
controller features a circuit-breaking mechanism, which circumvents potential
safety breach and guarantees the convergence of the system parameter estimate,
but is shown to be triggered only finitely often and hence has negligible
effect on the asymptotic performance of the controller. The proposed controller
is also validated via simulation on Tennessee Eastman Process~(TEP), a commonly
used industrial process example.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using Image Transformations to Learn Network Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.03419v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.03419v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brayan Ortiz, Amitabh Sinha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many learning tasks require observing a sequence of images and making a
decision. In a transportation problem of designing and planning for shipping
boxes between nodes, we show how to treat the network of nodes and the flows
between them as images. These images have useful structural information that
can be statistically summarized. Using image compression techniques, we reduce
an image down to a set of numbers that contain interpretable geographic
information that we call geographic signatures. Using geographic signatures, we
learn network structure that can be utilized to recommend future network
connectivity. We develop a Bayesian reinforcement algorithm that takes
advantage of statistically summarized network information as priors and
user-decisions to reinforce an agent's probabilistic decision. Additionally, we
show how reinforcement learning can be used with compression directly without
interpretation in simple tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, 5 tables, In Submission with Springer Nature,
  Computer Science Journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Energy-aware and Fault-tolerant Deep Reinforcement Learning based
  approach for Multi-agent Patrolling Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08230v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08230v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenhao Tong, Aaron Harwood, Maria A. Rodriguez, Richard O. Sinnott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autonomous vehicles are suited for continuous area patrolling problems.
However, finding an optimal patrolling strategy can be challenging for many
reasons. Firstly, patrolling environments are often complex and can include
unknown environmental factors, such as wind or landscape. Secondly, autonomous
vehicles can have failures or hardware constraints, such as limited battery
life. Importantly, patrolling large areas often requires multiple agents that
need to collectively coordinate their actions. In this work, we consider these
limitations and propose an approach based on model-free, deep multi-agent
reinforcement learning. In this approach, the agents are trained to patrol an
environment with various unknown dynamics and factors. They can automatically
recharge themselves to support continuous collective patrolling. A distributed
homogeneous multi-agent architecture is proposed, where all patrolling agents
execute identical policies locally based on their local observations and shared
location information. This architecture provides a patrolling system that can
tolerate agent failures and allow supplementary agents to be added to replace
failed agents or to increase the overall patrol performance. The solution is
validated through simulation experiments from multiple perspectives, including
the overall patrol performance, the efficiency of battery recharging
strategies, the overall fault tolerance, and the ability to cooperate with
supplementary agents.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Approach to Synchronization Problems over Subgroups of the
  Orthogonal Group 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2009.07514v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2009.07514v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Huikang Liu, Man-Chung Yue, Anthony Man-Cho So
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of synchronization over a group $\mathcal{G}$ aims to estimate a
collection of group elements $G^*_1, \dots, G^*_n \in \mathcal{G}$ based on
noisy observations of a subset of all pairwise ratios of the form $G^*_i
{G^*_j}^{-1}$. Such a problem has gained much attention recently and finds many
applications across a wide range of scientific and engineering areas. In this
paper, we consider the class of synchronization problems in which the group is
a closed subgroup of the orthogonal group. This class covers many group
synchronization problems that arise in practice. Our contribution is fivefold.
First, we propose a unified approach for solving this class of group
synchronization problems, which consists of a suitable initialization step and
an iterative refinement step based on the generalized power method, and show
that it enjoys a strong theoretical guarantee on the estimation error under
certain assumptions on the group, measurement graph, noise, and initialization.
Second, we formulate two geometric conditions that are required by our approach
and show that they hold for various practically relevant subgroups of the
orthogonal group. The conditions are closely related to the error-bound
geometry of the subgroup -- an important notion in optimization. Third, we
verify the assumptions on the measurement graph and noise for standard random
graph and random matrix models. Fourth, based on the classic notion of metric
entropy, we develop and analyze a novel spectral-type estimator. Finally, we
show via extensive numerical experiments that our proposed non-convex approach
outperforms existing approaches in terms of computational speed, scalability,
and/or estimation error.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalizable Low-Resource Activity Recognition with Diverse and
  Discriminative Representation Learning <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xin Qin, Jindong Wang, Shuo Ma, Wang Lu, Yongchun Zhu, Xing Xie, Yiqiang Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human activity recognition (HAR) is a time series classification task that
focuses on identifying the motion patterns from human sensor readings. Adequate
data is essential but a major bottleneck for training a generalizable HAR
model, which assists customization and optimization of online web applications.
However, it is costly in time and economy to collect large-scale labeled data
in reality, i.e., the low-resource challenge. Meanwhile, data collected from
different persons have distribution shifts due to different living habits, body
shapes, age groups, etc. The low-resource and distribution shift challenges are
detrimental to HAR when applying the trained model to new unseen subjects. In
this paper, we propose a novel approach called Diverse and Discriminative
representation Learning (DDLearn) for generalizable low-resource HAR. DDLearn
simultaneously considers diversity and discrimination learning. With the
constructed self-supervised learning task, DDLearn enlarges the data diversity
and explores the latent activity properties. Then, we propose a diversity
preservation module to preserve the diversity of learned features by enlarging
the distribution divergence between the original and augmented domains.
Meanwhile, DDLearn also enhances semantic discrimination by learning
discriminative representations with supervised contrastive learning. Extensive
experiments on three public HAR datasets demonstrate that our method
significantly outperforms state-of-art methods by an average accuracy
improvement of 9.5% under the low-resource distribution shift scenarios, while
being a generic, explainable, and flexible framework. Code is available at:
https://github.com/microsoft/robustlearn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGKDD 2023 Research track; 12 pages; Code is available
  at: https://github.com/microsoft/robustlearn</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-06-08T00:00:00Z">2023-06-08</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">83</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MIMIC-IT: Multi-Modal In-Context Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Li, Yuanhan Zhang, Liangyu Chen, Jinghao Wang, Fanyi Pu, Jingkang Yang, Chunyuan Li, Ziwei Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  High-quality instructions and responses are essential for the zero-shot
performance of large language models on interactive natural language tasks. For
interactive vision-language tasks involving intricate visual scenes, a large
quantity of diverse and creative instruction-response pairs should be
imperative to tune vision-language models (VLMs). Nevertheless, the current
availability of vision-language instruction-response pairs in terms of
quantity, diversity, and creativity remains limited, posing challenges to the
generalization of interactive VLMs. Here we present MultI-Modal In-Context
Instruction Tuning (MIMIC-IT), a dataset comprising 2.8 million multimodal
instruction-response pairs, with 2.2 million unique instructions derived from
images and videos. Each pair is accompanied by multi-modal in-context
information, forming conversational contexts aimed at empowering VLMs in
perception, reasoning, and planning. The instruction-response collection
process, dubbed as Syphus, is scaled using an automatic annotation pipeline
that combines human expertise with GPT's capabilities. Using the MIMIC-IT
dataset, we train a large VLM named Otter. Based on extensive evaluations
conducted on vision-language benchmarks, it has been observed that Otter
demonstrates remarkable proficiency in multi-modal perception, reasoning, and
in-context learning. Human evaluation reveals it effectively aligns with the
user's intentions. We release the MIMIC-IT dataset, instruction-response
collection pipeline, benchmarks, and the Otter model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://otter-ntu.github.io/ Dataset & code:
  https://github.com/Luodian/otter Initial release, work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixture-of-Domain-Adapters: Decoupling and Injecting Domain Knowledge to
  <span class="highlight-title">Pre-train</span>ed Language Models Memories <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shizhe Diao, Tianyang Xu, Ruijia Xu, Jiawei Wang, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained language models (PLMs) demonstrate excellent abilities to
understand texts in the generic domain while struggling in a specific domain.
Although continued pre-training on a large domain-specific corpus is effective,
it is costly to tune all the parameters on the domain. In this paper, we
investigate whether we can adapt PLMs both effectively and efficiently by only
tuning a few parameters. Specifically, we decouple the feed-forward networks
(FFNs) of the Transformer architecture into two parts: the original pre-trained
FFNs to maintain the old-domain knowledge and our novel domain-specific
adapters to inject domain-specific knowledge in parallel. Then we adopt a
mixture-of-adapters gate to fuse the knowledge from different domain adapters
dynamically. Our proposed Mixture-of-Domain-Adapters (MixDA) employs a
two-stage adapter-tuning strategy that leverages both unlabeled data and
labeled data to help the domain adaptation: i) domain-specific adapter on
unlabeled data; followed by ii) the task-specific adapter on labeled data.
MixDA can be seamlessly plugged into the pretraining-finetuning paradigm and
our experiments demonstrate that MixDA achieves superior performance on
in-domain tasks (GLUE), out-of-domain tasks (ChemProt, RCT, IMDB, Amazon), and
knowledge-intensive tasks (KILT). Further analyses demonstrate the reliability,
scalability, and efficiency of our method. The code is available at
https://github.com/Amano-Aki/Mixture-of-Domain-Adapters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modular Visual Question Answering via Code Generation <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05392v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05392v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjay Subramanian, Medhini Narasimhan, Kushal Khangaonkar, Kevin Yang, Arsha Nagrani, Cordelia Schmid, Andy Zeng, Trevor Darrell, Dan Klein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a framework that formulates visual question answering as modular
code generation. In contrast to prior work on modular approaches to VQA, our
approach requires no additional training and relies on pre-trained language
models (LMs), visual models pre-trained on image-caption pairs, and fifty VQA
examples used for in-context learning. The generated Python programs invoke and
compose the outputs of the visual models using arithmetic and conditional
logic. Our approach improves accuracy on the COVR dataset by at least 3% and on
the GQA dataset by roughly 2% compared to the few-shot baseline that does not
employ code generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utterance Emotion Dynamics in Children's Poems: Emotional Changes Across
  Age 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniela Teodorescu, Alona Fyshe, Saif M. Mohammad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emerging psychopathology studies are showing that patterns of changes in
emotional state -- emotion dynamics -- are associated with overall well-being
and mental health. More recently, there has been some work in tracking emotion
dynamics through one's utterances, allowing for data to be collected on a
larger scale across time and people. However, several questions about how
emotion dynamics change with age, especially in children, and when determined
through children's writing, remain unanswered. In this work, we use both a
lexicon and a machine learning based approach to quantify characteristics of
emotion dynamics determined from poems written by children of various ages. We
show that both approaches point to similar trends: consistent increasing
intensities for some emotions (e.g., anger, fear, joy, sadness, arousal, and
dominance) with age and a consistent decreasing valence with age. We also find
increasing emotional variability, rise rates (i.e., emotional reactivity), and
recovery rates (i.e., emotional regulation) with age. These results act as a
useful baselines for further research in how patterns of emotions expressed by
children change with age, and their association with mental health.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The ADAIO System at the BEA-2023 Shared Task on Generating AI Teacher
  Responses in Educational Dialogues <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05360v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05360v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adaeze Adigwe, Zheng Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the ADAIO team's system entry in the Building Educational
Applications (BEA) 2023 Shared Task on Generating AI Teacher Responses in
Educational Dialogues. The task aims to assess the performance of
state-of-the-art generative models as AI teachers in producing suitable
responses within a student-teacher dialogue. Our system comprises evaluating
various baseline models using OpenAI GPT-3 and designing diverse prompts to
prompt the OpenAI models for teacher response generation. After the challenge,
our system achieved second place by employing a few-shot prompt-based approach
with the OpenAI text-davinci-003 model. The results highlight the few-shot
learning capabilities of large-language models, particularly OpenAI's GPT-3, in
the role of AI teachers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the BEA workshop at ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Italian Biomedical Information Extraction with Large Language
  Models: Methodological Insights and Multicenter Practical Application 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claudio Crema, Tommaso Mario Buonocore, Silvia Fostinelli, Enea Parimbelli, Federico Verde, Cira Fundarò, Marina Manera, Matteo Cotta Ramusino, Marco Capelli, Alfredo Costa, Giuliano Binetti, Riccardo Bellazzi, Alberto Redolfi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of computerized medical records in hospitals has reduced
burdensome operations like manual writing and information fetching. However,
the data contained in medical records are still far underutilized, primarily
because extracting them from unstructured textual medical records takes time
and effort. Information Extraction, a subfield of Natural Language Processing,
can help clinical practitioners overcome this limitation, using automated
text-mining pipelines. In this work, we created the first Italian
neuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to
develop a Large Language Model for this task. Moreover, we conducted several
experiments with three external independent datasets to implement an effective
multicenter model, with overall F1-score 84.77%, Precision 83.16%, Recall
86.44%. The lessons learned are: (i) the crucial role of a consistent
annotation process and (ii) a fine-tuning strategy that combines classical
methods with a "few-shot" approach. This allowed us to establish methodological
guidelines that pave the way for future implementations in this field and allow
Italian hospitals to tap into important research opportunities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KIT's Multilingual Speech Translation System for IWSLT 2023 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danni Liu, Thai Binh Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, Alexander Waibel, Jan Niehues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many existing speech translation benchmarks focus on native-English speech in
high-quality recording conditions, which often do not match the conditions in
real-life use-cases. In this paper, we describe our speech translation system
for the multilingual track of IWSLT 2023, which focuses on the translation of
scientific conference talks. The test condition features accented input speech
and terminology-dense contents. The tasks requires translation into 10
languages of varying amounts of resources. In absence of training data from the
target domain, we use a retrieval-based approach (kNN-MT) for effective
adaptation (+0.8 BLEU for speech translation). We also use adapters to easily
integrate incremental training data from data augmentation, and show that it
matches the performance of re-training. We observe that cascaded systems are
more easily adaptable towards specific target domains, due to their separate
modules. Our cascaded speech system substantially outperforms its end-to-end
counterpart on scientific talk translation, although their performance remains
similar on TED talks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IWSLT 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CUED at ProbSum 2023: Hierarchical Ensemble of Summarization Models <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Potsawee Manakul, Yassir Fathullah, Adian Liusie, Vyas Raina, Vatsal Raina, Mark Gales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the challenge of summarizing patients' medical
progress notes in a limited data setting. For the Problem List Summarization
(shared task 1A) at the BioNLP Workshop 2023, we demonstrate that Clinical-T5
fine-tuned to 765 medical clinic notes outperforms other extractive,
abstractive and zero-shot baselines, yielding reasonable baseline systems for
medical note summarization. Further, we introduce Hierarchical Ensemble of
Summarization Models (HESM), consisting of token-level ensembles of diverse
fine-tuned Clinical-T5 models, followed by Minimum Bayes Risk (MBR) decoding.
Our HESM approach lead to a considerable summarization performance boost, and
when evaluated on held-out challenge data achieved a ROUGE-L of 32.77, which
was the best-performing system at the top of the shared task leaderboard.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BioNLP Workshop @ ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are fairness metric scores enough to assess discrimination biases in
  machine learning? <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanny Jourdan, Laurent Risser, Jean-Michel Loubes, Nicholas Asher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents novel experiments shedding light on the shortcomings of
current metrics for assessing biases of gender discrimination made by machine
learning algorithms on textual data. We focus on the Bios dataset, and our
learning task is to predict the occupation of individuals, based on their
biography. Such prediction tasks are common in commercial Natural Language
Processing (NLP) applications such as automatic job recommendations. We address
an important limitation of theoretical discussions dealing with group-wise
fairness metrics: they focus on large datasets, although the norm in many
industrial NLP applications is to use small to reasonably large linguistic
datasets for which the main practical constraint is to get a good prediction
accuracy. We then question how reliable are different popular measures of bias
when the size of the training set is simply sufficient to learn reasonably
accurate predictions. Our experiments sample the Bios dataset and learn more
than 200 models on different sample sizes. This allows us to statistically
study our results and to confirm that common gender bias indices provide
diverging and sometimes unreliable results when applied to relatively small
training and test samples. This highlights the crucial importance of variance
calculations for providing sound results in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at Third Workshop on Trustworthy Natural
  Language Processing, ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ToolAlpaca: Generalized Tool Learning for Language Models with 3000
  Simulated Cases 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05301v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05301v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiaoyu Tang, Ziliang Deng, Hongyu Lin, Xianpei Han, Qiao Liang, Le Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Enabling large language models to effectively utilize real-world tools is
crucial for achieving embodied intelligence. Existing approaches to tool
learning have primarily relied on either extremely large language models, such
as GPT-4, to attain generalized tool-use abilities in a zero-shot manner, or
have utilized supervised learning to train limited types of tools on compact
models. However, it remains uncertain whether smaller language models can
achieve generalized tool-use abilities without specific tool-specific training.
To address this question, this paper introduces ToolAlpaca, a novel framework
designed to automatically generate a tool-use corpus and learn generalized
tool-use abilities on compact language models with minimal human intervention.
Specifically, ToolAlpaca first collects a comprehensive dataset by building a
multi-agent simulation environment, which contains 3938 tool-use instances from
more than 400 real-world tool APIs spanning 50 distinct categories.
Subsequently, the constructed corpus is employed to fine-tune compact language
models, resulting in two models, namely ToolAlpaca-7B and ToolAlpaca-13B,
respectively. Finally, we evaluate the ability of these models to utilize
previously unseen tools without specific training. Experimental results
demonstrate that ToolAlpaca achieves effective generalized tool-use
capabilities comparable to those of extremely large language models like
GPT-3.5. This validation supports the notion that learning generalized tool-use
abilities is feasible for compact language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisit Few-shot Intent Classification with PLMs: Direct Fine-tuning vs.
  Continual <span class="highlight-title">Pre-train</span>ing <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05278v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05278v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haode Zhang, Haowen Liang, Liming Zhan, Xiao-Ming Wu, Albert Y. S. Lam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the task of few-shot intent detection, which involves training a
deep learning model to classify utterances based on their underlying intents
using only a small amount of labeled data. The current approach to address this
problem is through continual pre-training, i.e., fine-tuning pre-trained
language models (PLMs) on external resources (e.g., conversational corpora,
public intent detection datasets, or natural language understanding datasets)
before using them as utterance encoders for training an intent classifier. In
this paper, we show that continual pre-training may not be essential, since the
overfitting problem of PLMs on this task may not be as serious as expected.
Specifically, we find that directly fine-tuning PLMs on only a handful of
labeled examples already yields decent results compared to methods that employ
continual pre-training, and the performance gap diminishes rapidly as the
number of labeled data increases. To maximize the utilization of the limited
available data, we propose a context augmentation method and leverage
sequential self-distillation to boost performance. Comprehensive experiments on
real-world benchmarks show that given only two or more labeled samples per
class, direct fine-tuning outperforms many strong baselines that utilize
external data sources for continual pre-training. The code can be found at
https://github.com/hdzhang-code/DFTPlus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023, Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extensive Evaluation of <span class="highlight-title">Transformer</span>-based Architectures for Adverse Drug
  Events Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05276v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05276v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Scaboro, Beatrice Portellia, Emmanuele Chersoni, Enrico Santus, Giuseppe Serra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adverse Event (ADE) extraction is one of the core tasks in digital
pharmacovigilance, especially when applied to informal texts. This task has
been addressed by the Natural Language Processing community using large
pre-trained language models, such as BERT. Despite the great number of
Transformer-based architectures used in the literature, it is unclear which of
them has better performances and why. Therefore, in this paper we perform an
extensive evaluation and analysis of 19 Transformer-based models for ADE
extraction on informal texts. We compare the performance of all the considered
models on two datasets with increasing levels of informality (forums posts and
tweets). We also combine the purely Transformer-based models with two
commonly-used additional processing layers (CRF and LSTM), and analyze their
effect on the models performance. Furthermore, we use a well-established
feature importance technique (SHAP) to correlate the performance of the models
with a set of features that describe them: model category (AutoEncoding,
AutoRegressive, Text-to-Text), pretraining domain, training from scratch, and
model size in number of parameters. At the end of our analyses, we identify a
list of take-home messages that can be derived from the experimental data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Overview</span> of the Problem List Summarization (ProbSum) 2023 Shared Task on
  Summarizing Patients' Active Diagnoses and Problems from Electronic Health
  Record Progress Notes <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05270v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05270v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yanjun Gao, Dmitriy Dligach, Timothy Miller, Matthew M. Churpek, Majid Afshar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The BioNLP Workshop 2023 initiated the launch of a shared task on Problem
List Summarization (ProbSum) in January 2023. The aim of this shared task is to
attract future research efforts in building NLP models for real-world
diagnostic decision support applications, where a system generating relevant
and accurate diagnoses will augment the healthcare providers decision-making
process and improve the quality of care for patients. The goal for participants
is to develop models that generated a list of diagnoses and problems using
input from the daily care notes collected from the hospitalization of
critically ill patients. Eight teams submitted their final systems to the
shared task leaderboard. In this paper, we describe the tasks, datasets,
evaluation metrics, and baseline systems. Additionally, the techniques and
results of the evaluation of the different approaches tried by the
participating teams are summarized.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Proceedings of the 5th BioNLP Workshop at ACL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Factorized Contrastive Learning: Going Beyond Multi-view Redundancy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Pu Liang, Zihao Deng, Martin Ma, James Zou, Louis-Philippe Morency, Ruslan Salakhutdinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a wide range of multimodal tasks, contrastive learning has become a
particularly appealing approach since it can successfully learn representations
from abundant unlabeled data with only pairing information (e.g., image-caption
or video-audio pairs). Underpinning these approaches is the assumption of
multi-view redundancy - that shared information between modalities is necessary
and sufficient for downstream tasks. However, in many real-world settings,
task-relevant information is also contained in modality-unique regions:
information that is only present in one modality but still relevant to the
task. How can we learn self-supervised multimodal representations to capture
both shared and unique information relevant to downstream tasks? This paper
proposes FactorCL, a new multimodal representation learning method to go beyond
multi-view redundancy. FactorCL is built from three new contributions: (1)
factorizing task-relevant information into shared and unique representations,
(2) capturing task-relevant information via maximizing MI lower bounds and
removing task-irrelevant information via minimizing MI upper bounds, and (3)
multimodal data augmentations to approximate task relevance without labels. On
large-scale real-world datasets, FactorCL captures both shared and unique
information and achieves state-of-the-art results on six benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at: https://github.com/pliang279/FactorCL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dealing with Semantic Underspecification in Multimodal NLP <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05240v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05240v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sandro Pezzelle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent systems that aim at mastering language as humans do must deal
with its semantic underspecification, namely, the possibility for a linguistic
signal to convey only part of the information needed for communication to
succeed. Consider the usages of the pronoun they, which can leave the gender
and number of its referent(s) underspecified. Semantic underspecification is
not a bug but a crucial language feature that boosts its storage and processing
efficiency. Indeed, human speakers can quickly and effortlessly integrate
semantically-underspecified linguistic signals with a wide range of
non-linguistic information, e.g., the multimodal context, social or cultural
conventions, and shared knowledge. Standard NLP models have, in principle, no
or limited access to such extra information, while multimodal systems grounding
language into other modalities, such as vision, are naturally equipped to
account for this phenomenon. However, we show that they struggle with it, which
could negatively affect their performance and lead to harmful consequences when
used for applications. In this position paper, we argue that our community
should be aware of semantic underspecification if it aims to develop language
technology that can successfully interact with human users. We discuss some
applications where mastering it is crucial and outline a few directions toward
achieving this goal.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Proceedings of ACL 2023 (main conference). 13 pages,
  3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Long Context Document-Level Machine Translation <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Herold, Hermann Ney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document-level context for neural machine translation (NMT) is crucial to
improve the translation consistency and cohesion, the translation of ambiguous
inputs, as well as several other linguistic phenomena. Many works have been
published on the topic of document-level NMT, but most restrict the system to
only local context, typically including just the one or two preceding sentences
as additional information. This might be enough to resolve some ambiguous
inputs, but it is probably not sufficient to capture some document-level
information like the topic or style of a conversation. When increasing the
context size beyond just the local context, there are two challenges: (i)
the~memory usage increases exponentially (ii) the translation performance
starts to degrade. We argue that the widely-used attention mechanism is
responsible for both issues. Therefore, we propose a constrained attention
variant that focuses the attention on the most relevant parts of the sequence,
while simultaneously reducing the memory consumption. For evaluation, we
utilize targeted test sets in combination with novel evaluation techniques to
analyze the translations in regards to specific discourse-related phenomena. We
find that our approach is a good compromise between sentence-level NMT vs
attending to the full context, especially in low resource scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at CODI 2023 (ACL workshop)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M3Exam: A Multilingual, Multimodal, Multilevel Benchmark for Examining
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05179v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05179v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenxuan Zhang, Sharifah Mahani Aljunied, Chang Gao, Yew Ken Chia, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the existence of various benchmarks for evaluating natural language
processing models, we argue that human exams are a more suitable means of
evaluating general intelligence for large language models (LLMs), as they
inherently demand a much wider range of abilities such as language
understanding, domain knowledge, and problem-solving skills. To this end, we
introduce M3Exam, a novel benchmark sourced from real and official human exam
questions for evaluating LLMs in a multilingual, multimodal, and multilevel
context. M3Exam exhibits three unique characteristics: (1) multilingualism,
encompassing questions from multiple countries that require strong multilingual
proficiency and cultural knowledge; (2) multimodality, accounting for the
multimodal nature of many exam questions to test the model's multimodal
understanding capability; and (3) multilevel structure, featuring exams from
three critical educational periods to comprehensively assess a model's
proficiency at different levels. In total, M3Exam contains 12,317 questions in
9 diverse languages with three educational levels, where about 23\% of the
questions require processing images for successful solving. We assess the
performance of top-performing LLMs on M3Exam and find that current models,
including GPT-4, still struggle with multilingual text, particularly in
low-resource and non-Latin script languages. Multimodal LLMs also perform
poorly with complex multimodal questions. We believe that M3Exam can be a
valuable resource for comprehensively evaluating LLMs by examining their
multilingual and multimodal abilities and tracking their development. Data and
evaluation code is available at \url{https://github.com/DAMO-NLP-SG/M3Exam}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RRWKV: Capturing Long-range Dependencies in RWKV 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leilei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Owing to the impressive dot-product attention, the Transformers have been the
dominant architectures in various natural language processing (NLP) tasks.
Recently, the Receptance Weighted Key Value (RWKV) architecture follows a
non-transformer architecture to eliminate the drawbacks of dot-product
attention, where memory and computational complexity exhibits quadratic scaling
with sequence length. Although RWKV has exploited a linearly tensor-product
attention mechanism and achieved parallelized computations by deploying the
time-sequential mode, it fails to capture long-range dependencies because of
its limitation on looking back at previous information, compared with full
information obtained by direct interactions in the standard transformer.
Therefore, the paper devises the Retrospected Receptance Weighted Key Value
(RRWKV) architecture via incorporating the retrospecting ability into the RWKV
to effectively absorb information, which maintains memory and computational
efficiency as well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mapping Brains with Language Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05126v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05126v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonia Karamolegkou, Mostafa Abdou, Anders Søgaard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the years, many researchers have seemingly made the same observation:
Brain and language model activations exhibit some structural similarities,
enabling linear partial mappings between features extracted from neural
recordings and computational language models. In an attempt to evaluate how
much evidence has been accumulated for this observation, we survey over 30
studies spanning 10 datasets and 8 metrics. How much evidence has been
accumulated, and what, if anything, is missing before we can draw conclusions?
Our analysis of the evaluation methods used in the literature reveals that some
of the metrics are less conservative. We also find that the accumulated
evidence, for now, remains ambiguous, but correlations with model size and
quality provide grounds for cautious optimism.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reference Matters: Benchmarking Factual Error Correction for Dialogue
  Summarization with Fine-grained Evaluation Framework <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05119v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05119v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingqi Gao, Xiaojun Wan, Jia Su, Zhefeng Wang, Baoxing Huai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Factuality is important to dialogue summarization. Factual error correction
(FEC) of model-generated summaries is one way to improve factuality. Current
FEC evaluation that relies on factuality metrics is not reliable and detailed
enough. To address this problem, we are the first to manually annotate a FEC
dataset for dialogue summarization containing 4000 items and propose FERRANTI,
a fine-grained evaluation framework based on reference correction that
automatically evaluates the performance of FEC models on different error
categories. Using this evaluation framework, we conduct sufficient experiments
with FEC approaches under a variety of settings and find the best training
modes and significant differences in the performance of the existing approaches
on different factual error categories.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2023 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Search Strategies for Document-Level Neural Machine Translation <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Herold, Hermann Ney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to sentence-level systems, document-level neural machine translation
(NMT) models produce a more consistent output across a document and are able to
better resolve ambiguities within the input. There are many works on
document-level NMT, mostly focusing on modifying the model architecture or
training strategy to better accommodate the additional context-input. On the
other hand, in most works, the question on how to perform search with the
trained model is scarcely discussed, sometimes not mentioned at all. In this
work, we aim to answer the question how to best utilize a context-aware
translation model in decoding. We start with the most popular document-level
NMT approach and compare different decoding schemes, some from the literature
and others proposed by us. In the comparison, we are using both, standard
automatic metrics, as well as specific linguistic phenomena on three standard
document-level translation benchmarks. We find that most commonly used decoding
strategies perform similar to each other and that higher quality context
information has the potential to further improve the translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2023 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Closing the Loop: Testing Chat<span class="highlight-title">GPT</span> to Generate Model Explanations to
  Improve Human Labelling of Sponsored Content on Social Media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05115v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05115v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thales Bertaglia, Stefan Huber, Catalina Goanta, Gerasimos Spanakis, Adriana Iamnitchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Regulatory bodies worldwide are intensifying their efforts to ensure
transparency in influencer marketing on social media through instruments like
the Unfair Commercial Practices Directive (UCPD) in the European Union, or
Section 5 of the Federal Trade Commission Act. Yet enforcing these obligations
has proven to be highly problematic due to the sheer scale of the influencer
market. The task of automatically detecting sponsored content aims to enable
the monitoring and enforcement of such regulations at scale. Current research
in this field primarily frames this problem as a machine learning task,
focusing on developing models that achieve high classification performance in
detecting ads. These machine learning tasks rely on human data annotation to
provide ground truth information. However, agreement between annotators is
often low, leading to inconsistent labels that hinder the reliability of
models. To improve annotation accuracy and, thus, the detection of sponsored
content, we propose using chatGPT to augment the annotation process with
phrases identified as relevant features and brief explanations. Our experiments
show that this approach consistently improves inter-annotator agreement and
annotation accuracy. Additionally, our survey of user experience in the
annotation task indicates that the explanations improve the annotators'
confidence and streamline the process. Our proposed methods can ultimately lead
to more transparency and alignment with regulatory requirements in sponsored
content detection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to The World Conference on eXplainable Artificial
  Intelligence, Lisbon, Portugal, July 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The ART of Conversation: Measuring Phonetic Convergence and Deliberate
  Imitation in L2-Speech with a Siamese RNN <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Yuan, Aldo Pastore, Dorina de Jong, Hao Xu, Luciano Fadiga, Alessandro D'Ausilio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phonetic convergence describes the automatic and unconscious speech
adaptation of two interlocutors in a conversation. This paper proposes a
Siamese recurrent neural network (RNN) architecture to measure the convergence
of the holistic spectral characteristics of speech sounds in an L2-L2
interaction. We extend an alternating reading task (the ART) dataset by adding
20 native Slovak L2 English speakers. We train and test the Siamese RNN model
to measure phonetic convergence of L2 English speech from three different
native language groups: Italian (9 dyads), French (10 dyads) and Slovak (10
dyads). Our results indicate that the Siamese RNN model effectively captures
the dynamics of phonetic convergence and the speaker's imitation ability.
Moreover, this text-independent model is scalable and capable of handling
L1-induced speaker variability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PandaLM: An Automatic Evaluation Benchmark for LLM Instruction Tuning
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yidong Wang, Zhuohao Yu, Zhengran Zeng, Linyi Yang, Cunxiang Wang, Hao Chen, Chaoya Jiang, Rui Xie, Jindong Wang, Xing Xie, Wei Ye, Shikun Zhang, Yue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning large language models (LLMs) remains a challenging task,
owing to the complexity of hyperparameter selection and the difficulty involved
in evaluating the tuned models. To determine the optimal hyperparameters, an
automatic, robust, and reliable evaluation benchmark is essential. However,
establishing such a benchmark is not a trivial task due to the challenges
associated with evaluation accuracy and privacy protection. In response to
these challenges, we introduce a judge large language model, named PandaLM,
which is trained to distinguish the superior model given several LLMs.
PandaLM's focus extends beyond just the objective correctness of responses,
which is the main focus of traditional evaluation datasets. It addresses vital
subjective factors such as relative conciseness, clarity, adherence to
instructions, comprehensiveness, and formality. To ensure the reliability of
PandaLM, we collect a diverse human-annotated test dataset, where all contexts
are generated by humans and labels are aligned with human preferences. Our
results indicate that PandaLM-7B achieves 93.75% of GPT-3.5's evaluation
ability and 88.28% of GPT-4's in terms of F1-score on our test dataset. PandaLM
enables the evaluation of LLM to be fairer but with less cost, evidenced by
significant improvements achieved by models tuned through PandaLM compared to
their counterparts trained with default Alpaca's hyperparameters. In addition,
PandaLM does not depend on API-based evaluations, thus avoiding potential data
leakage. All resources of PandaLM are released at
https://github.com/WeOpenML/PandaLM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revealing the Blind Spot of Sentence Encoder Evaluation by HEROS <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05083v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05083v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng-Han Chiang, Yung-Sung Chuang, James Glass, Hung-yi Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing sentence textual similarity benchmark datasets only use a single
number to summarize how similar the sentence encoder's decision is to humans'.
However, it is unclear what kind of sentence pairs a sentence encoder (SE)
would consider similar. Moreover, existing SE benchmarks mainly consider
sentence pairs with low lexical overlap, so it is unclear how the SEs behave
when two sentences have high lexical overlap. We introduce a high-quality SE
diagnostic dataset, HEROS. HEROS is constructed by transforming an original
sentence into a new sentence based on certain rules to form a \textit{minimal
pair}, and the minimal pair has high lexical overlaps. The rules include
replacing a word with a synonym, an antonym, a typo, a random word, and
converting the original sentence into its negation. Different rules yield
different subsets of HEROS. By systematically comparing the performance of over
60 supervised and unsupervised SEs on HEROS, we reveal that most unsupervised
sentence encoders are insensitive to negation. We find the datasets used to
train the SE are the main determinants of what kind of sentence pairs an SE
considers similar. We also show that even if two SEs have similar performance
on STS benchmarks, they can have very different behavior on HEROS. Our result
reveals the blind spot of traditional STS benchmarks when evaluating SEs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 repl4nlp (representation learning for NLP) workshop poster
  paper. Dataset at https://huggingface.co/datasets/dcml0714/Heros</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Robustness of AI Offensive Code Generators via Data
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristina Improta, Pietro Liguori, Roberto Natella, Bojan Cukic, Domenico Cotroneo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a method to add perturbations to the code
descriptions, i.e., new inputs in natural language (NL) from well-intentioned
developers, in the context of security-oriented code, and analyze how and to
what extent perturbations affect the performance of AI offensive code
generators. Our experiments show that the performance of the code generators is
highly affected by perturbations in the NL descriptions. To enhance the
robustness of the code generators, we use the method to perform data
augmentation, i.e., to increase the variability and diversity of the training
data, proving its effectiveness against both perturbed and non-perturbed code
descriptions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Language Model Integration for Neural Machine Translation <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Herold, Yingbo Gao, Mohammad Zeineldeen, Hermann Ney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of language models for neural machine translation has been
extensively studied in the past. It has been shown that an external language
model, trained on additional target-side monolingual data, can help improve
translation quality. However, there has always been the assumption that the
translation model also learns an implicit target-side language model during
training, which interferes with the external language model at decoding time.
Recently, some works on automatic speech recognition have demonstrated that, if
the implicit language model is neutralized in decoding, further improvements
can be gained when integrating an external language model. In this work, we
transfer this concept to the task of machine translation and compare with the
most prominent way of including additional monolingual data - namely
back-translation. We find that accounting for the implicit language model
significantly boosts the performance of language model fusion, although this
approach is still outperformed by back-translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at ACL2023 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DLAMA: A Framework for Curating Culturally Diverse Facts for Probing the
  Knowledge of <span class="highlight-title">Pretrain</span>ed Language Models <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amr Keleg, Walid Magdy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A few benchmarking datasets have been released to evaluate the factual
knowledge of pretrained language models. These benchmarks (e.g., LAMA, and
ParaRel) are mainly developed in English and later are translated to form new
multilingual versions (e.g., mLAMA, and mParaRel). Results on these
multilingual benchmarks suggest that using English prompts to recall the facts
from multilingual models usually yields significantly better and more
consistent performance than using non-English prompts. Our analysis shows that
mLAMA is biased toward facts from Western countries, which might affect the
fairness of probing models. We propose a new framework for curating factual
triples from Wikidata that are culturally diverse. A new benchmark DLAMA-v1 is
built of factual triples from three pairs of contrasting cultures having a
total of 78,259 triples from 20 relation predicates. The three pairs comprise
facts representing the (Arab and Western), (Asian and Western), and (South
American and Western) countries respectively. Having a more balanced benchmark
(DLAMA-v1) supports that mBERT performs better on Western facts than
non-Western ones, while monolingual Arabic, English, and Korean models tend to
perform better on their culturally proximate facts. Moreover, both monolingual
and multilingual models tend to make a prediction that is culturally or
geographically relevant to the correct label, even if the prediction is wrong.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2023 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LCT-1 at SemEval-2023 Task 10: <span class="highlight-title">Pre-train</span>ing and Multi-task Learning for
  Sexism Detection and Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05075v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05075v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantin Chernyshev, Ekaterina Garanina, Duygu Bayram, Qiankun Zheng, Lukas Edman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Misogyny and sexism are growing problems in social media. Advances have been
made in online sexism detection but the systems are often uninterpretable.
SemEval-2023 Task 10 on Explainable Detection of Online Sexism aims at
increasing explainability of the sexism detection, and our team participated in
all the proposed subtasks. Our system is based on further domain-adaptive
pre-training (Gururangan et al., 2020). Building on the Transformer-based
models with the domain adaptation, we compare fine-tuning with multi-task
learning and show that each subtask requires a different system configuration.
In our experiments, multi-task learning performs on par with standard
fine-tuning for sexism detection and noticeably better for coarse-grained
sexism classification, while fine-tuning is preferable for fine-grained
classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning A Foundation Language Model for Geoscience Knowledge
  Understanding and Utilization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05064v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05064v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cheng Deng, Tianhang Zhang, Zhongmou He, Qiyuan Chen, Yuanyuan Shi, Le Zhou, Luoyi Fu, Weinan Zhang, Xinbing Wang, Chenghu Zhou, Zhouhan Lin, Junxian He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs)have achieved great success in general domains of
natural language processing. In this paper, we bring LLMs to the realm of
geoscience, with the objective of advancing research and applications in this
field. To this end, we present the first-ever LLM in geoscience, K2, alongside
a suite of resources developed to further promote LLM research within
geoscience. For instance, we have curated the first geoscience instruction
tuning dataset, GeoSignal, which aims to align LLM responses to
geoscience-related user queries. Additionally, we have established the first
geoscience benchmark, GeoBenchmark, to evaluate LLMs in the context of
geoscience. In this work, we experiment with a complete recipe to adapt a
pretrained general-domain LLM to the geoscience domain. Specifically, we
further train the LLaMA-7B model on over 1 million pieces of geoscience
literature and utilize GeoSignal's supervised data to fine-tune the model.
Moreover, we share a protocol that can efficiently gather domain-specific data
and construct domain-supervised data, even in situations where manpower is
scarce. Experiments conducted on the GeoBenchmark demonstrate the the
effectiveness of our approach and datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Medical Diagnostics with Structured Data Extraction by
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksa Bisercic, Mladen Nikolic, Mihaela van der Schaar, Boris Delibasic, Pietro Lio, Andrija Petrovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular data is often hidden in text, particularly in medical diagnostic
reports. Traditional machine learning (ML) models designed to work with tabular
data, cannot effectively process information in such form. On the other hand,
large language models (LLMs) which excel at textual tasks, are probably not the
best tool for modeling tabular data. Therefore, we propose a novel, simple, and
effective methodology for extracting structured tabular data from textual
medical reports, called TEMED-LLM. Drawing upon the reasoning capabilities of
LLMs, TEMED-LLM goes beyond traditional extraction techniques, accurately
inferring tabular features, even when their names are not explicitly mentioned
in the text. This is achieved by combining domain-specific reasoning guidelines
with a proposed data validation and reasoning correction feedback loop. By
applying interpretable ML models such as decision trees and logistic regression
over the extracted and validated data, we obtain end-to-end interpretable
predictions. We demonstrate that our approach significantly outperforms
state-of-the-art text classification models in medical diagnostics. Given its
predictive performance, simplicity, and interpretability, TEMED-LLM underscores
the potential of leveraging LLMs to improve the performance and trustworthiness
of ML models in medical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ T3L: Translate-and-Test Transfer Learning for Cross-Lingual Text
  Classification <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Inigo Jauregi Unanue, Gholamreza Haffari, Massimo Piccardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-lingual text classification leverages text classifiers trained in a
high-resource language to perform text classification in other languages with
no or minimal fine-tuning (zero/few-shots cross-lingual transfer). Nowadays,
cross-lingual text classifiers are typically built on large-scale, multilingual
language models (LMs) pretrained on a variety of languages of interest.
However, the performance of these models vary significantly across languages
and classification tasks, suggesting that the superposition of the language
modelling and classification tasks is not always effective. For this reason, in
this paper we propose revisiting the classic "translate-and-test" pipeline to
neatly separate the translation and classification stages. The proposed
approach couples 1) a neural machine translator translating from the targeted
language to a high-resource language, with 2) a text classifier trained in the
high-resource language, but the neural machine translator generates "soft"
translations to permit end-to-end backpropagation during fine-tuning of the
pipeline. Extensive experiments have been carried out over three cross-lingual
text classification datasets (XNLI, MLDoc and MultiEURLEX), with the results
showing that the proposed approach has significantly improved performance over
a competitive baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by the Transactions of the Association for Computational
  Linguistics (TACL), pre-MIT Press publication version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Assessing Phrase Break of ESL Speech with <span class="highlight-title">Pre-train</span>ed Language Models
  and Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyi Wang, Shaoguang Mao, Wenshan Wu, Yan Xia, Yan Deng, Jonathan Tien
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces approaches to assessing phrase breaks in ESL learners'
speech using pre-trained language models (PLMs) and large language models
(LLMs). There are two tasks: overall assessment of phrase break for a speech
clip and fine-grained assessment of every possible phrase break position. To
leverage NLP models, speech input is first force-aligned with texts, and then
pre-processed into a token sequence, including words and phrase break
information. To utilize PLMs, we propose a pre-training and fine-tuning
pipeline with the processed tokens. This process includes pre-training with a
replaced break token detection module and fine-tuning with text classification
and sequence labeling. To employ LLMs, we design prompts for ChatGPT. The
experiments show that with the PLMs, the dependence on labeled training data
has been greatly reduced, and the performance has improved. Meanwhile, we
verify that ChatGPT, a renowned LLM, has potential for further advancement in
this area.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by InterSpeech 2023. arXiv admin note: substantial text
  overlap with arXiv:2210.16029</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Actively Supervised Clustering for Open Relation Extraction <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Zhao, Yongxin Zhang, Qi Zhang, Tao Gui, Zhongyu Wei, Minlong Peng, Mingming Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current clustering-based Open Relation Extraction (OpenRE) methods usually
adopt a two-stage pipeline. The first stage simultaneously learns relation
representations and assignments. The second stage manually labels several
instances and thus names the relation for each cluster. However, unsupervised
objectives struggle to optimize the model to derive accurate clustering
assignments, and the number of clusters has to be supplied in advance. In this
paper, we present a novel setting, named actively supervised clustering for
OpenRE. Our insight lies in that clustering learning and relation labeling can
be alternately performed, providing the necessary guidance for clustering
without a significant increase in human effort. The key to the setting is
selecting which instances to label. Instead of using classical active labeling
strategies designed for fixed known classes, we propose a new strategy, which
is applicable to dynamically discover clusters of unknown relations.
Experimental results show that our method is able to discover almost all
relational clusters in the data and improve the SOTA methods by 10.3\% and
5.2\%, on two datasets respectively.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Language Identification to Enhance Code-Mixed Text
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gauri Takawane, Abhishek Phaltankar, Varad Patwardhan, Aryan Patil, Raviraj Joshi, Mukta S. Takalikar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The usage of more than one language in the same text is referred to as Code
Mixed. It is evident that there is a growing degree of adaption of the use of
code-mixed data, especially English with a regional language, on social media
platforms. Existing deep-learning models do not take advantage of the implicit
language information in the code-mixed text. Our study aims to improve
BERT-based models performance on low-resource Code-Mixed Hindi-English Datasets
by experimenting with language augmentation approaches. We propose a pipeline
to improve code-mixed systems that comprise data preprocessing, word-level
language identification, language augmentation, and model training on
downstream tasks like sentiment analysis. For language augmentation in BERT
models, we explore word-level interleaving and post-sentence placement of
language information. We have examined the performance of vanilla BERT-based
models and their code-mixed HingBERT counterparts on respective benchmark
datasets, comparing their results with and without using word-level language
information. The models were evaluated using metrics such as accuracy,
precision, recall, and F1 score. Our findings show that the proposed language
augmentation approaches work well across different BERT models. We demonstrate
the importance of augmenting code-mixed text with language information on five
different code-mixed Hindi-English downstream datasets based on sentiment
analysis, hate speech detection, and emotion detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RE-Matching: A Fine-Grained Semantic Matching Method for Zero-Shot
  Relation Extraction <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Zhao, Wenyu Zhan, Xin Zhao, Qi Zhang, Tao Gui, Zhongyu Wei, Junzhe Wang, Minlong Peng, Mingming Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Semantic matching is a mainstream paradigm of zero-shot relation extraction,
which matches a given input with a corresponding label description. The
entities in the input should exactly match their hypernyms in the description,
while the irrelevant contexts should be ignored when matching. However, general
matching methods lack explicit modeling of the above matching pattern. In this
work, we propose a fine-grained semantic matching method tailored for zero-shot
relation extraction. Following the above matching pattern, we decompose the
sentence-level similarity score into entity and context matching scores. Due to
the lack of explicit annotations of the redundant components, we design a
feature distillation module to adaptively identify the relation-irrelevant
features and reduce their negative impact on context matching. Experimental
results show that our method achieves higher matching $F_1$ score and has an
inference speed 10 times faster, when compared with the state-of-the-art
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Open Set Relation Extraction via Unknown-Aware Training <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jun Zhao, Xin Zhao, Wenyu Zhan, Qi Zhang, Tao Gui, Zhongyu Wei, Yunwen Chen, Xiang Gao, Xuanjing Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The existing supervised relation extraction methods have achieved impressive
performance in a closed-set setting, where the relations during both training
and testing remain the same. In a more realistic open-set setting, unknown
relations may appear in the test set. Due to the lack of supervision signals
from unknown relations, a well-performing closed-set relation extractor can
still confidently misclassify them into known relations. In this paper, we
propose an unknown-aware training method, regularizing the model by dynamically
synthesizing negative instances. To facilitate a compact decision boundary,
``difficult'' negative instances are necessary. Inspired by text adversarial
attacks, we adaptively apply small but critical perturbations to original
training instances and thus synthesizing negative instances that are more
likely to be mistaken by the model as known relations. Experimental results
show that this method achieves SOTA unknown relation detection without
compromising the classification of known relations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A modified model for topic detection from a corpus and a new metric
  evaluating the understandability of topics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomoya Kitano, Yuto Miyatake, Daisuke Furihata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a modified neural model for topic detection from a corpus
and proposes a new metric to evaluate the detected topics. The new model builds
upon the embedded topic model incorporating some modifications such as document
clustering. Numerical experiments suggest that the new model performs
favourably regardless of the document's length. The new metric, which can be
computed more efficiently than widely-used metrics such as topic coherence,
provides variable information regarding the understandability of the detected
topics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Info<span class="highlight-title">Prompt</span>: Information-Theoretic Soft <span class="highlight-title">Prompt</span> Tuning for Natural
  Language Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04933v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04933v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junda Wu, Tong Yu, Rui Wang, Zhao Song, Ruiyi Zhang, Handong Zhao, Chaochao Lu, Shuai Li, Ricardo Henao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Soft prompt tuning achieves superior performances across a wide range of
few-shot tasks. However, the performances of prompt tuning can be highly
sensitive to the initialization of the prompts. We also empirically observe
that conventional prompt tuning methods cannot encode and learn sufficient
task-relevant information from prompt tokens. In this work, we develop an
information-theoretic framework that formulates soft prompt tuning as
maximizing mutual information between prompts and other model parameters (or
encoded representations). This novel view helps us to develop a more efficient,
accurate and robust soft prompt tuning method InfoPrompt. With this framework,
we develop two novel mutual information based loss functions, to (i) discover
proper prompt initialization for the downstream tasks and learn sufficient
task-relevant information from prompt tokens and (ii) encourage the output
representation from the pretrained language model to be more aware of the
task-relevant information captured in the learnt prompt. Extensive experiments
validate that InfoPrompt can significantly accelerate the convergence of the
prompt tuning and outperform traditional prompt tuning methods. Finally, we
provide a formal theoretical result for showing to show that gradient descent
type algorithm can be used to train our mutual information loss.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ covLLM: Large Language Models for COVID-19 Biomedical Literature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04926v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04926v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yousuf A. Khan, Clarisse Hokia, Jennifer Xu, Ben Ehlert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The COVID-19 pandemic led to 1.1 million deaths in the United States, despite
the explosion of coronavirus research. These new findings are slow to translate
to clinical interventions, leading to poorer patient outcomes and unnecessary
deaths. One reason is that clinicians, overwhelmed by patients, struggle to
keep pace with the rate of new coronavirus literature. A potential solution is
developing a tool for evaluating coronavirus literature using large language
models (LLMs) -- neural networks that are deployed for natural language
processing. LLMs can be used to summarize and extract user-specified
information. The greater availability and advancement of LLMs and pre-processed
coronavirus literature databases provide the opportunity to assist clinicians
in evaluating coronavirus literature through a coronavirus literature specific
LLM (covLLM), a tool that directly takes an inputted research article and a
user query to return an answer. Using the COVID-19 Open Research Dataset
(CORD-19), we produced two datasets: (1) synCovid, which uses a combination of
handwritten prompts and synthetic prompts generated using OpenAI, and (2) real
abstracts, which contains abstract and title pairs. covLLM was trained with
LLaMA 7B as a baseline model to produce three models trained on (1) the Alpaca
and synCovid datasets, (2) the synCovid dataset, and (3) the synCovid and real
abstract datasets. These models were evaluated by two human evaluators and
ChatGPT. Results demonstrate that training covLLM on the synCovid and abstract
pairs datasets performs competitively with ChatGPT and outperforms covLLM
trained primarily using the Alpaca dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prefer to Classify: Improving Text Classifiers via Auxiliary Preference
  Learning <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04925v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04925v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaehyung Kim, Jinwoo Shin, Dongyeop Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of largely human-annotated benchmarks has driven the success
of deep neural networks in various NLP tasks. To enhance the effectiveness of
existing benchmarks, collecting new additional input-output pairs is often too
costly and challenging, particularly considering their marginal impact on
improving the current model accuracy. Instead, additional or complementary
annotations on the existing input texts in the benchmarks can be preferable as
an efficient way to pay the additional human cost. In this paper, we
investigate task-specific preferences between pairs of input texts as a new
alternative way for such auxiliary data annotation. From 'pair-wise'
comparisons with respect to the task, the auxiliary preference learning enables
the model to learn an additional informative training signal that cannot be
captured with 'instance-wise' task labels. To this end, we propose a novel
multi-task learning framework, called prefer-to-classify (P2C), which can enjoy
the cooperative effect of learning both the given classification task and the
auxiliary preferences. Here, we provide three different ways to collect
preference signals in practice: (a) implicitly extracting from annotation
records (for free, but often unavailable), (b) collecting explicitly from crowd
workers (high paid), or (c) pre-trained large language models such as GPT-3
(low paid). Given existing classification NLP benchmarks, we demonstrate that
the proposed auxiliary preference learning via P2C on them is effective in
improving text classifiers. Our codes are publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, accepted at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NOWJ at COLIEE 2023 -- Multi-Task and Ensemble Approaches in Legal
  Information Processing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04903v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04903v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thi-Hai-Yen Vuong, Hai-Long Nguyen, Tan-Minh Nguyen, Hoang-Trung Nguyen, Thai-Binh Nguyen, Ha-Thanh Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the NOWJ team's approach to the COLIEE 2023 Competition,
which focuses on advancing legal information processing techniques and applying
them to real-world legal scenarios. Our team tackles the four tasks in the
competition, which involve legal case retrieval, legal case entailment, statute
law retrieval, and legal textual entailment. We employ state-of-the-art machine
learning models and innovative approaches, such as BERT, Longformer,
BM25-ranking algorithm, and multi-task learning models. Although our team did
not achieve state-of-the-art results, our findings provide valuable insights
and pave the way for future improvements in legal information processing.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>COLIEE 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ In-Context Learning through the Bayesian Prism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04891v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04891v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kabir Ahuja, Madhur Panwar, Navin Goyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning is one of the surprising and useful features of large
language models. How it works is an active area of research. Recently, stylized
meta-learning-like setups have been devised that train these models on a
sequence of input-output pairs $(x, f(x))$ from a function class using the
language modeling loss and observe generalization to unseen functions from the
same class. One of the main discoveries in this line of research has been that
for several problems such as linear regression, trained transformers learn
algorithms for learning functions in context. However, the inductive biases of
these models resulting in this behavior are not clearly understood. A model
with unlimited training data and compute is a Bayesian predictor: it learns the
pretraining distribution. It has been shown that high-capacity transformers
mimic the Bayesian predictor for linear regression. In this paper, we show
empirical evidence of transformers exhibiting the behavior of this ideal
learner across different linear and non-linear function classes. We also extend
the previous setups to work in the multitask setting and verify that
transformers can do in-context learning in this setup as well and the Bayesian
perspective sheds light on this setting also. Finally, via the example of
learning Fourier series, we study the inductive bias for in-context learning.
We find that in-context learning may or may not have simplicity bias depending
on the pretraining data distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Expanding Scope: Adapting English Adversarial Attacks to Chinese <span class="chip">ACL23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04874v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04874v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanyu Liu, Chengyuan Cai, Yanjun Qi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have revealed that NLP predictive models are vulnerable to
adversarial attacks. Most existing studies focused on designing attacks to
evaluate the robustness of NLP models in the English language alone. Literature
has seen an increasing need for NLP solutions for other languages. We,
therefore, ask one natural question: whether state-of-the-art (SOTA) attack
methods generalize to other languages. This paper investigates how to adapt
SOTA adversarial attack algorithms in English to the Chinese language. Our
experiments show that attack methods previously applied to English NLP can
generate high-quality adversarial examples in Chinese when combined with proper
text segmentation and linguistic constraints. In addition, we demonstrate that
the generated adversarial examples can achieve high fluency and semantic
consistency by focusing on the Chinese language's morphology and phonology,
which in turn can be used to improve the adversarial robustness of Chinese NLP
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages; in ACL23 TrustNLP 2023: TrustNLP: Third Workshop on
  Trustworthy Natural Language Processing Colocated with the Annual Conference
  of the Association for Computational Linguistics (ACL 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixture-of-Supernets: Improving Weight-Sharing Supernet Training with
  Architecture-Routed Mixture-of-Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04845v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04845v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganesh Jawahar, Haichuan Yang, Yunyang Xiong, Zechun Liu, Dilin Wang, Fei Sun, Meng Li, Aasish Pappu, Barlas Oguz, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan, Raghuraman Krishnamoorthi, Vikas Chandra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weight-sharing supernet has become a vital component for performance
estimation in the state-of-the-art (SOTA) neural architecture search (NAS)
frameworks. Although supernet can directly generate different subnetworks
without retraining, there is no guarantee for the quality of these subnetworks
because of weight sharing. In NLP tasks such as machine translation and
pre-trained language modeling, we observe that given the same model
architecture, there is a large performance gap between supernet and training
from scratch. Hence, supernet cannot be directly used and retraining is
necessary after finding the optimal architectures.
  In this work, we propose mixture-of-supernets, a generalized supernet
formulation where mixture-of-experts (MoE) is adopted to enhance the expressive
power of the supernet model, with negligible training overhead. In this way,
different subnetworks do not share the model weights directly, but through an
architecture-based routing mechanism. As a result, model weights of different
subnetworks are customized towards their specific architectures and the weight
generation is learned by gradient descent. Compared to existing weight-sharing
supernet for NLP, our method can minimize the retraining time, greatly
improving training efficiency. In addition, the proposed method achieves the
SOTA performance in NAS for building fast machine translation models, yielding
better latency-BLEU tradeoff compared to HAT, state-of-the-art NAS for MT. We
also achieve the SOTA performance in NAS for building memory-efficient
task-agnostic BERT models, outperforming NAS-BERT and AutoDistil in various
model sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Vietnamese Legal Question--Answering System based on Automatic
  Data Enrichment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04841v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04841v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thi-Hai-Yen Vuong, Ha-Thanh Nguyen, Quang-Huy Nguyen, Le-Minh Nguyen, Xuan-Hieu Phan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Question answering (QA) in law is a challenging problem because legal
documents are much more complicated than normal texts in terms of terminology,
structure, and temporal and logical relationships. It is even more difficult to
perform legal QA for low-resource languages like Vietnamese where labeled data
are rare and pre-trained language models are still limited. In this paper, we
try to overcome these limitations by implementing a Vietnamese article-level
retrieval-based legal QA system and introduce a novel method to improve the
performance of language models by improving data quality through weak labeling.
Our hypothesis is that in contexts where labeled data are limited, efficient
data enrichment can help increase overall performance. Our experiments are
designed to test multiple aspects, which demonstrate the effectiveness of the
proposed technique.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>JURISIN 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LOST: A Mental Health <span class="highlight-title">Dataset</span> of Low Self-esteem in Reddit Posts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muskan Garg, Manas Gaur, Raxit Goswami, Sunghwan Sohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low self-esteem and interpersonal needs (i.e., thwarted belongingness (TB)
and perceived burdensomeness (PB)) have a major impact on depression and
suicide attempts. Individuals seek social connectedness on social media to
boost and alleviate their loneliness. Social media platforms allow people to
express their thoughts, experiences, beliefs, and emotions. Prior studies on
mental health from social media have focused on symptoms, causes, and
disorders. Whereas an initial screening of social media content for
interpersonal risk factors and low self-esteem may raise early alerts and
assign therapists to at-risk users of mental disturbance. Standardized scales
measure self-esteem and interpersonal needs from questions created using
psychological theories. In the current research, we introduce a
psychology-grounded and expertly annotated dataset, LoST: Low Self esTeem, to
study and detect low self-esteem on Reddit. Through an annotation approach
involving checks on coherence, correctness, consistency, and reliability, we
ensure gold-standard for supervised learning. We present results from different
deep language models tested using two data augmentation techniques. Our
findings suggest developing a class of language models that infuses
psychological and clinical knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privacy- and Utility-Preserving NLP with Anonymized Data: A case study
  of Pseudonymization <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oleksandr Yermilov, Vipul Raheja, Artem Chernodub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work investigates the effectiveness of different pseudonymization
techniques, ranging from rule-based substitutions to using pre-trained Large
Language Models (LLMs), on a variety of datasets and models used for two widely
used NLP tasks: text classification and summarization. Our work provides
crucial insights into the gaps between original and anonymized data (focusing
on the pseudonymization technique) and model quality and fosters future
research into higher-quality anonymization techniques to better balance the
trade-offs between data protection and utility preservation. We make our code,
pseudonymized datasets, and downstream models publicly available
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages. Accepted for TrustNLP workshop at ACL2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion and Sentiment Guided Paraphrasing <span class="chip">WASSA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin J. Xie, Ameeta Agrawal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Paraphrase generation, a.k.a. paraphrasing, is a common and important task in
natural language processing. Emotional paraphrasing, which changes the emotion
embodied in a piece of text while preserving its meaning, has many potential
applications, including moderating online dialogues and preventing
cyberbullying. We introduce a new task of fine-grained emotional paraphrasing
along emotion gradients, that is, altering the emotional intensities of the
paraphrases in fine-grained settings following smooth variations in affective
dimensions while preserving the meaning of the original text. We reconstruct
several widely used paraphrasing datasets by augmenting the input and target
texts with their fine-grained emotion labels. Then, we propose a framework for
emotion and sentiment guided paraphrasing by leveraging pre-trained language
models for conditioned text generation. Extensive evaluation of the fine-tuned
models suggests that including fine-grained emotion labels in the paraphrase
task significantly improves the likelihood of obtaining high-quality
paraphrases that reflect the desired emotions while achieving consistently
better scores in paraphrase metrics such as BLEU, ROUGE, and METEOR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13th Workshop on Computational Approaches to Subjectivity, Sentiment
  & Social Media Analysis (WASSA) 2023 at The 61st Annual Meeting of the
  Association for Computational Linguistics (ACL) 2023. arXiv admin note:
  substantial text overlap with arXiv:2212.03297</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bias Against 93 Stigmatized Groups in Masked Language Models and
  Downstream Sentiment Classification Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05550v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05550v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katelyn X. Mei, Sonia Fereidooni, Aylin Caliskan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid deployment of artificial intelligence (AI) models demands a
thorough investigation of biases and risks inherent in these models to
understand their impact on individuals and society. This study extends the
focus of bias evaluation in extant work by examining bias against social
stigmas on a large scale. It focuses on 93 stigmatized groups in the United
States, including a wide range of conditions related to disease, disability,
drug use, mental illness, religion, sexuality, socioeconomic status, and other
relevant factors. We investigate bias against these groups in English
pre-trained Masked Language Models (MLMs) and their downstream sentiment
classification tasks. To evaluate the presence of bias against 93 stigmatized
conditions, we identify 29 non-stigmatized conditions to conduct a comparative
analysis. Building upon a psychology scale of social rejection, the Social
Distance Scale, we prompt six MLMs: RoBERTa-base, RoBERTa-large, XLNet-large,
BERTweet-base, BERTweet-large, and DistilBERT. We use human annotations to
analyze the predicted words from these models, with which we measure the extent
of bias against stigmatized groups. When prompts include stigmatized
conditions, the probability of MLMs predicting negative words is approximately
20 percent higher than when prompts have non-stigmatized conditions. In the
sentiment classification tasks, when sentences include stigmatized conditions
related to diseases, disability, education, and mental illness, they are more
likely to be classified as negative. We also observe a strong correlation
between bias in MLMs and their downstream sentiment classifiers (r =0.79). The
evidence indicates that MLMs and their downstream sentiment classification
tasks exhibit biases against socially stigmatized groups.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages,12 figures,2 tables; ACM FAccT 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span> Injection attack against LLM-integrated Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05499v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05499v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs), renowned for their superior proficiency in
language comprehension and generation, stimulate a vibrant ecosystem of
applications around them. However, their extensive assimilation into various
services introduces significant security risks. This study deconstructs the
complexities and implications of prompt injection attacks on actual
LLM-integrated applications. Initially, we conduct an exploratory analysis on
ten commercial applications, highlighting the constraints of current attack
strategies in practice. Prompted by these limitations, we subsequently
formulate HouYi, a novel black-box prompt injection attack technique, which
draws inspiration from traditional web injection attacks. HouYi is
compartmentalized into three crucial elements: a seamlessly-incorporated
pre-constructed prompt, an injection prompt inducing context partition, and a
malicious payload designed to fulfill the attack objectives. Leveraging HouYi,
we unveil previously unknown and severe attack outcomes, such as unrestricted
arbitrary LLM usage and uncomplicated application prompt theft. We deploy HouYi
on 36 actual LLM-integrated applications and discern 31 applications
susceptible to prompt injection. 10 vendors have validated our discoveries,
including Notion, which has the potential to impact millions of users. Our
investigation illuminates both the possible risks of prompt injection attacks
and the possible tactics for mitigation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hexatagging: Projective Dependency Parsing as Tagging <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05477v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05477v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afra Amini, Tianyu Liu, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel dependency parser, the hexatagger, that constructs
dependency trees by tagging the words in a sentence with elements from a finite
set of possible tags. In contrast to many approaches to dependency parsing, our
approach is fully parallelizable at training time, i.e., the structure-building
actions needed to build a dependency parse can be predicted in parallel to each
other. Additionally, exact decoding is linear in time and space complexity.
Furthermore, we derive a probabilistic dependency parser that predicts hexatags
using no more than a linear model with features from a pretrained language
model, i.e., we forsake a bespoke architecture explicitly designed for the
task. Despite the generality and simplicity of our approach, we achieve
state-of-the-art performance of 96.4 LAS and 97.4 UAS on the Penn Treebank test
set. Additionally, our parser's linear time complexity and parallelism
significantly improve computational efficiency, with a roughly 10-times
speed-up over previous state-of-the-art models during decoding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Latent Phrase Matching for Dysarthric Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05446v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05446v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Colin Lea, Dianna Yee, Jaya Narain, Zifang Huang, Lauren Tooley, Jeffrey P. Bigham, Leah Findlater
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many consumer speech recognition systems are not tuned for people with speech
disabilities, resulting in poor recognition and user experience, especially for
severe speech differences. Recent studies have emphasized interest in
personalized speech models from people with atypical speech patterns. We
propose a query-by-example-based personalized phrase recognition system that is
trained using small amounts of speech, is language agnostic, does not assume a
traditional pronunciation lexicon, and generalizes well across speech
difference severities. On an internal dataset collected from 32 people with
dysarthria, this approach works regardless of severity and shows a 60%
improvement in recall relative to a commercial speech recognition system. On
the public EasyCall dataset of dysarthric speech, our approach improves
accuracy by 30.5%. Performance degrades as the number of phrases increases, but
consistently outperforms ASR systems when trained with 50 unique phrases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PIXIU: A Large Language Model, Instruction Data and Evaluation Benchmark
  for Finance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, Jimin Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although large language models (LLMs) has shown great performance on natural
language processing (NLP) in the financial domain, there are no publicly
available financial tailtored LLMs, instruction tuning datasets, and evaluation
benchmarks, which is critical for continually pushing forward the open-source
development of financial artificial intelligence (AI). This paper introduces
PIXIU, a comprehensive framework including the first financial LLM based on
fine-tuning LLaMA with instruction data, the first instruction data with 136K
data samples to support the fine-tuning, and an evaluation benchmark with 5
tasks and 9 datasets. We first construct the large-scale multi-task instruction
data considering a variety of financial tasks, financial document types, and
financial data modalities. We then propose a financial LLM called FinMA by
fine-tuning LLaMA with the constructed dataset to be able to follow
instructions for various financial tasks. To support the evaluation of
financial LLMs, we propose a standardized benchmark that covers a set of
critical financial tasks, including five financial NLP tasks and one financial
prediction task. With this benchmark, we conduct a detailed analysis of FinMA
and several existing LLMs, uncovering their strengths and weaknesses in
handling critical financial tasks. The model, datasets, benchmark, and
experimental results are open-sourced to facilitate future research in
financial AI.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 1 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do language models have coherent mental models of everyday things? <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10029v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10029v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuling Gu, Bhavana Dalvi Mishra, Peter Clark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When people think of everyday things like an egg, they typically have a
mental image associated with it. This allows them to correctly judge, for
example, that "the yolk surrounds the shell" is a false statement. Do language
models similarly have a coherent picture of such everyday things? To
investigate this, we propose a benchmark dataset consisting of 100 everyday
things, their parts, and the relationships between these parts, expressed as
11,720 "X relation Y?" true/false questions. Using these questions as probes,
we observe that state-of-the-art pre-trained language models (LMs) like GPT-3
and Macaw have fragments of knowledge about these everyday things, but do not
have fully coherent "parts mental models" (54-59% accurate, 19-43% conditional
constraint violation). We propose an extension where we add a constraint
satisfaction layer on top of the LM's raw predictions to apply commonsense
constraints. As well as removing inconsistencies, we find that this also
significantly improves accuracy (by 16-20%), suggesting how the incoherence of
the LM's pictures of everyday things can be significantly reduced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models are In-Context Semantic Reasoners rather than
  Symbolic Reasoners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14825v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14825v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaojuan Tang, Zilong Zheng, Jiaqi Li, Fanxu Meng, Song-Chun Zhu, Yitao Liang, Muhan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The emergent few-shot reasoning capabilities of Large Language Models (LLMs)
have excited the natural language and machine learning community over recent
years. Despite of numerous successful applications, the underlying mechanism of
such in-context capabilities still remains unclear. In this work, we
hypothesize that the learned \textit{semantics} of language tokens do the most
heavy lifting during the reasoning process. Different from human's symbolic
reasoning process, the semantic representations of LLMs could create strong
connections among tokens, thus composing a superficial logical chain. To test
our hypothesis, we decouple semantics from the language reasoning process and
evaluate three kinds of reasoning abilities, i.e., deduction, induction and
abduction. Our findings reveal that semantics play a vital role in LLMs'
in-context reasoning -- LLMs perform significantly better when semantics are
consistent with commonsense but struggle to solve symbolic or
counter-commonsense reasoning tasks by leveraging in-context new knowledge. The
surprising observations question whether modern LLMs have mastered the
inductive, deductive and abductive reasoning abilities as in human
intelligence, and motivate research on unveiling the magic existing within the
black-box LLMs. On the whole, our analysis provides a novel perspective on the
role of semantics in developing and evaluating language models' reasoning
abilities. Code is available at {\url{https://github.com/XiaojuanTang/ICSR}}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ One does not fit all! On the Complementarity of Vision Encoders for
  Vision and Language Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06379v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06379v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gregor Geigle, Chen Cecilia Liu, Jonas Pfeiffer, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current multimodal models, aimed at solving Vision and Language (V+L) tasks,
predominantly repurpose Vision Encoders (VE) as feature extractors. While many
VEs -- of different architectures, trained on different data and objectives --
are publicly available, they are not designed for the downstream V+L tasks.
Nonetheless, most current work assumes that a \textit{single} pre-trained VE
can serve as a general-purpose encoder. In this work, we focus on analysis and
aim to understand whether the information stored within different VEs is
complementary, i.e. if providing the model with features from multiple VEs can
improve the performance on a target task, and how they are combined. We
exhaustively experiment with three popular VEs on six downstream V+L tasks and
analyze the attention and VE-dropout patterns. Our analyses suggest that
diverse VEs complement each other, resulting in improved downstream V+L task
performance, where the improvements are not due to simple ensemble effects
(i.e. the performance does not always improve when increasing the number of
encoders). We demonstrate that future VEs, which are not \textit{repurposed},
but explicitly \textit{designed} for V+L tasks, have the potential of improving
performance on the target V+L tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Repl4NLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Hidden Mystery of OCR in Large Multimodal Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07895v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07895v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuliang Liu, Zhang Li, Hongliang Li, Wenwen Yu, Mingxin Huang, Dezhi Peng, Mingyu Liu, Mingrui Chen, Chunyuan Li, Cheng-lin Liu, Lianwen Jin, Xiang Bai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large models have recently played a dominant role in natural language
processing and multimodal vision-language learning. It remains less explored
about their efficacy in text-related visual tasks. We conducted a comprehensive
study of existing publicly available multimodal models, evaluating their
performance in text recognition (document text, artistic text, handwritten
text, scene text), text-based visual question answering (document text, scene
text, and bilingual text), key information extraction (receipts, documents, and
nutrition facts) and handwritten mathematical expression recognition. Our
findings reveal strengths and weaknesses in these models, which primarily rely
on semantic understanding for word recognition and exhibit inferior perception
of individual character shapes. They also display indifference towards text
length and have limited capabilities in detecting finegrained features in
images. Consequently, these results demonstrate that even the current most
powerful large multimodal models cannot match domain-specific methods in
traditional text tasks and face greater challenges in more complex tasks. Most
importantly, the baseline results showcased in this study could provide a
foundational framework for the conception and assessment of innovative
strategies targeted at enhancing zero-shot multimodal techniques. Evaluation
pipeline is available at https://github.com/Yuliang-Liu/MultimodalOCR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Supplementary Features of BiLSTM for Enhanced Sequence Labeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19928v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19928v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Conglei Xu, Kun Shen, Hongguang Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequence labeling tasks require the computation of sentence representations
for each word within a given sentence. With the rise of advanced pretrained
language models; one common approach involves incorporating a BiLSTM layer to
enhance the sequence structure information at the output level. Nevertheless,
it has been empirically demonstrated (P.-H. Li, 2020) that BiLSTM's potential
for generating sentence representations for sequence labeling tasks is
constrained, primarily due to the integration of fragments from past and future
sentence representations to form a complete sentence representation. In this
study, we observed that the entire sentence representation, found in both the
first and last cells of BiLSTM, can supplement each cell's sentence
representation. Accordingly, we devised a global context mechanism to integrate
entire future and past sentence representations into each cell's sentence
representation within BiLSTM, leading to a significant improvement in both F1
score and accuracy. By embedding the BERT model within BiLSTM as a
demonstration, and conducting exhaustive experiments on nine datasets for
sequence labeling tasks, including named entity recognition (NER), part of
speech (POS) tagging and End-to-End Aspect-Based sentiment analysis (E2E-ABSA).
We noted significant improvements in F1 scores and accuracy across all examined
datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Asymmetric feature interaction for interpreting model predictions <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07224v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07224v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolei Lu, Jianghong Ma, Haode Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In natural language processing (NLP), deep neural networks (DNNs) could model
complex interactions between context and have achieved impressive results on a
range of NLP tasks. Prior works on feature interaction attribution mainly focus
on studying symmetric interaction that only explains the additional influence
of a set of words in combination, which fails to capture asymmetric influence
that contributes to model prediction. In this work, we propose an asymmetric
feature interaction attribution explanation model that aims to explore
asymmetric higher-order feature interactions in the inference of deep neural
NLP models. By representing our explanation with an directed interaction graph,
we experimentally demonstrate interpretability of the graph to discover
asymmetric feature interactions. Experimental results on two sentiment
classification datasets show the superiority of our model against the
state-of-the-art feature interaction attribution methods in identifying
influential features for model predictions. Our code is available at
https://github.com/StillLu/ASIV.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Findings of the Association for Computational
  Linguistics: ACL 2023 (long paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ M$^3$IT: A Large-Scale <span class="highlight-title">Dataset</span> towards Multi-Modal Multilingual
  Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04387v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04387v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, Lingpeng Kong, Qi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning has significantly advanced large language models (LLMs)
such as ChatGPT, enabling them to align with human instructions across diverse
tasks. However, progress in open vision-language models (VLMs) has been limited
due to the scarcity of high-quality instruction datasets. To tackle this
challenge and promote research in the vision-language field, we introduce the
Multi-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to
optimize VLM alignment with human instructions. Our M$^3$IT dataset comprises
40 carefully curated datasets, including 2.4 million instances and 400 manually
written task instructions, reformatted into a vision-to-text structure. Key
tasks are translated into 80 languages with an advanced translation system,
ensuring broader accessibility. M$^3$IT surpasses previous datasets regarding
task coverage, instruction number and instance scale. Moreover, we develop
Ying-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potential
to answer complex questions requiring world knowledge, generalize to unseen
video tasks, and comprehend unseen instructions in Chinese. We have
open-sourced the dataset to encourage further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fix dataset url: https://huggingface.co/datasets/MMInstruction/M3IT
  Project: https://m3-it.github.io/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two Stage Contextual Word Filtering for Context bias in Unified
  Streaming and Non-streaming Transducer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06735v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06735v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhanheng Yang, Sining Sun, Xiong Wang, Yike Zhang, Long Ma, Lei Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is difficult for an E2E ASR system to recognize words such as entities
appearing infrequently in the training data. A widely used method to mitigate
this issue is feeding contextual information into the acoustic model. Previous
works have proven that a compact and accurate contextual list can boost the
performance significantly. In this paper, we propose an efficient approach to
obtain a high quality contextual list for a unified streaming/non-streaming
based E2E model. Specifically, we make use of the phone-level streaming output
to first filter the predefined contextual word list then fuse it into
non-casual encoder and decoder to generate the final recognition results. Our
approach improve the accuracy of the contextual ASR system and speed up the
inference process. Experiments on two datasets demonstrates over 20% CER
reduction comparing to the baseline system. Meanwhile, the RTF of our system
can be stabilized within 0.15 when the size of the contextual word list grows
over 6,000.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Think Twice: A Human-like Two-stage Conversational Agent for Emotional
  Response Generation <span class="chip">AAMAS2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04907v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04907v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushan Qian, Bo Wang, Shangzhao Ma, Wu Bin, Shuo Zhang, Dongming Zhao, Kun Huang, Yuexian Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Towards human-like dialogue systems, current emotional dialogue approaches
jointly model emotion and semantics with a unified neural network. This
strategy tends to generate safe responses due to the mutual restriction between
emotion and semantics, and requires rare emotion-annotated large-scale dialogue
corpus. Inspired by the "think twice" behavior in human dialogue, we propose a
two-stage conversational agent for the generation of emotional dialogue.
Firstly, a dialogue model trained without the emotion-annotated dialogue corpus
generates a prototype response that meets the contextual semantics. Secondly,
the first-stage prototype is modified by a controllable emotion refiner with
the empathy hypothesis. Experimental results on the DailyDialog and
EmpatheticDialogues datasets demonstrate that the proposed conversational
outperforms the comparison models in emotion generation and maintains the
semantic performance in automatic and human evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to AAMAS2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Relational Sentence Embedding for Flexible Semantic Matching <span class="chip">RepL4NLP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08802v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08802v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bin Wang, Haizhou Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Relational Sentence Embedding (RSE), a new paradigm to further
discover the potential of sentence embeddings. Prior work mainly models the
similarity between sentences based on their embedding distance. Because of the
complex semantic meanings conveyed, sentence pairs can have various relation
types, including but not limited to entailment, paraphrasing, and
question-answer. It poses challenges to existing embedding methods to capture
such relational information. We handle the problem by learning associated
relational embeddings. Specifically, a relation-wise translation operation is
applied to the source sentence to infer the corresponding target sentence with
a pre-trained Siamese-based encoder. The fine-grained relational similarity
scores can be computed from learned embeddings. We benchmark our method on 19
datasets covering a wide range of tasks, including semantic textual similarity,
transfer, and domain-specific tasks. Experimental results show that our method
is effective and flexible in modeling sentence relations and outperforms a
series of state-of-the-art sentence embedding methods.
https://github.com/BinWang28/RSE
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>RepL4NLP at ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BabySLM: language-acquisition-friendly benchmark of <span class="highlight-title">self-supervised</span>
  spoken language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marvin Lavechin, Yaya Sy, Hadrien Titeux, María Andrea Cruz Blandón, Okko Räsänen, Hervé Bredin, Emmanuel Dupoux, Alejandrina Cristia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised techniques for learning speech representations have been
shown to develop linguistic competence from exposure to speech without the need
for human labels. In order to fully realize the potential of these approaches
and further our understanding of how infants learn language, simulations must
closely emulate real-life situations by training on developmentally plausible
corpora and benchmarking against appropriate test sets. To this end, we propose
a language-acquisition-friendly benchmark to probe spoken language models at
the lexical and syntactic levels, both of which are compatible with the
vocabulary typical of children's language experiences. This paper introduces
the benchmark and summarizes a range of experiments showing its usefulness. In
addition, we highlight two exciting challenges that need to be addressed for
further progress: bridging the gap between text and speech and between clean
speech and in-the-wild speech.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Semi-supervised Approach for a Better Translation of Sentiment in
  Dialectical Arabic UGT <span class="chip">WANLP2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.11899v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.11899v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hadeel Saadany, Constantin Orasan, Emad Mohamed, Ashraf Tantawy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the online world, Machine Translation (MT) systems are extensively used to
translate User-Generated Text (UGT) such as reviews, tweets, and social media
posts, where the main message is often the author's positive or negative
attitude towards the topic of the text. However, MT systems still lack accuracy
in some low-resource languages and sometimes make critical translation errors
that completely flip the sentiment polarity of the target word or phrase and
hence delivers a wrong affect message. This is particularly noticeable in texts
that do not follow common lexico-grammatical standards such as the dialectical
Arabic (DA) used on online platforms. In this research, we aim to improve the
translation of sentiment in UGT written in the dialectical versions of the
Arabic language to English. Given the scarcity of gold-standard parallel data
for DA-EN in the UGT domain, we introduce a semi-supervised approach that
exploits both monolingual and parallel data for training an NMT system
initialised by a cross-lingual language model trained with supervised and
unsupervised modeling objectives. We assess the accuracy of sentiment
translation by our proposed system through a numerical 'sentiment-closeness'
measure as well as human evaluation. We will show that our semi-supervised MT
system can significantly help with correcting sentiment errors detected in the
online translation of dialectical Arabic UGT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>WANLP2022 at EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CoRRPUS: Code-based Structured <span class="highlight-title">Prompt</span>ing for Neurosymbolic Story
  Understanding <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10754v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10754v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yijiang River Dong, Lara J. Martin, Chris Callison-Burch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Story generation and understanding -- as with all NLG/NLU tasks -- has seen a
surge in neurosymbolic work. Researchers have recognized that, while large
language models (LLMs) have tremendous utility, they can be augmented with
symbolic means to be even better and to make up for any flaws that the neural
networks might have. However, symbolic methods are extremely costly in terms of
the amount of time and expertise needed to create them. In this work, we
capitalize on state-of-the-art Code-LLMs, such as Codex, to bootstrap the use
of symbolic methods for tracking the state of stories and aiding in story
understanding. We show that our CoRRPUS system and abstracted prompting
procedures can beat current state-of-the-art structured LLM techniques on
pre-existing story understanding tasks (bAbI Task 2 and Re^3) with minimal hand
engineering. We hope that this work can help highlight the importance of
symbolic representations and specialized prompting for LLMs as these models
require some guidance for performing reasoning tasks properly.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BanglaBook: A Large-scale Bangla <span class="highlight-title">Dataset</span> for Sentiment Analysis from
  Book <span class="highlight-title">Review</span>s <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06595v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06595v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohsinul Kabir, Obayed Bin Mahfuz, Syed Rifat Raiyan, Hasan Mahmud, Md Kamrul Hasan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The analysis of consumer sentiment, as expressed through reviews, can provide
a wealth of insight regarding the quality of a product. While the study of
sentiment analysis has been widely explored in many popular languages,
relatively less attention has been given to the Bangla language, mostly due to
a lack of relevant data and cross-domain adaptability. To address this
limitation, we present BanglaBook, a large-scale dataset of Bangla book reviews
consisting of 158,065 samples classified into three broad categories: positive,
negative, and neutral. We provide a detailed statistical analysis of the
dataset and employ a range of machine learning models to establish baselines
including SVM, LSTM, and Bangla-BERT. Our findings demonstrate a substantial
performance advantage of pre-trained models over models that rely on manually
crafted features, emphasizing the necessity for additional training resources
in this domain. Additionally, we conduct an in-depth error analysis by
examining sentiment unigrams, which may provide insight into common
classification errors in under-resourced languages like Bangla. Our codes and
data are publicly available at https://github.com/mohsinulkabir14/BanglaBook.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Findings of the Association for Computational
  Linguistics: ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controlled Text Generation with Natural Language Instructions <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14293v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14293v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models generate fluent texts and can follow natural language
instructions to solve a wide range of tasks without task-specific training.
Nevertheless, it is notoriously difficult to control their generation to
satisfy the various constraints required by different applications. In this
work, we present InstructCTG, a controlled text generation framework that
incorporates different constraints by conditioning on natural language
descriptions and demonstrations of the constraints. In particular, we first
extract the underlying constraints of natural texts through a combination of
off-the-shelf NLP tools and simple heuristics. We then verbalize the
constraints into natural language instructions to form weakly supervised
training data. By prepending natural language descriptions of the constraints
and a few demonstrations, we fine-tune a pre-trained language model to
incorporate various types of constraints. Compared to existing search-based or
score-based methods, InstructCTG is more flexible to different constraint types
and has a much smaller impact on the generation quality and speed because it
does not modify the decoding procedure. Additionally, InstructCTG allows the
model to adapt to new constraints without re-training through the use of
few-shot task generalization and in-context learning abilities of
instruction-tuned language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese
  Medical Exam <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03030v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03030v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang, Chenyu You, Zhenhua Guo, Lei Zhu, Michael Lingzhi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have transformed the
field of question answering (QA). However, evaluating LLMs in the medical field
is challenging due to the lack of standardized and comprehensive datasets. To
address this gap, we introduce CMExam, sourced from the Chinese National
Medical Licensing Examination. CMExam consists of 60K+ multiple-choice
questions for standardized and objective evaluations, as well as solution
explanations for model reasoning evaluation in an open-ended manner. For
in-depth analyses of LLMs, we invited medical professionals to label five
additional question-wise annotations, including disease groups, clinical
departments, medical disciplines, areas of competency, and question difficulty
levels. Alongside the dataset, we further conducted thorough experiments with
representative LLMs and QA algorithms on CMExam. The results show that GPT-4
had the best accuracy of 61.6% and a weighted F1 score of 0.617. These results
highlight a great disparity when compared to human accuracy, which stood at
71.6%. For explanation tasks, while LLMs could generate relevant reasoning and
demonstrate improved performance after finetuning, they fall short of a desired
standard, indicating ample room for improvement. To the best of our knowledge,
CMExam is the first Chinese medical exam dataset to provide comprehensive
medical annotations. The experiments and findings of LLM evaluation also
provide valuable insights into the challenges and potential solutions in
developing Chinese medical QA systems and LLM evaluation pipelines. The dataset
and relevant code are available at https://github.com/williamliujl/CMExam.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">GPT</span> Self-Supervision for a Better Data Annotator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohuan Pei, Yanxi Li, Chang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of annotating data into concise summaries poses a significant
challenge across various domains, frequently requiring the allocation of
significant time and specialized knowledge by human experts. Despite existing
efforts to use large language models for annotation tasks, significant problems
such as limited applicability to unlabeled data, the absence of self-supervised
methods, and the lack of focus on complex structured data still persist. In
this work, we propose a GPT self-supervision annotation method, which embodies
a generating-recovering paradigm that leverages the one-shot learning
capabilities of the Generative Pretrained Transformer (GPT). The proposed
approach comprises a one-shot tuning phase followed by a generation phase. In
the one-shot tuning phase, we sample a data from the support set as part of the
prompt for GPT to generate a textual summary, which is then used to recover the
original data. The alignment score between the recovered and original data
serves as a self-supervision navigator to refine the process. In the generation
stage, the optimally selected one-shot sample serves as a template in the
prompt and is applied to generating summaries from challenging datasets. The
annotation performance is evaluated by tuning several human feedback reward
networks and by calculating alignment scores between original and recovered
data at both sentence and structure levels. Our self-supervised annotation
method consistently achieves competitive scores, convincingly demonstrating its
robust strength in various data-to-summary annotation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Whitening-based Contrastive Learning of Sentence Embeddings <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17746v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17746v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Zhuo, Yifan Sun, Xiaohan Wang, Linchao Zhu, Yi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a whitening-based contrastive learning method for
sentence embedding learning (WhitenedCSE), which combines contrastive learning
with a novel shuffled group whitening. Generally, contrastive learning pulls
distortions of a single sample (i.e., positive samples) close and push negative
samples far away, correspondingly facilitating the alignment and uniformity in
the feature space. A popular alternative to the "pushing'' operation is
whitening the feature space, which scatters all the samples for uniformity.
Since the whitening and the contrastive learning have large redundancy w.r.t.
the uniformity, they are usually used separately and do not easily work
together. For the first time, this paper integrates whitening into the
contrastive learning scheme and facilitates two benefits. 1) Better uniformity.
We find that these two approaches are not totally redundant but actually have
some complementarity due to different uniformity mechanism. 2) Better
alignment. We randomly divide the feature into multiple groups along the
channel axis and perform whitening independently within each group. By
shuffling the group division, we derive multiple distortions of a single sample
and thus increase the positive sample diversity. Consequently, using multiple
positive samples with enhanced diversity further improves contrastive learning
due to better alignment. Extensive experiments on seven semantic textual
similarity tasks show our method achieves consistent improvement over the
contrastive learning baseline and sets new states of the art, e.g., 78.78\%
(+2.53\% based on BERT\ba) Spearman correlation on STS tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 Main Conference(Oral)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Systematic Study and Comprehensive Evaluation of Chat<span class="highlight-title">GPT</span> on Benchmark
  <span class="highlight-title">Dataset</span>s <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18486v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18486v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, Jimmy Xiangji Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of large language models (LLMs) such as ChatGPT has brought a
lot of attention recently. However, their evaluation in the benchmark academic
datasets remains under-explored due to the difficulty of evaluating the
generative outputs produced by this model against the ground truth. In this
paper, we aim to present a thorough evaluation of ChatGPT's performance on
diverse academic datasets, covering tasks like question-answering, text
summarization, code generation, commonsense reasoning, mathematical
problem-solving, machine translation, bias detection, and ethical
considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze
255K responses it generates in these datasets. This makes our work the largest
evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate
the strengths and weaknesses of ChatGPT in various tasks and provide insights
for future research using LLMs. We also report a new emergent ability to follow
multi-query instructions that we mostly found in ChatGPT and other
instruction-tuned models. Our extensive evaluation shows that even though
ChatGPT is capable of performing a wide variety of tasks, and may obtain
impressive performance in several benchmark datasets, it is still far from
achieving the ability to reliably solve many challenging tasks. By providing a
thorough assessment of ChatGPT's performance across diverse NLP tasks, this
paper sets the stage for a targeted deployment of ChatGPT-like LLMs in
real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2023 Findings. The first three authors contributed
  equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridge the Gap Between CV and NLP! A Gradient-based Textual Adversarial
  Attack Framework <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.15317v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.15317v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lifan Yuan, Yichi Zhang, Yangyi Chen, Wei Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite recent success on various tasks, deep learning techniques still
perform poorly on adversarial examples with small perturbations. While
optimization-based methods for adversarial attacks are well-explored in the
field of computer vision, it is impractical to directly apply them in natural
language processing due to the discrete nature of the text. To address the
problem, we propose a unified framework to extend the existing
optimization-based adversarial attack methods in the vision domain to craft
textual adversarial samples. In this framework, continuously optimized
perturbations are added to the embedding layer and amplified in the forward
propagation process. Then the final perturbed latent representations are
decoded with a masked language model head to obtain potential adversarial
samples. In this paper, we instantiate our framework with an attack algorithm
named Textual Projected Gradient Descent (T-PGD). We find our algorithm
effective even using proxy gradient information. Therefore, we perform the more
challenging transfer black-box attack and conduct comprehensive experiments to
evaluate our attack algorithm with several models on three benchmark datasets.
Experimental results demonstrate that our method achieves overall better
performance and produces more fluent and grammatical adversarial samples
compared to strong baseline methods. The code and data are available at
\url{https://github.com/Phantivia/T-PGD}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of ACL 2023. Codes are available at:
  https://github.com/Phantivia/T-PGD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Empirical Study on Challenging Math Problem Solving with <span class="highlight-title">GPT</span>-4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01337v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01337v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiran Wu, Feiran Jia, Shaokun Zhang, Hangyu Li, Erkang Zhu, Yue Wang, Yin Tat Lee, Richard Peng, Qingyun Wu, Chi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Employing Large Language Models (LLMs) to address mathematical problems is an
intriguing research endeavor, considering the abundance of math problems
expressed in natural language across numerous science and engineering fields.
While several prior works have investigated solving elementary mathematics
using LLMs, this work explores the frontier of using GPT-4 for solving more
complex and challenging math problems. We evaluate various ways of using GPT-4.
Some of them are adapted from existing work, and one is MathChat, a
conversational problem-solving framework newly proposed in this work. We
perform the evaluation on difficult high school competition problems from the
MATH dataset, which shows the advantage of the proposed conversational
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fix minor errors, update github link</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sensitivity and Robustness of Large Language Models to <span class="highlight-title">Prompt</span> Template
  in Japanese Text Classification Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08714v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08714v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengguang Gan, Tatsunori Mori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt engineering relevance research has seen a notable surge in recent
years, primarily driven by advancements in pre-trained language models and
large language models. However, a critical issue has been identified within
this domain: the inadequate of sensitivity and robustness of these models
towards Prompt Templates, particularly in lesser-studied languages such as
Japanese. This paper explores this issue through a comprehensive evaluation of
several representative Large Language Models (LLMs) and a widely-utilized
pre-trained model(PLM). These models are scrutinized using a benchmark dataset
in Japanese, with the aim to assess and analyze the performance of the current
multilingual models in this context. Our experimental results reveal startling
discrepancies. A simple modification in the sentence structure of the Prompt
Template led to a drastic drop in the accuracy of GPT-4 from 49.21 to 25.44.
This observation underscores the fact that even the highly performance GPT-4
model encounters significant stability issues when dealing with diverse
Japanese prompt templates, rendering the consistency of the model's output
results questionable. In light of these findings, we conclude by proposing
potential research trajectories to further enhance the development and
performance of Large Language Models in their current stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review. 11 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards
  and Ethical Behavior in the MACHIAVELLI Benchmark <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03279v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03279v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Pan, Jun Shern Chan, Andy Zou, Nathaniel Li, Steven Basart, Thomas Woodside, Jonathan Ng, Hanlin Zhang, Scott Emmons, Dan Hendrycks
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Artificial agents have traditionally been trained to maximize reward, which
may incentivize power-seeking and deception, analogous to how next-token
prediction in language models (LMs) may incentivize toxicity. So do agents
naturally learn to be Machiavellian? And how do we measure these behaviors in
general-purpose models such as GPT-4? Towards answering these questions, we
introduce MACHIAVELLI, a benchmark of 134 Choose-Your-Own-Adventure games
containing over half a million rich, diverse scenarios that center on social
decision-making. Scenario labeling is automated with LMs, which are more
performant than human annotators. We mathematize dozens of harmful behaviors
and use our annotations to evaluate agents' tendencies to be power-seeking,
cause disutility, and commit ethical violations. We observe some tension
between maximizing reward and behaving ethically. To improve this trade-off, we
investigate LM-based methods to steer agents' towards less harmful behaviors.
Our results show that agents can both act competently and morally, so concrete
progress can currently be made in machine ethics--designing agents that are
Pareto improvements in both safety and capabilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023 Oral (camera-ready); 31 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Socratic <span class="highlight-title">Pretrain</span>ing: Question-Driven <span class="highlight-title">Pretrain</span>ing for Controllable
  Summarization <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10449v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10449v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Artidoro Pagnoni, Alexander R. Fabbri, Wojciech Kryściński, Chien-Sheng Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In long document controllable summarization, where labeled data is scarce,
pretrained models struggle to adapt to the task and effectively respond to user
queries. In this paper, we introduce Socratic pretraining, a question-driven,
unsupervised pretraining objective specifically designed to improve
controllability in summarization tasks. By training a model to generate and
answer relevant questions in a given context, Socratic pretraining enables the
model to more effectively adhere to user-provided queries and identify relevant
content to be summarized. We demonstrate the effectiveness of this approach
through extensive experimentation on two summarization domains, short stories
and dialogue, and multiple control strategies: keywords, questions, and factoid
QA pairs. Our pretraining method relies only on unlabeled documents and a
question generation system and outperforms pre-finetuning approaches that use
additional supervised data. Furthermore, our results show that Socratic
pretraining cuts task-specific labeled data requirements in half, is more
faithful to user-provided queries, and achieves state-of-the-art performance on
QMSum and SQuALITY.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncertainty-Aware Bootstrap Learning for Joint Extraction on
  Distantly-Supervised Data <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yufei Li, Xiao Yu, Yanchi Liu, Haifeng Chen, Cong Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Jointly extracting entity pairs and their relations is challenging when
working on distantly-supervised data with ambiguous or noisy labels. To
mitigate such impact, we propose uncertainty-aware bootstrap learning, which is
motivated by the intuition that the higher uncertainty of an instance, the more
likely the model confidence is inconsistent with the ground truths.
Specifically, we first explore instance-level data uncertainty to create an
initial high-confident examples. Such subset serves as filtering noisy
instances and facilitating the model to converge fast at the early stage.
During bootstrap learning, we propose self-ensembling as a regularizer to
alleviate inter-model uncertainty produced by noisy labels. We further define
probability variance of joint tagging probabilities to estimate inner-model
parametric uncertainty, which is used to select and build up new reliable
training instances for the next iteration. Experimental results on two large
datasets reveal that our approach outperforms existing strong baselines and
related methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 main conference short paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Party Chat: Conversational Agents in Group Settings with Humans
  and Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.13835v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.13835v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jimmy Wei, Kurt Shuster, Arthur Szlam, Jason Weston, Jack Urbanek, Mojtaba Komeili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current dialogue research primarily studies pairwise (two-party)
conversations, and does not address the everyday setting where more than two
speakers converse together. In this work, we both collect and evaluate
multi-party conversations to study this more general case. We use the LIGHT
environment to construct grounded conversations, where each participant has an
assigned character to role-play. We thus evaluate the ability of language
models to act as one or more characters in such conversations. Models require
two skills that pairwise-trained models appear to lack: (1) being able to
decide when to talk; (2) producing coherent utterances grounded on multiple
characters. We compare models trained on our new dataset to existing
pairwise-trained dialogue models, as well as large language models with
few-shot prompting. We find that our new dataset, MultiLIGHT, which we will
publicly release, can help bring significant improvements in the group setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Delving Deeper into Cross-lingual Visual Question Answering <span class="chip">EACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.07630v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.07630v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Liu, Jonas Pfeiffer, Anna Korhonen, Ivan Vulić, Iryna Gurevych
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual question answering (VQA) is one of the crucial vision-and-language
tasks. Yet, existing VQA research has mostly focused on the English language,
due to a lack of suitable evaluation resources. Previous work on cross-lingual
VQA has reported poor zero-shot transfer performance of current multilingual
multimodal Transformers with large gaps to monolingual performance, without any
deeper analysis. In this work, we delve deeper into the different aspects of
cross-lingual VQA, aiming to understand the impact of 1) modeling methods and
choices, including architecture, inductive bias, fine-tuning; 2) learning
biases: including question types and modality biases in cross-lingual setups.
The key results of our analysis are: 1) We show that simple modifications to
the standard training setup can substantially reduce the transfer gap to
monolingual English performance, yielding +10 accuracy points over existing
methods. 2) We analyze cross-lingual VQA across different question types of
varying complexity for different multilingual multimodal Transformers, and
identify question types that are the most difficult to improve on. 3) We
provide an analysis of modality biases present in training data and models,
revealing why zero-shot performance gaps remain for certain question types and
languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of EACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context-NER : Contextual Phrase Generation at Scale <span class="chip">NeurIPS
  2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.08079v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.08079v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Himanshu Gupta, Shreyas Verma, Santosh Mashetty, Swaroop Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Named Entity Recognition (NER) has seen significant progress in recent years,
with numerous state-of-the-art (SOTA) models achieving high performance.
However, very few studies have focused on the generation of entities' context.
In this paper, we introduce CONTEXT-NER, a task that aims to generate the
relevant context for entities in a sentence, where the context is a phrase
describing the entity but not necessarily present in the sentence. To
facilitate research in this task, we also present the EDGAR10-Q dataset, which
consists of annual and quarterly reports from the top 1500 publicly traded
companies. The dataset is the largest of its kind, containing 1M sentences,
2.8M entities, and an average of 35 tokens per sentence, making it a
challenging dataset. We propose a baseline approach that combines a phrase
generation algorithm with inferencing using a 220M language model, achieving a
ROUGE-L score of 27% on the test split. Additionally, we perform a one-shot
inference with ChatGPT, which obtains a 30% ROUGE-L, highlighting the
difficulty of the dataset. We also evaluate models such as T5 and BART, which
achieve a maximum ROUGE-L of 49% after supervised finetuning on EDGAR10-Q. We
also find that T5-large, when pre-finetuned on EDGAR10-Q, achieve SOTA results
on downstream finance tasks such as Headline, FPB, and FiQA SA, outperforming
vanilla version by 10.81 points. To our surprise, this 66x smaller
pre-finetuned model also surpasses the finance-specific LLM BloombergGPT-50B by
15 points. We hope that our dataset and generated artifacts will encourage
further research in this direction, leading to the development of more
sophisticated language models for financial text analysis
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 5 Figures, 2 AlgorithmS, 17 Tables. Accepted in NeurIPS
  2022 - Efficient Natural Language and Speech Processing (ENLSP) Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-Preserving Domain Adaptation of Semantic Parsers <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10520v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10520v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemehsadat Mireshghallah, Yu Su, Tatsunori Hashimoto, Jason Eisner, Richard Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented dialogue systems often assist users with personal or
confidential matters. For this reason, the developers of such a system are
generally prohibited from observing actual usage. So how can they know where
the system is failing and needs more training data or new functionality? In
this work, we study ways in which realistic user utterances can be generated
synthetically, to help increase the linguistic and functional coverage of the
system, without compromising the privacy of actual users. To this end, we
propose a two-stage Differentially Private (DP) generation method which first
generates latent semantic parses, and then generates utterances based on the
parses. Our proposed approach improves MAUVE by 2.5$\times$ and parse tree
function type overlap by 1.3$\times$ relative to current approaches for private
synthetic data generation, improving both on fluency and semantic coverage. We
further validate our approach on a realistic domain adaptation task of adding
new functionality from private user data to a semantic parser, and show overall
gains of 8.5% points in accuracy with the new feature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Optimization and Control <span class="chip" style="font-size: 60%">32</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Movement Optimization of Robotic Arms for Energy and Time Reduction
  using Evolutionary Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05329v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05329v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abolfazl Akbari, Saeed Mozaffari, Rajmeet Singh, Majid Ahmadi, Shahpour Alirezaee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Trajectory optimization of a robot manipulator consists of both optimization
of the robot movement as well as optimization of the robot end-effector path.
This paper aims to find optimum movement parameters including movement type,
speed, and acceleration to minimize robot energy. Trajectory optimization by
minimizing the energy would increase the longevity of robotic manipulators. We
utilized the particle swarm optimization method to find the movement parameters
leading to minimum energy consumption. The effectiveness of the proposed method
is demonstrated on different trajectories. Experimental results show that 49%
efficiency was obtained using a UR5 robotic arm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe Adaptive Multi-Agent Coverage Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Bai, Yujie Wang, Xiaogang Xiong, Mikhail Svinin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a safe adaptive coverage controller for multi-agent
systems with actuator faults and time-varying uncertainties. The centroidal
Voronoi tessellation (CVT) is applied to generate an optimal configuration of
multi-agent systems for covering an area of interest. As a conventional
CVT-based controller cannot prevent collisions between agents with non-zero
size, a control barrier function (CBF) based controller is developed to ensure
collision avoidance with a function approximation technique (FAT) based design
to deal with system uncertainties. The proposed controller is verified under
simulations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Identification and Optimization of Nonsmooth Superposition
  Operators in Semilinear Elliptic PDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constantin Christof, Julia Kowalczyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study an infinite-dimensional optimization problem that aims to identify
the Nemytskii operator in the nonlinear part of a prototypical semilinear
elliptic partial differential equation (PDE) which minimizes the distance
between the PDE-solution and a given desired state. In contrast to previous
works, we consider this identification problem in a low-regularity regime in
which the function inducing the Nemytskii operator is a-priori only known to be
an element of $H^1_{loc}(\mathbb{R})$. This makes the studied problem class a
suitable point of departure for the rigorous analysis of training problems for
learning-informed PDEs in which an unknown superposition operator is
approximated by means of a neural network with nonsmooth activation functions
(ReLU, leaky-ReLU, etc.). We establish that, despite the low regularity of the
controls, it is possible to derive a classical stationarity system for local
minimizers and to solve the considered problem by means of a gradient
projection method. The convergence of the resulting algorithm is proven in the
function space setting. It is also shown that the established first-order
necessary optimality conditions imply that locally optimal superposition
operators share various characteristic properties with commonly used activation
functions: They are always sigmoidal, continuously differentiable away from the
origin, and typically possess a distinct kink at zero. The paper concludes with
numerical experiments which confirm the theoretical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Optimization of Expensive Nested Grey-Box Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Xu, Yuning Jiang, Bratislav Svetozarevic, Colin N. Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of optimizing a grey-box objective function, i.e.,
nested function composed of both black-box and white-box functions. A general
formulation for such grey-box problems is given, which covers the existing
grey-box optimization formulations as special cases. We then design an
optimism-driven algorithm to solve it. Under certain regularity assumptions,
our algorithm achieves similar regret bound as that for the standard black-box
Bayesian optimization algorithm, up to a constant multiplicative term depending
on the Lipschitz constants of the functions considered. We further extend our
method to the constrained case and discuss several special cases. For the
commonly used kernel functions, the regret bounds allow us to derive a
convergence rate to the optimal solution. Experimental results show that our
grey-box optimization method empirically improves the speed of finding the
global optimal solution significantly, as compared to the standard black-box
optimization algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-sum stopper vs. singular-controller games with constrained control
  directions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05113v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05113v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Bovo, Tiziano De Angelis, Jan Palczewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a class of zero-sum stopper vs.\ singular-controller games in
which the controller can only act on a subset $d_0<d$ of the $d$ coordinates of
a controlled diffusion. Due to the constraint on the control directions these
games fall outside the framework of recently studied variational methods. In
this paper we develop an approximation procedure, based on $L^1$-stability
estimates for the controlled diffusion process and almost sure convergence of
suitable stopping times. That allows us to prove existence of the game's value
and to obtain an optimal strategy for the stopper, under continuity and growth
conditions on the payoff functions. This class of games is a natural extension
of (single-agent) singular control problems, studied in the literature, with
similar constraints on the admissible controls.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Communication-Efficient Gradient Descent-Accent Methods for Distributed
  Variational Inequalities: Unified Analysis and Local Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Zhang, Sayantan Choudhury, Sebastian U Stich, Nicolas Loizou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed and federated learning algorithms and techniques associated
primarily with minimization problems. However, with the increase of minimax
optimization and variational inequality problems in machine learning, the
necessity of designing efficient distributed/federated learning approaches for
these problems is becoming more apparent. In this paper, we provide a unified
convergence analysis of communication-efficient local training methods for
distributed variational inequality problems (VIPs). Our approach is based on a
general key assumption on the stochastic estimates that allows us to propose
and analyze several novel local training algorithms under a single framework
for solving a class of structured non-monotone VIPs. We present the first local
gradient descent-accent algorithms with provable improved communication
complexity for solving distributed variational inequalities on heterogeneous
data. The general algorithmic framework recovers state-of-the-art algorithms
and their sharp convergence guarantees when the setting is specialized to
minimization or minimax optimization problems. Finally, we demonstrate the
strong performance of the proposed algorithms compared to state-of-the-art
methods when solving federated minimax optimization problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recovering Simultaneously Structured Data via Non-Convex Iteratively
  Reweighted Least Squares 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Kümmerle, Johannes Maly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new algorithm for the problem of recovering data that adheres to
multiple, heterogeneous low-dimensional structures from linear observations.
Focusing on data matrices that are simultaneously row-sparse and low-rank, we
propose and analyze an iteratively reweighted least squares (IRLS) algorithm
that is able to leverage both structures. In particular, it optimizes a
combination of non-convex surrogates for row-sparsity and rank, a balancing of
which is built into the algorithm. We prove locally quadratic convergence of
the iterates to a simultaneously structured data matrix in a regime of minimal
sample complexity (up to constants and a logarithmic factor), which is known to
be impossible for a combination of convex surrogates. In experiments, we show
that the IRLS method exhibits favorable empirical convergence, identifying
simultaneously row-sparse and low-rank matrices from fewer measurements than
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpreting and Improving Diffusion Models Using the Euclidean Distance
  Function 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frank Permenter, Chenyang Yuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Denoising is intuitively related to projection. Indeed, under the manifold
hypothesis, adding random noise is approximately equivalent to orthogonal
perturbation. Hence, learning to denoise is approximately learning to project.
In this paper, we use this observation to reinterpret denoising diffusion
models as approximate gradient descent applied to the Euclidean distance
function. We then provide straight-forward convergence analysis of the DDIM
sampler under simple assumptions on the projection-error of the denoiser.
Finally, we propose a new sampler based on two simple modifications to DDIM
using insights from our theoretical results. In as few as 5-10 function
evaluations, our sampler achieves state-of-the-art FID scores on pretrained
CIFAR-10 and CelebA models and can generate high quality samples on latent
diffusion models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Risk-aware Urban Air Mobility Network Design with Overflow Redundancy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qinshuang Wei, Zhenyu Gao, John-Paul Clarke, Ufuk Topcu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Urban Air Mobility (UAM), as envisioned by researchers and practitioners,
will be achieved through the use of highly automated aircraft that operate and
transport passengers and cargo at low altitudes within urban and suburban
areas. To operate in complex urban environment, precise air traffic management,
in particular the management of traffic overflows due to operational
disruptions will be critical to ensuring system safety and efficiency. To this
end, we propose a methodology for the design of UAM networks with reserve
capacity, i.e., a design where alternative landing options and flight corridors
are explicitly considered as a means of improving contingency management and
reducing risk. Similar redundancy considerations are incorporated in the design
of many critical infrastructures, yet remain unexploited in the air
transportation literature. In our methodology, we first model how disruptions
to a given on-demand UAM network might impact on the nominal traffic flow and
how this flow might be re-accommodated on an extended network with reserve
capacity. Then, through an optimization problem, we select the locations and
capacities for the backup vertiports with the maximal expected throughput of
the extended network over all possible disruption scenarios, while the
throughput is the maximal amount of flights that the network can accommodate
per unit of time. We show that we can obtain the solution for the corresponding
bi-level and bi-linear optimization problem by solving a mixed-integer linear
program. We demonstrate our methodology in the case study using networks from
Milwaukee, Atlanta, and Dallas--Fort Worth metropolitan areas and show how the
throughput and flexibility of the UAM networks with reserve capacity can
outcompete those without.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>43 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Enhanced Control Engineering Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05545v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05545v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ion Matei, Raj Minhas, Johan de Kleer, Alexander Felman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AI and machine learning based approaches are becoming ubiquitous in almost
all engineering fields. Control engineering cannot escape this trend. In this
paper, we explore how AI tools can be useful in control applications. The core
tool we focus on is automatic differentiation. Two immediate applications are
linearization of system dynamics for local stability analysis or for state
estimation using Kalman filters. We also explore other usages such as
conversion of differential algebraic equations to ordinary differential
equations for control design. In addition, we explore the use of machine
learning models for global parameterizations of state vectors and control
inputs in model predictive control applications. For each considered use case,
we give examples and results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Driven Near-Optimal Control of Nonlinear Systems Over Finite
  Horizon 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05482v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05482v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vasanth Reddy, Hoda Eldardiry, Almuatazbellah Boker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine the problem of two-point boundary optimal control of nonlinear
systems over finite-horizon time periods with unknown model dynamics by
employing reinforcement learning. We use techniques from singular perturbation
theory to decompose the control problem over the finite horizon into two
sub-problems, each solved over an infinite horizon. In the process, we avoid
the need to solve the time-varying Hamilton-Jacobi-Bellman equation. Using a
policy iteration method, which is made feasible as a result of this
decomposition, it is now possible to learn the controller gains of both
sub-problems. The overall control is then formed by piecing together the
solutions to the two sub-problems. We show that the performance of the proposed
closed-loop system approaches that of the model-based optimal performance as
the time horizon gets long. Finally, we provide three simulation scenarios to
support the paper's claims.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safety-Critical Control for Systems with Impulsive Actuators and Dwell
  Time Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10243v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10243v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Breeden, Dimitra Panagou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents extensions of control barrier function (CBF) and control
Lyapunov function (CLF) theory to systems wherein all actuators cause impulsive
changes to the state trajectory, and can only be used again after a minimum
dwell time has elapsed. These rules define a hybrid system, wherein the
controller must at each control cycle choose whether to remain on the current
state flow or to jump to a new trajectory. We first derive a sufficient
condition to render a specified set forward invariant using extensions of CBF
theory. We then derive related conditions to ensure asymptotic stability in
such systems, and apply both conditions online in an optimization-based control
law with aperiodic impulses. We simulate both results on a spacecraft docking
problem with multiple obstacles.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Control Systems Letters, extended version includes
  full proof of Corollary 1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Target-based Surrogates for Stochastic Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02607v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02607v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Wilder Lavington, Sharan Vaswani, Reza Babanezhad, Mark Schmidt, Nicolas Le Roux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider minimizing functions for which it is expensive to compute the
(possibly stochastic) gradient. Such functions are prevalent in reinforcement
learning, imitation learning and adversarial training. Our target optimization
framework uses the (expensive) gradient computation to construct surrogate
functions in a \emph{target space} (e.g. the logits output by a linear model
for classification) that can be minimized efficiently. This allows for multiple
parameter updates to the model, amortizing the cost of gradient computation. In
the full-batch setting, we prove that our surrogate is a global upper-bound on
the loss, and can be (locally) minimized using a black-box optimization
algorithm. We prove that the resulting majorization-minimization algorithm
ensures convergence to a stationary point of the loss. Next, we instantiate our
framework in the stochastic setting and propose the $SSO$ algorithm, which can
be viewed as projected stochastic gradient descent in the target space. This
connection enables us to prove theoretical guarantees for $SSO$ when minimizing
convex functions. Our framework allows the use of standard stochastic
optimization algorithms to construct surrogates which can be minimized by any
deterministic optimization method. To evaluate our framework, we consider a
suite of supervised learning and imitation learning problems. Our experiments
indicate the benefits of target optimization and the effectiveness of $SSO$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On packing dijoins in digraphs and weighted digraphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.00392v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.00392v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Abdi, Gérard Cornuéjols, Michael Zlatin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Let $D=(V,A)$ be a digraph. A dicut is a cut $\delta^+(U)\subseteq A$ for
some nonempty proper vertex subset $U$ such that $\delta^-(U)=\emptyset$, a
dijoin is an arc subset that intersects every dicut at least once, and more
generally a $k$-dijoin is an arc subset that intersects every dicut at least
$k$ times. Our first result is that $A$ can be partitioned into a dijoin and a
$(\tau-1)$-dijoin where $\tau$ denotes the smallest size of a dicut. Woodall
conjectured the stronger statement that $A$ can be partitioned into $\tau$
dijoins.
  Let $w\in \mathbb{Z}^A_{\geq 0}$ and suppose every dicut has weight at least
$\tau$, for some integer $\tau\geq 2$. Let
$\rho(\tau,D,w):=\frac{1}{\tau}\sum_{v\in V} m_v$, where each $m_v$ is the
integer in $\{0,1,\ldots,\tau-1\}$ equal to $w(\delta^+(v))-w(\delta^-(v))$ mod
$\tau$. We prove the following results: (i) If $\rho(\tau,D,w)\in \{0,1\}$,
then there is an equitable $w$-weighted packing of dijoins of size $\tau$. (ii)
If $\rho(\tau,D,w)= 2$, then there is a $w$-weighted packing of dijoins of size
$\tau$. (iii) If $\rho(\tau,D,w)=3$, $\tau=3$, and $w={\bf 1}$, then $A$ can be
partitioned into three dijoins.
  Each result is best possible: (i) does not hold for $\rho(\tau,D,w)=2$ even
if $w=\1$, (ii) does not hold for $\rho(\tau,D,w)=3$, and (iii) do not hold for
general $w$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>69 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data-driven Optimal Computing Budget Allocation under Input Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.11809v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.11809v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuhao Wang, Enlu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a fixed budget ranking and Selection (R&S) problem, one aims to identify
the best design among a finite number of candidates by efficiently allocating
the given simulation budget to evaluate design performance. Classical methods
for fixed budget R&S usually assume known input distributions, which are the
distributions that model the randomness in the system and drive the simulation.
In this paper, we consider the practical scenario where the input distribution
is unknown but can be estimated from streaming input data that arrive in
batches over time. We model the R&S problem in this dynamic setting as a
multi-stage problem, where the input distribution estimate is updated at each
stage and a stage-wise optimization problem is formulated to allocate the
simulation budget. We characterize the optimality conditions for the stage-wise
budget allocation problem by applying the large deviations theory to maximize
the decay rate of the probability of false selection. Based on the optimality
conditions and combined with the updating of input distribution estimates, we
design two sequential budget allocation procedures for R&S under streaming
input data. We theoretically guarantee the consistency and asymptotic
optimality of the two proposed procedures. We also demonstrate the practical
efficiency through numerical experiments in comparison with the equal
allocation policy and two extensions of the Optimal Computing Budget Allocation
(OCBA) algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ End-of-Horizon Load Balancing Problems: Algorithms and Insights 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01968v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01968v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Freund, Chamsi Hssaine, Jiayu Kamessi Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective load balancing is at the heart of many applications in operations.
Often tackled via the balls-into-bins paradigm, seminal results have shown that
a limited amount of flexibility goes a long way in order to maintain
(approximately) balanced loads throughout the decision-making horizon. This
paper is motivated by the fact that balance across time is too stringent a
requirement for some applications; rather, the only desideratum is approximate
balance at the end of the horizon. In this work we design
``limited-flexibility'' algorithms for three instantiations of the
end-of-horizon balance problem: the balls-into-bins problem, opaque selling
strategies for inventory management, and parcel delivery for e-commerce
fulfillment. For the balls-into-bins model, we show that a simple policy which
begins exerting flexibility toward the end of the time horizon (i.e., when
$\Theta\left(\sqrt{T\log T}\right)$ periods remain), suffices to achieve an
approximately balanced load (i.e., a maximum load within ${O}(1)$ of the
average load). Moreover, with just a small amount of adaptivity, a threshold
policy achieves the same result, while only exerting flexibility in
${O}\left(\sqrt{T}\right)$ periods, matching a natural lower bound. We then
adapt these algorithms to develop order-wise optimal policies for the opaque
selling problem. Finally, we show via a data-driven case study that the
adaptive policy designed for the balls-into-bins model can be modified to (i)
achieve approximate balance at the end of the horizon and (ii) yield
significant cost savings relative to policies which either never exert
flexibility, or exert flexibility aggressively enough to achieve anytime
balance. The unifying motivation behind our algorithms is the observation that
exerting flexibility at the beginning of the horizon is likely wasted when
system balance is only evaluated at the end.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cyclic Coordinate Dual Averaging with Extrapolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.13244v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.13244v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaobing Song, Jelena Diakonikolas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cyclic block coordinate methods are a fundamental class of optimization
methods widely used in practice and implemented as part of standard software
packages for statistical learning. Nevertheless, their convergence is generally
not well understood and so far their good practical performance has not been
explained by existing convergence analyses. In this work, we introduce a new
block coordinate method that applies to the general class of variational
inequality (VI) problems with monotone operators. This class includes composite
convex optimization problems and convex-concave min-max optimization problems
as special cases and has not been addressed by the existing work. The resulting
convergence bounds match the optimal convergence bounds of full gradient
methods, but are provided in terms of a novel gradient Lipschitz condition
w.r.t.~a Mahalanobis norm. For $m$ coordinate blocks, the resulting gradient
Lipschitz constant in our bounds is never larger than a factor $\sqrt{m}$
compared to the traditional Euclidean Lipschitz constant, while it is possible
for it to be much smaller. Further, for the case when the operator in the VI
has finite-sum structure, we propose a variance reduced variant of our method
which further decreases the per-iteration cost and has better convergence rates
in certain regimes. To obtain these results, we use a gradient extrapolation
strategy that allows us to view a cyclic collection of block coordinate-wise
gradients as one implicit gradient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 2 figures. Accepted to SIAM Journal on Optimization.
  Version prior to final copy editing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Branch-and-Bound Performance Estimation Programming: A Unified
  Methodology for Constructing Optimal Optimization Methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.07305v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.07305v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuvomoy Das Gupta, Bart P. G. Van Parys, Ernest K. Ryu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present the Branch-and-Bound Performance Estimation Programming (BnB-PEP),
a unified methodology for constructing optimal first-order methods for convex
and nonconvex optimization. BnB-PEP poses the problem of finding the optimal
optimization method as a nonconvex but practically tractable quadratically
constrained quadratic optimization problem and solves it to certifiable global
optimality using a customized branch-and-bound algorithm. By directly
confronting the nonconvexity, BnB-PEP offers significantly more flexibility and
removes the many limitations of the prior methodologies. Our customized
branch-and-bound algorithm, through exploiting specific problem structures,
outperforms the latest off-the-shelf implementations by orders of magnitude,
accelerating the solution time from hours to seconds and weeks to minutes. We
apply BnB-PEP to several setups for which the prior methodologies do not apply
and obtain methods with bounds that improve upon prior state-of-the-art
results. Finally, we use the BnB-PEP methodology to find proofs with potential
function structures, thereby systematically generating analytical convergence
proofs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Mathematical Programming Series A</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Theoretical Analysis of Optimistic Proximal Policy Optimization in
  Linear Markov Decision Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhong, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proximal policy optimization (PPO) algorithm stands as one of the most
prosperous methods in the field of reinforcement learning (RL). Despite its
success, the theoretical understanding of PPO remains deficient. Specifically,
it is unclear whether PPO or its optimistic variants can effectively solve
linear Markov decision processes (MDPs), which are arguably the simplest models
in RL with function approximation. To bridge this gap, we propose an optimistic
variant of PPO for episodic adversarial linear MDPs with full-information
feedback, and establish a $\tilde{\mathcal{O}}(d^{3/4}H^2K^{3/4})$ regret for
it. Here $d$ is the ambient dimension of linear MDPs, $H$ is the length of each
episode, and $K$ is the number of episodes. Compared with existing policy-based
algorithms, we achieve the state-of-the-art regret bound in both stochastic
linear MDPs and adversarial linear MDPs with full information. Additionally,
our algorithm design features a novel multi-batched updating mechanism and the
theoretical analysis utilizes a new covering number argument of value and
policy classes, which might be of independent interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the optimal control of kinetic epidemic models with uncertain social
  features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09201v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09201v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Franceschi, Andrea Medaglia, Mattia Zanella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is recognized that social heterogeneities in terms of the contact
distribution have a strong influence on the spread of infectious diseases.
Nevertheless, few data are available on the group composition of social
contacts, and their statistical description does not possess universal patterns
and may vary spatially and temporally. It is therefore essential to design
robust control strategies, mimicking the effects of non-pharmaceutical
interventions, to limit efficiently the number of infected cases. In this work,
starting from a recently introduced kinetic model for epidemiological dynamics
that takes into account the impact of social contacts of individuals, we
consider an uncertain contact formation dynamics leading to slim-tailed as well
as fat-tailed distributions of contacts. Hence, we analyse the effects of an
optimally robust control strategy of the system of agents. Thanks to classical
methods of kinetic theory, we couple uncertainty quantification methods with
the introduced mathematical model to assess the effects of social limitations.
Finally, using the proposed modelling approach and starting from available
data, we show the effectiveness of the proposed selective measures to dampen
uncertainties together with the epidemic trends.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Semismoothness for Solution Operators of Obstacle-Type Variational
  Inequalities with Applications in Optimal Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.12018v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.12018v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constantin Christof, Gerd Wachsmuth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We prove that solution operators of elliptic obstacle-type variational
inequalities (or, more generally, locally Lipschitz continuous functions
possessing certain pointwise-a.e. convexity properties) are Newton
differentiable when considered as maps between suitable Lebesgue spaces and
equipped with the strong-weak Bouligand differential as a generalized
set-valued derivative. It is shown that this Newton differentiability allows to
solve optimal control problems with H1-cost terms and one-sided pointwise
control constraints by means of a semismooth Newton method. The superlinear
convergence of the resulting algorithm is proved in the infinite-dimensional
setting and its mesh independence is demonstrated in numerical experiments. We
expect that the findings of this paper are also helpful for the design of
numerical solution procedures for quasi-variational inequalities and the
optimal control of obstacle-type variational problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Minor revisions. Published in SIAM Journal on Control and
  Optimization, Vol. 61, Iss. 3 (2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Efficient sample selection for safe learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.14104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.14104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta Zagorowska, Efe C. Balta, Varsha Behrunani, Alisa Rupenyan, John Lygeros
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ensuring safety in industrial control systems usually involves imposing
constraints at the design stage of the control algorithm. Enforcing constraints
is challenging if the underlying functional form is unknown. The challenge can
be addressed by using surrogate models, such as Gaussian processes, which
provide confidence intervals used to find solutions that can be considered
safe. This in turn involves an exhaustive search on the entire search space.
That approach can quickly become computationally expensive. We reformulate the
exhaustive search as a series of optimization problems to find the next
recommended points. We show that the proposed reformulation allows using a wide
range of available optimization solvers, such as derivative-free methods. We
show that by exploiting the properties of the solver, we enable the
introduction of new stopping criteria into safe learning methods and increase
flexibility in trading off solver accuracy and computational time. The results
from a non-convex optimization problem and an application for controller tuning
confirm the flexibility and the performance of the proposed reformulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Automatic Scenario Generation for Robust Optimal Control Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.14145v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.14145v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marta Zagorowska, Paola Falugi, Edward O'Dwyer, Eric C. Kerrigan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing methods for nonlinear robust control often use scenario-based
approaches to formulate the control problem as nonlinear optimization problems.
Increasing the number of scenarios improves robustness, while increasing the
size of the optimization problems. Mitigating the size of the problem by
reducing the number of scenarios requires knowledge about how the uncertainty
affects the system. This paper draws from local reduction methods used in
semi-infinite optimization to solve robust optimal control problems with
parametric uncertainty. We show that nonlinear robust optimal control problems
are equivalent to semi-infinite optimization problems and can be solved by
local reduction. By iteratively adding interim globally worst-case scenarios to
the problem, methods based on local reduction provide a way to manage the total
number of scenarios. In particular, we show that local reduction methods find
worst case scenarios that are not on the boundary of the uncertainty set. The
proposed approach is illustrated with a case study with both parametric and
additive time-varying uncertainty. The number of scenarios obtained from local
reduction is 101, smaller than in the case when all $2^{14+3\times192}$
boundary scenarios are considered. A validation with randomly drawn scenarios
shows that our proposed approach reduces the number of scenarios and ensures
robustness even if local solvers are used.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Primal Dual Hybrid Gradient Algorithm with Adaptive
  Step-Sizes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02511v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02511v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonin Chambolle, Claire Delplancke, Matthias J. Ehrhardt, Carola-Bibiane Schönlieb, Junqi Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we propose a new primal-dual algorithm with adaptive step-sizes.
The stochastic primal-dual hybrid gradient (SPDHG) algorithm with constant
step-sizes has become widely applied in large-scale convex optimization across
many scientific fields due to its scalability. While the product of the primal
and dual step-sizes is subject to an upper-bound in order to ensure
convergence, the selection of the ratio of the step-sizes is critical in
applications. Up-to-now there is no systematic and successful way of selecting
the primal and dual step-sizes for SPDHG. In this work, we propose a general
class of adaptive SPDHG (A-SPDHG) algorithms, and prove their convergence under
weak assumptions. We also propose concrete parameters-updating strategies which
satisfy the assumptions of our theory and thereby lead to convergent
algorithms. Numerical examples on computed tomography demonstrate the
effectiveness of the proposed schemes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regret Bounds for Markov Decision Processes with Recursive Optimized
  Certainty Equivalents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12601v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12601v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Xu, Xuefeng Gao, Xuedong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The optimized certainty equivalent (OCE) is a family of risk measures that
cover important examples such as entropic risk, conditional value-at-risk and
mean-variance models. In this paper, we propose a new episodic risk-sensitive
reinforcement learning formulation based on tabular Markov decision processes
with recursive OCEs. We design an efficient learning algorithm for this problem
based on value iteration and upper confidence bound. We derive an upper bound
on the regret of the proposed algorithm, and also establish a minimax lower
bound. Our bounds show that the regret rate achieved by our proposed algorithm
has optimal dependence on the number of episodes and the number of actions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attentional-Biased Stochastic Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.06951v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.06951v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Qi, Yi Xu, Rong Jin, Wotao Yin, Tianbao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a simple yet effective provable method (named
ABSGD) for addressing the data imbalance or label noise problem in deep
learning. Our method is a simple modification to momentum SGD where we assign
an individual importance weight to each sample in the mini-batch. The
individual-level weight of sampled data is systematically proportional to the
exponential of a scaled loss value of the data, where the scaling factor is
interpreted as the regularization parameter in the framework of
distributionally robust optimization (DRO). Depending on whether the scaling
factor is positive or negative, ABSGD is guaranteed to converge to a stationary
point of an information-regularized min-max or min-min DRO problem,
respectively. Compared with existing class-level weighting schemes, our method
can capture the diversity between individual examples within each class.
Compared with existing individual-level weighting methods using meta-learning
that require three backward propagations for computing mini-batch stochastic
gradients, our method is more efficient with only one backward propagation at
each iteration as in standard deep learning methods. ABSGD is flexible enough
to combine with other robust losses without any additional cost. Our empirical
studies on several benchmark datasets demonstrate the effectiveness of the
proposed method.\footnote{Code is available
at:\url{https://github.com/qiqi-helloworld/ABSGD/}}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximate Newton policy gradient algorithms <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.02398v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.02398v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoya Li, Samarth Gupta, Hsiangfu Yu, Lexing Ying, Inderjit Dhillon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policy gradient algorithms have been widely applied to Markov decision
processes and reinforcement learning problems in recent years. Regularization
with various entropy functions is often used to encourage exploration and
improve stability. This paper proposes an approximate Newton method for the
policy gradient algorithm with entropy regularization. In the case of Shannon
entropy, the resulting algorithm reproduces the natural policy gradient
algorithm. For other entropy functions, this method results in brand-new policy
gradient algorithms. We prove that all these algorithms enjoy Newton-type
quadratic convergence and that the corresponding gradient flow converges
globally to the optimal solution. We use synthetic and industrial-scale
examples to demonstrate that the proposed approximate Newton method typically
converges in single-digit iterations, often orders of magnitude faster than
other state-of-the-art algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 15 figures, v6 accepted by SIAM SISC</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Framework for Analyzing and Optimizing a Class of Convex
  Inequity Measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13427v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13427v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Man Yiu Tsang, Karmel S. Shehadeh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new unified framework for analyzing a new parameterized class of
convex inequity measures suitable for optimization contexts. First, we propose
a new class of order-based inequity measures, discuss their properties, and
derive axiomatic characterizations for such measures. Then, we introduce our
proposed class of convex inequity measures, discuss their theoretical
properties in an absolute and relative sense, and derive an equivalent dual
representation of these measures as a robustified order-based inequity measure
over their dual sets. Importantly, this dual representation renders a unified
mathematical expression and an alternative geometric characterization for
convex inequity measures through their dual sets. Using this representation, we
propose a unified framework for optimization problems with a convex inequity
measure objective or constraint, including reformulations and solution methods.
Finally, we provide stability results that quantify the impact of employing
different convex inequity measures on the optimal value and solution of the
resulting optimization problem. Our numerical results demonstrate the
computational efficiency of our proposed framework over existing approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Certification of Bottleneck Task Assignment with Shortest Path Criteria 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12724v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12724v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tony A. Wood, Maryam Kamgarpour
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Minimising the longest travel distance for a group of mobile robots with
interchangeable goals requires knowledge of the shortest length paths between
all robots and goal destinations. Determining the exact length of the shortest
paths in an environment with obstacles is NP-hard however. In this paper, we
investigate when polynomial-time approximations of the shortest path search are
sufficient to determine the optimal assignment of robots to goals. In
particular, we propose an algorithm in which the accuracy of the path planning
is iteratively increased. The approach provides a certificate when the
uncertainties on estimates of the shortest paths become small enough to
guarantee the optimality of the goal assignment. To this end, we apply results
from assignment sensitivity assuming upper and lower bounds on the length of
the shortest paths. We then provide polynomial-time methods to find such bounds
by applying sampling-based path planning. The upper bounds are given by
feasible paths, the lower bounds are obtained by expanding the sample set and
leveraging the knowledge of the sample dispersion. We demonstrate the
application of the proposed method with a multi-robot path-planning case study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning with symmetric positive definite matrices via generalized
  Bures-Wasserstein geometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.10464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.10464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andi Han, Bamdev Mishra, Pratik Jawanpuria, Junbin Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning with symmetric positive definite (SPD) matrices has many
applications in machine learning. Consequently, understanding the Riemannian
geometry of SPD matrices has attracted much attention lately. A particular
Riemannian geometry of interest is the recently proposed Bures-Wasserstein (BW)
geometry which builds on the Wasserstein distance between the Gaussian
densities. In this paper, we propose a novel generalization of the BW geometry,
which we call the GBW geometry. The proposed generalization is parameterized by
a symmetric positive definite matrix $\mathbf{M}$ such that when $\mathbf{M} =
\mathbf{I}$, we recover the BW geometry. We provide a rigorous treatment to
study various differential geometric notions on the proposed novel generalized
geometry which makes it amenable to various machine learning applications. We
also present experiments that illustrate the efficacy of the proposed GBW
geometry over the BW geometry.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Negative curvature obstructs acceleration for strongly geodesically
  convex optimization, even with exact first-order oracles <span class="chip">COLT 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.13263v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.13263v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Criscitiello, Nicolas Boumal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hamilton and Moitra (2021) showed that, in certain regimes, it is not
possible to accelerate Riemannian gradient descent in the hyperbolic plane if
we restrict ourselves to algorithms which make queries in a (large) bounded
domain and which receive gradients and function values corrupted by a (small)
amount of noise. We show that acceleration remains unachievable for any
deterministic algorithm which receives exact gradient and function-value
information (unbounded queries, no noise). Our results hold for the classes of
strongly and nonstrongly geodesically convex functions, and for a large class
of Hadamard manifolds including hyperbolic spaces and the symmetric space
$\mathrm{SL}(n) / \mathrm{SO}(n)$ of positive definite $n \times n$ matrices of
determinant one. This cements a surprising gap between the complexity of convex
optimization and geodesically convex optimization: for hyperbolic spaces,
Riemannian gradient descent is optimal on the class of smooth and and strongly
geodesically convex functions, in the regime where the condition number scales
with the radius of the optimization domain. The key idea for proving the lower
bound consists of perturbing the hard functions of Hamilton and Moitra (2021)
with sums of bump functions chosen by a resisting oracle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>v2 to v3: Updated and shortened to reflect COLT 2022 version. Results
  on nonstrongly g-convex case (former Sec. 5) and reduction to Euclidean
  convexity (former Sec. 6) are now in Sec. 3 and App. D of "Curvature and
  Complexity: Better lower bounds for geodesically convex optimization", COLT
  2023 (arxiv.org/abs/2306.02959). v3 to v4: Added word "strongly" to title to
  match COLT 2022 version; Proceedings of Thirty Fifth Conference on Learning
  Theory, PMLR 178:496-542, 2022,
  https://proceedings.mlr.press/v178/criscitiello22a</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Kullback-Leibler-Quadratic Optimal Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2004.01798v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2004.01798v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Neil Cammardella, Ana Bušić, Sean Meyn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents approaches to mean-field control, motivated by
distributed control of multi-agent systems. Control solutions are based on a
convex optimization problem, whose domain is a convex set of probability mass
functions (pmfs). The main contributions follow: 1. Kullback-Leibler-Quadratic
(KLQ) optimal control is a special case, in which the objective function is
composed of a control cost in the form of Kullback-Leibler divergence between a
candidate pmf and the nominal, plus a quadratic cost on the sequence of
marginals. Theory in this paper extends prior work on deterministic control
systems, establishing that the optimal solution is an exponential tilting of
the nominal pmf. Transform techniques are introduced to reduce complexity of
the KLQ solution, motivated by the need to consider time horizons that are much
longer than the inter-sampling times required for reliable control. 2.
Infinite-horizon KLQ leads to a state feedback control solution with attractive
properties. It can be expressed as either state feedback, in which the state is
the sequence of marginal pmfs, or an open loop solution is obtained that is
more easily computed. 3. Numerical experiments are surveyed in an application
of distributed control of residential loads to provide grid services, similar
to utility-scale battery storage. The results show that KLQ optimal control
enables the aggregate power consumption of a collection of flexible loads to
track a time-varying reference signal, while simultaneously ensuring each
individual load satisfies its own quality of service constraints. Keywords:
Mean field games, distributed control, Markov decision processes, Demand
Dispatch.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended version of paper to appear in SIAM Journal on Control and
  Optimization (SICON)</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SequenceMatch: Imitation Learning for Autoregressive Sequence Modelling
  with Backtracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05426v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05426v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chris Cundy, Stefano Ermon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many domains, autoregressive models can achieve low log-likelihood on the
task of predicting the next observation. However, this maximum-likelihood (MLE)
objective does not necessarily match a downstream use-case of autoregressively
generating high-quality sequences. The MLE objective weights sequences
proportionally to their frequency under the data distribution, with no guidance
for the model's behaviour out of distribution (OOD): leading to compounding
error during autoregressive generation. In order to address this compounding
error problem, we formulate sequence generation as an imitation learning (IL)
problem. This allows us to minimize a variety of divergences between the
distribution of sequences generated by an autoregressive model and sequences
from a dataset, including divergences with weight on OOD generated sequences.
The IL framework also allows us to incorporate backtracking by introducing a
backspace action into the generation process. This further mitigates the
compounding error problem by allowing the model to revert a sampled token if it
takes the sequence OOD. Our resulting method, SequenceMatch, can be implemented
without adversarial training or major architectural changes. We identify the
SequenceMatch-$\chi^2$ divergence as a more suitable training objective for
autoregressive models which are used for generation. We show that empirically,
SequenceMatch training leads to improvements over MLE on text generation with
language models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scaling Spherical CNNs <span class="chip">ICML'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carlos Esteves, Jean-Jacques Slotine, Ameesh Makadia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spherical CNNs generalize CNNs to functions on the sphere, by using spherical
convolutions as the main linear operation. The most accurate and efficient way
to compute spherical convolutions is in the spectral domain (via the
convolution theorem), which is still costlier than the usual planar
convolutions. For this reason, applications of spherical CNNs have so far been
limited to small problems that can be approached with low model capacity. In
this work, we show how spherical CNNs can be scaled for much larger problems.
To achieve this, we make critical improvements including novel variants of
common model components, an implementation of core operations to exploit
hardware accelerator characteristics, and application-specific input
representations that exploit the properties of our model. Experiments show our
larger spherical CNNs reach state-of-the-art on several targets of the QM9
molecular benchmark, which was previously dominated by equivariant graph neural
networks, and achieve competitive performance on multiple weather forecasting
tasks. Our code is available at
https://github.com/google-research/spherical-cnn.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TopoMask: Instance-Mask-Based Formulation for the Road Topology Problem
  via <span class="highlight-title">Transformer</span>-Based Architecture 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05419v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05419v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        M. Esat Kalfaoglu, Halil Ibrahim Ozturk, Ozsel Kilinc, Alptekin Temizel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Driving scene understanding task involves detecting static elements such as
lanes, traffic signs, and traffic lights, and their relationships with each
other. To facilitate the development of comprehensive scene understanding
solutions using multiple camera views, a new dataset called Road Genome
(OpenLane-V2) has been released. This dataset allows for the exploration of
complex road connections and situations where lane markings may be absent.
Instead of using traditional lane markings, the lanes in this dataset are
represented by centerlines, which offer a more suitable representation of lanes
and their connections. In this study, we have introduced a new approach called
TopoMask for predicting centerlines in road topology. Unlike existing
approaches in the literature that rely on keypoints or parametric methods,
TopoMask utilizes an instance-mask based formulation with a transformer-based
architecture and, in order to enrich the mask instances with flow information,
a direction label representation is proposed. TopoMask have ranked 4th in the
OpenLane-V2 Score (OLS) and ranked 2nd in the F1 score of centerline prediction
in OpenLane Topology Challenge 2023. In comparison to the current
state-of-the-art method, TopoNet, the proposed method has achieved similar
performance in Frechet-based lane detection and outperformed TopoNet in
Chamfer-based lane detection without utilizing its scene graph neural network.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4th in OLS and 2nd in the F1-score in OpenLane Topology Challenge
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal normalizing flows: from theory to practice 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Adrián Javaloy, Pablo Sánchez-Martín, Isabel Valera
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we deepen on the use of normalizing flows for causal reasoning.
Specifically, we first leverage recent results on non-linear ICA to show that
causal models are identifiable from observational data given a causal ordering,
and thus can be recovered using autoregressive normalizing flows (NFs). Second,
we analyze different design and learning choices for causal normalizing flows
to capture the underlying causal data-generating process. Third, we describe
how to implement the do-operator in causal NFs, and thus, how to answer
interventional and counterfactual questions. Finally, in our experiments, we
validate our design and training choices through a comprehensive ablation
study; compare causal NFs to other approaches for approximating causal models;
and empirically demonstrate that causal NFs can be used to address real-world
problems, where the presence of mixed discrete-continuous data and partial
knowledge on the causal graph is the norm. The code for this work can be found
at https://github.com/psanch21/causal-flows.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 15 figures. Under submission</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Offline Prioritized Experience Replay 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05412v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05412v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Yue, Bingyi Kang, Xiao Ma, Gao Huang, Shiji Song, Shuicheng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL) is challenged by the distributional shift
problem. To address this problem, existing works mainly focus on designing
sophisticated policy constraints between the learned policy and the behavior
policy. However, these constraints are applied equally to well-performing and
inferior actions through uniform sampling, which might negatively affect the
learned policy. To alleviate this issue, we propose Offline Prioritized
Experience Replay (OPER), featuring a class of priority functions designed to
prioritize highly-rewarding transitions, making them more frequently visited
during training. Through theoretical analysis, we show that this class of
priority functions induce an improved behavior policy, and when constrained to
this improved policy, a policy-constrained offline RL algorithm is likely to
yield a better solution. We develop two practical strategies to obtain priority
weights by estimating advantages based on a fitted value network (OPER-A) or
utilizing trajectory returns (OPER-R) for quick computation. OPER is a
plug-and-play component for offline RL algorithms. As case studies, we evaluate
OPER on five different algorithms, including BC, TD3+BC, Onestep RL, CQL, and
IQL. Extensive experiments demonstrate that both OPER-A and OPER-R
significantly improve the performance for all baseline methods. Codes and
priority weights are availiable at https://github.com/sail-sg/OPER.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RDumb: A simple approach that questions our progress in continual
  test-time adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ori Press, Steffen Schneider, Matthias Kümmerer, Matthias Bethge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-Time Adaptation (TTA) allows to update pretrained models to changing
data distributions at deployment time. While early work tested these algorithms
for individual fixed distribution shifts, recent work proposed and applied
methods for continual adaptation over long timescales. To examine the reported
progress in the field, we propose the Continuously Changing Corruptions (CCC)
benchmark to measure asymptotic performance of TTA techniques. We find that
eventually all but one state-of-the-art methods collapse and perform worse than
a non-adapting model, including models specifically proposed to be robust to
performance collapse. In addition, we introduce a simple baseline, "RDumb",
that periodically resets the model to its pretrained state. RDumb performs
better or on par with the previously proposed state-of-the-art in all
considered benchmarks. Our results show that previous TTA approaches are
neither effective at regularizing adaptation to avoid collapse nor able to
outperform a simplistic resetting strategy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ordinal Potential-based Player Rating 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nelson Vadori, Rahul Savani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A two-player symmetric zero-sum game is transitive if for any pure strategies
$x$, $y$, $z$, if $x$ is better than $y$, and $y$ is better than $z$, then $x$
is better than $z$. It was recently observed that the Elo rating fails at
preserving transitive relations among strategies and therefore cannot correctly
extract the transitive component of a game. Our first contribution is to show
that the Elo rating actually does preserve transitivity when computed in the
right space. Precisely, using a suitable invertible mapping $\varphi$, we first
apply $\varphi$ to the game, then compute Elo ratings, then go back to the
original space by applying $\varphi^{-1}$. We provide a characterization of
transitive games as a weak variant of ordinal potential games with additively
separable potential functions. Leveraging this insight, we introduce the
concept of transitivity order, the minimum number of invertible mappings
required to transform the payoff of a transitive game into (differences of) its
potential function. The transitivity order is a tool to classify transitive
games, with Elo games being an example of transitive games of order one. Most
real-world games have both transitive and non-transitive (cyclic) components,
and we use our analysis of transitivity to extract the transitive (potential)
component of an arbitrary game. We link transitivity to the known concept of
sign-rank: transitive games have sign-rank two; arbitrary games may have higher
sign-rank. Using a neural network-based architecture, we learn a decomposition
of an arbitrary game into transitive and cyclic components that prioritises
capturing the sign pattern of the game. In particular, a transitive game always
has just one component in its decomposition, the potential component. We
provide a comprehensive evaluation of our methodology using both toy examples
and empirical data from real-world games.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subject clustering by IF-PCA and several recent methods 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05363v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05363v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dieyi Chen, Jiashun Jin, Zheng Tracy Ke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subject clustering (i.e., the use of measured features to cluster subjects,
such as patients or cells, into multiple groups) is a problem of great
interest. In recent years, many approaches were proposed, among which
unsupervised deep learning (UDL) has received a great deal of attention. Two
interesting questions are (a) how to combine the strengths of UDL and other
approaches, and (b) how these approaches compare to one other.
  We combine Variational Auto-Encoder (VAE), a popular UDL approach, with the
recent idea of Influential Feature PCA (IF-PCA), and propose IF-VAE as a new
method for subject clustering. We study IF-VAE and compare it with several
other methods (including IF-PCA, VAE, Seurat, and SC3) on $10$ gene microarray
data sets and $8$ single-cell RNA-seq data sets. We find that IF-VAE
significantly improves over VAE, but still underperforms IF-PCA. We also find
that IF-PCA is quite competitive, which slightly outperforms Seurat and SC3
over the $8$ single-cell data sets. IF-PCA is conceptually simple and permits
delicate analysis. We demonstrate that IF-PCA is capable of achieving the phase
transition in a Rare/Weak model. Comparatively, Seurat and SC3 are more complex
and theoretically difficult to analyze (for these reasons, their optimality
remains unclear).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Compositional Concepts Discovery with Text-to-Image
  Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Liu, Yilun Du, Shuang Li, Joshua B. Tenenbaum, Antonio Torralba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text-to-image generative models have enabled high-resolution image synthesis
across different domains, but require users to specify the content they wish to
generate. In this paper, we consider the inverse problem -- given a collection
of different images, can we discover the generative concepts that represent
each image? We present an unsupervised approach to discover generative concepts
from a collection of images, disentangling different art styles in paintings,
objects, and lighting from kitchen scenes, and discovering image classes given
ImageNet images. We show how such generative concepts can accurately represent
the content of images, be recombined and composed to generate new artistic and
hybrid images, and be further used as a representation for downstream
classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project Webpage:
  https://energy-based-model.github.io/unsupervised-concept-discovery/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Crystal-Specific <span class="highlight-title">Pre-Train</span>ing Framework for Crystal Material Property
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05344v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05344v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haomin Yu, Yanru Song, Jilin Hu, Chenjuan Guo, Bin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crystal property prediction is a crucial aspect of developing novel
materials. However, there are two technical challenges to be addressed for
speeding up the investigation of crystals. First, labeling crystal properties
is intrinsically difficult due to the high cost and time involved in physical
simulations or lab experiments. Second, crystals adhere to a specific quantum
chemical principle known as periodic invariance, which is often not captured by
existing machine learning methods. To overcome these challenges, we propose the
crystal-specific pre-training framework for learning crystal representations
with self-supervision. The framework designs a mutex mask strategy for
enhancing representation learning so as to alleviate the limited labels
available for crystal property prediction. Moreover, we take into account the
specific periodic invariance in crystal structures by developing a periodic
invariance multi-graph module and periodic attribute learning within our
framework. This framework has been tested on eight different tasks. The
experimental results on these tasks show that the framework achieves promising
prediction performance and is able to outperform recent strong baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Learning under Covariate Shifts with Generalization Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05325v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05325v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Ramezani-Kebrya, Fanghui Liu, Thomas Pethick, Grigorios Chrysos, Volkan Cevher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses intra-client and inter-client covariate shifts in
federated learning (FL) with a focus on the overall generalization performance.
To handle covariate shifts, we formulate a new global model training paradigm
and propose Federated Importance-Weighted Empirical Risk Minimization (FTW-ERM)
along with improving density ratio matching methods without requiring perfect
knowledge of the supremum over true ratios. We also propose the
communication-efficient variant FITW-ERM with the same level of privacy
guarantees as those of classical ERM in FL. We theoretically show that FTW-ERM
achieves smaller generalization error than classical ERM under certain
settings. Experimental results demonstrate the superiority of FTW-ERM over
existing FL baselines in challenging imbalanced federated settings in terms of
data distribution shifts across clients.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (TMLR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Advancing Italian Biomedical Information Extraction with Large Language
  Models: Methodological Insights and Multicenter Practical Application 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05323v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05323v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claudio Crema, Tommaso Mario Buonocore, Silvia Fostinelli, Enea Parimbelli, Federico Verde, Cira Fundarò, Marina Manera, Matteo Cotta Ramusino, Marco Capelli, Alfredo Costa, Giuliano Binetti, Riccardo Bellazzi, Alberto Redolfi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The introduction of computerized medical records in hospitals has reduced
burdensome operations like manual writing and information fetching. However,
the data contained in medical records are still far underutilized, primarily
because extracting them from unstructured textual medical records takes time
and effort. Information Extraction, a subfield of Natural Language Processing,
can help clinical practitioners overcome this limitation, using automated
text-mining pipelines. In this work, we created the first Italian
neuropsychiatric Named Entity Recognition dataset, PsyNIT, and used it to
develop a Large Language Model for this task. Moreover, we conducted several
experiments with three external independent datasets to implement an effective
multicenter model, with overall F1-score 84.77%, Precision 83.16%, Recall
86.44%. The lessons learned are: (i) the crucial role of a consistent
annotation process and (ii) a fine-tuning strategy that combines classical
methods with a "few-shot" approach. This allowed us to establish methodological
guidelines that pave the way for future implementations in this field and allow
Italian hospitals to tap into important research opportunities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Real-time whole-heart electromechanical simulations using Latent Neural
  Ordinary Differential Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05321v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05321v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Salvador, Marina Strocchi, Francesco Regazzoni, Luca Dede', Steven Niederer, Alfio Quarteroni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cardiac digital twins provide a physics and physiology informed framework to
deliver predictive and personalized medicine. However, high-fidelity
multi-scale cardiac models remain a barrier to adoption due to their extensive
computational costs and the high number of model evaluations needed for
patient-specific personalization. Artificial Intelligence-based methods can
make the creation of fast and accurate whole-heart digital twins feasible. In
this work, we use Latent Neural Ordinary Differential Equations (LNODEs) to
learn the temporal pressure-volume dynamics of a heart failure patient. Our
surrogate model based on LNODEs is trained from 400 3D-0D whole-heart
closed-loop electromechanical simulations while accounting for 43 model
parameters, describing single cell through to whole organ and cardiovascular
hemodynamics. The trained LNODEs provides a compact and efficient
representation of the 3D-0D model in a latent space by means of a feedforward
fully-connected Artificial Neural Network that retains 3 hidden layers with 13
neurons per layer and allows for 300x real-time numerical simulations of the
cardiac function on a single processor of a standard laptop. This surrogate
model is employed to perform global sensitivity analysis and robust parameter
estimation with uncertainty quantification in 3 hours of computations, still on
a single processor. We match pressure and volume time traces unseen by the
LNODEs during the training phase and we calibrate 4 to 11 model parameters
while also providing their posterior distribution. This paper introduces the
most advanced surrogate model of cardiac function available in the literature
and opens new important venues for parameter calibration in cardiac digital
twins.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RNN-Based GNSS Positioning using Satellite Measurement Features and
  Pseudorange Residuals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ibrahim Sbeity, Christophe Villien, Benoît Denis, E. Veronica Belmega
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the Global Navigation Satellite System (GNSS) context, the growing number
of available satellites has lead to many challenges when it comes to choosing
the most accurate pseudorange contributions, given the strong impact of biased
measurements on positioning accuracy, particularly in single-epoch scenarios.
This work leverages the potential of machine learning in predicting link-wise
measurement quality factors and, hence, optimize measurement weighting. For
this purpose, we use a customized matrix composed of heterogeneous features
such as conditional pseudorange residuals and per-link satellite metrics (e.g.,
carrier-to-noise power density ratio and its empirical statistics, satellite
elevation, carrier phase lock time). This matrix is then fed as an input to a
recurrent neural network (RNN) (i.e., a long-short term memory (LSTM) network).
Our experimental results on real data, obtained from extensive field
measurements, demonstrate the high potential of our proposed solution being
able to outperform traditional measurements weighting and selection strategies
from state-of-the-art.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A framework for dynamically training and adapting deep reinforcement
  learning models to different, low-compute, and continuously changing
  radiology deployment environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05310v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05310v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangyao Zheng, Shuhao Lai, Vladimir Braverman, Michael A. Jacobs, Vishwa S. Parekh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Deep Reinforcement Learning has been widely researched in medical
imaging, the training and deployment of these models usually require powerful
GPUs. Since imaging environments evolve rapidly and can be generated by edge
devices, the algorithm is required to continually learn and adapt to changing
environments, and adjust to low-compute devices. To this end, we developed
three image coreset algorithms to compress and denoise medical images for
selective experience replayed-based lifelong reinforcement learning. We
implemented neighborhood averaging coreset, neighborhood sensitivity-based
sampling coreset, and maximum entropy coreset on full-body DIXON water and
DIXON fat MRI images. All three coresets produced 27x compression with
excellent performance in localizing five anatomical landmarks: left knee, right
trochanter, left kidney, spleen, and lung across both imaging environments.
Maximum entropy coreset obtained the best performance of $11.97\pm 12.02$
average distance error, compared to the conventional lifelong learning
framework's $19.24\pm 50.77$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Are fairness metric scores enough to assess discrimination biases in
  machine learning? <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fanny Jourdan, Laurent Risser, Jean-Michel Loubes, Nicholas Asher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents novel experiments shedding light on the shortcomings of
current metrics for assessing biases of gender discrimination made by machine
learning algorithms on textual data. We focus on the Bios dataset, and our
learning task is to predict the occupation of individuals, based on their
biography. Such prediction tasks are common in commercial Natural Language
Processing (NLP) applications such as automatic job recommendations. We address
an important limitation of theoretical discussions dealing with group-wise
fairness metrics: they focus on large datasets, although the norm in many
industrial NLP applications is to use small to reasonably large linguistic
datasets for which the main practical constraint is to get a good prediction
accuracy. We then question how reliable are different popular measures of bias
when the size of the training set is simply sufficient to learn reasonably
accurate predictions. Our experiments sample the Bios dataset and learn more
than 200 models on different sample sizes. This allows us to statistically
study our results and to confirm that common gender bias indices provide
diverging and sometimes unreliable results when applied to relatively small
training and test samples. This highlights the crucial importance of variance
calculations for providing sound results in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at Third Workshop on Trustworthy Natural
  Language Processing, ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Optimisation of Functions on Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05304v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05304v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingchen Wan, Pierre Osselin, Henry Kenlay, Binxin Ru, Michael A. Osborne, Xiaowen Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing availability of graph-structured data motivates the task of
optimising over functions defined on the node set of graphs. Traditional graph
search algorithms can be applied in this case, but they may be
sample-inefficient and do not make use of information about the function
values; on the other hand, Bayesian optimisation is a class of promising
black-box solvers with superior sample efficiency, but it has been scarcely
been applied to such novel setups. To fill this gap, we propose a novel
Bayesian optimisation framework that optimises over functions defined on
generic, large-scale and potentially unknown graphs. Through the learning of
suitable kernels on graphs, our framework has the advantage of adapting to the
behaviour of the target function. The local modelling approach further
guarantees the efficiency of our method. Extensive experiments on both
synthetic and real-world graphs demonstrate the effectiveness of the proposed
optimisation framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 9 figures, 1 table (23 pages, 24 figures, 1 table including
  references and appendices)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Correlated Noise in Epoch-Based Stochastic Gradient Descent:
  Implications for Weight Variances 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05300v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05300v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Kühn, Bernd Rosenow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic gradient descent (SGD) has become a cornerstone of neural network
optimization, yet the noise introduced by SGD is often assumed to be
uncorrelated over time, despite the ubiquity of epoch-based training. In this
work, we challenge this assumption and investigate the effects of epoch-based
noise correlations on the stationary distribution of discrete-time SGD with
momentum, limited to a quadratic loss. Our main contributions are twofold:
first, we calculate the exact autocorrelation of the noise for training in
epochs under the assumption that the noise is independent of small fluctuations
in the weight vector; second, we explore the influence of correlations
introduced by the epoch-based learning scheme on SGD dynamics. We find that for
directions with a curvature greater than a hyperparameter-dependent crossover
value, the results for uncorrelated noise are recovered. However, for
relatively flat directions, the weight variance is significantly reduced. We
provide an intuitive explanation for these results based on a crossover between
correlation times, contributing to a deeper understanding of the dynamics of
SGD in the presence of epoch-based noise correlations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Safe Collaborative Filtering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05292v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05292v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Riku Togashi, Tatsushi Oka, Naoto Ohsaka, Tetsuro Morimura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Excellent tail performance is crucial for modern machine learning tasks, such
as algorithmic fairness, class imbalance, and risk-sensitive decision making,
as it ensures the effective handling of challenging samples within a dataset.
Tail performance is also a vital determinant of success for personalised
recommender systems to reduce the risk of losing users with low satisfaction.
This study introduces a "safe" collaborative filtering method that prioritises
recommendation quality for less-satisfied users rather than focusing on the
average performance. Our approach minimises the conditional value at risk
(CVaR), which represents the average risk over the tails of users' loss. To
overcome computational challenges for web-scale recommender systems, we develop
a robust yet practical algorithm that extends the most scalable method,
implicit alternating least squares (iALS). Empirical evaluation on real-world
datasets demonstrates the excellent tail performance of our approach while
maintaining competitive computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simple and Controllable Music Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05284v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05284v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jade Copet, Felix Kreuk, Itai Gat, Tal Remez, David Kant, Gabriel Synnaeve, Yossi Adi, Alexandre Défossez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the task of conditional music generation. We introduce MusicGen, a
single Language Model (LM) that operates over several streams of compressed
discrete music representation, i.e., tokens. Unlike prior work, MusicGen is
comprised of a single-stage transformer LM together with efficient token
interleaving patterns, which eliminates the need for cascading several models,
e.g., hierarchically or upsampling. Following this approach, we demonstrate how
MusicGen can generate high-quality samples, while being conditioned on textual
description or melodic features, allowing better controls over the generated
output. We conduct extensive empirical evaluation, considering both automatic
and human studies, showing the proposed approach is superior to the evaluated
baselines on a standard text-to-music benchmark. Through ablation studies, we
shed light over the importance of each of the components comprising MusicGen.
Music samples, code, and models are available at
https://github.com/facebookresearch/audiocraft.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Linear Contextual Bandits with User-level Differential Privacy <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05275v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05275v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiquan Huang, Huanyu Zhang, Luca Melis, Milan Shen, Meisam Hajzinia, Jing Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies federated linear contextual bandits under the notion of
user-level differential privacy (DP). We first introduce a unified federated
bandits framework that can accommodate various definitions of DP in the
sequential decision-making setting. We then formally introduce user-level
central DP (CDP) and local DP (LDP) in the federated bandits framework, and
investigate the fundamental trade-offs between the learning regrets and the
corresponding DP guarantees in a federated linear contextual bandits model. For
CDP, we propose a federated algorithm termed as \robin and show that it is
near-optimal in terms of the number of clients $M$ and the privacy budget
$\varepsilon$ by deriving nearly-matching upper and lower regret bounds when
user-level DP is satisfied. For LDP, we obtain several lower bounds, indicating
that learning under user-level $(\varepsilon,\delta)$-LDP must suffer a regret
blow-up factor at least {$\min\{1/\varepsilon,M\}$ or
$\min\{1/\sqrt{\varepsilon},\sqrt{M}\}$} under different conditions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Clustering via the Principle of Rate Reduction in the Age of
  <span class="highlight-title">Pretrain</span>ed Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianzhe Chu, Shengbang Tong, Tianjiao Ding, Xili Dai, Benjamin David Haeffele, Rene Vidal, Yi Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The advent of large pre-trained models has brought about a paradigm shift in
both visual representation learning and natural language processing. However,
clustering unlabeled images, as a fundamental and classic machine learning
problem, still lacks effective solution, particularly for large-scale datasets.
In this paper, we propose a novel image clustering pipeline that leverages the
powerful feature representation of large pre-trained models such as CLIP and
cluster images effectively and efficiently at scale. We show that the
pre-trained features are significantly more structured by further optimizing
the rate reduction objective. The resulting features may significantly improve
the clustering accuracy, e.g., from 57\% to 66\% on ImageNet-1k. Furthermore,
by leveraging CLIP's image-text binding, we show how the new clustering method
leads to a simple yet effective self-labeling algorithm that successfully works
on unlabeled large datasets such as MS-COCO and LAION-Aesthetics. We will
release the code in https://github.com/LeslieTrue/CPP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Factorized Contrastive Learning: Going Beyond Multi-view Redundancy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Pu Liang, Zihao Deng, Martin Ma, James Zou, Louis-Philippe Morency, Ruslan Salakhutdinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a wide range of multimodal tasks, contrastive learning has become a
particularly appealing approach since it can successfully learn representations
from abundant unlabeled data with only pairing information (e.g., image-caption
or video-audio pairs). Underpinning these approaches is the assumption of
multi-view redundancy - that shared information between modalities is necessary
and sufficient for downstream tasks. However, in many real-world settings,
task-relevant information is also contained in modality-unique regions:
information that is only present in one modality but still relevant to the
task. How can we learn self-supervised multimodal representations to capture
both shared and unique information relevant to downstream tasks? This paper
proposes FactorCL, a new multimodal representation learning method to go beyond
multi-view redundancy. FactorCL is built from three new contributions: (1)
factorizing task-relevant information into shared and unique representations,
(2) capturing task-relevant information via maximizing MI lower bounds and
removing task-irrelevant information via minimizing MI upper bounds, and (3)
multimodal data augmentations to approximate task relevance without labels. On
large-scale real-world datasets, FactorCL captures both shared and unique
information and achieves state-of-the-art results on six benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at: https://github.com/pliang279/FactorCL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representing and Learning Functions Invariant Under Crystallographic
  Groups 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryan P. Adams, Peter Orbanz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Crystallographic groups describe the symmetries of crystals and other
repetitive structures encountered in nature and the sciences. These groups
include the wallpaper and space groups. We derive linear and nonlinear
representations of functions that are (1) smooth and (2) invariant under such a
group. The linear representation generalizes the Fourier basis to
crystallographically invariant basis functions. We show that such a basis
exists for each crystallographic group, that it is orthonormal in the relevant
$L_2$ space, and recover the standard Fourier basis as a special case for pure
shift groups. The nonlinear representation embeds the orbit space of the group
into a finite-dimensional Euclidean space. We show that such an embedding
exists for every crystallographic group, and that it factors functions through
a generalization of a manifold called an orbifold. We describe algorithms that,
given a standardized description of the group, compute the Fourier basis and an
embedding map. As examples, we construct crystallographically invariant neural
networks, kernel machines, and Gaussian processes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comprehensive evaluation of deep and graph learning on drug-drug
  interactions prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05257v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05257v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuan Lin, Lichang Dai, Yafang Zhou, Zu-Guo Yu, Wen Zhang, Jian-Yu Shi, Dong-Sheng Cao, Li Zeng, Haowen Chen, Bosheng Song, Philip S. Yu, Xiangxiang Zeng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances and achievements of artificial intelligence (AI) as well as
deep and graph learning models have established their usefulness in biomedical
applications, especially in drug-drug interactions (DDIs). DDIs refer to a
change in the effect of one drug to the presence of another drug in the human
body, which plays an essential role in drug discovery and clinical research.
DDIs prediction through traditional clinical trials and experiments is an
expensive and time-consuming process. To correctly apply the advanced AI and
deep learning, the developer and user meet various challenges such as the
availability and encoding of data resources, and the design of computational
methods. This review summarizes chemical structure based, network based, NLP
based and hybrid methods, providing an updated and accessible guide to the
broad researchers and development community with different domain knowledge. We
introduce widely-used molecular representation and describe the theoretical
frameworks of graph neural network models for representing molecular
structures. We present the advantages and disadvantages of deep and graph
learning methods by performing comparative experiments. We discuss the
potential technical challenges and highlight future directions of deep and
graph learning models for accelerating DDIs prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Briefings in Bioinformatics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unscented Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05256v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05256v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Faris Janjoš, Lars Rosenbaum, Maxim Dolgov, J. Marius Zöllner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Variational Autoencoder (VAE) is a seminal approach in deep generative
modeling with latent variables. Interpreting its reconstruction process as a
nonlinear transformation of samples from the latent posterior distribution, we
apply the Unscented Transform (UT) -- a well-known distribution approximation
used in the Unscented Kalman Filter (UKF) from the field of filtering. A finite
set of statistics called sigma points, sampled deterministically, provides a
more informative and lower-variance posterior representation than the
ubiquitous noise-scaling of the reparameterization trick, while ensuring
higher-quality reconstruction. We further boost the performance by replacing
the Kullback-Leibler (KL) divergence with the Wasserstein distribution metric
that allows for a sharper posterior. Inspired by the two components, we derive
a novel, deterministic-sampling flavor of the VAE, the Unscented Autoencoder
(UAE), trained purely with regularization-like terms on the per-sample
posterior. We empirically show competitive performance in Fr\'echet Inception
Distance (FID) scores over closely-related models, in addition to a lower
training variance than the VAE.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward more accurate and generalizable brain deformation estimators for
  traumatic brain injury detection with unsupervised domain adaptation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xianghao Zhan, Jiawei Sun, Yuzhe Liu, Nicholas J. Cecchi, Enora Le Flao, Olivier Gevaert, Michael M. Zeineh, David B. Camarillo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning head models (MLHMs) are developed to estimate brain
deformation for early detection of traumatic brain injury (TBI). However, the
overfitting to simulated impacts and the lack of generalizability caused by
distributional shift of different head impact datasets hinders the broad
clinical applications of current MLHMs. We propose brain deformation estimators
that integrates unsupervised domain adaptation with a deep neural network to
predict whole-brain maximum principal strain (MPS) and MPS rate (MPSR). With
12,780 simulated head impacts, we performed unsupervised domain adaptation on
on-field head impacts from 302 college football (CF) impacts and 457 mixed
martial arts (MMA) impacts using domain regularized component analysis (DRCA)
and cycle-GAN-based methods. The new model improved the MPS/MPSR estimation
accuracy, with the DRCA method significantly outperforming other domain
adaptation methods in prediction accuracy (p<0.001): MPS RMSE: 0.027 (CF) and
0.037 (MMA); MPSR RMSE: 7.159 (CF) and 13.022 (MMA). On another two hold-out
test sets with 195 college football impacts and 260 boxing impacts, the DRCA
model significantly outperformed the baseline model without domain adaptation
in MPS and MPSR estimation accuracy (p<0.001). The DRCA domain adaptation
reduces the MPS/MPSR estimation error to be well below TBI thresholds, enabling
accurate brain deformation estimation to detect TBI in future clinical
applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Matching Latent Encoding for Audio-Text based Keyword Spotting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05245v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05245v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kumari Nishu, Minsik Cho, Devang Naik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using audio and text embeddings jointly for Keyword Spotting (KWS) has shown
high-quality results, but the key challenge of how to semantically align two
embeddings for multi-word keywords of different sequence lengths remains
largely unsolved. In this paper, we propose an audio-text-based end-to-end
model architecture for flexible keyword spotting (KWS), which builds upon
learned audio and text embeddings. Our architecture uses a novel dynamic
programming-based algorithm, Dynamic Sequence Partitioning (DSP), to optimally
partition the audio sequence into the same length as the word-based text
sequence using the monotonic alignment of spoken content. Our proposed model
consists of an encoder block to get audio and text embeddings, a projector
block to project individual embeddings to a common latent space, and an
audio-text aligner containing a novel DSP algorithm, which aligns the audio and
text embeddings to determine if the spoken content is the same as the text.
Experimental results show that our DSP is more effective than other
partitioning schemes, and the proposed architecture outperformed the
state-of-the-art results on the public dataset in terms of Area Under the ROC
Curve (AUC) and Equal-Error-Rate (EER) by 14.4 % and 28.9%, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ownership Protection of Generative Adversarial Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hailong Hu, Jun Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative adversarial networks (GANs) have shown remarkable success in image
synthesis, making GAN models themselves commercially valuable to legitimate
model owners. Therefore, it is critical to technically protect the intellectual
property of GANs. Prior works need to tamper with the training set or training
process, and they are not robust to emerging model extraction attacks. In this
paper, we propose a new ownership protection method based on the common
characteristics of a target model and its stolen models. Our method can be
directly applicable to all well-trained GANs as it does not require retraining
target models. Extensive experimental results show that our new method can
achieve the best protection performance, compared to the state-of-the-art
methods. Finally, we demonstrate the effectiveness of our method with respect
to the number of generations of model extraction attacks, the number of
generated samples, different datasets, as well as adaptive attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting Adversarial Transferability by Achieving Flat Local Maxima 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijin Ge, Fanhua Shang, Hongying Liu, Yuanyuan Liu, Xiaosen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transfer-based attack adopts the adversarial examples generated on the
surrogate model to attack various models, making it applicable in the physical
world and attracting increasing interest. Recently, various adversarial attacks
have emerged to boost adversarial transferability from different perspectives.
In this work, inspired by the fact that flat local minima are correlated with
good generalization, we assume and empirically validate that adversarial
examples at a flat local region tend to have good transferability by
introducing a penalized gradient norm to the original loss function. Since
directly optimizing the gradient regularization norm is computationally
expensive and intractable for generating adversarial examples, we propose an
approximation optimization method to simplify the gradient update of the
objective function. Specifically, we randomly sample an example and adopt the
first-order gradient to approximate the second-order Hessian matrix, which
makes computing more efficient by interpolating two Jacobian matrices.
Meanwhile, in order to obtain a more stable gradient direction, we randomly
sample multiple examples and average the gradients of these examples to reduce
the variance due to random sampling during the iterative process. Extensive
experimental results on the ImageNet-compatible dataset show that the proposed
method can generate adversarial examples at flat local regions, and
significantly improve the adversarial transferability on either normally
trained models or adversarially trained models than the state-of-the-art
attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 5 figures, 6 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Boosting-based Construction of BDDs for Linear Threshold Functions and
  Its Application to Verification of Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05211v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05211v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiping Tang, Kohei Hatano, Eiji Takimoto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the characteristics of neural networks is important but
difficult due to their complex structures and behaviors. Some previous work
proposes to transform neural networks into equivalent Boolean expressions and
apply verification techniques for characteristics of interest. This approach is
promising since rich results of verification techniques for circuits and other
Boolean expressions can be readily applied. The bottleneck is the time
complexity of the transformation. More precisely, (i) each neuron of the
network, i.e., a linear threshold function, is converted to a Binary Decision
Diagram (BDD), and (ii) they are further combined into some final form, such as
Boolean circuits. For a linear threshold function with $n$ variables, an
existing method takes $O(n2^{\frac{n}{2}})$ time to construct an ordered BDD of
size $O(2^{\frac{n}{2}})$ consistent with some variable ordering. However, it
is non-trivial to choose a variable ordering producing a small BDD among $n!$
candidates.
  We propose a method to convert a linear threshold function to a specific form
of a BDD based on the boosting approach in the machine learning literature. Our
method takes $O(2^n \text{poly}(1/\rho))$ time and outputs BDD of size
$O(\frac{n^2}{\rho^4}\ln{\frac{1}{\rho}})$, where $\rho$ is the margin of some
consistent linear threshold function. Our method does not need to search for
good variable orderings and produces a smaller expression when the margin of
the linear threshold function is large. More precisely, our method is based on
our new boosting algorithm, which is of independent interest. We also propose a
method to combine them into the final Boolean expression representing the
neural network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PriSampler: Mitigating Property Inference of Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hailong Hu, Jun Pang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have been remarkably successful in data synthesis. Such
successes have also driven diffusion models to apply to sensitive data, such as
human face data, but this might bring about severe privacy concerns. In this
work, we systematically present the first privacy study about property
inference attacks against diffusion models, in which adversaries aim to extract
sensitive global properties of the training set from a diffusion model, such as
the proportion of the training data for certain sensitive properties.
Specifically, we consider the most practical attack scenario: adversaries are
only allowed to obtain synthetic data. Under this realistic scenario, we
evaluate the property inference attacks on different types of samplers and
diffusion models. A broad range of evaluations shows that various diffusion
models and their samplers are all vulnerable to property inference attacks.
Furthermore, one case study on off-the-shelf pre-trained diffusion models also
demonstrates the effectiveness of the attack in practice. Finally, we propose a
new model-agnostic plug-in method PriSampler to mitigate the property inference
of diffusion models. PriSampler can be directly applied to well-trained
diffusion models and support both stochastic and deterministic sampling.
Extensive experiments illustrate the effectiveness of our defense and it makes
adversaries infer the proportion of properties as close as random guesses.
PriSampler also shows its significantly superior performance to diffusion
models trained with differential privacy on both model utility and defense
performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ EMO: Episodic Memory Optimization for Few-Shot Meta-Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yingjun Du, Jiayi Shen, Xiantong Zhen, Cee G. M. Snoek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot meta-learning presents a challenge for gradient descent optimization
due to the limited number of training samples per task. To address this issue,
we propose an episodic memory optimization for meta-learning, we call
\emph{EMO}, which is inspired by the human ability to recall past learning
experiences from the brain's memory. EMO retains the gradient history of past
experienced tasks in external memory, enabling few-shot learning in a
memory-augmented way. By learning to retain and recall the learning process of
past training tasks, EMO nudges parameter updates in the right direction, even
when the gradients provided by a limited number of examples are uninformative.
We prove theoretically that our algorithm converges for smooth, strongly convex
objectives. EMO is generic, flexible, and model-agnostic, making it a simple
plug-and-play optimizer that can be seamlessly embedded into existing
optimization-based few-shot meta-learning approaches. Empirical results show
that EMO scales well with most few-shot classification benchmarks and improves
the performance of optimization-based meta-learning methods, resulting in
accelerated convergence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CoLLAs 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Identification and Optimization of Nonsmooth Superposition
  Operators in Semilinear Elliptic PDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05185v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05185v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Constantin Christof, Julia Kowalczyk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study an infinite-dimensional optimization problem that aims to identify
the Nemytskii operator in the nonlinear part of a prototypical semilinear
elliptic partial differential equation (PDE) which minimizes the distance
between the PDE-solution and a given desired state. In contrast to previous
works, we consider this identification problem in a low-regularity regime in
which the function inducing the Nemytskii operator is a-priori only known to be
an element of $H^1_{loc}(\mathbb{R})$. This makes the studied problem class a
suitable point of departure for the rigorous analysis of training problems for
learning-informed PDEs in which an unknown superposition operator is
approximated by means of a neural network with nonsmooth activation functions
(ReLU, leaky-ReLU, etc.). We establish that, despite the low regularity of the
controls, it is possible to derive a classical stationarity system for local
minimizers and to solve the considered problem by means of a gradient
projection method. The convergence of the resulting algorithm is proven in the
function space setting. It is also shown that the established first-order
necessary optimality conditions imply that locally optimal superposition
operators share various characteristic properties with commonly used activation
functions: They are always sigmoidal, continuously differentiable away from the
origin, and typically possess a distinct kink at zero. The paper concludes with
numerical experiments which confirm the theoretical findings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Long Context Document-Level Machine Translation <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05183v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05183v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Herold, Hermann Ney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document-level context for neural machine translation (NMT) is crucial to
improve the translation consistency and cohesion, the translation of ambiguous
inputs, as well as several other linguistic phenomena. Many works have been
published on the topic of document-level NMT, but most restrict the system to
only local context, typically including just the one or two preceding sentences
as additional information. This might be enough to resolve some ambiguous
inputs, but it is probably not sufficient to capture some document-level
information like the topic or style of a conversation. When increasing the
context size beyond just the local context, there are two challenges: (i)
the~memory usage increases exponentially (ii) the translation performance
starts to degrade. We argue that the widely-used attention mechanism is
responsible for both issues. Therefore, we propose a constrained attention
variant that focuses the attention on the most relevant parts of the sequence,
while simultaneously reducing the memory consumption. For evaluation, we
utilize targeted test sets in combination with novel evaluation techniques to
analyze the translations in regards to specific discourse-related phenomena. We
find that our approach is a good compromise between sentence-level NMT vs
attending to the full context, especially in low resource scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at CODI 2023 (ACL workshop)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large-scale <span class="highlight-title">Dataset</span> Pruning with Dynamic Uncertainty 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05175v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05175v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muyang He, Shuo Yang, Tiejun Huang, Bo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The state of the art of many learning tasks, e.g., image classification, is
advanced by collecting larger datasets and then training larger models on them.
As the outcome, the increasing computational cost is becoming unaffordable. In
this paper, we investigate how to prune the large-scale datasets, and thus
produce an informative subset for training sophisticated deep models with
negligible performance drop. We propose a simple yet effective dataset pruning
method by exploring both the prediction uncertainty and training dynamics. To
our knowledge, this is the first work to study dataset pruning on large-scale
datasets, i.e., ImageNet-1K and ImageNet-21K, and advanced models, i.e., Swin
Transformer and ConvNeXt. Extensive experimental results indicate that our
method outperforms the state of the art and achieves 75% lossless compression
ratio on both ImageNet-1K and ImageNet-21K. The code and pruned datasets are
available at https://github.com/BAAI-DCAI/Dataset-Pruning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FLEdge: Benchmarking Federated Machine Learning Applications in Edge
  Computing Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05172v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05172v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Herbert Woisetschläger, Alexander Isenko, Ruben Mayer, Hans-Arno Jacobsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Machine Learning (FL) has received considerable attention in recent
years. FL benchmarks are predominantly explored in either simulated systems or
data center environments, neglecting the setups of real-world systems, which
are often closely linked to edge computing. We close this research gap by
introducing FLEdge, a benchmark targeting FL workloads in edge computing
systems. We systematically study hardware heterogeneity, energy efficiency
during training, and the effect of various differential privacy levels on
training in FL systems. To make this benchmark applicable to real-world
scenarios, we evaluate the impact of client dropouts on state-of-the-art FL
strategies with failure rates as high as 50%. FLEdge provides new insights,
such as that training state-of-the-art FL workloads on older GPU-accelerated
embedded devices is up to 3x more energy efficient than on modern server-grade
GPUs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decision S4: Efficient Sequence-Based RL via State Spaces Layers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05167v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05167v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shmuel Bar-David, Itamar Zimerman, Eliya Nachmani, Lior Wolf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, sequence learning methods have been applied to the problem of
off-policy Reinforcement Learning, including the seminal work on Decision
Transformers, which employs transformers for this task. Since transformers are
parameter-heavy, cannot benefit from history longer than a fixed window size,
and are not computed using recurrence, we set out to investigate the
suitability of the S4 family of models, which are based on state-space layers
and have been shown to outperform transformers, especially in modeling
long-range dependencies. In this work we present two main algorithms: (i) an
off-policy training procedure that works with trajectories, while still
maintaining the training efficiency of the S4 model. (ii) An on-policy training
procedure that is trained in a recurrent manner, benefits from long-range
dependencies, and is based on a novel stable actor-critic mechanism. Our
results indicate that our method outperforms multiple variants of decision
transformers, as well as the other baseline methods on most tasks, while
reducing the latency, number of parameters, and training time by several orders
of magnitude, making our approach more suitable for real-world RL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages,13 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Optimization of Expensive Nested Grey-Box Functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05150v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05150v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenjie Xu, Yuning Jiang, Bratislav Svetozarevic, Colin N. Jones
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of optimizing a grey-box objective function, i.e.,
nested function composed of both black-box and white-box functions. A general
formulation for such grey-box problems is given, which covers the existing
grey-box optimization formulations as special cases. We then design an
optimism-driven algorithm to solve it. Under certain regularity assumptions,
our algorithm achieves similar regret bound as that for the standard black-box
Bayesian optimization algorithm, up to a constant multiplicative term depending
on the Lipschitz constants of the functions considered. We further extend our
method to the constrained case and discuss several special cases. For the
commonly used kernel functions, the regret bounds allow us to derive a
convergence rate to the optimal solution. Experimental results show that our
grey-box optimization method empirically improves the speed of finding the
global optimal solution significantly, as compared to the standard black-box
optimization algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mesogeos: A multi-purpose <span class="highlight-title">dataset</span> for data-driven wildfire modeling in
  the Mediterranean 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05144v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05144v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Spyros Kondylatos, Ioannis Prapas, Gustau Camps-Valls, Ioannis Papoutsis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Mesogeos, a large-scale multi-purpose dataset for wildfire
modeling in the Mediterranean. Mesogeos integrates variables representing
wildfire drivers (meteorology, vegetation, human activity) and historical
records of wildfire ignitions and burned areas for 17 years (2006-2022). It is
designed as a cloud-friendly spatio-temporal dataset, namely a datacube,
harmonizing all variables in a grid of 1km x 1km x 1-day resolution. The
datacube structure offers opportunities to assess machine learning (ML) usage
in various wildfire modeling tasks. We extract two ML-ready datasets that
establish distinct tracks to demonstrate this potential: (1) short-term
wildfire danger forecasting and (2) final burned area estimation given the
point of ignition. We define appropriate metrics and baselines to evaluate the
performance of models in each track. By publishing the datacube, along with the
code to create the ML datasets and models, we encourage the community to foster
the implementation of additional tracks for mitigating the increasing threat of
wildfires in the Mediterranean.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Genomic Interpreter: A Hierarchical Genomic Deep Neural Network with 1D
  Shifted Window <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05143v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05143v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehui Li, Akashaditya Das, William A V Beardall, Yiren Zhao, Guy-Bart Stan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given the increasing volume and quality of genomics data, extracting new
insights requires interpretable machine-learning models. This work presents
Genomic Interpreter: a novel architecture for genomic assay prediction. This
model outperforms the state-of-the-art models for genomic assay prediction
tasks. Our model can identify hierarchical dependencies in genomic sites. This
is achieved through the integration of 1D-Swin, a novel Transformer-based block
designed by us for modelling long-range hierarchical data. Evaluated on a
dataset containing 38,171 DNA segments of 17K base pairs, Genomic Interpreter
demonstrates superior performance in chromatin accessibility and gene
expression prediction and unmasks the underlying `syntax' of gene regulation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal Prediction for Federated Uncertainty Quantification Under
  Label Shift <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05131v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05131v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vincent Plassier, Mehdi Makni, Aleksandr Rubashevskii, Eric Moulines, Maxim Panov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) is a machine learning framework where many clients
collaboratively train models while keeping the training data decentralized.
Despite recent advances in FL, the uncertainty quantification topic (UQ)
remains partially addressed. Among UQ methods, conformal prediction (CP)
approaches provides distribution-free guarantees under minimal assumptions. We
develop a new federated conformal prediction method based on quantile
regression and take into account privacy constraints. This method takes
advantage of importance weighting to effectively address the label shift
between agents and provides theoretical guarantees for both valid coverage of
the prediction sets and differential privacy. Extensive experimental studies
demonstrate that this method outperforms current competitors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Meta-Generation framework for Industrial System Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05123v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05123v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fouad Oubari, Raphael Meunier, Rodrigue Décatoire, Mathilde Mougeot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative design is an increasingly important tool in the industrial world.
It allows the designers and engineers to easily explore vast ranges of design
options, providing a cheaper and faster alternative to the trial and failure
approaches. Thanks to the flexibility they offer, Deep Generative Models are
gaining popularity amongst Generative Design technologies. However, developing
and evaluating these models can be challenging. The field lacks accessible
benchmarks, in order to evaluate and compare objectively different Deep
Generative Models architectures. Moreover, vanilla Deep Generative Models
appear to be unable to accurately generate multi-components industrial systems
that are controlled by latent design constraints. To address these challenges,
we propose an industry-inspired use case that incorporates actual industrial
system characteristics. This use case can be quickly generated and used as a
benchmark. We propose a Meta-VAE capable of producing multi-component
industrial systems and showcase its application on the proposed use case.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Search Strategies for Document-Level Neural Machine Translation <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Herold, Hermann Ney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to sentence-level systems, document-level neural machine translation
(NMT) models produce a more consistent output across a document and are able to
better resolve ambiguities within the input. There are many works on
document-level NMT, mostly focusing on modifying the model architecture or
training strategy to better accommodate the additional context-input. On the
other hand, in most works, the question on how to perform search with the
trained model is scarcely discussed, sometimes not mentioned at all. In this
work, we aim to answer the question how to best utilize a context-aware
translation model in decoding. We start with the most popular document-level
NMT approach and compare different decoding schemes, some from the literature
and others proposed by us. In the comparison, we are using both, standard
automatic metrics, as well as specific linguistic phenomena on three standard
document-level translation benchmarks. We find that most commonly used decoding
strategies perform similar to each other and that higher quality context
information has the potential to further improve the translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2023 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Yet Another ICU Benchmark: A Flexible Multi-Center Framework for
  Clinical ML 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05109v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05109v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Robin van de Water, Hendrik Schmidt, Paul Elbers, Patrick Thoral, Bert Arnrich, Patrick Rockenschaub
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical applications of machine learning (ML) have experienced a surge in
popularity in recent years. The intensive care unit (ICU) is a natural habitat
for ML given the abundance of available data from electronic health records.
Models have been proposed to address numerous ICU prediction tasks like the
early detection of complications. While authors frequently report
state-of-the-art performance, it is challenging to verify claims of
superiority. Datasets and code are not always published, and cohort
definitions, preprocessing pipelines, and training setups are difficult to
reproduce. This work introduces Yet Another ICU Benchmark (YAIB), a modular
framework that allows researchers to define reproducible and comparable
clinical ML experiments; we offer an end-to-end solution from cohort definition
to model evaluation. The framework natively supports most open-access ICU
datasets (MIMIC III/IV, eICU, HiRID, AUMCdb) and is easily adaptable to future
ICU datasets. Combined with a transparent preprocessing pipeline and extensible
training code for multiple ML and deep learning models, YAIB enables unified
model development. Our benchmark comes with five predefined established
prediction tasks (mortality, acute kidney injury, sepsis, kidney function, and
length of stay) developed in collaboration with clinicians. Adding further
tasks is straightforward by design. Using YAIB, we demonstrate that the choice
of dataset, cohort definition, and preprocessing have a major impact on the
prediction performance - often more so than model class - indicating an urgent
need for YAIB as a holistic benchmarking tool. We provide our work to the
clinical ML community to accelerate method development and enable real-world
clinical implementations. Software Repository:
https://github.com/rvandewater/YAIB.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Main benchmark: https://github.com/rvandewater/YAIB, Cohort
  generation: https://github.com/rvandewater/YAIB-cohorts, Models:
  https://github.com/rvandewater/YAIB-models</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hybrid Graph: A Unified Graph Representation with <span class="highlight-title">Dataset</span>s and
  Benchmarks for Complex Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05108v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05108v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zehui Li, Xiangyu Zhao, Mingzhu Shen, Guy-Bart Stan, Pietro Liò, Yiren Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graphs are widely used to encapsulate a variety of data formats, but
real-world networks often involve complex node relations beyond only being
pairwise. While hypergraphs and hierarchical graphs have been developed and
employed to account for the complex node relations, they cannot fully represent
these complexities in practice. Additionally, though many Graph Neural Networks
(GNNs) have been proposed for representation learning on higher-order graphs,
they are usually only evaluated on simple graph datasets. Therefore, there is a
need for a unified modelling of higher-order graphs, and a collection of
comprehensive datasets with an accessible evaluation framework to fully
understand the performance of these algorithms on complex graphs. In this
paper, we introduce the concept of hybrid graphs, a unified definition for
higher-order graphs, and present the Hybrid Graph Benchmark (HGB). HGB contains
23 real-world hybrid graph datasets across various domains such as biology,
social media, and e-commerce. Furthermore, we provide an extensible evaluation
framework and a supporting codebase to facilitate the training and evaluation
of GNNs on HGB. Our empirical study of existing GNNs on HGB reveals various
research opportunities and gaps, including (1) evaluating the actual
performance improvement of hypergraph GNNs over simple graph GNNs; (2)
comparing the impact of different sampling strategies on hybrid graph learning
methods; and (3) exploring ways to integrate simple graph and hypergraph
information. We make our source code and full datasets publicly available at
https://zehui127.github.io/hybrid-graph-benchmark/.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review. 16 pages, 5 figures, 11 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sy-CON: Symmetric Contrastive Loss for Continual <span class="highlight-title">Self-Supervised</span>
  Representation Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sungmin Cha, Taesup Moon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a novel and general loss function, called Symmetric Contrastive
(Sy-CON) loss, for effective continual self-supervised learning (CSSL). We
first argue that the conventional loss form of continual learning which
consists of single task-specific loss (for plasticity) and a regularizer (for
stability) may not be ideal for contrastive loss based CSSL that focus on
representation learning. Our reasoning is that, in contrastive learning based
methods, the task-specific loss would suffer from decreasing diversity of
negative samples and the regularizer may hinder learning new distinctive
representations. To that end, we propose Sy-CON that consists of two losses
(one for plasticity and the other for stability) with symmetric dependence on
current and past models' negative sample embeddings. We argue our model can
naturally find good trade-off between the plasticity and stability without any
explicit hyperparameter tuning. We validate the effectiveness of our approach
through extensive experiments, demonstrating that MoCo-based implementation of
Sy-CON loss achieves superior performance compared to other state-of-the-art
CSSL methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Communication-Efficient Gradient Descent-Accent Methods for Distributed
  Variational Inequalities: Unified Analysis and Local Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05100v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05100v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siqi Zhang, Sayantan Choudhury, Sebastian U Stich, Nicolas Loizou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributed and federated learning algorithms and techniques associated
primarily with minimization problems. However, with the increase of minimax
optimization and variational inequality problems in machine learning, the
necessity of designing efficient distributed/federated learning approaches for
these problems is becoming more apparent. In this paper, we provide a unified
convergence analysis of communication-efficient local training methods for
distributed variational inequality problems (VIPs). Our approach is based on a
general key assumption on the stochastic estimates that allows us to propose
and analyze several novel local training algorithms under a single framework
for solving a class of structured non-monotone VIPs. We present the first local
gradient descent-accent algorithms with provable improved communication
complexity for solving distributed variational inequalities on heterogeneous
data. The general algorithmic framework recovers state-of-the-art algorithms
and their sharp convergence guarantees when the setting is specialized to
minimization or minimax optimization problems. Finally, we demonstrate the
strong performance of the proposed algorithms compared to state-of-the-art
methods when solving federated minimax optimization problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Re-aligning Shadow Models can Improve White-box Membership Inference
  Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05093v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05093v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ana-Maria Cretu, Daniel Jones, Yves-Alexandre de Montjoye, Shruti Tople
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models have been shown to leak sensitive information about
their training datasets. As models are being increasingly used, on devices, to
automate tasks and power new applications, there have been concerns that such
white-box access to its parameters, as opposed to the black-box setting which
only provides query access to the model, increases the attack surface. Directly
extending the shadow modelling technique from the black-box to the white-box
setting has been shown, in general, not to perform better than black-box only
attacks. A key reason is misalignment, a known characteristic of deep neural
networks. We here present the first systematic analysis of the causes of
misalignment in shadow models and show the use of a different weight
initialisation to be the main cause of shadow model misalignment. Second, we
extend several re-alignment techniques, previously developed in the model
fusion literature, to the shadow modelling context, where the goal is to
re-align the layers of a shadow model to those of the target model.We show
re-alignment techniques to significantly reduce the measured misalignment
between the target and shadow models. Finally, we perform a comprehensive
evaluation of white-box membership inference attacks (MIA). Our analysis
reveals that (1) MIAs suffer from misalignment between shadow models, but that
(2) re-aligning the shadow models improves, sometimes significantly, MIA
performance. On the CIFAR10 dataset with a false positive rate of 1\%,
white-box MIA using re-aligned shadow models improves the true positive rate by
4.5\%.Taken together, our results highlight that on-device deployment increase
the attack surface and that the newly available information can be used by an
attacker.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The ART of Conversation: Measuring Phonetic Convergence and Deliberate
  Imitation in L2-Speech with a Siamese RNN <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheng Yuan, Aldo Pastore, Dorina de Jong, Hao Xu, Luciano Fadiga, Alessandro D'Ausilio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phonetic convergence describes the automatic and unconscious speech
adaptation of two interlocutors in a conversation. This paper proposes a
Siamese recurrent neural network (RNN) architecture to measure the convergence
of the holistic spectral characteristics of speech sounds in an L2-L2
interaction. We extend an alternating reading task (the ART) dataset by adding
20 native Slovak L2 English speakers. We train and test the Siamese RNN model
to measure phonetic convergence of L2 English speech from three different
native language groups: Italian (9 dyads), French (10 dyads) and Slovak (10
dyads). Our results indicate that the Siamese RNN model effectively captures
the dynamics of phonetic convergence and the speaker's imitation ability.
Moreover, this text-independent model is scalable and capable of handling
L1-induced speaker variability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Importance of Time in Causal Algorithmic Recourse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05082v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05082v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Isacco Beretta, Martina Cinquini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The application of Algorithmic Recourse in decision-making is a promising
field that offers practical solutions to reverse unfavorable decisions.
However, the inability of these methods to consider potential dependencies
among variables poses a significant challenge due to the assumption of feature
independence. Recent advancements have incorporated knowledge of causal
dependencies, thereby enhancing the quality of the recommended recourse
actions. Despite these improvements, the inability to incorporate the temporal
dimension remains a significant limitation of these approaches. This is
particularly problematic as identifying and addressing the root causes of
undesired outcomes requires understanding time-dependent relationships between
variables. In this work, we motivate the need to integrate the temporal
dimension into causal algorithmic recourse methods to enhance recommendations'
plausibility and reliability. The experimental evaluation highlights the
significance of the role of time in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for xAI Conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Robustness of AI Offensive Code Generators via Data
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristina Improta, Pietro Liguori, Roberto Natella, Bojan Cukic, Domenico Cotroneo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we present a method to add perturbations to the code
descriptions, i.e., new inputs in natural language (NL) from well-intentioned
developers, in the context of security-oriented code, and analyze how and to
what extent perturbations affect the performance of AI offensive code
generators. Our experiments show that the performance of the code generators is
highly affected by perturbations in the NL descriptions. To enhance the
robustness of the code generators, we use the method to perform data
augmentation, i.e., to increase the variability and diversity of the training
data, proving its effectiveness against both perturbed and non-perturbed code
descriptions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Language Model Integration for Neural Machine Translation <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05077v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05077v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Herold, Yingbo Gao, Mohammad Zeineldeen, Hermann Ney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of language models for neural machine translation has been
extensively studied in the past. It has been shown that an external language
model, trained on additional target-side monolingual data, can help improve
translation quality. However, there has always been the assumption that the
translation model also learns an implicit target-side language model during
training, which interferes with the external language model at decoding time.
Recently, some works on automatic speech recognition have demonstrated that, if
the implicit language model is neutralized in decoding, further improvements
can be gained when integrating an external language model. In this work, we
transfer this concept to the task of machine translation and compare with the
most prominent way of including additional monolingual data - namely
back-translation. We find that accounting for the implicit language model
significantly boosts the performance of language model fusion, although this
approach is still outperformed by back-translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at ACL2023 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Causal Framework for Decomposing Spurious Variations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05071v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05071v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Drago Plecko, Elias Bareinboim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the fundamental challenges found throughout the data sciences is to
explain why things happen in specific ways, or through which mechanisms a
certain variable $X$ exerts influences over another variable $Y$. In statistics
and machine learning, significant efforts have been put into developing
machinery to estimate correlations across variables efficiently. In causal
inference, a large body of literature is concerned with the decomposition of
causal effects under the rubric of mediation analysis. However, many variations
are spurious in nature, including different phenomena throughout the applied
sciences. Despite the statistical power to estimate correlations and the
identification power to decompose causal effects, there is still little
understanding of the properties of spurious associations and how they can be
decomposed in terms of the underlying causal mechanisms. In this manuscript, we
develop formal tools for decomposing spurious variations in both Markovian and
Semi-Markovian models. We prove the first results that allow a non-parametric
decomposition of spurious effects and provide sufficient conditions for the
identification of such decompositions. The described approach has several
applications, ranging from explainable and fair AI to questions in epidemiology
and medicine, and we empirically demonstrate its use on a real-world dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Shedding light on underrepresentation and Sampling Bias in machine
  learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05068v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05068v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sami Zhioua, Rūta Binkytė
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurately measuring discrimination is crucial to faithfully assessing
fairness of trained machine learning (ML) models. Any bias in measuring
discrimination leads to either amplification or underestimation of the existing
disparity. Several sources of bias exist and it is assumed that bias resulting
from machine learning is born equally by different groups (e.g. females vs
males, whites vs blacks, etc.). If, however, bias is born differently by
different groups, it may exacerbate discrimination against specific
sub-populations. Sampling bias, is inconsistently used in the literature to
describe bias due to the sampling procedure. In this paper, we attempt to
disambiguate this term by introducing clearly defined variants of sampling
bias, namely, sample size bias (SSB) and underrepresentation bias (URB). We
show also how discrimination can be decomposed into variance, bias, and noise.
Finally, we challenge the commonly accepted mitigation approach that
discrimination can be addressed by collecting more samples of the
underrepresented group.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Visual <span class="highlight-title">Prompt</span> Tuning for <span class="highlight-title">Self-supervised</span> Vision <span class="highlight-title">Transformer</span>s <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungryong Yoo, Eunji Kim, Dahuin Jung, Jungbeom Lee, Sungroh Yoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Visual Prompt Tuning (VPT) is an effective tuning method for adapting
pretrained Vision Transformers (ViTs) to downstream tasks. It leverages extra
learnable tokens, known as prompts, which steer the frozen pretrained ViTs.
Although VPT has demonstrated its applicability with supervised vision
transformers, it often underperforms with self-supervised ones. Through
empirical observations, we deduce that the effectiveness of VPT hinges largely
on the ViT blocks with which the prompt tokens interact. Specifically, VPT
shows improved performance on image classification tasks for MAE and MoCo v3
when the prompt tokens are inserted into later blocks rather than the first
block. These observations suggest that there exists an optimal location of
blocks for the insertion of prompt tokens. Unfortunately, identifying the
optimal blocks for prompts within each self-supervised ViT for diverse future
scenarios is a costly process. To mitigate this problem, we propose a simple
yet effective method that learns a gate for each ViT block to adjust its
intervention into the prompt tokens. With our method, prompt tokens are
selectively influenced by blocks that require steering for task adaptation. Our
method outperforms VPT variants in FGVC and VTAB image classification and
ADE20K semantic segmentation. The code is available at
https://github.com/ryongithub/GatedPromptTuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Machine Learning (ICML) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Fairness for Outcome Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Drago Plecko, Elias Bareinboim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As society transitions towards an AI-based decision-making infrastructure, an
ever-increasing number of decisions once under control of humans are now
delegated to automated systems. Even though such developments make various
parts of society more efficient, a large body of evidence suggests that a great
deal of care needs to be taken to make such automated decision-making systems
fair and equitable, namely, taking into account sensitive attributes such as
gender, race, and religion. In this paper, we study a specific decision-making
task called outcome control in which an automated system aims to optimize an
outcome variable $Y$ while being fair and equitable. The interest in such a
setting ranges from interventions related to criminal justice and welfare, all
the way to clinical decision-making and public health. In this paper, we first
analyze through causal lenses the notion of benefit, which captures how much a
specific individual would benefit from a positive decision, counterfactually
speaking, when contrasted with an alternative, negative one. We introduce the
notion of benefit fairness, which can be seen as the minimal fairness
requirement in decision-making, and develop an algorithm for satisfying it. We
then note that the benefit itself may be influenced by the protected attribute,
and propose causal tools which can be used to analyze this. Finally, if some of
the variations of the protected attribute in the benefit are considered as
discriminatory, the notion of benefit fairness may need to be strengthened,
which leads us to articulating a notion of causal benefit fairness. Using this
notion, we develop a new optimization procedure capable of maximizing $Y$ while
ascertaining causal fairness in the decision process.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Precision-aware Latency and Energy Balancing on Multi-Accelerator
  Platforms for DNN Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05060v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05060v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matteo Risso, Alessio Burrello, Giuseppe Maria Sarda, Luca Benini, Enrico Macii, Massimo Poncino, Marian Verhelst, Daniele Jahier Pagliari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The need to execute Deep Neural Networks (DNNs) at low latency and low power
at the edge has spurred the development of new heterogeneous Systems-on-Chips
(SoCs) encapsulating a diverse set of hardware accelerators. How to optimally
map a DNN onto such multi-accelerator systems is an open problem. We propose
ODiMO, a hardware-aware tool that performs a fine-grain mapping across
different accelerators on-chip, splitting individual layers and executing them
in parallel, to reduce inference energy consumption or latency, while taking
into account each accelerator's quantization precision to maintain accuracy.
Pareto-optimal networks in the accuracy vs. energy or latency space are pursued
for three popular dataset/DNN pairs, and deployed on the DIANA heterogeneous
ultra-low power edge AI SoC. We show that ODiMO reduces energy/latency by up to
33%/31% with limited accuracy drop (-0.53%/-0.32%) compared to manual heuristic
mappings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at 2023 ACM/IEEE International Symposium on Low Power
  Electronics and Design (ISLPED)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Reconciling Predictive and Statistical Parity: A Causal Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Drago Plecko, Elias Bareinboim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since the rise of fair machine learning as a critical field of inquiry, many
different notions on how to quantify and measure discrimination have been
proposed in the literature. Some of these notions, however, were shown to be
mutually incompatible. Such findings make it appear that numerous different
kinds of fairness exist, thereby making a consensus on the appropriate measure
of fairness harder to reach, hindering the applications of these tools in
practice. In this paper, we investigate one of these key impossibility results
that relates the notions of statistical and predictive parity. Specifically, we
derive a new causal decomposition formula for the fairness measures associated
with predictive parity, and obtain a novel insight into how this criterion is
related to statistical parity through the legal doctrines of disparate
treatment, disparate impact, and the notion of business necessity. Our results
show that through a more careful causal analysis, the notions of statistical
and predictive parity are not really mutually exclusive, but complementary and
spanning a spectrum of fairness notions through the concept of business
necessity. Finally, we demonstrate the importance of our findings on a
real-world example.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuro-Symbolic Approaches for Context-Aware Human Activity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05058v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05058v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Luca Arrotta, Gabriele Civitarese, Claudio Bettini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Learning models are a standard solution for sensor-based Human Activity
Recognition (HAR), but their deployment is often limited by labeled data
scarcity and models' opacity. Neuro-Symbolic AI (NeSy) provides an interesting
research direction to mitigate these issues by infusing knowledge about context
information into HAR deep learning classifiers. However, existing NeSy methods
for context-aware HAR require computationally expensive symbolic reasoners
during classification, making them less suitable for deployment on
resource-constrained devices (e.g., mobile devices). Additionally, NeSy
approaches for context-aware HAR have never been evaluated on in-the-wild
datasets, and their generalization capabilities in real-world scenarios are
questionable. In this work, we propose a novel approach based on a semantic
loss function that infuses knowledge constraints in the HAR model during the
training phase, avoiding symbolic reasoning during classification. Our results
on scripted and in-the-wild datasets show the impact of different semantic loss
functions in outperforming a purely data-driven model. We also compare our
solution with existing NeSy methods and analyze each approach's strengths and
weaknesses. Our semantic loss remains the only NeSy solution that can be
deployed as a single DNN without the need for symbolic reasoning modules,
reaching recognition rates close (and better in some cases) to existing
approaches.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Magnitude Attention-based Dynamic Pruning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05056v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05056v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihye Back, Namhyuk Ahn, Jangho Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing pruning methods utilize the importance of each weight based on
specified criteria only when searching for a sparse structure but do not
utilize it during training. In this work, we propose a novel approach -
\textbf{M}agnitude \textbf{A}ttention-based Dynamic \textbf{P}runing (MAP)
method, which applies the importance of weights throughout both the forward and
backward paths to explore sparse model structures dynamically. Magnitude
attention is defined based on the magnitude of weights as continuous
real-valued numbers enabling a seamless transition from a redundant to an
effective sparse network by promoting efficient exploration. Additionally, the
attention mechanism ensures more effective updates for important layers within
the sparse network. In later stages of training, our approach shifts from
exploration to exploitation, exclusively updating the sparse model composed of
crucial weights based on the explored structure, resulting in pruned models
that not only achieve performance comparable to dense models but also
outperform previous pruning methods on CIFAR-10/100 and ImageNet.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Medical Diagnostics with Structured Data Extraction by
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksa Bisercic, Mladen Nikolic, Mihaela van der Schaar, Boris Delibasic, Pietro Lio, Andrija Petrovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tabular data is often hidden in text, particularly in medical diagnostic
reports. Traditional machine learning (ML) models designed to work with tabular
data, cannot effectively process information in such form. On the other hand,
large language models (LLMs) which excel at textual tasks, are probably not the
best tool for modeling tabular data. Therefore, we propose a novel, simple, and
effective methodology for extracting structured tabular data from textual
medical reports, called TEMED-LLM. Drawing upon the reasoning capabilities of
LLMs, TEMED-LLM goes beyond traditional extraction techniques, accurately
inferring tabular features, even when their names are not explicitly mentioned
in the text. This is achieved by combining domain-specific reasoning guidelines
with a proposed data validation and reasoning correction feedback loop. By
applying interpretable ML models such as decision trees and logistic regression
over the extracted and validated data, we obtain end-to-end interpretable
predictions. We demonstrate that our approach significantly outperforms
state-of-the-art text classification models in medical diagnostics. Given its
predictive performance, simplicity, and interpretability, TEMED-LLM underscores
the potential of leveraging LLMs to improve the performance and trustworthiness
of ML models in medical applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Gradient-based Approach for Online Robust Deep Neural Network Training
  with Noisy Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05046v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05046v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Yang, Alec Koppel, Zheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning with noisy labels is an important topic for scalable training in
many real-world scenarios. However, few previous research considers this
problem in the online setting, where the arrival of data is streaming. In this
paper, we propose a novel gradient-based approach to enable the detection of
noisy labels for the online learning of model parameters, named Online
Gradient-based Robust Selection (OGRS). In contrast to the previous sample
selection approach for the offline training that requires the estimation of a
clean ratio of the dataset before each epoch of training, OGRS can
automatically select clean samples by steps of gradient update from datasets
with varying clean ratios without changing the parameter setting. During the
training process, the OGRS method selects clean samples at each iteration and
feeds the selected sample to incrementally update the model parameters. We
provide a detailed theoretical analysis to demonstrate data selection process
is converging to the low-loss region of the sample space, by introducing and
proving the sub-linear local Lagrangian regret of the non-convex constrained
optimization problem. Experimental results show that it outperforms
state-of-the-art methods in different settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-autoregressive Conditional Diffusion Models for Time Series
  Prediction <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lifeng Shen, James Kwok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, denoising diffusion models have led to significant breakthroughs in
the generation of images, audio and text. However, it is still an open question
on how to adapt their strong modeling ability to model time series. In this
paper, we propose TimeDiff, a non-autoregressive diffusion model that achieves
high-quality time series prediction with the introduction of two novel
conditioning mechanisms: future mixup and autoregressive initialization.
Similar to teacher forcing, future mixup allows parts of the ground-truth
future predictions for conditioning, while autoregressive initialization helps
better initialize the model with basic time series patterns such as short-term
trends. Extensive experiments are performed on nine real-world datasets.
Results show that TimeDiff consistently outperforms existing time series
diffusion models, and also achieves the best overall performance across a
variety of the existing strong baselines (including transformers and FiLM).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2023 (Poster)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Energy-Efficient Downlink Semantic Generative Communication with
  Text-to-Image Generators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05041v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05041v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyein Lee, Jihong Park, Sooyoung Kim, Jinho Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce a novel semantic generative communication (SGC)
framework, where generative users leverage text-to-image (T2I) generators to
create images locally from downloaded text prompts, while non-generative users
directly download images from a base station (BS). Although generative users
help reduce downlink transmission energy at the BS, they consume additional
energy for image generation and for uploading their generator state information
(GSI). We formulate the problem of minimizing the total energy consumption of
the BS and the users, and devise a generative user selection algorithm.
Simulation results corroborate that our proposed algorithm reduces total energy
by up to 54% compared to a baseline with all non-generative users.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 7 figures. arXiv admin note: text overlap with
  arXiv:2302.02498</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Long-Term Series Forecasting Need Complex Attention and Extra Long
  Inputs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05035v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05035v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daojun Liang, Haixia Zhang, Dongfeng Yuan, Xiaoyan Ma, Dongyang Li, Minggao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As Transformer-based models have achieved impressive performance on various
time series tasks, Long-Term Series Forecasting (LTSF) tasks have also received
extensive attention in recent years. However, due to the inherent computational
complexity and long sequences demanding of Transformer-based methods, its
application on LTSF tasks still has two major issues that need to be further
investigated: 1) Whether the sparse attention mechanism designed by these
methods actually reduce the running time on real devices; 2) Whether these
models need extra long input sequences to guarantee their performance? The
answers given in this paper are negative. Therefore, to better copy with these
two issues, we design a lightweight Period-Attention mechanism (Periodformer),
which renovates the aggregation of long-term subseries via explicit periodicity
and short-term subseries via built-in proximity. Meanwhile, a gating mechanism
is embedded into Periodformer to regulate the influence of the attention module
on the prediction results. Furthermore, to take full advantage of GPUs for fast
hyperparameter optimization (e.g., finding the suitable input length), a
Multi-GPU Asynchronous parallel algorithm based on Bayesian Optimization (MABO)
is presented. MABO allocates a process to each GPU via a queue mechanism, and
then creates multiple trials at a time for asynchronous parallel search, which
greatly reduces the search time. Compared with the state-of-the-art methods,
the prediction error of Periodformer reduced by 13% and 26% for multivariate
and univariate forecasting, respectively. In addition, MABO reduces the average
search time by 46% while finding better hyperparameters. As a conclusion, this
paper indicates that LTSF may not need complex attention and extra long input
sequences. The source code will be open source on Github.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable and Adaptive Log-based Anomaly Detection with Expert in the
  Loop 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05032v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05032v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinyang Liu, Junjie Huang, Yintong Huo, Zhihan Jiang, Jiazhen Gu, Zhuangbin Chen, Cong Feng, Minzhi Yan, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  System logs play a critical role in maintaining the reliability of software
systems. Fruitful studies have explored automatic log-based anomaly detection
and achieved notable accuracy on benchmark datasets. However, when applied to
large-scale cloud systems, these solutions face limitations due to high
resource consumption and lack of adaptability to evolving logs. In this paper,
we present an accurate, lightweight, and adaptive log-based anomaly detection
framework, referred to as SeaLog. Our method introduces a Trie-based Detection
Agent (TDA) that employs a lightweight, dynamically-growing trie structure for
real-time anomaly detection. To enhance TDA's accuracy in response to evolving
log data, we enable it to receive feedback from experts. Interestingly, our
findings suggest that contemporary large language models, such as ChatGPT, can
provide feedback with a level of consistency comparable to human experts, which
can potentially reduce manual verification efforts. We extensively evaluate
SeaLog on two public datasets and an industrial dataset. The results show that
SeaLog outperforms all baseline methods in terms of effectiveness, runs 2X to
10X faster and only consumes 5% to 41% of the memory resource.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalizable Lightweight Proxy for Robust NAS against Diverse
  Perturbations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05031v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05031v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyeonjeong Ha, Minseon Kim, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent neural architecture search (NAS) frameworks have been successful in
finding optimal architectures for given conditions (e.g., performance or
latency). However, they search for optimal architectures in terms of their
performance on clean images only, while robustness against various types of
perturbations or corruptions is crucial in practice. Although there exist
several robust NAS frameworks that tackle this issue by integrating adversarial
training into one-shot NAS, however, they are limited in that they only
consider robustness against adversarial attacks and require significant
computational resources to discover optimal architectures for a single task,
which makes them impractical in real-world scenarios. To address these
challenges, we propose a novel lightweight robust zero-cost proxy that
considers the consistency across features, parameters, and gradients of both
clean and perturbed images at the initialization state. Our approach
facilitates an efficient and rapid search for neural architectures capable of
learning generalizable features that exhibit robustness across diverse
perturbations. The experimental results demonstrate that our proxy can rapidly
and efficiently search for neural architectures that are consistently robust
against various perturbations on multiple benchmark datasets and diverse search
spaces, largely outperforming existing clean zero-shot NAS and robust NAS with
reduced search cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-level Multiple Instance Learning with <span class="highlight-title">Transformer</span> for Whole Slide
  Image Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05029v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05029v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruijie Zhang, Qiaozhe Zhang, Yingzhuang Liu, Hao Xin, Yan Liu, Xinggang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whole slide image (WSI) refers to a type of high-resolution scanned tissue
image, which is extensively employed in computer-assisted diagnosis (CAD). The
extremely high resolution and limited availability of region-level annotations
make it challenging to employ deep learning methods for WSI-based digital
diagnosis. Multiple instance learning (MIL) is a powerful tool to address the
weak annotation problem, while Transformer has shown great success in the field
of visual tasks. The combination of both should provide new insights for deep
learning based image diagnosis. However, due to the limitations of single-level
MIL and the attention mechanism's constraints on sequence length, directly
applying Transformer to WSI-based MIL tasks is not practical. To tackle this
issue, we propose a Multi-level MIL with Transformer (MMIL-Transformer)
approach. By introducing a hierarchical structure to MIL, this approach enables
efficient handling of MIL tasks that involve a large number of instances. To
validate its effectiveness, we conducted a set of experiments on WSIs
classification task, where MMIL-Transformer demonstrate superior performance
compared to existing state-of-the-art methods. Our proposed approach achieves
test AUC 94.74% and test accuracy 93.41% on CAMELYON16 dataset, test AUC 99.04%
and test accuracy 94.37% on TCGA-NSCLC dataset, respectively. All code and
pre-trained models are available at: https://github.com/hustvl/MMIL-Transformer
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Posterior Collapse in Linear Conditional and Hierarchical Variational
  Autoencoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05023v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05023v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hien Dang, Tho Tran, Tan Nguyen, Nhat Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The posterior collapse phenomenon in variational autoencoders (VAEs), where
the variational posterior distribution closely matches the prior distribution,
can hinder the quality of the learned latent variables. As a consequence of
posterior collapse, the latent variables extracted by the encoder in VAEs
preserve less information from the input data and thus fail to produce
meaningful representations as input to the reconstruction process in the
decoder. While this phenomenon has been an actively addressed topic related to
VAEs performance, the theory for posterior collapse remains underdeveloped,
especially beyond the standard VAEs. In this work, we advance the theoretical
understanding of posterior collapse to two important and prevalent yet less
studied classes of VAEs: conditional VAEs and hierarchical VAEs. Specifically,
via a non-trivial theoretical analysis of linear conditional VAEs and
hierarchical VAEs with two levels of latent, we prove that the cause of
posterior collapses in these models includes the correlation between the input
and output of the conditional VAEs and the effect of learnable encoder variance
in the hierarchical VAEs. We empirically validate our theoretical findings for
linear conditional and hierarchical VAEs and demonstrate that these results are
also predictive for non-linear cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>50 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mixed-TD: Efficient Neural Network Accelerator with Layer-Specific
  Tensor Decomposition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhewen Yu, Christos-Savvas Bouganis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Network designs are quite diverse, from VGG-style to ResNet-style, and
from Convolutional Neural Networks to Transformers. Towards the design of
efficient accelerators, many works have adopted a dataflow-based, inter-layer
pipelined architecture, with a customised hardware towards each layer,
achieving ultra high throughput and low latency. The deployment of neural
networks to such dataflow architecture accelerators is usually hindered by the
available on-chip memory as it is desirable to preload the weights of neural
networks on-chip to maximise the system performance. To address this, networks
are usually compressed before the deployment through methods such as pruning,
quantization and tensor decomposition. In this paper, a framework for mapping
CNNs onto FPGAs based on a novel tensor decomposition method called Mixed-TD is
proposed. The proposed method applies layer-specific Singular Value
Decomposition (SVD) and Canonical Polyadic Decomposition (CPD) in a mixed
manner, achieving 1.73x to 10.29x throughput per DSP to state-of-the-art CNNs.
Our work is open-sourced: https://github.com/Yu-Zhewen/Mixed-TD
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Non-Intrusive Load Monitoring (NILM) using Deep Neural Networks: A
  <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05017v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05017v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Irani Azad, Roozbeh Rajabi, Abouzar Estebsari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Demand-side management now encompasses more residential loads. To efficiently
apply demand response strategies, it's essential to periodically observe the
contribution of various domestic appliances to total energy consumption.
Non-intrusive load monitoring (NILM), also known as load disaggregation, is a
method for decomposing the total energy consumption profile into individual
appliance load profiles within the household. It has multiple applications in
demand-side management, energy consumption monitoring, and analysis. Various
methods, including machine learning and deep learning, have been used to
implement and improve NILM algorithms. This paper reviews some recent NILM
methods based on deep learning and introduces the most accurate methods for
residential loads. It summarizes public databases for NILM evaluation and
compares methods using standard performance metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, EEEIC 2023 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Closed-form Equations for Subgrid-scale Closures from
  High-fidelity Data: Promises and Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Karan Jakhar, Yifei Guan, Rambod Mojgani, Ashesh Chattopadhyay, Pedram Hassanzadeh, Laura Zanna
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  There is growing interest in discovering interpretable, closed-form equations
for subgrid-scale (SGS) closures/parameterizations of complex processes in
Earth system. Here, we apply a common equation-discovery technique with
expansive libraries to learn closures from filtered direct numerical
simulations of 2D forced turbulence and Rayleigh-B\'enard convection (RBC).
Across common filters, we robustly discover closures of the same form for
momentum and heat fluxes. These closures depend on nonlinear combinations of
gradients of filtered variables (velocity, temperature), with constants that
are independent of the fluid/flow properties and only depend on filter
type/size. We show that these closures are the nonlinear gradient model (NGM),
which is derivable analytically using Taylor-series expansions. In fact, we
suggest that with common (physics-free) equation-discovery algorithms,
regardless of the system/physics, discovered closures are always consistent
with the Taylor-series. Like previous studies, we find that large-eddy
simulations with NGM closures are unstable, despite significant similarities
between the true and NGM-predicted fluxes (pattern correlations $> 0.95$). We
identify two shortcomings as reasons for these instabilities: in 2D, NGM
produces zero kinetic energy transfer between resolved and subgrid scales,
lacking both diffusion and backscattering. In RBC, backscattering of potential
energy is poorly predicted. Moreover, we show that SGS fluxes diagnosed from
data, presumed the "truth" for discovery, depend on filtering procedures and
are not unique. Accordingly, to learn accurate, stable closures from
high-fidelity data in future work, we propose several ideas around using
physics-informed libraries, loss functions, and metrics. These findings are
relevant beyond turbulence to closure modeling of any multi-scale system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>40 pages, 4 figures. The codes and data used in this work can be
  found at https://github.com/jakharkaran/EqsDiscovery_2D-FHIT_RBC and
  https://doi.org/10.5281/zenodo.7500647, respectively</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequence-to-Sequence Model with <span class="highlight-title">Transformer</span>-based Attention Mechanism
  and Temporal Pooling for Non-Intrusive Load Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05012v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05012v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Irani Azad, Roozbeh Rajabi, Abouzar Estebsari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel Sequence-to-Sequence (Seq2Seq) model based on a
transformer-based attention mechanism and temporal pooling for Non-Intrusive
Load Monitoring (NILM) of smart buildings. The paper aims to improve the
accuracy of NILM by using a deep learning-based method. The proposed method
uses a Seq2Seq model with a transformer-based attention mechanism to capture
the long-term dependencies of NILM data. Additionally, temporal pooling is used
to improve the model's accuracy by capturing both the steady-state and
transient behavior of appliances. The paper evaluates the proposed method on a
publicly available dataset and compares the results with other state-of-the-art
NILM techniques. The results demonstrate that the proposed method outperforms
the existing methods in terms of both accuracy and computational efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, EEEIC 2023 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Attention Weighted Mixture of Experts with Contrastive Learning for
  Personalized Ranking in E-commerce <span class="chip">ICDE2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05011v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05011v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juan Gong, Zhenlin Chen, Chaoyi Ma, Zhuojian Xiao, Haonan Wang, Guoyu Tang, Lin Liu, Sulong Xu, Bo Long, Yunjiang Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ranking model plays an essential role in e-commerce search and
recommendation. An effective ranking model should give a personalized ranking
list for each user according to the user preference. Existing algorithms
usually extract a user representation vector from the user behavior sequence,
then feed the vector into a feed-forward network (FFN) together with other
features for feature interactions, and finally produce a personalized ranking
score. Despite tremendous progress in the past, there is still room for
improvement. Firstly, the personalized patterns of feature interactions for
different users are not explicitly modeled. Secondly, most of existing
algorithms have poor personalized ranking results for long-tail users with few
historical behaviors due to the data sparsity. To overcome the two challenges,
we propose Attention Weighted Mixture of Experts (AW-MoE) with contrastive
learning for personalized ranking. Firstly, AW-MoE leverages the MoE framework
to capture personalized feature interactions for different users. To model the
user preference, the user behavior sequence is simultaneously fed into expert
networks and the gate network. Within the gate network, one gate unit and one
activation unit are designed to adaptively learn the fine-grained activation
vector for experts using an attention mechanism. Secondly, a random masking
strategy is applied to the user behavior sequence to simulate long-tail users,
and an auxiliary contrastive loss is imposed to the output of the gate network
to improve the model generalization for these users. This is validated by a
higher performance gain on the long-tail user test set. Experiment results on a
JD real production dataset and a public dataset demonstrate the effectiveness
of AW-MoE, which significantly outperforms state-of-art methods. Notably,
AW-MoE has been successfully deployed in the JD e-commerce search engine, ...
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICDE2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COURIER: Contrastive User Intention Reconstruction for Large-Scale
  <span class="highlight-title">Pre-Train</span> of Image Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05001v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05001v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Qi Yang, Chenglei Dai, OU Dan, Ju Huang, De-Chuan Zhan, Qingwen Liu, Xiaoyi Zeng, Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the development of the multi-media internet, visual characteristics have
become an important factor affecting user interests. Thus, incorporating visual
features is a promising direction for further performance improvements in
click-through rate (CTR) prediction. However, we found that simply injecting
the image embeddings trained with established pre-training methods only has
marginal improvements. We attribute the failure to two reasons: First, The
pre-training methods are designed for well-defined computer vision tasks
concentrating on semantic features, and they cannot learn personalized interest
in recommendations. Secondly, pre-trained image embeddings only containing
semantic information have little information gain, considering we already have
semantic features such as categories and item titles as inputs in the CTR
prediction task. We argue that a pre-training method tailored for
recommendation is necessary for further improvements. To this end, we propose a
recommendation-aware image pre-training method that can learn visual features
from user click histories. Specifically, we propose a user interest
reconstruction module to mine visual features related to user interests from
behavior histories. We further propose a contrastive training method to avoid
collapsing of embedding vectors. We conduct extensive experiments to verify
that our method can learn users' visual interests, and our method achieves
$0.46\%$ improvement in offline AUC and $0.88\%$ improvement in Taobao online
GMV with p-value$<0.01$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Ambulance Demand Prediction via Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04994v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04994v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximiliane Rautenstrauß, Maximilian Schiffer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Minimizing response times is crucial for emergency medical services to reduce
patients' waiting times and to increase their survival rates. Many models exist
to optimize operational tasks such as ambulance allocation and dispatching.
Including accurate demand forecasts in such models can improve operational
decision-making. Against this background, we present a novel convolutional
neural network (CNN) architecture that transforms time series data into
heatmaps to predict ambulance demand. Applying such predictions requires
incorporating external features that influence ambulance demands. We contribute
to the existing literature by providing a flexible, generic CNN architecture,
allowing for the inclusion of external features with varying dimensions.
Additionally, we provide a feature selection and hyperparameter optimization
framework utilizing Bayesian optimization. We integrate historical ambulance
demand and external information such as weather, events, holidays, and time. To
show the superiority of the developed CNN architecture over existing
approaches, we conduct a case study for Seattle's 911 call data and include
external information. We show that the developed CNN architecture outperforms
existing state-of-the-art methods and industry practice by more than 9%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Beyond Probability Partitions: Calibrating Neural Networks with Semantic
  Aware Grouping 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04985v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04985v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jia-Qi Yang, De-Chuan Zhan, Le Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Research has shown that deep networks tend to be overly optimistic about
their predictions, leading to an underestimation of prediction errors. Due to
the limited nature of data, existing studies have proposed various methods
based on model prediction probabilities to bin the data and evaluate
calibration error. We propose a more generalized definition of calibration
error called Partitioned Calibration Error (PCE), revealing that the key
difference among these calibration error metrics lies in how the data space is
partitioned. We put forth an intuitive proposition that an accurate model
should be calibrated across any partition, suggesting that the input space
partitioning can extend beyond just the partitioning of prediction
probabilities, and include partitions directly related to the input. Through
semantic-related partitioning functions, we demonstrate that the relationship
between model accuracy and calibration lies in the granularity of the
partitioning function. This highlights the importance of partitioning criteria
for training a calibrated and accurate model. To validate the aforementioned
analysis, we propose a method that involves jointly learning a semantic aware
grouping function based on deep model features and logits to partition the data
space into subsets. Subsequently, a separate calibration function is learned
for each subset. Experimental results demonstrate that our approach achieves
significant performance improvements across multiple datasets and network
architectures, thus highlighting the importance of the partitioning function
for calibration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ G$^2$uardFL: Safeguarding Federated Learning Against Backdoor Attacks
  through Attributed Client Graph Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yu, Chuan Ma, Meng Liu, Xinwang Liu, Zhe Liu, Ming Ding
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As a collaborative paradigm, Federated Learning (FL) empowers clients to
engage in collective model training without exchanging their respective local
data. Nevertheless, FL remains vulnerable to backdoor attacks in which an
attacker compromises malicious clients, and injects poisoned model weights into
the aggregation process to yield attacker-chosen predictions for particular
samples. Existing countermeasures, mainly based on anomaly detection, may
erroneously reject legitimate weights while accepting malicious ones, which is
due to inadequacies in quantifying client model similarities. Other defense
mechanisms prove effective exclusively when confronted with a restricted number
of malicious clients, e.g., less than 10%. To address these vulnerabilities, we
present G$^2$uardFL, a protective framework that reframes the detection of
malicious clients as an attributed graph clustering problem, thereby
safeguarding FL systems. This framework employs a client graph clustering
technique to identify malicious clients and incorporates an adaptive method to
amplify the disparity between the aggregated model and poisoned client models,
thereby eliminating previously embedded backdoors. A theoretical analysis of
convergence is also performed to demonstrate that the global model closely
approximates the model untouched by any backdoor. Through empirical evaluation
compared to cutting-edge defenses and against various backdoor attacks, our
experimental results indicate that G$^2$uardFL considerably undermines the
effectiveness of backdoor attacks while maintaining a negligible impact on the
benign sample performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoCo: A Coupled Contrastive Framework for Unsupervised Domain Adaptive
  Graph Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04979v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04979v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nan Yin, Li Shen, Mengzhu Wang, Long Lan, Zeyu Ma, Chong Chen, Xian-Sheng Hua, Xiao Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although graph neural networks (GNNs) have achieved impressive achievements
in graph classification, they often need abundant task-specific labels, which
could be extensively costly to acquire. A credible solution is to explore
additional labeled graphs to enhance unsupervised learning on the target
domain. However, how to apply GNNs to domain adaptation remains unsolved owing
to the insufficient exploration of graph topology and the significant domain
discrepancy. In this paper, we propose \underline{Co}upled
\underline{Co}ntrastive Graph Representation Learning (\method{}), which
extracts the topological information from coupled learning branches and reduces
the domain discrepancy with coupled contrastive learning. \method{} contains a
graph convolutional network branch and a hierarchical graph kernel network
branch, which explore graph topology in implicit and explicit manners. Besides,
we incorporate coupled branches into a holistic multi-view contrastive learning
framework, which not only incorporates graph representations learned from
complementary views for enhanced understanding, but also encourages the
similarity between cross-domain example pairs with the same semantics for
domain alignment. Extensive experiments on various popular datasets show that
\method{} outperforms these competing baselines by 5.7\% to 21.0\% generally.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conservative Prediction via Data-Driven Confidence Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caroline Choi, Fahim Tajwar, Yoonho Lee, Huaxiu Yao, Ananya Kumar, Chelsea Finn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Errors of machine learning models are costly, especially in safety-critical
domains such as healthcare, where such mistakes can prevent the deployment of
machine learning altogether. In these settings, conservative models -- models
which can defer to human judgment when they are likely to make an error -- may
offer a solution. However, detecting unusual or difficult examples is notably
challenging, as it is impossible to anticipate all potential inputs at test
time. To address this issue, prior work has proposed to minimize the model's
confidence on an auxiliary pseudo-OOD dataset. We theoretically analyze the
effect of confidence minimization and show that the choice of auxiliary dataset
is critical. Specifically, if the auxiliary dataset includes samples from the
OOD region of interest, confidence minimization provably separates ID and OOD
inputs by predictive confidence. Taking inspiration from this result, we
present data-driven confidence minimization (DCM), which minimizes confidence
on an uncertainty dataset containing examples that the model is likely to
misclassify at test time. Our experiments show that DCM consistently
outperforms state-of-the-art OOD detection methods on 8 ID-OOD dataset pairs,
reducing FPR (at TPR 95%) by 6.3% and 58.1% on CIFAR-10 and CIFAR-100, and
outperforms existing selective classification approaches on 4 datasets in
conditions of distribution shift.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Melting Pot of Evolution and Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moshe Sipper, Achiya Elyasaf, Tomer Halperin, Zvika Haramaty, Raz Lapid, Eyal Segal, Itai Tzruia, Snir Vitrack Tamam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We survey eight recent works by our group, involving the successful blending
of evolutionary algorithms with machine learning and deep learning: 1. Binary
and Multinomial Classification through Evolutionary Symbolic Regression, 2.
Classy Ensemble: A Novel Ensemble Algorithm for Classification, 3. EC-KitY:
Evolutionary Computation Tool Kit in Python, 4. Evolution of Activation
Functions for Deep Learning-Based Image Classification, 5. Adaptive Combination
of a Genetic Algorithm and Novelty Search for Deep Neuroevolution, 6. An
Evolutionary, Gradient-Free, Query-Efficient, Black-Box Algorithm for
Generating Adversarial Instances in Deep Networks, 7. Foiling Explanations in
Deep Neural Networks, 8. Patch of Invisibility: Naturalistic Black-Box
Adversarial Attacks on Object Detectors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To Appear in Proceedings of Genetic Programming Theory & Practice XX,
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Language Identification to Enhance Code-Mixed Text
  Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04964v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04964v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gauri Takawane, Abhishek Phaltankar, Varad Patwardhan, Aryan Patil, Raviraj Joshi, Mukta S. Takalikar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The usage of more than one language in the same text is referred to as Code
Mixed. It is evident that there is a growing degree of adaption of the use of
code-mixed data, especially English with a regional language, on social media
platforms. Existing deep-learning models do not take advantage of the implicit
language information in the code-mixed text. Our study aims to improve
BERT-based models performance on low-resource Code-Mixed Hindi-English Datasets
by experimenting with language augmentation approaches. We propose a pipeline
to improve code-mixed systems that comprise data preprocessing, word-level
language identification, language augmentation, and model training on
downstream tasks like sentiment analysis. For language augmentation in BERT
models, we explore word-level interleaving and post-sentence placement of
language information. We have examined the performance of vanilla BERT-based
models and their code-mixed HingBERT counterparts on respective benchmark
datasets, comparing their results with and without using word-level language
information. The models were evaluated using metrics such as accuracy,
precision, recall, and F1 score. Our findings show that the proposed language
augmentation approaches work well across different BERT models. We demonstrate
the importance of augmenting code-mixed text with language information on five
different code-mixed Hindi-English downstream datasets based on sentiment
analysis, hate speech detection, and emotion detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ arXiv4TGC: Large-Scale <span class="highlight-title">Dataset</span>s for Temporal Graph Clustering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04962v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04962v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meng Liu, Ke Liang, Yue Liu, Siwei Wang, Sihang Zhou, Xinwang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Temporal graph clustering (TGC) is a crucial task in temporal graph learning.
Its focus is on node clustering on temporal graphs, and it offers greater
flexibility for large-scale graph structures due to the mechanism of temporal
graph methods. However, the development of TGC is currently constrained by a
significant problem: the lack of suitable and reliable large-scale temporal
graph datasets to evaluate clustering performance. In other words, most
existing temporal graph datasets are in small sizes, and even large-scale
datasets contain only a limited number of available node labels. It makes
evaluating models for large-scale temporal graph clustering challenging. To
address this challenge, we build arXiv4TGC, a set of novel academic datasets
(including arXivAI, arXivCS, arXivMath, arXivPhy, and arXivLarge) for
large-scale temporal graph clustering. In particular, the largest dataset,
arXivLarge, contains 1.3 million labeled available nodes and 10 million
temporal edges. We further compare the clustering performance with typical
temporal graph learning models on both previous classic temporal graph datasets
and the new datasets proposed in this paper. The clustering performance on
arXiv4TGC can be more apparent for evaluating different models, resulting in
higher clustering confidence and more suitable for large-scale temporal graph
clustering. The arXiv4TGC datasets are publicly available at:
https://github.com/MGitHubL/arXiv4TGC.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recovering Simultaneously Structured Data via Non-Convex Iteratively
  Reweighted Least Squares 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04961v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04961v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Kümmerle, Johannes Maly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new algorithm for the problem of recovering data that adheres to
multiple, heterogeneous low-dimensional structures from linear observations.
Focusing on data matrices that are simultaneously row-sparse and low-rank, we
propose and analyze an iteratively reweighted least squares (IRLS) algorithm
that is able to leverage both structures. In particular, it optimizes a
combination of non-convex surrogates for row-sparsity and rank, a balancing of
which is built into the algorithm. We prove locally quadratic convergence of
the iterates to a simultaneously structured data matrix in a regime of minimal
sample complexity (up to constants and a logarithmic factor), which is known to
be impossible for a combination of convex surrogates. In experiments, we show
that the IRLS method exhibits favorable empirical convergence, identifying
simultaneously row-sparse and low-rank matrices from fewer measurements than
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive Fake Audio Detection with Low-Rank Model Squeezing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04956v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04956v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohui Zhang, Jiangyan Yi, Jianhua Tao, Chenlong Wang, Le Xu, Ruibo Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid advancement of spoofing algorithms necessitates the development of
robust detection methods capable of accurately identifying emerging fake audio.
Traditional approaches, such as finetuning on new datasets containing these
novel spoofing algorithms, are computationally intensive and pose a risk of
impairing the acquired knowledge of known fake audio types. To address these
challenges, this paper proposes an innovative approach that mitigates the
limitations associated with finetuning. We introduce the concept of training
low-rank adaptation matrices tailored specifically to the newly emerging fake
audio types. During the inference stage, these adaptation matrices are combined
with the existing model to generate the final prediction output. Extensive
experimentation is conducted to evaluate the efficacy of the proposed method.
The results demonstrate that our approach effectively preserves the prediction
accuracy of the existing model for known fake audio types. Furthermore, our
approach offers several advantages, including reduced storage memory
requirements and lower equal error rates compared to conventional finetuning
methods, particularly on specific spoofing algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Degraded Polygons Raise Fundamental Questions of Neural Network
  Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leonard Tang, Dan Ley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It is well-known that modern computer vision systems often exhibit behaviors
misaligned with those of humans: from adversarial attacks to image corruptions,
deep learning vision models suffer in a variety of settings that humans capably
handle. In light of these phenomena, here we introduce another, orthogonal
perspective studying the human-machine vision gap. We revisit the task of
recovering images under degradation, first introduced over 30 years ago in the
Recognition-by-Components theory of human vision. Specifically, we study the
performance and behavior of neural networks on the seemingly simple task of
classifying regular polygons at varying orders of degradation along their
perimeters. To this end, we implement the Automated Shape Recoverability Test
for rapidly generating large-scale datasets of perimeter-degraded regular
polygons, modernizing the historically manual creation of image recoverability
experiments. We then investigate the capacity of neural networks to recognize
and recover such degraded shapes when initialized with different priors.
Ultimately, we find that neural networks' behavior on this simple task
conflicts with human behavior, raising a fundamental question of the robustness
and learning capabilities of modern computer vision models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entropy-based Training Methods for Scalable Neural Implicit Sampler 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04952v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04952v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weijian Luo, Boya Zhang, Zhihua Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficiently sampling from un-normalized target distributions is a fundamental
problem in scientific computing and machine learning. Traditional approaches
like Markov Chain Monte Carlo (MCMC) guarantee asymptotically unbiased samples
from such distributions but suffer from computational inefficiency,
particularly when dealing with high-dimensional targets, as they require
numerous iterations to generate a batch of samples. In this paper, we propose
an efficient and scalable neural implicit sampler that overcomes these
limitations. Our sampler can generate large batches of samples with low
computational costs by leveraging a neural transformation that directly maps
easily sampled latent vectors to target samples without the need for iterative
procedures. To train the neural implicit sampler, we introduce two novel
methods: the KL training method and the Fisher training method. The former
minimizes the Kullback-Leibler divergence, while the latter minimizes the
Fisher divergence. By employing these training methods, we effectively optimize
the neural implicit sampler to capture the desired target distribution. To
demonstrate the effectiveness, efficiency, and scalability of our proposed
samplers, we evaluate them on three sampling benchmarks with different scales.
These benchmarks include sampling from 2D targets, Bayesian inference, and
sampling from high-dimensional energy-based models (EBMs). Notably, in the
experiment involving high-dimensional EBMs, our sampler produces samples that
are comparable to those generated by MCMC-based methods while being more than
100 times more efficient, showcasing the efficiency of our neural sampler. We
believe that the theoretical and empirical contributions presented in this work
will stimulate further research on developing efficient samplers for various
applications beyond the ones explored in this study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Robust Learning with Progressive Data Expansion Against Spurious
  Correlation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04949v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04949v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihe Deng, Yu Yang, Baharan Mirzasoleiman, Quanquan Gu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep learning models have shown remarkable performance in various
tasks, they are susceptible to learning non-generalizable spurious features
rather than the core features that are genuinely correlated to the true label.
In this paper, beyond existing analyses of linear models, we theoretically
examine the learning process of a two-layer nonlinear convolutional neural
network in the presence of spurious features. Our analysis suggests that
imbalanced data groups and easily learnable spurious features can lead to the
dominance of spurious features during the learning process. In light of this,
we propose a new training algorithm called PDE that efficiently enhances the
model's robustness for a better worst-group performance. PDE begins with a
group-balanced subset of training data and progressively expands it to
facilitate the learning of the core features. Experiments on synthetic and
real-world benchmark datasets confirm the superior performance of our method on
models such as ResNets and Transformers. On average, our method achieves a 2.8%
improvement in worst-group accuracy compared with the state-of-the-art method,
while enjoying up to 10x faster training efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ShuttleSet: A Human-Annotated Stroke-Level Singles <span class="highlight-title">Dataset</span> for Badminton
  Tactical Analysis <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei-Yao Wang, Yung-Chang Huang, Tsi-Ui Ik, Wen-Chih Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the recent progress in sports analytics, deep learning approaches have
demonstrated the effectiveness of mining insights into players' tactics for
improving performance quality and fan engagement. This is attributed to the
availability of public ground-truth datasets. While there are a few available
datasets for turn-based sports for action detection, these datasets severely
lack structured source data and stroke-level records since these require
high-cost labeling efforts from domain experts and are hard to detect using
automatic techniques. Consequently, the development of artificial intelligence
approaches is significantly hindered when existing models are applied to more
challenging structured turn-based sequences. In this paper, we present
ShuttleSet, the largest publicly-available badminton singles dataset with
annotated stroke-level records. It contains 104 sets, 3,685 rallies, and 36,492
strokes in 44 matches between 2018 and 2021 with 27 top-ranking men's singles
and women's singles players. ShuttleSet is manually annotated with a
computer-aided labeling tool to increase the labeling efficiency and
effectiveness of selecting the shot type with a choice of 18 distinct classes,
the corresponding hitting locations, and the locations of both players at each
stroke. In the experiments, we provide multiple benchmarks (i.e., stroke
influence, stroke forecasting, and movement forecasting) with baselines to
illustrate the practicability of using ShuttleSet for turn-based analytics,
which is expected to stimulate both academic and sports communities. Over the
past two years, a visualization platform has been deployed to illustrate the
variability of analysis cases from ShuttleSet for coaches to delve into
players' tactical preferences with human-interactive interfaces, which was also
used by national badminton teams during multiple international high-ranking
matches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD 2023. Project page: https://github.com/wywyWang/CoachAI-Projects</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A modified model for topic detection from a corpus and a new metric
  evaluating the understandability of topics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04941v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04941v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomoya Kitano, Yuto Miyatake, Daisuke Furihata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a modified neural model for topic detection from a corpus
and proposes a new metric to evaluate the detected topics. The new model builds
upon the embedded topic model incorporating some modifications such as document
clustering. Numerical experiments suggest that the new model performs
favourably regardless of the document's length. The new metric, which can be
computed more efficiently than widely-used metrics such as topic coherence,
provides variable information regarding the understandability of the detected
topics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Layer-level activation mechanism 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04940v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04940v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yoon Kihyuk, Lim Chiehyeon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we propose a novel activation mechanism aimed at establishing
layer-level activation (LayerAct) functions. These functions are designed to be
more noise-robust compared to traditional element-level activation functions by
reducing the layer-level fluctuation of the activation outputs due to shift in
inputs. Moreover, the LayerAct functions achieve a zero-like mean activation
output without restricting the activation output space. We present an analysis
and experiments demonstrating that LayerAct functions exhibit superior
noise-robustness compared to element-level activation functions, and
empirically show that these functions have a zero-like mean activation.
Experimental results on three benchmark image classification tasks show that
LayerAct functions excel in handling noisy image datasets, outperforming
element-level activation functions, while the performance on clean datasets is
also superior in most cases.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, 4 tables except appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ When to <span class="highlight-title">Pre-Train</span> Graph Neural Networks? From Data Generation
  Perspective! 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16458v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16458v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Cao, Jiarong Xu, Carl Yang, Jiaan Wang, Yunchao Zhang, Chunping Wang, Lei Chen, Yang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, graph pre-training has gained significant attention,
focusing on acquiring transferable knowledge from unlabeled graph data to
improve downstream performance. Despite these recent endeavors, the problem of
negative transfer remains a major concern when utilizing graph pre-trained
models to downstream tasks. Previous studies made great efforts on the issue of
what to pre-train and how to pre-train by designing a variety of graph
pre-training and fine-tuning strategies. However, there are cases where even
the most advanced "pre-train and fine-tune" paradigms fail to yield distinct
benefits. This paper introduces a generic framework W2PGNN to answer the
crucial question of when to pre-train (i.e., in what situations could we take
advantage of graph pre-training) before performing effortful pre-training or
fine-tuning. We start from a new perspective to explore the complex generative
mechanisms from the pre-training data to downstream data. In particular, W2PGNN
first fits the pre-training data into graphon bases, each element of graphon
basis (i.e., a graphon) identifies a fundamental transferable pattern shared by
a collection of pre-training graphs. All convex combinations of graphon bases
give rise to a generator space, from which graphs generated form the solution
space for those downstream data that can benefit from pre-training. In this
manner, the feasibility of pre-training can be quantified as the generation
probability of the downstream data from any generator in the generator space.
W2PGNN offers three broad applications: providing the application scope of
graph pre-trained models, quantifying the feasibility of pre-training, and
assistance in selecting pre-training data to enhance downstream performance. We
provide a theoretically sound solution for the first application and extensive
empirical justifications for the latter two applications.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Target-based Surrogates for Stochastic Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02607v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02607v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Wilder Lavington, Sharan Vaswani, Reza Babanezhad, Mark Schmidt, Nicolas Le Roux
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider minimizing functions for which it is expensive to compute the
(possibly stochastic) gradient. Such functions are prevalent in reinforcement
learning, imitation learning and adversarial training. Our target optimization
framework uses the (expensive) gradient computation to construct surrogate
functions in a \emph{target space} (e.g. the logits output by a linear model
for classification) that can be minimized efficiently. This allows for multiple
parameter updates to the model, amortizing the cost of gradient computation. In
the full-batch setting, we prove that our surrogate is a global upper-bound on
the loss, and can be (locally) minimized using a black-box optimization
algorithm. We prove that the resulting majorization-minimization algorithm
ensures convergence to a stationary point of the loss. Next, we instantiate our
framework in the stochastic setting and propose the $SSO$ algorithm, which can
be viewed as projected stochastic gradient descent in the target space. This
connection enables us to prove theoretical guarantees for $SSO$ when minimizing
convex functions. Our framework allows the use of standard stochastic
optimization algorithms to construct surrogates which can be minimized by any
deterministic optimization method. To evaluate our framework, we consider a
suite of supervised learning and imitation learning problems. Our experiments
indicate the benefits of target optimization and the effectiveness of $SSO$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reduce, Reuse, Recycle: Compositional Generation with Energy-Based
  Diffusion Models and MCMC <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11552v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11552v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, Will Grathwohl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since their introduction, diffusion models have quickly become the prevailing
approach to generative modeling in many domains. They can be interpreted as
learning the gradients of a time-varying sequence of log-probability density
functions. This interpretation has motivated classifier-based and
classifier-free guidance as methods for post-hoc control of diffusion models.
In this work, we build upon these ideas using the score-based interpretation of
diffusion models, and explore alternative ways to condition, modify, and reuse
diffusion models for tasks involving compositional generation and guidance. In
particular, we investigate why certain types of composition fail using current
techniques and present a number of solutions. We conclude that the sampler (not
the model) is responsible for this failure and propose new samplers, inspired
by MCMC, which enable successful compositional generation. Further, we propose
an energy-based parameterization of diffusion models which enables the use of
new compositional operators and more sophisticated, Metropolis-corrected
samplers. Intriguingly we find these samplers lead to notable improvements in
compositional generation across a wide set of problems such as
classifier-guided ImageNet modeling and compositional text-to-image generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023, Project Webpage:
  https://energy-based-model.github.io/reduce-reuse-recycle/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Parallel Sampling of Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16317v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16317v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andy Shih, Suneel Belkhale, Stefano Ermon, Dorsa Sadigh, Nima Anari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are powerful generative models but suffer from slow
sampling, often taking 1000 sequential denoising steps for one sample. As a
result, considerable efforts have been directed toward reducing the number of
denoising steps, but these methods hurt sample quality. Instead of reducing the
number of denoising steps (trading quality for speed), in this paper we explore
an orthogonal approach: can we run the denoising steps in parallel (trading
compute for speed)? In spite of the sequential nature of the denoising steps,
we show that surprisingly it is possible to parallelize sampling via Picard
iterations, by guessing the solution of future denoising steps and iteratively
refining until convergence. With this insight, we present ParaDiGMS, a novel
method to accelerate the sampling of pretrained diffusion models by denoising
multiple steps in parallel. ParaDiGMS is the first diffusion sampling method
that enables trading compute for speed and is even compatible with existing
fast sampling techniques such as DDIM and DPMSolver. Using ParaDiGMS, we
improve sampling speed by 2-4x across a range of robotics and image generation
models, giving state-of-the-art sampling speeds of 0.2s on 100-step
DiffusionPolicy and 16s on 1000-step StableDiffusion-v2 with no measurable
degradation of task reward, FID score, or CLIP score.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust Subtask Learning for Compositional Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02984v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02984v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kishor Jothimurugan, Steve Hsu, Osbert Bastani, Rajeev Alur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compositional reinforcement learning is a promising approach for training
policies to perform complex long-horizon tasks. Typically, a high-level task is
decomposed into a sequence of subtasks and a separate policy is trained to
perform each subtask. In this paper, we focus on the problem of training
subtask policies in a way that they can be used to perform any task; here, a
task is given by a sequence of subtasks. We aim to maximize the worst-case
performance over all tasks as opposed to the average-case performance. We
formulate the problem as a two agent zero-sum game in which the adversary picks
the sequence of subtasks. We propose two RL algorithms to solve this game: one
is an adaptation of existing multi-agent RL algorithms to our setting and the
other is an asynchronous version which enables parallel training of subtask
policies. We evaluate our approach on two multi-task environments with
continuous states and actions and demonstrate that our algorithms outperform
state-of-the-art baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Leveraging Diffusion For Strong and High Quality Face Morphing Attacks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04218v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04218v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zander Blasingame, Chen Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Face morphing attacks seek to deceive a Face Recognition (FR) system by
presenting a morphed image consisting of the biometric qualities from two
different identities with the aim of triggering a false acceptance with one of
the two identities, thereby presenting a significant threat to biometric
systems. The success of a morphing attack is dependent on the ability of the
morphed image to represent the biometric characteristics of both identities
that were used to create the image. We present a novel morphing attack that
uses a Diffusion-based architecture to improve the visual fidelity of the image
and the ability of the morphing attack to represent characteristics from both
identities. We demonstrate the effectiveness of the proposed attack by
evaluating its visual fidelity via the Frechet Inception Distance (FID). Also,
extensive experiments are conducted to measure the vulnerability of FR systems
to the proposed attack. The ability of a morphing attack detector to detect the
proposed attack is measured and compared against two state-of-the-art GAN-based
morphing attacks along with two Landmark-based attacks. Additionally, a novel
metric to measure the relative strength between different morphing attacks is
introduced and evaluated.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DP-Fast MH: Private, Fast, and Accurate Metropolis-Hastings for
  Large-Scale Bayesian Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06171v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06171v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanrong Zhang, Ruqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian inference provides a principled framework for learning from complex
data and reasoning under uncertainty. It has been widely applied in machine
learning tasks such as medical diagnosis, drug design, and policymaking. In
these common applications, data can be highly sensitive. Differential privacy
(DP) offers data analysis tools with powerful worst-case privacy guarantees and
has been developed as the leading approach in privacy-preserving data analysis.
In this paper, we study Metropolis-Hastings (MH), one of the most fundamental
MCMC methods, for large-scale Bayesian inference under differential privacy.
While most existing private MCMC algorithms sacrifice accuracy and efficiency
to obtain privacy, we provide the first exact and fast DP MH algorithm, using
only a minibatch of data in most iterations. We further reveal, for the first
time, a three-way trade-off among privacy, scalability (i.e. the batch size),
and efficiency (i.e. the convergence rate), theoretically characterizing how
privacy affects the utility and computational cost in Bayesian inference. We
empirically demonstrate the effectiveness and efficiency of our algorithm in
various experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Bandits without Graph Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11401v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11401v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mikhail Konobeev, Jalal Etesami, Negar Kiyavash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the causal bandit problem when the causal graph is unknown and
develop an efficient algorithm for finding the parent node of the reward node
using atomic interventions. We derive the exact equation for the expected
number of interventions performed by the algorithm and show that under certain
graphical conditions it could perform either logarithmically fast or, under
more general assumptions, slower but still sublinearly in the number of
variables. We formally show that our algorithm is optimal as it meets the
universal lower bound we establish for any algorithm that performs atomic
interventions. Finally, we extend our algorithm to the case when the reward
node has multiple parents. Using this algorithm together with a standard
algorithm from bandit literature leads to improved regret bounds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multitask Learning and Bandits via Robust Statistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.14233v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.14233v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kan Xu, Hamsa Bastani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision-makers often simultaneously face many related but heterogeneous
learning problems. For instance, a large retailer may wish to learn product
demand at different stores to solve pricing or inventory problems, making it
desirable to learn jointly for stores serving similar customers; alternatively,
a hospital network may wish to learn patient risk at different providers to
allocate personalized interventions, making it desirable to learn jointly for
hospitals serving similar patient populations. Motivated by real datasets, we
study a natural setting where the unknown parameter in each learning instance
can be decomposed into a shared global parameter plus a sparse
instance-specific term. We propose a novel two-stage multitask learning
estimator that exploits this structure in a sample-efficient way, using a
unique combination of robust statistics (to learn across similar instances) and
LASSO regression (to debias the results). Our estimator yields improved sample
complexity bounds in the feature dimension $d$ relative to commonly-employed
estimators; this improvement is exponential for "data-poor" instances, which
benefit the most from multitask learning. We illustrate the utility of these
results for online learning by embedding our multitask estimator within
simultaneous contextual bandit algorithms. We specify a dynamic calibration of
our estimator to appropriately balance the bias-variance tradeoff over time,
improving the resulting regret bounds in the context dimension $d$. Finally, we
illustrate the value of our approach on synthetic and real datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph-based Time-Series Anomaly Detection: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00058v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00058v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thi Kieu Khanh Ho, Ali Karami, Narges Armanfard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the recent advances in technology, a wide range of systems continue to
collect a large amount of data over time and thus generate time series.
Time-Series Anomaly Detection (TSAD) is an important task in various
time-series applications such as e-commerce, cybersecurity, vehicle
maintenance, and healthcare monitoring. However, this task is very challenging
as it requires considering both the intra-variable dependency and the
inter-variable dependency, where a variable can be defined as an observation in
time series data. Recent graph-based approaches have made impressive progress
in tackling the challenges of this field. In this survey, we conduct a
comprehensive and up-to-date review of Graph-based TSAD (G-TSAD). First, we
explore the significant potential of graph representation learning for
time-series data. Then, we review state-of-the-art graph anomaly detection
techniques in the context of time series and discuss their strengths and
drawbacks. Finally, we discuss the technical challenges and potential future
directions for possible improvements in this research field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Learning Meets Sparse Regularization: A Signal Processing
  Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.09554v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.09554v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rahul Parhi, Robert D. Nowak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning has been wildly successful in practice and most
state-of-the-art machine learning methods are based on neural networks.
Lacking, however, is a rigorous mathematical theory that adequately explains
the amazing performance of deep neural networks. In this article, we present a
relatively new mathematical framework that provides the beginning of a deeper
understanding of deep learning. This framework precisely characterizes the
functional properties of neural networks that are trained to fit to data. The
key mathematical tools which support this framework include transform-domain
sparse regularization, the Radon transform of computed tomography, and
approximation theory, which are all techniques deeply rooted in signal
processing. This framework explains the effect of weight decay regularization
in neural network training, the use of skip connections and low-rank weight
matrices in network architectures, the role of sparsity in neural networks, and
explains why neural networks can perform well in high-dimensional problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Simple Proof of the Mixing of Metropolis-Adjusted Langevin Algorithm
  under Smoothness and Isoperimetry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04095v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04095v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuansi Chen, Khashayar Gatmiry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the mixing time of Metropolis-Adjusted Langevin algorithm (MALA) for
sampling a target density on $\mathbb{R}^d$. We assume that the target density
satisfies $\psi_\mu$-isoperimetry and that the operator norm and trace of its
Hessian are bounded by $L$ and $\Upsilon$ respectively. Our main result
establishes that, from a warm start, to achieve $\epsilon$-total variation
distance to the target density, MALA mixes in
$O\left(\frac{(L\Upsilon)^{\frac12}}{\psi_\mu^2}
\log\left(\frac{1}{\epsilon}\right)\right)$ iterations. Notably, this result
holds beyond the log-concave sampling setting and the mixing time depends on
only $\Upsilon$ rather than its upper bound $L d$. In the $m$-strongly
logconcave and $L$-log-smooth sampling setting, our bound recovers the previous
minimax mixing bound of MALA~\cite{wu2021minimax}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cyclic Coordinate Dual Averaging with Extrapolation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2102.13244v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2102.13244v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaobing Song, Jelena Diakonikolas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cyclic block coordinate methods are a fundamental class of optimization
methods widely used in practice and implemented as part of standard software
packages for statistical learning. Nevertheless, their convergence is generally
not well understood and so far their good practical performance has not been
explained by existing convergence analyses. In this work, we introduce a new
block coordinate method that applies to the general class of variational
inequality (VI) problems with monotone operators. This class includes composite
convex optimization problems and convex-concave min-max optimization problems
as special cases and has not been addressed by the existing work. The resulting
convergence bounds match the optimal convergence bounds of full gradient
methods, but are provided in terms of a novel gradient Lipschitz condition
w.r.t.~a Mahalanobis norm. For $m$ coordinate blocks, the resulting gradient
Lipschitz constant in our bounds is never larger than a factor $\sqrt{m}$
compared to the traditional Euclidean Lipschitz constant, while it is possible
for it to be much smaller. Further, for the case when the operator in the VI
has finite-sum structure, we propose a variance reduced variant of our method
which further decreases the per-iteration cost and has better convergence rates
in certain regimes. To obtain these results, we use a gradient extrapolation
strategy that allows us to view a cyclic collection of block coordinate-wise
gradients as one implicit gradient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 2 figures. Accepted to SIAM Journal on Optimization.
  Version prior to final copy editing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Learning of Label and Environment Causal Independence for Graph
  Out-of-Distribution Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01103v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01103v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shurui Gui, Meng Liu, Xiner Li, Youzhi Luo, Shuiwang Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We tackle the problem of graph out-of-distribution (OOD) generalization.
Existing graph OOD algorithms either rely on restricted assumptions or fail to
exploit environment information in training data. In this work, we propose to
simultaneously incorporate label and environment causal independence (LECI) to
fully make use of label and environment information, thereby addressing the
challenges faced by prior methods on identifying causal and invariant
subgraphs. We further develop an adversarial training strategy to jointly
optimize these two properties for causal subgraph discovery with theoretical
guarantees. Extensive experiments and analysis show that LECI significantly
outperforms prior methods on both synthetic and real-world datasets,
establishing LECI as a practical and effective solution for graph OOD
generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deploying clinical machine learning? Consider the following... <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.06919v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.06919v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Lu, Ken Chang, Praveer Singh, Stuart Pomerantz, Sean Doyle, Sujay Kakarmath, Christopher Bridge, Jayashree Kalpathy-Cramer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the intense attention and considerable investment into clinical
machine learning research, relatively few applications have been deployed at a
large-scale in a real-world clinical environment. While research is important
in advancing the state-of-the-art, translation is equally important in bringing
these techniques and technologies into a position to ultimately impact
healthcare. We believe a lack of appreciation for several considerations are a
major cause for this discrepancy between expectation and reality. To better
characterize a holistic perspective among researchers and practitioners, we
survey several practitioners with commercial experience in developing CML for
clinical deployment. Using these insights, we identify several main categories
of challenges in order to better design and develop clinical machine learning
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Trustworthy AI for Healthcare workshop at AAAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating <span class="highlight-title">Self-Supervised</span> Learning for Molecular Graph Embeddings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08005v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08005v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanchen Wang, Jean Kaddour, Shengchao Liu, Jian Tang, Joan Lasenby, Qi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Self-Supervised Learning (GSSL) provides a robust pathway for acquiring
embeddings without expert labelling, a capability that carries profound
implications for molecular graphs due to the staggering number of potential
molecules and the high cost of obtaining labels. However, GSSL methods are
designed not for optimisation within a specific domain but rather for
transferability across a variety of downstream tasks. This broad applicability
complicates their evaluation. Addressing this challenge, we present "Molecular
Graph Representation Evaluation" (MOLGRAPHEVAL), generating detailed profiles
of molecular graph embeddings with interpretable and diversified attributes.
MOLGRAPHEVAL offers a suite of probing tasks grouped into three categories: (i)
generic graph, (ii) molecular substructure, and (iii) embedding space
properties. By leveraging MOLGRAPHEVAL to benchmark existing GSSL methods
against both current downstream datasets and our suite of tasks, we uncover
significant inconsistencies between inferences drawn solely from existing
datasets and those derived from more nuanced probing. These findings suggest
that current evaluation methodologies fail to capture the entirety of the
landscape.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>update results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic noise can be helpful for variational quantum algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06723v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06723v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junyu Liu, Frederik Wilde, Antonio Anna Mele, Liang Jiang, Jens Eisert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Saddle points constitute a crucial challenge for first-order gradient descent
algorithms. In notions of classical machine learning, they are avoided for
example by means of stochastic gradient descent methods. In this work, we
provide evidence that the saddle points problem can be naturally avoided in
variational quantum algorithms by exploiting the presence of stochasticity. We
prove convergence guarantees and present practical examples in numerical
simulations and on quantum hardware. We argue that the natural stochasticity of
variational algorithms can be beneficial for avoiding strict saddle points,
i.e., those saddle points with at least one negative Hessian eigenvalue. This
insight that some levels of shot noise could help is expected to add a new
perspective to notions of near-term variational quantum algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 14 figures, presentation improved, proofs extended</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A fermion neural network with efficient optimization and quantum
  applicability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05793v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05793v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pei-Lin Zheng, Jia-Bao Wang, Yi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical artificial neural networks have witnessed widespread successes in
machine-learning applications. Here, we propose fermion neural networks (FNNs)
whose physical properties, such as local density of states or conditional
conductance, serve as outputs, once the inputs are incorporated as an initial
layer. Comparable to back-propagation, we establish an efficient optimization,
which entitles FNNs to competitive performance on challenging machine-learning
benchmarks. FNNs also directly apply to quantum systems, including hard ones
with interactions, and offer in-situ analysis without preprocessing or
presumption. Following machine learning, FNNs precisely determine topological
phases and emergent charge orders. Their quantum nature also brings various
advantages: quantum correlation entitles more general network connectivity and
insight into the vanishing gradient problem, quantum entanglement opens up
novel avenues for interpretable machine learning, etc.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Lipschitz Bandits Approach for Continuous Hyperparameter Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01539v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01539v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasong Feng, Weijian Luo, Yimin Huang, Tianyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the most critical problems in machine learning is HyperParameter
Optimization (HPO), since choice of hyperparameters has a significant impact on
final model performance. Although there are many HPO algorithms, they either
have no theoretical guarantees or require strong assumptions. To this end, we
introduce BLiE -- a Lipschitz-bandit-based algorithm for HPO that only assumes
Lipschitz continuity of the objective function. BLiE exploits the landscape of
the objective function to adaptively search over the hyperparameter space.
Theoretically, we show that $(i)$ BLiE finds an $\epsilon$-optimal
hyperparameter with $\mathcal{O} \left( \epsilon^{-(d_z + \beta)}\right)$ total
budgets, where $d_z$ and $\beta$ are problem intrinsic; $(ii)$ BLiE is highly
parallelizable. Empirically, we demonstrate that BLiE outperforms the
state-of-the-art HPO algorithms on benchmark tasks. We also apply BLiE to
search for noise schedule of diffusion models. Comparison with the default
schedule shows that BLiE schedule greatly improves the sampling speed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Some preliminaries and backgrounds are drawn from arXiv:2110.09722 by
  the first author and the last author, and their coauthor Z. Huang</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Classification of Stress via Ambulatory ECG and GSR Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.04705v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.04705v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zachary Dair, Muhammad Muneeb Saad, Urja Pawar, Samantha Dockray, Ruairi O'Reilly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In healthcare, detecting stress and enabling individuals to monitor their
mental health and wellbeing is challenging. Advancements in wearable technology
now enable continuous physiological data collection. This data can provide
insights into mental health and behavioural states through psychophysiological
analysis. However, automated analysis is required to provide timely results due
to the quantity of data collected. Machine learning has shown efficacy in
providing an automated classification of physiological data for health
applications in controlled laboratory environments. Ambulatory uncontrolled
environments, however, provide additional challenges requiring further
modelling to overcome. This work empirically assesses several approaches
utilising machine learning classifiers to detect stress using physiological
data recorded in an ambulatory setting with self-reported stress annotations. A
subset of the training portion SMILE dataset enables the evaluation of
approaches before submission. The optimal stress detection approach achieves
90.77% classification accuracy, 91.24 F1-Score, 90.42 Sensitivity and 91.08
Specificity, utilising an ExtraTrees classifier and feature imputation methods.
Meanwhile, accuracy on the challenge data is much lower at 59.23% (submission
#54 from BEaTS-MTU, username ZacDair). The cause of the performance disparity
is explored in this work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Associated Code to enable reproducible experimental work -
  https://github.com/ZacDair/EMBC_Release SMILE dataset provided by
  Computational Wellbeing Group (COMPWELL)
  https://compwell.rice.edu/workshops/embc2022/dataset -
  https://compwell.rice.edu/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ EquiMod: An Equivariance Module to Improve <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01244v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01244v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Devillers, Mathieu Lefort
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised visual representation methods are closing the gap with
supervised learning performance. These methods rely on maximizing the
similarity between embeddings of related synthetic inputs created through data
augmentations. This can be seen as a task that encourages embeddings to leave
out factors modified by these augmentations, i.e. to be invariant to them.
However, this only considers one side of the trade-off in the choice of the
augmentations: they need to strongly modify the images to avoid simple solution
shortcut learning (e.g. using only color histograms), but on the other hand,
augmentations-related information may be lacking in the representations for
some downstream tasks (e.g. color is important for birds and flower
classification). Few recent works proposed to mitigate the problem of using
only an invariance task by exploring some form of equivariance to
augmentations. This has been performed by learning additional embeddings
space(s), where some augmentation(s) cause embeddings to differ, yet in a
non-controlled way. In this work, we introduce EquiMod a generic equivariance
module that structures the learned latent space, in the sense that our module
learns to predict the displacement in the embedding space caused by the
augmentations. We show that applying that module to state-of-the-art invariance
models, such as SimCLR and BYOL, increases the performances on CIFAR10 and
ImageNet datasets. Moreover, while our model could collapse to a trivial
equivariance, i.e. invariance, we observe that it instead automatically learns
to keep some augmentations-related information beneficial to the
representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Emotion-Conditioned Melody Harmonization with Hierarchical Variational
  Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shulei Ji, Xinyu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing melody harmonization models have made great progress in improving
the quality of generated harmonies, but most of them ignored the emotions
beneath the music. Meanwhile, the variability of harmonies generated by
previous methods is insufficient. To solve these problems, we propose a novel
LSTM-based Hierarchical Variational Auto-Encoder (LHVAE) to investigate the
influence of emotional conditions on melody harmonization, while improving the
quality of generated harmonies and capturing the abundant variability of chord
progressions. Specifically, LHVAE incorporates latent variables and emotional
conditions at different levels (piece- and bar-level) to model the global and
local music properties. Additionally, we introduce an attention-based melody
context vector at each step to better learn the correspondence between melodies
and harmonies. Experimental results of the objective evaluation show that our
proposed model outperforms other LSTM-based models. Through subjective
evaluation, we conclude that only altering the chords hardly changes the
overall emotion of the music. The qualitative analysis demonstrates the ability
of our model to generate variable harmonies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE SMC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Simplicity Bias Leads to Amplified Performance Disparities 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.06641v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.06641v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel J. Bell, Levent Sagun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Which parts of a dataset will a given model find difficult? Recent work has
shown that SGD-trained models have a bias towards simplicity, leading them to
prioritize learning a majority class, or to rely upon harmful spurious
correlations. Here, we show that the preference for "easy" runs far deeper: A
model may prioritize any class or group of the dataset that it finds simple-at
the expense of what it finds complex-as measured by performance difference on
the test set. When subsets with different levels of complexity align with
demographic groups, we term this difficulty disparity, a phenomenon that occurs
even with balanced datasets that lack group/label associations. We show how
difficulty disparity is a model-dependent quantity, and is further amplified in
commonly-used models as selected by typical average performance scores. We
quantify an amplification factor across a range of settings in order to compare
disparity of different models on a fixed dataset. Finally, we present two
real-world examples of difficulty amplification in action, resulting in
worse-than-expected performance disparities between groups even when using a
balanced dataset. The existence of such disparities in balanced datasets
demonstrates that merely balancing sample sizes of groups is not sufficient to
ensure unbiased performance. We hope this work presents a step towards
measurable understanding of the role of model bias as it interacts with the
structure of data, and call for additional model-dependent mitigation methods
to be deployed alongside dataset audits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In 2023 ACM Conference on Fairness, Accountability, and Transparency
  (FAccT '23). ACM</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can <span class="highlight-title">Self-Supervised</span> Neural Representations <span class="highlight-title">Pre-Train</span>ed on Human Speech
  distinguish Animal Callers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14035v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14035v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eklavya Sarkar, Mathew Magimai. -Doss
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) models use only the intrinsic structure of a
given signal, independent of its acoustic domain, to extract essential
information from the input to an embedding space. This implies that the utility
of such representations is not limited to modeling human speech alone. Building
on this understanding, this paper explores the cross-transferability of SSL
neural representations learned from human speech to analyze bio-acoustic
signals. We conduct a caller discrimination analysis and a caller detection
study on Marmoset vocalizations using eleven SSL models pre-trained with
various pretext tasks. The results show that the embedding spaces carry
meaningful caller information and can successfully distinguish the individual
identities of Marmoset callers without fine-tuning. This demonstrates that
representations pre-trained on human speech can be effectively applied to the
bio-acoustics domain, providing valuable insights for future investigations in
this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stability of implicit neural networks for long-term forecasting in
  dynamical systems <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17155v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17155v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leon Migus, Julien Salomon, Patrick Gallinari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Forecasting physical signals in long time range is among the most challenging
tasks in Partial Differential Equations (PDEs) research. To circumvent
limitations of traditional solvers, many different Deep Learning methods have
been proposed. They are all based on auto-regressive methods and exhibit
stability issues. Drawing inspiration from the stability property of implicit
numerical schemes, we introduce a stable auto-regressive implicit neural
network. We develop a theory based on the stability definition of schemes to
ensure the stability in forecasting of this network. It leads us to introduce
hard constraints on its weights and propagate the dynamics in the latent space.
Our experimental results validate our stability property, and show improved
results at long-term forecasting for two transports PDEs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023 Workshop on Physics for Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FARE: Provably Fair Representation Learning with Practical Certificates <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07213v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07213v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikola Jovanović, Mislav Balunović, Dimitar I. Dimitrov, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fair representation learning (FRL) is a popular class of methods aiming to
produce fair classifiers via data preprocessing. Recent regulatory directives
stress the need for FRL methods that provide practical certificates, i.e.,
provable upper bounds on the unfairness of any downstream classifier trained on
preprocessed data, which directly provides assurance in a practical scenario.
Creating such FRL methods is an important challenge that remains unsolved. In
this work, we address that challenge and introduce FARE (Fairness with
Restricted Encoders), the first FRL method with practical fairness
certificates. FARE is based on our key insight that restricting the
representation space of the encoder enables the derivation of practical
guarantees, while still permitting favorable accuracy-fairness tradeoffs for
suitable instantiations, such as one we propose based on fair trees. To produce
a practical certificate, we develop and apply a statistical procedure that
computes a finite sample high-confidence upper bound on the unfairness of any
downstream classifier trained on FARE embeddings. In our comprehensive
experimental evaluation, we demonstrate that FARE produces practical
certificates that are tight and often even comparable with purely empirical
results obtained by prior methods, which establishes the practical value of our
approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Message-passing selection: Towards interpretable GNNs for graph
  classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02081v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02081v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenda Li, Kaixuan Chen, Shunyu Liu, Wenjie Huang, Haofei Zhang, Yingjie Tian, Yun Su, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we strive to develop an interpretable GNNs' inference
paradigm, termed MSInterpreter, which can serve as a plug-and-play scheme
readily applicable to various GNNs' baselines. Unlike the most existing
explanation methods, MSInterpreter provides a Message-passing Selection
scheme(MSScheme) to select the critical paths for GNNs' message aggregations,
which aims at reaching the self-explaination instead of post-hoc explanations.
In detail, the elaborate MSScheme is designed to calculate weight factors of
message aggregation paths by considering the vanilla structure and node
embedding components, where the structure base aims at weight factors among
node-induced substructures; on the other hand, the node embedding base focuses
on weight factors via node embeddings obtained by one-layer GNN.Finally, we
demonstrate the effectiveness of our approach on graph classification
benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 1 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Unrolled Graph Learning for Multi-Agent Collaboration 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.17101v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.17101v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enpei Zhang, Shuo Tang, Xiaowen Dong, Siheng Chen, Yanfeng Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-agent learning has gained increasing attention to tackle distributed
machine learning scenarios under constrictions of data exchanging. However,
existing multi-agent learning models usually consider data fusion under fixed
and compulsory collaborative relations among agents, which is not as flexible
and autonomous as human collaboration. To fill this gap, we propose a
distributed multi-agent learning model inspired by human collaboration, in
which the agents can autonomously detect suitable collaborators and refer to
collaborators' model for better performance. To implement such adaptive
collaboration, we use a collaboration graph to indicate the pairwise
collaborative relation. The collaboration graph can be obtained by graph
learning techniques based on model similarity between different agents. Since
model similarity can not be formulated by a fixed graphical optimization, we
design a graph learning network by unrolling, which can learn underlying
similar features among potential collaborators. By testing on both regression
and classification tasks, we validate that our proposed collaboration model can
figure out accurate collaborative relationship and greatly improve agents'
learning performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work was accepted to be presented at the Graph Signal Processing
  Workshop 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Capturing Conversion Rate Fluctuation during Sales Promotions: A Novel
  Historical Data Reuse Approach <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12837v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12837v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangming Chan, Yu Zhang, Shuguang Han, Yong Bai, Xiang-Rong Sheng, Siyuan Lou, Jiacen Hu, Baolin Liu, Yuning Jiang, Jian Xu, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversion rate (CVR) prediction is one of the core components in online
recommender systems, and various approaches have been proposed to obtain
accurate and well-calibrated CVR estimation. However, we observe that a
well-trained CVR prediction model often performs sub-optimally during sales
promotions. This can be largely ascribed to the problem of the data
distribution shift, in which the conventional methods no longer work. To this
end, we seek to develop alternative modeling techniques for CVR prediction.
Observing similar purchase patterns across different promotions, we propose
reusing the historical promotion data to capture the promotional conversion
patterns. Herein, we propose a novel \textbf{H}istorical \textbf{D}ata
\textbf{R}euse (\textbf{HDR}) approach that first retrieves historically
similar promotion data and then fine-tunes the CVR prediction model with the
acquired data for better adaptation to the promotion mode. HDR consists of
three components: an automated data retrieval module that seeks similar data
from historical promotions, a distribution shift correction module that
re-weights the retrieved data for better aligning with the target promotion,
and a TransBlock module that quickly fine-tunes the original model for better
adaptation to the promotion mode. Experiments conducted with real-world data
demonstrate the effectiveness of HDR, as it improves both ranking and
calibration metrics to a large extent. HDR has also been deployed on the
display advertising system in Alibaba, bringing a lift of $9\%$ RPM and $16\%$
CVR during Double 11 Sales in 2022.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at KDD 2023. This work has already been deployed on the
  display advertising system in Alibaba, bringing substantial economic gains</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Theoretical Analysis of Optimistic Proximal Policy Optimization in
  Linear Markov Decision Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08841v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08841v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhong, Tong Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proximal policy optimization (PPO) algorithm stands as one of the most
prosperous methods in the field of reinforcement learning (RL). Despite its
success, the theoretical understanding of PPO remains deficient. Specifically,
it is unclear whether PPO or its optimistic variants can effectively solve
linear Markov decision processes (MDPs), which are arguably the simplest models
in RL with function approximation. To bridge this gap, we propose an optimistic
variant of PPO for episodic adversarial linear MDPs with full-information
feedback, and establish a $\tilde{\mathcal{O}}(d^{3/4}H^2K^{3/4})$ regret for
it. Here $d$ is the ambient dimension of linear MDPs, $H$ is the length of each
episode, and $K$ is the number of episodes. Compared with existing policy-based
algorithms, we achieve the state-of-the-art regret bound in both stochastic
linear MDPs and adversarial linear MDPs with full information. Additionally,
our algorithm design features a novel multi-batched updating mechanism and the
theoretical analysis utilizes a new covering number argument of value and
policy classes, which might be of independent interest.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving PDEs with Unmeasurable Source Terms Using Coupled
  Physics-Informed Neural Network with Recurrent Prediction in Soft Sensor
  Modeling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08618v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08618v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aina Wang, Pan Qin, Xi-Ming Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Nonhomogeneous partial differential equations (PDEs) are an applicable model
in soft sensor modeling for describing spatiotemporal industrial systems with
unmeasurable source terms, which cannot be well solved by existing
physics-informed neural networks (PINNs). To this end, a coupled PINN (CPINN)
with a recurrent prediction (RP) learning strategy (CPINN-RP) is proposed for
soft sensor modeling in spatiotemporal industrial processes, such as vibration
displacement. First, CPINN containing NetU and NetG is proposed. NetU is used
to approximate the solutions to PDEs under study and NetG is used to regularize
the training of NetU. The two networks are integrated into a
data-physics-hybrid loss function. Then, we theoretically prove that the
proposed CPINN has a satisfying approximation capacity to the PDEs solutions.
Besides the theoretical aspects, we propose a hierarchical training strategy to
optimize and couple the two networks to achieve the parameters of CPINN.
Secondly, NetU-RP is achieved by NetU compensated by RP, the recurrently
delayed output of CPINN, to further improve the soft sensor performance.
Finally, simulations and experiment verify the effectiveness and practical
applications of CPINN-RP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust online active learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00422v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00422v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Cacciarelli, Murat Kulahci, John Sølve Tyssedal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many industrial applications, obtaining labeled observations is not
straightforward as it often requires the intervention of human experts or the
use of expensive testing equipment. In these circumstances, active learning can
be highly beneficial in suggesting the most informative data points to be used
when fitting a model. Reducing the number of observations needed for model
development alleviates both the computational burden required for training and
the operational expenses related to labeling. Online active learning, in
particular, is useful in high-volume production processes where the decision
about the acquisition of the label for a data point needs to be taken within an
extremely short time frame. However, despite the recent efforts to develop
online active learning strategies, the behavior of these methods in the
presence of outliers has not been thoroughly examined. In this work, we
investigate the performance of online active linear regression in contaminated
data streams. Our study shows that the currently available query strategies are
prone to sample outliers, whose inclusion in the training set eventually
degrades the predictive performance of the models. To address this issue, we
propose a solution that bounds the search area of a conditional D-optimal
algorithm and uses a robust estimator. Our approach strikes a balance between
exploring unseen regions of the input space and protecting against outliers.
Through numerical simulations, we show that the proposed method is effective in
improving the performance of online active learning in the presence of
outliers, thus expanding the potential applications of this powerful tool.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Quality and Reliability Engineering International (2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stream-based active learning with linear models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.09874v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.09874v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Cacciarelli, Murat Kulahci, John Sølve Tyssedal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of automated data collection schemes and the advances in
sensorics are increasing the amount of data we are able to monitor in
real-time. However, given the high annotation costs and the time required by
quality inspections, data is often available in an unlabeled form. This is
fostering the use of active learning for the development of soft sensors and
predictive models. In production, instead of performing random inspections to
obtain product information, labels are collected by evaluating the information
content of the unlabeled data. Several query strategy frameworks for regression
have been proposed in the literature but most of the focus has been dedicated
to the static pool-based scenario. In this work, we propose a new strategy for
the stream-based scenario, where instances are sequentially offered to the
learner, which must instantaneously decide whether to perform the quality check
to obtain the label or discard the instance. The approach is inspired by the
optimal experimental design theory and the iterative aspect of the
decision-making process is tackled by setting a threshold on the
informativeness of the unlabeled data points. The proposed approach is
evaluated using numerical simulations and the Tennessee Eastman Process
simulator. The results confirm that selecting the examples suggested by the
proposed algorithm allows for a faster reduction in the prediction error.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Knowledge-Based Systems (2022)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RGCVAE: Relational Graph Conditioned Variational Autoencoder for
  Molecule Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11699v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11699v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Davide Rigoni, Nicolò Navarin, Alessandro Sperduti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Identifying molecules that exhibit some pre-specified properties is a
difficult problem to solve. In the last few years, deep generative models have
been used for molecule generation. Deep Graph Variational Autoencoders are
among the most powerful machine learning tools with which it is possible to
address this problem. However, existing methods struggle in capturing the true
data distribution and tend to be computationally expensive. In this work, we
propose RGCVAE, an efficient and effective Graph Variational Autoencoder based
on: (i) an encoding network exploiting a new powerful Relational Graph
Isomorphism Network; (ii) a novel probabilistic decoding component. Compared to
several state-of-the-art VAE methods on two widely adopted datasets, RGCVAE
shows state-of-the-art molecule generation performance while being
significantly faster to train.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Look Beneath the Surface: Exploiting Fundamental Symmetry for
  Sample-Efficient Offline RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04220v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04220v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Cheng, Xianyuan Zhan, Zhihao Wu, Wenjia Zhang, Shoucheng Song, Han Wang, Youfang Lin, Li Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL) offers an appealing approach to
real-world tasks by learning policies from pre-collected datasets without
interacting with the environment. However, the performance of existing offline
RL algorithms heavily depends on the scale and state-action space coverage of
datasets. Real-world data collection is often expensive and uncontrollable,
leading to small and narrowly covered datasets and posing significant
challenges for practical deployments of offline RL. In this paper, we provide a
new insight that leveraging the fundamental symmetry of system dynamics can
substantially enhance offline RL performance under small datasets.
Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced
Dynamics Model (TDM), which establishes consistency between a pair of forward
and reverse latent dynamics. TDM provides both well-behaved representations for
small datasets and a new reliability measure for OOD samples based on
compliance with the T-symmetry. These can be readily used to construct a new
offline RL algorithm (TSRL) with less conservative policy constraints and a
reliable latent space data augmentation procedure. Based on extensive
experiments, we find TSRL achieves great performance on small benchmark
datasets with as few as 1% of the original samples, which significantly
outperforms the recent offline RL algorithms in terms of data efficiency and
generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Skin<span class="highlight-title">GPT</span>-4: An Interactive Dermatology Diagnostic System with Visual
  Large Language Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Juexiao Zhou, Xiaonan He, Liyuan Sun, Jiannan Xu, Xiuying Chen, Yuetan Chu, Longxi Zhou, Xingyu Liao, Bin Zhang, Xin Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Skin and subcutaneous diseases rank high among the leading contributors to
the global burden of nonfatal diseases, impacting a considerable portion of the
population. Nonetheless, the field of dermatology diagnosis faces three
significant hurdles. Firstly, there is a shortage of dermatologists accessible
to diagnose patients, particularly in rural regions. Secondly, accurately
interpreting skin disease images poses a considerable challenge. Lastly,
generating patient-friendly diagnostic reports is usually a time-consuming and
labor-intensive task for dermatologists. To tackle these challenges, we present
SkinGPT-4, which is the world's first interactive dermatology diagnostic system
powered by an advanced visual large language model. SkinGPT-4 leverages a
fine-tuned version of MiniGPT-4, trained on an extensive collection of skin
disease images (comprising 52,929 publicly available and proprietary images)
along with clinical concepts and doctors' notes. We designed a two-step
training process to allow SkinGPT to express medical features in skin disease
images with natural language and make accurate diagnoses of the types of skin
diseases. With SkinGPT-4, users could upload their own skin photos for
diagnosis, and the system could autonomously evaluate the images, identifies
the characteristics and categories of the skin conditions, performs in-depth
analysis, and provides interactive treatment recommendations. Meanwhile,
SkinGPT-4's local deployment capability and commitment to user privacy also
render it an appealing choice for patients in search of a dependable and
precise diagnosis of their skin ailments. To demonstrate the robustness of
SkinGPT-4, we conducted quantitative evaluations on 150 real-life cases, which
were independently reviewed by certified dermatologists, and showed that
SkinGPT-4 could provide accurate diagnoses of skin diseases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Adjusting Weighted Expected Improvement for Bayesian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04262v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04262v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carolin Benjamins, Elena Raponi, Anja Jankovic, Carola Doerr, Marius Lindauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian Optimization (BO) is a class of surrogate-based, sample-efficient
algorithms for optimizing black-box problems with small evaluation budgets. The
BO pipeline itself is highly configurable with many different design choices
regarding the initial design, surrogate model, and acquisition function (AF).
Unfortunately, our understanding of how to select suitable components for a
problem at hand is very limited. In this work, we focus on the definition of
the AF, whose main purpose is to balance the trade-off between exploring
regions with high uncertainty and those with high promise for good solutions.
We propose Self-Adjusting Weighted Expected Improvement (SAWEI), where we let
the exploration-exploitation trade-off self-adjust in a data-driven manner,
based on a convergence criterion for BO. On the noise-free black-box BBOB
functions of the COCO benchmarking platform, our method exhibits a favorable
any-time performance compared to handcrafted baselines and serves as a robust
default choice for any problem structure. The suitability of our method also
transfers to HPOBench. With SAWEI, we are a step closer to on-the-fly,
data-driven, and robust BO designs that automatically adjust their sampling
behavior to the problem at hand.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AutoML Conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Hybrid <span class="highlight-title">Self-Supervised</span> Learning Framework for Vertical Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.08934v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.08934v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuanqin He, Yan Kang, Xinyuan Zhao, Jiahuan Luo, Lixin Fan, Yuxing Han, Qiang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Vertical federated learning (VFL), a variant of Federated Learning (FL), has
recently drawn increasing attention as the VFL matches the enterprises' demands
of leveraging more valuable features to achieve better model performance.
However, conventional VFL methods may run into data deficiency as they exploit
only aligned and labeled samples (belonging to different parties), leaving
often the majority of unaligned and unlabeled samples unused. The data
deficiency hampers the effort of the federation.
  In this work, we propose a Federated Hybrid Self-Supervised Learning
framework, named FedHSSL, that utilizes cross-party views (i.e., dispersed
features) of samples aligned among parties and local views (i.e., augmentation)
of unaligned samples within each party to improve the representation learning
capability of the VFL joint model. FedHSSL further exploits invariant features
across parties to boost the performance of the joint model through partial
model aggregation. FedHSSL, as a framework, can work with various
representative SSL methods. We empirically demonstrate that FedHSSL methods
outperform baselines by large margins. We provide an in-depth analysis of
FedHSSL regarding label leakage, which is rarely investigated in existing
self-supervised VFL works. The experimental results show that, with proper
protection, FedHSSL achieves the best privacy-utility trade-off against the
state-of-the-art label inference attack compared with baselines. Code is
available at \url{https://github.com/jorghyq2016/FedHSSL}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Add preliminaries and experiments</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Robustness of Random Forest Against Untargeted Data Poisoning: An
  Ensemble-Based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.14013v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.14013v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Anisetti, Claudio A. Ardagna, Alessandro Balestrucci, Nicola Bena, Ernesto Damiani, Chan Yeob Yeun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning is becoming ubiquitous. From finance to medicine, machine
learning models are boosting decision-making processes and even outperforming
humans in some tasks. This huge progress in terms of prediction quality does
not however find a counterpart in the security of such models and corresponding
predictions, where perturbations of fractions of the training set (poisoning)
can seriously undermine the model accuracy. Research on poisoning attacks and
defenses received increasing attention in the last decade, leading to several
promising solutions aiming to increase the robustness of machine learning.
Among them, ensemble-based defenses, where different models are trained on
portions of the training set and their predictions are then aggregated, provide
strong theoretical guarantees at the price of a linear overhead. Surprisingly,
ensemble-based defenses, which do not pose any restrictions on the base model,
have not been applied to increase the robustness of random forest models. The
work in this paper aims to fill in this gap by designing and implementing a
novel hash-based ensemble approach that protects random forest against
untargeted, random poisoning attacks. An extensive experimental evaluation
measures the performance of our approach against a variety of attacks, as well
as its sustainability in terms of resource consumption and performance, and
compares it with a traditional monolithic model based on random forest. A final
discussion presents our main findings and compares our approach with existing
poisoning defenses targeting random forests.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faithful Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04431v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04431v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom A. Lamb, Rudy Brunel, Krishnamurthy DJ Dvijotham, M. Pawan Kumar, Philip H. S. Torr, Francisco Eiras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) has received much attention due to its success in
compressing networks to allow for their deployment in resource-constrained
systems. While the problem of adversarial robustness has been studied before in
the KD setting, previous works overlook what we term the relative calibration
of the student network with respect to its teacher in terms of soft
confidences. In particular, we focus on two crucial questions with regard to a
teacher-student pair: (i) do the teacher and student disagree at points close
to correctly classified dataset examples, and (ii) is the distilled student as
confident as the teacher around dataset examples? These are critical questions
when considering the deployment of a smaller student network trained from a
robust teacher within a safety-critical setting. To address these questions, we
introduce a faithful imitation framework to discuss the relative calibration of
confidences, as well as provide empirical and certified methods to evaluate the
relative calibration of a student w.r.t. its teacher. Further, to verifiably
align the relative calibration incentives of the student to those of its
teacher, we introduce faithful distillation. Our experiments on the MNIST and
Fashion-MNIST datasets demonstrate the need for such an analysis and the
advantages of the increased verifiability of faithful distillation over
alternative adversarial distillation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12pgs (main content), 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Generalization of Auto-Regressive Hidden Markov Models to Non-Linear
  Dynamics and Unit Quaternion Observation Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11834v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11834v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michele Ginesi, Paolo Fiorini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latent variable models are widely used to perform unsupervised segmentation
of time series in different context such as robotics, speech recognition, and
economics. One of the most widely used latent variable model is the
Auto-Regressive Hidden Markov Model (ARHMM), which combines a latent mode
governed by a Markov chain dynamics with a linear Auto-Regressive dynamics of
the observed state.
  In this work, we propose two generalizations of the ARHMM. First, we propose
a more general AR dynamics in Cartesian space, described as a linear
combination of non-linear basis functions. Second, we propose a linear dynamics
in unit quaternion space, in order to properly describe orientations. These
extensions allow to describe more complex dynamics of the observed state.
  Although this extension is proposed for the ARHMM, it can be easily extended
to other latent variable models with AR dynamics in the observed space, such as
Auto-Regressive Hidden semi-Markov Models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MSCDA: Multi-level Semantic-guided Contrast Improves Unsupervised Domain
  Adaptation for Breast MRI Segmentation in Small <span class="highlight-title">Dataset</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.02554v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.02554v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheng Kuang, Henry C. Woodruff, Renee Granzier, Thiemo J. A. van Nijnatten, Marc B. I. Lobbes, Marjolein L. Smidt, Philippe Lambin, Siamak Mehrkanoon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning (DL) applied to breast tissue segmentation in magnetic
resonance imaging (MRI) has received increased attention in the last decade,
however, the domain shift which arises from different vendors, acquisition
protocols, and biological heterogeneity, remains an important but challenging
obstacle on the path towards clinical implementation. In this paper, we propose
a novel Multi-level Semantic-guided Contrastive Domain Adaptation (MSCDA)
framework to address this issue in an unsupervised manner. Our approach
incorporates self-training with contrastive learning to align feature
representations between domains. In particular, we extend the contrastive loss
by incorporating pixel-to-pixel, pixel-to-centroid, and centroid-to-centroid
contrasts to better exploit the underlying semantic information of the image at
different levels. To resolve the data imbalance problem, we utilize a
category-wise cross-domain sampling strategy to sample anchors from target
images and build a hybrid memory bank to store samples from source images. We
have validated MSCDA with a challenging task of cross-domain breast MRI
segmentation between datasets of healthy volunteers and invasive breast cancer
patients. Extensive experiments show that MSCDA effectively improves the
model's feature alignment capabilities between domains, outperforming
state-of-the-art methods. Furthermore, the framework is shown to be
label-efficient, achieving good performance with a smaller source dataset. The
code is publicly available at \url{https://github.com/ShengKuangCN/MSCDA}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Q-Diffusion: Quantizing Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04304v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04304v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiuyu Li, Yijiang Liu, Long Lian, Huanrui Yang, Zhen Dong, Daniel Kang, Shanghang Zhang, Kurt Keutzer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved great success in image synthesis through
iterative noise estimation using deep neural networks. However, the slow
inference, high memory consumption, and computation intensity of the noise
estimation model hinder the efficient adoption of diffusion models. Although
post-training quantization (PTQ) is considered a go-to compression method for
other tasks, it does not work out-of-the-box on diffusion models. We propose a
novel PTQ method specifically tailored towards the unique multi-timestep
pipeline and model architecture of the diffusion models, which compresses the
noise estimation network to accelerate the generation process. We identify the
key difficulty of diffusion model quantization as the changing output
distributions of noise estimation networks over multiple time steps and the
bimodal activation distribution of the shortcut layers within the noise
estimation network. We tackle these challenges with timestep-aware calibration
and split shortcut quantization in this work. Experimental results show that
our proposed method is able to quantize full-precision unconditional diffusion
models into 4-bit while maintaining comparable performance (small FID change of
at most 2.34 compared to >100 for traditional PTQ) in a training-free manner.
Our approach can also be applied to text-guided image generation, where we can
run stable diffusion in 4-bit weights with high generation quality for the
first time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The code is available at https://github.com/Xiuyu-Li/q-diffusion</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarial Sample Detection Through Neural Network Transport Dynamics <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04252v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04252v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Skander Karkar, Patrick Gallinari, Alain Rakotomamonjy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a detector of adversarial samples that is based on the view of
neural networks as discrete dynamic systems. The detector tells clean inputs
from abnormal ones by comparing the discrete vector fields they follow through
the layers. We also show that regularizing this vector field during training
makes the network more regular on the data distribution's support, thus making
the activations of clean inputs more distinguishable from those of abnormal
ones. Experimentally, we compare our detector favorably to other detectors on
seen and unseen attacks, and show that the regularization of the network's
dynamics improves the performance of adversarial detectors that use the
internal embeddings as inputs, while also improving test accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECML PKDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward Enhanced Robustness in Unsupervised Graph Representation
  Learning: A Graph Information Bottleneck Perspective 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.08557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.08557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihong Wang, Minnan Luo, Jundong Li, Ziqi Liu, Jun Zhou, Qinghua Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent studies have revealed that GNNs are vulnerable to adversarial attacks.
Most existing robust graph learning methods measure model robustness based on
label information, rendering them infeasible when label information is not
available. A straightforward direction is to employ the widely used Infomax
technique from typical Unsupervised Graph Representation Learning (UGRL) to
learn robust unsupervised representations. Nonetheless, directly transplanting
the Infomax technique from typical UGRL to robust UGRL may involve a biased
assumption. In light of the limitation of Infomax, we propose a novel unbiased
robust UGRL method called Robust Graph Information Bottleneck (RGIB), which is
grounded in the Information Bottleneck (IB) principle. Our RGIB attempts to
learn robust node representations against adversarial perturbations by
preserving the original information in the benign graph while eliminating the
adversarial information in the adversarial graph. There are mainly two
challenges to optimize RGIB: 1) high complexity of adversarial attack to
perturb node features and graph structure jointly in the training procedure; 2)
mutual information estimation upon adversarially attacked graphs. To tackle
these problems, we further propose an efficient adversarial training strategy
with only feature perturbations and an effective mutual information estimator
with subgraph-level summary. Moreover, we theoretically establish a connection
between our proposed RGIB and the robustness of downstream classifiers,
revealing that RGIB can provide a lower bound on the adversarial risk of
downstream classifiers. Extensive experiments over several benchmarks and
downstream tasks demonstrate the effectiveness and superiority of our proposed
method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regret Bounds for Markov Decision Processes with Recursive Optimized
  Certainty Equivalents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12601v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12601v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenhao Xu, Xuefeng Gao, Xuedong He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The optimized certainty equivalent (OCE) is a family of risk measures that
cover important examples such as entropic risk, conditional value-at-risk and
mean-variance models. In this paper, we propose a new episodic risk-sensitive
reinforcement learning formulation based on tabular Markov decision processes
with recursive OCEs. We design an efficient learning algorithm for this problem
based on value iteration and upper confidence bound. We derive an upper bound
on the regret of the proposed algorithm, and also establish a minimax lower
bound. Our bounds show that the regret rate achieved by our proposed algorithm
has optimal dependence on the number of episodes and the number of actions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Continual Learning with <span class="highlight-title">Pretrain</span>ed Backbones by Tuning in the Input
  Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02947v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02947v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Marullo, Matteo Tiezzi, Marco Gori, Stefano Melacci, Tinne Tuytelaars
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The intrinsic difficulty in adapting deep learning models to non-stationary
environments limits the applicability of neural networks to real-world tasks.
This issue is critical in practical supervised learning settings, such as the
ones in which a pre-trained model computes projections toward a latent space
where different task predictors are sequentially learned over time. As a matter
of fact, incrementally fine-tuning the whole model to better adapt to new tasks
usually results in catastrophic forgetting, with decreasing performance over
the past experiences and losing valuable knowledge from the pre-training stage.
In this paper, we propose a novel strategy to make the fine-tuning procedure
more effective, by avoiding to update the pre-trained part of the network and
learning not only the usual classification head, but also a set of
newly-introduced learnable parameters that are responsible for transforming the
input data. This process allows the network to effectively leverage the
pre-training knowledge and find a good trade-off between plasticity and
stability with modest computational efforts, thus especially suitable for
on-the-edge settings. Our experiments on four image classification problems in
a continual learning setting confirm the quality of the proposed approach when
compared to several fine-tuning procedures and to popular continual learning
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Maximize Mutual Information for Dynamic Feature Selection <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00557v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00557v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ian Covert, Wei Qiu, Mingyu Lu, Nayoon Kim, Nathan White, Su-In Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Feature selection helps reduce data acquisition costs in ML, but the standard
approach is to train models with static feature subsets. Here, we consider the
dynamic feature selection (DFS) problem where a model sequentially queries
features based on the presently available information. DFS is often addressed
with reinforcement learning, but we explore a simpler approach of greedily
selecting features based on their conditional mutual information. This method
is theoretically appealing but requires oracle access to the data distribution,
so we develop a learning approach based on amortized optimization. The proposed
method is shown to recover the greedy policy when trained to optimality, and it
outperforms numerous existing feature selection methods in our experiments,
thus validating it as a simple but powerful approach for this problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023 camera-ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balanced Audiovisual <span class="highlight-title">Dataset</span> for Imbalance Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10912v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10912v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wenke Xia, Xu Zhao, Xincheng Pang, Changqing Zhang, Di Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The imbalance problem is widespread in the field of machine learning, which
also exists in multimodal learning areas caused by the intrinsic discrepancy
between modalities of samples. Recent works have attempted to solve the
modality imbalance problem from algorithm perspective, however, they do not
fully analyze the influence of modality bias in datasets. Concretely, existing
multimodal datasets are usually collected under specific tasks, where one
modality tends to perform better than other ones in most conditions. In this
work, to comprehensively explore the influence of modality bias, we first split
existing datasets into different subsets by estimating sample-wise modality
discrepancy. We surprisingly find that: the multimodal models with existing
imbalance algorithms consistently perform worse than the unimodal one on
specific subsets, in accordance with the modality bias. To further explore the
influence of modality bias and analyze the effectiveness of existing imbalance
algorithms, we build a balanced audiovisual dataset, with uniformly distributed
modality discrepancy over the whole dataset. We then conduct extensive
experiments to re-evaluate existing imbalance algorithms and draw some
interesting findings: existing algorithms only provide a compromise between
modalities and suffer from the large modality discrepancy of samples. We hope
that these findings could facilitate future research on the modality imbalance
problem.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>website:https://gewu-lab.github.io/Balanced-Audiovisual-Dataset/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controlled Text Generation with Natural Language Instructions <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14293v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14293v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangchunshu Zhou, Yuchen Eleanor Jiang, Ethan Wilcox, Ryan Cotterell, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models generate fluent texts and can follow natural language
instructions to solve a wide range of tasks without task-specific training.
Nevertheless, it is notoriously difficult to control their generation to
satisfy the various constraints required by different applications. In this
work, we present InstructCTG, a controlled text generation framework that
incorporates different constraints by conditioning on natural language
descriptions and demonstrations of the constraints. In particular, we first
extract the underlying constraints of natural texts through a combination of
off-the-shelf NLP tools and simple heuristics. We then verbalize the
constraints into natural language instructions to form weakly supervised
training data. By prepending natural language descriptions of the constraints
and a few demonstrations, we fine-tune a pre-trained language model to
incorporate various types of constraints. Compared to existing search-based or
score-based methods, InstructCTG is more flexible to different constraint types
and has a much smaller impact on the generation quality and speed because it
does not modify the decoding procedure. Additionally, InstructCTG allows the
model to adapt to new constraints without re-training through the use of
few-shot task generalization and in-context learning abilities of
instruction-tuned language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards biologically plausible Dreaming and Planning in recurrent
  spiking networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.10044v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.10044v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cristiano Capone, Pier Stanislao Paolucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans and animals can learn new skills after practicing for a few hours,
while current reinforcement learning algorithms require a large amount of data
to achieve good performances. Recent model-based approaches show promising
results by reducing the number of necessary interactions with the environment
to learn a desirable policy. However, these methods require biological
implausible ingredients, such as the detailed storage of older experiences, and
long periods of offline learning. The optimal way to learn and exploit
word-models is still an open question. Taking inspiration from biology, we
suggest that dreaming might be an efficient expedient to use an inner model. We
propose a two-module (agent and model) spiking neural network in which
"dreaming" (living new experiences in a model-based simulated environment)
significantly boosts learning. We also explore "planning", an online
alternative to dreaming, that shows comparable performances. Importantly, our
model does not require the detailed storage of experiences, and learns online
the world-model and the policy. Moreover, we stress that our network is
composed of spiking neurons, further increasing the biological plausibility and
implementability in neuromorphic hardware.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revising deep learning methods in parking lot occupancy detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04288v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04288v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasia Martynova, Mikhail Kuznetsov, Vadim Porvatov, Vladislav Tishin, Andrey Kuznetsov, Natalia Semenova, Ksenia Kuznetsova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parking guidance systems have recently become a popular trend as a part of
the smart cities' paradigm of development. The crucial part of such systems is
the algorithm allowing drivers to search for available parking lots across
regions of interest. The classic approach to this task is based on the
application of neural network classifiers to camera records. However, existing
systems demonstrate a lack of generalization ability and appropriate testing
regarding specific visual conditions. In this study, we extensively evaluate
state-of-the-art parking lot occupancy detection algorithms, compare their
prediction quality with the recently emerged vision transformers, and propose a
new pipeline based on EfficientNet architecture. Performed computational
experiments have demonstrated the performance increase in the case of our
model, which was evaluated on 5 different datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distilling BlackBox to Interpretable models for Efficient Transfer
  Learning <span class="chip">MICCAI</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17303v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17303v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shantanu Ghosh, Ke Yu, Kayhan Batmanghelich
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building generalizable AI models is one of the primary challenges in the
healthcare domain. While radiologists rely on generalizable descriptive rules
of abnormality, Neural Network (NN) models suffer even with a slight shift in
input distribution (e.g., scanner type). Fine-tuning a model to transfer
knowledge from one domain to another requires a significant amount of labeled
data in the target domain. In this paper, we develop an interpretable model
that can be efficiently fine-tuned to an unseen target domain with minimal
computational cost. We assume the interpretable component of NN to be
approximately domain-invariant. However, interpretable models typically
underperform compared to their Blackbox (BB) variants. We start with a BB in
the source domain and distill it into a \emph{mixture} of shallow interpretable
models using human-understandable concepts. As each interpretable model covers
a subset of data, a mixture of interpretable models achieves comparable
performance as BB. Further, we use the pseudo-labeling technique from
semi-supervised learning (SSL) to learn the concept classifier in the target
domain, followed by fine-tuning the interpretable models in the target domain.
We evaluate our model using a real-life large-scale chest-X-ray (CXR)
classification dataset. The code is available at:
\url{https://github.com/batmanlab/MICCAI-2023-Route-interpret-repeat-CXRs}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MICCAI, 2023, Early accept</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Attentional-Biased Stochastic Gradient Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2012.06951v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2012.06951v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qi Qi, Yi Xu, Rong Jin, Wotao Yin, Tianbao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a simple yet effective provable method (named
ABSGD) for addressing the data imbalance or label noise problem in deep
learning. Our method is a simple modification to momentum SGD where we assign
an individual importance weight to each sample in the mini-batch. The
individual-level weight of sampled data is systematically proportional to the
exponential of a scaled loss value of the data, where the scaling factor is
interpreted as the regularization parameter in the framework of
distributionally robust optimization (DRO). Depending on whether the scaling
factor is positive or negative, ABSGD is guaranteed to converge to a stationary
point of an information-regularized min-max or min-min DRO problem,
respectively. Compared with existing class-level weighting schemes, our method
can capture the diversity between individual examples within each class.
Compared with existing individual-level weighting methods using meta-learning
that require three backward propagations for computing mini-batch stochastic
gradients, our method is more efficient with only one backward propagation at
each iteration as in standard deep learning methods. ABSGD is flexible enough
to combine with other robust losses without any additional cost. Our empirical
studies on several benchmark datasets demonstrate the effectiveness of the
proposed method.\footnote{Code is available
at:\url{https://github.com/qiqi-helloworld/ABSGD/}}
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Statistical Inference for Fairness Auditing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03712v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03712v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John J. Cherian, Emmanuel J. Candès
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Before deploying a black-box model in high-stakes problems, it is important
to evaluate the model's performance on sensitive subpopulations. For example,
in a recidivism prediction task, we may wish to identify demographic groups for
which our prediction model has unacceptably high false positive rates or
certify that no such groups exist. In this paper, we frame this task, often
referred to as "fairness auditing," in terms of multiple hypothesis testing. We
show how the bootstrap can be used to simultaneously bound performance
disparities over a collection of groups with statistical guarantees. Our
methods can be used to flag subpopulations affected by model underperformance,
and certify subpopulations for which the model performs adequately. Crucially,
our audit is model-agnostic and applicable to nearly any performance metric or
group fairness criterion. Our methods also accommodate extremely rich -- even
infinite -- collections of subpopulations. Further, we generalize beyond
subpopulations by showing how to assess performance over certain distribution
shifts. We test the proposed methods on benchmark datasets in predictive
inference and algorithmic fairness and find that our audits can provide
interpretable and trustworthy guarantees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>44 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Set Encoding with Universal Mini-Batch Consistency and Unbiased
  Full Set Gradient Approximation <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.12401v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.12401v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeffrey Willette, Seanie Lee, Bruno Andreis, Kenji Kawaguchi, Juho Lee, Sung Ju Hwang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent work on mini-batch consistency (MBC) for set functions has brought
attention to the need for sequentially processing and aggregating chunks of a
partitioned set while guaranteeing the same output for all partitions. However,
existing constraints on MBC architectures lead to models with limited
expressive power. Additionally, prior work has not addressed how to deal with
large sets during training when the full set gradient is required. To address
these issues, we propose a Universally MBC (UMBC) class of set functions which
can be used in conjunction with arbitrary non-MBC components while still
satisfying MBC, enabling a wider range of function classes to be used in MBC
settings. Furthermore, we propose an efficient MBC training algorithm which
gives an unbiased approximation of the full set gradient and has a constant
memory overhead for any set size for both train- and test-time. We conduct
extensive experiments including image completion, text classification,
unsupervised clustering, and cancer detection on high-resolution images to
verify the efficiency and efficacy of our scalable set encoding framework. Our
code is available at github.com/jeffwillette/umbc
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Value Functions Factorization with Latent State Information Sharing in
  Decentralized Multi-Agent Policy Gradients 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.01247v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.01247v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanhan Zhou, Tian Lan, Vaneet Aggarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Value function factorization via centralized training and decentralized
execution is promising for solving cooperative multi-agent reinforcement tasks.
One of the approaches in this area, QMIX, has become state-of-the-art and
achieved the best performance on the StarCraft II micromanagement benchmark.
However, the monotonic-mixing of per agent estimates in QMIX is known to
restrict the joint action Q-values it can represent, as well as the
insufficient global state information for single agent value function
estimation, often resulting in suboptimality. To this end, we present LSF-SAC,
a novel framework that features a variational inference-based
information-sharing mechanism as extra state information to assist individual
agents in the value function factorization. We demonstrate that such latent
individual state information sharing can significantly expand the power of
value function factorization, while fully decentralized execution can still be
maintained in LSF-SAC through a soft-actor-critic design. We evaluate LSF-SAC
on the StarCraft II micromanagement challenge and demonstrate that it
outperforms several state-of-the-art methods in challenging collaborative
tasks. We further set extensive ablation studies for locating the key factors
accounting for its performance improvements. We believe that this new insight
can lead to new local value estimation methods and variational deep learning
algorithms. A demo video and code of implementation can be found at
https://sites.google.com/view/sacmm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Transactions on Emerging Topics in Computational
  Intelligence (TETCI)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximate Newton policy gradient algorithms <span class="chip">SC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2110.02398v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2110.02398v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoya Li, Samarth Gupta, Hsiangfu Yu, Lexing Ying, Inderjit Dhillon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Policy gradient algorithms have been widely applied to Markov decision
processes and reinforcement learning problems in recent years. Regularization
with various entropy functions is often used to encourage exploration and
improve stability. This paper proposes an approximate Newton method for the
policy gradient algorithm with entropy regularization. In the case of Shannon
entropy, the resulting algorithm reproduces the natural policy gradient
algorithm. For other entropy functions, this method results in brand-new policy
gradient algorithms. We prove that all these algorithms enjoy Newton-type
quadratic convergence and that the corresponding gradient flow converges
globally to the optimal solution. We use synthetic and industrial-scale
examples to demonstrate that the proposed approximate Newton method typically
converges in single-digit iterations, often orders of magnitude faster than
other state-of-the-art algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 15 figures, v6 accepted by SIAM SISC</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-06-07T00:00:00Z">2023-06-07</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">112</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ModuleFormer: Learning Modular Large Language Models From Uncurated Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikang Shen, Zheyu Zhang, Tianyou Cao, Shawn Tan, Zhenfang Chen, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved remarkable results. But existing
models are expensive to train and deploy, and it is also difficult to expand
their knowledge beyond pre-training data without forgetting previous knowledge.
This paper proposes a new neural network architecture, ModuleFormer, that
leverages modularity to improve the efficiency and flexibility of large
language models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE).
Unlike the previous SMoE-based modular language model [Gururangan et al.,
2021], which requires domain-labeled data to learn domain-specific experts,
ModuleFormer can induce modularity from uncurated data with its new load
balancing and load concentration losses. ModuleFormer is a modular architecture
that includes two different types of modules, new stick-breaking attention
heads, and feedforward experts. Different modules are sparsely activated
conditions on the input token during training and inference. In our experiment,
we found that the modular architecture enables three important abilities for
large pre-trained language models: 1) Efficiency, since ModuleFormer only
activates a subset of its modules for each input token, thus it could achieve
the same performance as dense LLMs with more than two times throughput; 2)
Extendability, ModuleFormer is more immune to catastrophic forgetting than
dense LLMs and can be easily extended with new modules to learn new knowledge
that is not included in the training data; 3) Specialisation, finetuning
ModuleFormer could specialize a subset of modules to the finetuning task, and
the task-unrelated modules could be easily pruned for a lightweight deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s as Statisticians: Provable In-Context Learning with
  In-Context Algorithm Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, Song Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural sequence models based on the transformer architecture have
demonstrated remarkable \emph{in-context learning} (ICL) abilities, where they
can perform new tasks when prompted with training and test examples, without
any parameter update to the model. This work first provides a comprehensive
statistical theory for transformers to perform ICL. Concretely, we show that
transformers can implement a broad class of standard machine learning
algorithms in context, such as least squares, ridge regression, Lasso, learning
generalized linear models, and gradient descent on two-layer neural networks,
with near-optimal predictive power on various in-context data distributions.
Using an efficient implementation of in-context gradient descent as the
underlying mechanism, our transformer constructions admit mild size bounds, and
can be learned with polynomially many pretraining sequences.
  Building on these ``base'' ICL algorithms, intriguingly, we show that
transformers can implement more complex ICL procedures involving
\emph{in-context algorithm selection}, akin to what a statistician can do in
real life -- A \emph{single} transformer can adaptively select different base
ICL algorithms -- or even perform qualitatively different tasks -- on different
input sequences, without any explicit prompting of the right algorithm or task.
We both establish this in theory by explicit constructions, and also observe
this phenomenon experimentally. In theory, we construct two general mechanisms
for algorithm selection with concrete examples: pre-ICL testing, and post-ICL
validation. As an example, we use the post-ICL validation mechanism to
construct a transformer that can perform nearly Bayes-optimal ICL on a
challenging task -- noisy linear models with mixed noise levels.
Experimentally, we demonstrate the strong in-context algorithm selection
capabilities of standard transformer architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Reliability of Watermarks for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, Tom Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are now deployed to everyday use and positioned
to produce large quantities of text in the coming decade. Machine-generated
text may displace human-written text on the internet and has the potential to
be used for malicious purposes, such as spearphishing attacks and social media
bots. Watermarking is a simple and effective strategy for mitigating such harms
by enabling the detection and documentation of LLM-generated text. Yet, a
crucial question remains: How reliable is watermarking in realistic settings in
the wild? There, watermarked text might be mixed with other text sources,
paraphrased by human writers or other language models, and used for
applications in a broad number of domains, both social and technical. In this
paper, we explore different detection schemes, quantify their power at
detecting watermarks, and determine how much machine-generated text needs to be
observed in each scenario to reliably detect the watermark. We especially
highlight our human study, where we investigate the reliability of watermarking
when faced with human paraphrasing. We compare watermark-based detection to
other detection strategies, finding overall that watermarking is a reliable
solution, especially because of its sample complexity - for all attacks we
consider, the watermark evidence compounds the more examples are given, and the
watermark is eventually detected.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages in the main body. Code is available at
  https://github.com/jwkirchenbauer/lm-watermarking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis,
  and LLMs Evaluations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reexamines the research on out-of-distribution (OOD) robustness in
the field of NLP. We find that the distribution shift settings in previous
studies commonly lack adequate challenges, hindering the accurate evaluation of
OOD robustness. To address these issues, we propose a benchmark construction
protocol that ensures clear differentiation and challenging distribution
shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution
robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we
conduct a series of experiments on pre-trained language models for analysis and
evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the
relationship between in-distribution (ID) and OOD performance. We identify
three typical types that unveil the inner learning mechanism, which could
potentially facilitate the forecasting of OOD robustness, correlating with the
advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and
find that, despite exhibiting some effectiveness in specific cases, they do not
offer significant improvement compared to vanilla fine-tuning. Further, we
evaluate 5 LLMs with various adaptation paradigms and find that when sufficient
ID data is available, fine-tuning domain-specific models outperform LLMs on ID
examples significantly. However, in the case of OOD instances, prioritizing
LLMs with in-context learning yields better results. We identify that both
fine-tuned small models and LLMs face challenges in effectively addressing
downstream tasks. The code is public at
\url{https://github.com/lifan-yuan/OOD_NLP}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at \url{https://github.com/lifan-yuan/OOD_NLP}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Two Word Test: A Semantic Benchmark for Large Language Models <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04610v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04610v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicholas Riccardi, Rutvik H. Desai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have shown remarkable abilities recently,
including passing advanced professional exams and demanding benchmark tests.
This performance has led many to suggest that they are close to achieving
humanlike or 'true' understanding of language, and even Artificial General
Intelligence (AGI). Here, we provide a new open-source benchmark that can
assess semantic abilities of LLMs using two-word phrases using a task that can
be performed relatively easily by humans without advanced training. Combining
multiple words into a single concept is a fundamental aspect of human language
and intelligence. The test requires meaningfulness judgments of 1768 noun-noun
combinations that have been rated as meaningful (e.g., baby boy) or not
meaningful (e.g., goat sky). by 150 human raters. We provide versions of the
task that probe meaningfulness ratings on a 0-4 scale as well as binary
judgments. We conducted a series of experiments using the TWT on GPT-4,
GPT-3.5, and Bard, with both versions. Results demonstrated that, compared to
humans, all models perform poorly at rating meaningfulness of these phrases.
GPT-3.5 and Bard are also unable to make binary discriminations between
sensible and nonsense phrases as making sense. GPT-4 makes a substantial
improvement in binary discrimination of combinatorial phrases but is still
significantly worse than human performance. The TWT can be used to understand
the limitations and weaknesses of current LLMs, and potentially improve them.
The test also reminds us that caution is warranted in attributing 'true
understanding' or AGI to LLMs. TWT is available at:
https://github.com/NickRiccardi/two-word-test
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 5 figures, 3 tables, submitted to NeurIPS 2023 Datasets and
  Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models Get a Gender Makeover: Mitigating Gender Bias with
  Few-Shot Data Interventions <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, Louis-Philippe Morency
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Societal biases present in pre-trained large language models are a critical
issue as these models have been shown to propagate biases in countless
downstream applications, rendering them unfair towards specific groups of
people. Since large-scale retraining of these models from scratch is both time
and compute-expensive, a variety of approaches have been previously proposed
that de-bias a pre-trained model. While the majority of current
state-of-the-art debiasing methods focus on changes to the training regime, in
this paper, we propose data intervention strategies as a powerful yet simple
technique to reduce gender bias in pre-trained models. Specifically, we
empirically show that by fine-tuning a pre-trained model on only 10 de-biased
(intervened) training examples, the tendency to favor any gender is
significantly reduced. Since our proposed method only needs a few training
examples, our few-shot debiasing approach is highly feasible and practical.
Through extensive experimentation, we show that our debiasing technique
performs better than competitive state-of-the-art baselines with minimal loss
in language modeling ability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2023 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gender, names and other mysteries: Towards the ambiguous for
  gender-inclusive translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04573v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04573v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danielle Saunders, Katrina Olsen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vast majority of work on gender in MT focuses on 'unambiguous' inputs,
where gender markers in the source language are expected to be resolved in the
output. Conversely, this paper explores the widespread case where the source
sentence lacks explicit gender markers, but the target sentence contains them
due to richer grammatical gender. We particularly focus on inputs containing
person names.
  Investigating such sentence pairs casts a new light on research into MT
gender bias and its mitigation. We find that many name-gender co-occurrences in
MT data are not resolvable with 'unambiguous gender' in the source language,
and that gender-ambiguous examples can make up a large proportion of training
examples. From this, we discuss potential steps toward gender-inclusive
translation which accepts the ambiguity in both gender and translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>GITT workshop at EAMT 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chat<span class="highlight-title">GPT</span> is fun, but it is not funny! Humor is still challenging Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophie Jentzsch, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humor is a central aspect of human communication that has not been solved for
artificial agents so far. Large language models (LLMs) are increasingly able to
capture implicit and contextual information. Especially, OpenAI's ChatGPT
recently gained immense public attention. The GPT3-based model almost seems to
communicate on a human level and can even tell jokes. Humor is an essential
component of human communication. But is ChatGPT really funny? We put ChatGPT's
sense of humor to the test. In a series of exploratory experiments around
jokes, i.e., generation, explanation, and detection, we seek to understand
ChatGPT's capability to grasp and reproduce human humor. Since the model itself
is not accessible, we applied prompt-based experiments. Our empirical evidence
indicates that jokes are not hard-coded but mostly also not newly generated by
the model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system
accurately explains valid jokes but also comes up with fictional explanations
for invalid jokes. Joke-typical characteristics can mislead ChatGPT in the
classification of jokes. ChatGPT has not solved computational humor yet but it
can be a big leap toward "funny" machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Task Training with In-Domain Language Models for Diagnostic
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brihat Sharma, Yanjun Gao, Timothy Miller, Matthew M. Churpek, Majid Afshar, Dmitriy Dligach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative artificial intelligence (AI) is a promising direction for
augmenting clinical diagnostic decision support and reducing diagnostic errors,
a leading contributor to medical errors. To further the development of clinical
AI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as a
comprehensive generative AI framework, comprised of six tasks representing key
components in clinical reasoning. We present a comparative analysis of
in-domain versus out-of-domain language models as well as multi-task versus
single task training with a focus on the problem summarization task in DR.BENCH
(Gao et al., 2023). We demonstrate that a multi-task, clinically trained
language model outperforms its general domain counterpart by a large margin,
establishing a new state-of-the-art performance, with a ROUGE-L score of 28.55.
This research underscores the value of domain-specific training for optimizing
clinical diagnostic reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2023 ClinicalNLP Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Bootstrapping for Label Refinement <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04544v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04544v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shudi Hou, Yu Xia, Muhao Chen, Sujian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traditional text classification typically categorizes texts into pre-defined
coarse-grained classes, from which the produced models cannot handle the
real-world scenario where finer categories emerge periodically for accurate
services. In this work, we investigate the setting where fine-grained
classification is done only using the annotation of coarse-grained categories
and the coarse-to-fine mapping. We propose a lightweight contrastive
clustering-based bootstrapping method to iteratively refine the labels of
passages. During clustering, it pulls away negative passage-prototype pairs
under the guidance of the mapping from both global and local perspectives.
Experiments on NYT and 20News show that our method outperforms the
state-of-the-art methods by a large margin.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Learning Without Labeled Multimodal Data: Guarantees and
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Pu Liang, Chun Kai Ling, Yun Cheng, Alex Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, Ruslan Salakhutdinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many machine learning systems that jointly learn from multiple modalities,
a core research question is to understand the nature of multimodal
interactions: the emergence of new task-relevant information during learning
from both modalities that was not present in either alone. We study this
challenge of interaction quantification in a semi-supervised setting with only
labeled unimodal data and naturally co-occurring multimodal data (e.g.,
unlabeled images and captions, video and corresponding audio) but when labeling
them is time-consuming. Using a precise information-theoretic definition of
interactions, our key contributions are the derivations of lower and upper
bounds to quantify the amount of multimodal interactions in this
semi-supervised setting. We propose two lower bounds based on the amount of
shared information between modalities and the disagreement between separately
trained unimodal classifiers, and derive an upper bound through connections to
approximate algorithms for min-entropy couplings. We validate these estimated
bounds and show how they accurately track true interactions. Finally, two
semi-supervised multimodal applications are explored based on these theoretical
results: (1) analyzing the relationship between multimodal performance and
estimated interactions, and (2) self-supervised learning that embraces
disagreement between modalities beyond agreement as is typically done.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at: https://github.com/pliang279/PID</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long-form analogies generated by chat<span class="highlight-title">GPT</span> lack human-like
  psycholinguistic properties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04537v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04537v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. M. Seals, Valerie L. Shalin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Psycholinguistic analyses provide a means of evaluating large language model
(LLM) output and making systematic comparisons to human-generated text. These
methods can be used to characterize the psycholinguistic properties of LLM
output and illustrate areas where LLMs fall short in comparison to
human-generated text. In this work, we apply psycholinguistic methods to
evaluate individual sentences from long-form analogies about biochemical
concepts. We compare analogies generated by human subjects enrolled in
introductory biochemistry courses to analogies generated by chatGPT. We perform
a supervised classification analysis using 78 features extracted from
Coh-metrix that analyze text cohesion, language, and readability (Graesser et.
al., 2004). Results illustrate high performance for classifying
student-generated and chatGPT-generated analogies. To evaluate which features
contribute most to model performance, we use a hierarchical clustering
approach. Results from this analysis illustrate several linguistic differences
between the two sources.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arxiv version of conference paper to appear at CogSci 2023 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>Attack: Probing Dialogue State Trackers with Adversarial <span class="highlight-title">Prompt</span>s <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiangjue Dong, Yun He, Ziwei Zhu, James Caverlee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key component of modern conversational systems is the Dialogue State
Tracker (or DST), which models a user's goals and needs. Toward building more
robust and reliable DSTs, we introduce a prompt-based learning approach to
automatically generate effective adversarial examples to probe DST models. Two
key characteristics of this approach are: (i) it only needs the output of the
DST with no need for model parameters, and (ii) it can learn to generate
natural language utterances that can target any DST. Through experiments over
state-of-the-art DSTs, the proposed framework leads to the greatest reduction
in accuracy and the best attack success rate while maintaining good fluency and
a low perturbation ratio. We also show how much the generated adversarial
examples can bolster a DST through adversarial training. These results indicate
the strength of prompt-based attacks on DSTs and leave open avenues for
continued refinement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in Findings of ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Lenient Evaluation of Japanese Speech Recognition: Modeling Naturally
  Occurring Spelling Inconsistency <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shigeki Karita, Richard Sproat, Haruko Ishikawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Word error rate (WER) and character error rate (CER) are standard metrics in
Speech Recognition (ASR), but one problem has always been alternative
spellings: If one's system transcribes adviser whereas the ground truth has
advisor, this will count as an error even though the two spellings really
represent the same word.
  Japanese is notorious for ``lacking orthography'': most words can be spelled
in multiple ways, presenting a problem for accurate ASR evaluation. In this
paper we propose a new lenient evaluation metric as a more defensible CER
measure for Japanese ASR. We create a lattice of plausible respellings of the
reference transcription, using a combination of lexical resources, a Japanese
text-processing system, and a neural machine translation model for
reconstructing kanji from hiragana or katakana. In a manual evaluation, raters
rated 95.4% of the proposed spelling variants as plausible. ASR results show
that our method, which does not penalize the system for choosing a valid
alternate spelling of a word, affords a 2.4%-3.1% absolute reduction in CER
depending on the task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL Workshop on Computation and Written Language (CAWL) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Can current NLI systems handle German word order? Investigating language
  model performance on a new German challenge set of minimal pairs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04523v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04523v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ines Reinig, Katja Markert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to English, German word order is freer and therefore poses
additional challenges for natural language inference (NLI). We create WOGLI
(Word Order in German Language Inference), the first adversarial NLI dataset
for German word order that has the following properties: (i) each premise has
an entailed and a non-entailed hypothesis; (ii) premise and hypotheses differ
only in word order and necessary morphological changes to mark case and number.
In particular, each premise andits two hypotheses contain exactly the same
lemmata. Our adversarial examples require the model to use morphological
markers in order to recognise or reject entailment. We show that current German
autoencoding models fine-tuned on translated NLI data can struggle on this
challenge set, reflecting the fact that translated NLI datasets will not mirror
all necessary language phenomena in the target language. We also examine
performance after data augmentation as well as on related word order phenomena
derived from WOGLI. Our datasets are publically available at
https://github.com/ireinig/wogli.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing In-Context Learning with Answer Feedback for Multi-Span
  Question Answering <span class="chip">NLPCC 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04508v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04508v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixian Huang, Jiaying Zhou, Gengyang Xiao, Gong Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whereas the recent emergence of large language models (LLMs) like ChatGPT has
exhibited impressive general performance, it still has a large gap with
fully-supervised models on specific tasks such as multi-span question
answering. Previous researches found that in-context learning is an effective
approach to exploiting LLM, by using a few task-related labeled data as
demonstration examples to construct a few-shot prompt for answering new
questions. A popular implementation is to concatenate a few questions and their
correct answers through simple templates, informing LLM of the desired output.
In this paper, we propose a novel way of employing labeled data such that it
also informs LLM of some undesired output, by extending demonstration examples
with feedback about answers predicted by an off-the-shelf model, e.g., correct,
incorrect, or incomplete. Experiments on three multi-span question answering
datasets as well as a keyphrase extraction dataset show that our new prompting
strategy consistently improves LLM's in-context learning performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, submitted to NLPCC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Chat<span class="highlight-title">GPT</span> on Biomedical Tasks: A Zero-Shot Comparison with
  Fine-Tuned Generative <span class="highlight-title">Transformer</span>s <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is a large language model developed by OpenAI. Despite its impressive
performance across various tasks, no prior work has investigated its capability
in the biomedical domain yet. To this end, this paper aims to evaluate the
performance of ChatGPT on various benchmark biomedical tasks, such as relation
extraction, document classification, question answering, and summarization. To
the best of our knowledge, this is the first work that conducts an extensive
evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on
our evaluation that in biomedical datasets that have smaller training sets,
zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative
transformer models, such as BioGPT and BioBART. This suggests that ChatGPT's
pre-training on large text corpora makes it quite specialized even in the
biomedical domain. Our findings demonstrate that ChatGPT has the potential to
be a valuable tool for various tasks in the biomedical domain that lack large
annotated data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by BioNLP@ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ STEPS: A Benchmark for Order Reasoning in Sequential Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04441v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04441v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Weizhi Wang, Hong Wang, Xifeng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Various human activities can be abstracted into a sequence of actions in
natural text, i.e. cooking, repairing, manufacturing, etc. Such action
sequences heavily depend on the executing order, while disorder in action
sequences leads to failure of further task execution by robots or AI agents.
Therefore, to verify the order reasoning capability of current neural models in
sequential tasks, we propose a challenging benchmark , named STEPS. STEPS
involves two subtask settings, focusing on determining the rationality of given
next step in recipes and selecting the reasonable step from the multi-choice
question, respectively. We describe the data construction and task
formulations, and benchmark most of significant Large Language Models (LLMs).
The experimental results demonstrate 1) The commonsense reasoning of action
orders in sequential tasks are challenging to resolve via zero-shot prompting
or few-shot in-context learning for LLMs; 2) Prompting method still
significantly lags behind tuning-based method on STEPS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in Progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zambezi Voice: A Multilingual Speech Corpus for Zambian Languages <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Claytone Sikasote, Kalinda Siaminwe, Stanly Mwape, Bangiwe Zulu, Mofya Phiri, Martin Phiri, David Zulu, Mayumbo Nyirenda, Antonios Anastasopoulos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces Zambezi Voice, an open-source multilingual speech
resource for Zambian languages. It contains two collections of datasets:
unlabelled audio recordings of radio news and talk shows programs (160 hours)
and labelled data (over 80 hours) consisting of read speech recorded from text
sourced from publicly available literature books. The dataset is created for
speech recognition but can be extended to multilingual speech processing
research for both supervised and unsupervised learning approaches. To our
knowledge, this is the first multilingual speech dataset created for Zambian
languages. We exploit pretraining and cross-lingual transfer learning by
finetuning the Wav2Vec2.0 large-scale multilingual pre-trained model to build
end-to-end (E2E) speech recognition models for our baseline models. The dataset
is released publicly under a Creative Commons BY-NC-ND 4.0 license and can be
accessed through the project repository. See
https://github.com/unza-speech-lab/zambezi-voice
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Examining Bias in Opinion Summarisation Through the Perspective of
  Opinion Diversity <span class="chip">WASSA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04424v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04424v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nannan Huang, Lin Tian, Haytham Fayek, Xiuzhen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Opinion summarisation is a task that aims to condense the information
presented in the source documents while retaining the core message and
opinions. A summary that only represents the majority opinions will leave the
minority opinions unrepresented in the summary. In this paper, we use the
stance towards a certain target as an opinion. We study bias in opinion
summarisation from the perspective of opinion diversity, which measures whether
the model generated summary can cover a diverse set of opinions. In addition,
we examine opinion similarity, a measure of how closely related two opinions
are in terms of their stance on a given topic, and its relationship with
opinion diversity. Through the lens of stances towards a topic, we examine
opinion diversity and similarity using three debatable topics under COVID-19.
Experimental results on these topics revealed that a higher degree of
similarity of opinions did not indicate good diversity or fairly cover the
various opinions originally presented in the source documents. We found that
BART and ChatGPT can better capture diverse opinions presented in the source
documents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, accepted at WASSA, ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer Learning of <span class="highlight-title">Transformer</span>-based Speech Recognition Models from
  Czech to Slovak 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04399v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04399v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Lehečka, Josef V. Psutka, Josef Psutka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we are comparing several methods of training the Slovak speech
recognition models based on the Transformers architecture. Specifically, we are
exploring the approach of transfer learning from the existing Czech pre-trained
Wav2Vec 2.0 model into Slovak. We are demonstrating the benefits of the
proposed approach on three Slovak datasets. Our Slovak models scored the best
results when initializing the weights from the Czech model at the beginning of
the pre-training phase. Our results show that the knowledge stored in the Cezch
pre-trained model can be successfully reused to solve tasks in Slovak while
outperforming even much larger public multilingual models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to TSD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ M$^3$IT: A Large-Scale <span class="highlight-title">Dataset</span> towards Multi-Modal Multilingual
  Instruction Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04387v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04387v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lei Li, Yuwei Yin, Shicheng Li, Liang Chen, Peiyi Wang, Shuhuai Ren, Mukai Li, Yazheng Yang, Jingjing Xu, Xu Sun, Lingpeng Kong, Qi Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning has significantly advanced large language models (LLMs)
such as ChatGPT, enabling them to align with human instructions across diverse
tasks. However, progress in open vision-language models (VLMs) has been limited
due to the scarcity of high-quality instruction datasets. To tackle this
challenge and promote research in the vision-language field, we introduce the
Multi-Modal, Multilingual Instruction Tuning (M$^3$IT) dataset, designed to
optimize VLM alignment with human instructions. Our M$^3$IT dataset comprises
40 carefully curated datasets, including 2.4 million instances and 400 manually
written task instructions, reformatted into a vision-to-text structure. Key
tasks are translated into 80 languages with an advanced translation system,
ensuring broader accessibility. M$^3$IT surpasses previous datasets regarding
task coverage, instruction number and instance scale. Moreover, we develop
Ying-VLM, a VLM model trained on our M$^3$IT dataset, showcasing its potential
to answer complex questions requiring world knowledge, generalize to unseen
video tasks, and comprehend unseen instructions in Chinese. To encourage
further research, we have open-sourced both the dataset and trained models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Dataset available at: https://huggingface.co/MMInstruction/M3IT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual Clinical NER: Translation or Cross-lingual Transfer? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Fontaine, Félix Gaschi, Parisa Rastin, Yannick Toussaint
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language tasks like Named Entity Recognition (NER) in the clinical
domain on non-English texts can be very time-consuming and expensive due to the
lack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent
this issue thanks to the ability of multilingual large language models to be
fine-tuned on a specific task in one language and to provide high accuracy for
the same task in another language. However, other methods leveraging
translation models can be used to perform NER without annotated data in the
target language, by either translating the training set or test set. This paper
compares cross-lingual transfer with these two alternative methods, to perform
clinical NER in French and in German without any training data in those
languages. To this end, we release MedNERF a medical NER test set extracted
from French drug prescriptions and annotated with the same guidelines as an
English dataset. Through extensive experiments on this dataset and on a German
medical dataset (Frei and Kramer, 2021), we show that translation-based methods
can achieve similar performance to CLT but require more care in their design.
And while they can take advantage of monolingual clinical language models,
those do not guarantee better results than large general-purpose multilingual
models, whether with cross-lingual transfer or translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, Proceedings of the 5th Clinical Natural Language Processing
  Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Label Aware Speech Representation Learning For Language Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shikhar Vashishth, Shikhar Bharadwaj, Sriram Ganapathy, Ankur Bapna, Min Ma, Wei Han, Vera Axelrod, Partha Talukdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech representation learning approaches for non-semantic tasks such as
language recognition have either explored supervised embedding extraction
methods using a classifier model or self-supervised representation learning
approaches using raw data. In this paper, we propose a novel framework of
combining self-supervised representation learning with the language label
information for the pre-training task. This framework, termed as Label Aware
Speech Representation (LASR) learning, uses a triplet based objective function
to incorporate language labels along with the self-supervised loss function.
The speech representations are further fine-tuned for the downstream task. The
language recognition experiments are performed on two public datasets - FLEURS
and Dhwani. In these experiments, we illustrate that the proposed LASR
framework improves over the state-of-the-art systems on language
identification. We also report an analysis of the robustness of LASR approach
to noisy/missing labels as well as its application to multi-lingual speech
recognition tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Arabic Dysarthric Speech Recognition Using Adversarial and Signal-Based
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04368v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04368v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Massa Baali, Ibrahim Almakky, Shady Shehata, Fakhri Karray
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite major advancements in Automatic Speech Recognition (ASR), the
state-of-the-art ASR systems struggle to deal with impaired speech even with
high-resource languages. In Arabic, this challenge gets amplified, with added
complexities in collecting data from dysarthric speakers. In this paper, we aim
to improve the performance of Arabic dysarthric automatic speech recognition
through a multi-stage augmentation approach. To this effect, we first propose a
signal-based approach to generate dysarthric Arabic speech from healthy Arabic
speech by modifying its speed and tempo. We also propose a second stage
Parallel Wave Generative (PWG) adversarial model that is trained on an English
dysarthric dataset to capture language-independant dysarthric speech patterns
and further augment the signal-adjusted speech samples. Furthermore, we propose
a fine-tuning and text-correction strategies for Arabic Conformer at different
dysarthric speech severity levels. Our fine-tuned Conformer achieved 18% Word
Error Rate (WER) and 17.2% Character Error Rate (CER) on synthetically
generated dysarthric speech from the Arabic commonvoice speech dataset. This
shows significant WER improvement of 81.8% compared to the baseline model
trained solely on healthy data. We perform further validation on real English
dysarthric speech showing a WER improvement of 124% compared to the baseline
trained only on healthy English LJSpeech dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Youku-mPLUG: A 10 Million Large-scale Chinese Video-Language <span class="highlight-title">Dataset</span> for
  <span class="highlight-title">Pre-train</span>ing and Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04362v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04362v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haiyang Xu, Qinghao Ye, Xuan Wu, Ming Yan, Yuan Miao, Jiabo Ye, Guohai Xu, Anwen Hu, Yaya Shi, Guangwei Xu, Chenliang Li, Qi Qian, Maofei Que, Ji Zhang, Xiao Zeng, Fei Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To promote the development of Vision-Language Pre-training (VLP) and
multimodal Large Language Model (LLM) in the Chinese community, we firstly
release the largest public Chinese high-quality video-language dataset named
Youku-mPLUG, which is collected from Youku, a well-known Chinese video-sharing
website, with strict criteria of safety, diversity, and quality. Youku-mPLUG
contains 10 million Chinese video-text pairs filtered from 400 million raw
videos across a wide range of 45 diverse categories for large-scale
pre-training. In addition, to facilitate a comprehensive evaluation of
video-language models, we carefully build the largest human-annotated Chinese
benchmarks covering three popular video-language tasks of cross-modal
retrieval, video captioning, and video category classification. Youku-mPLUG can
enable researchers to conduct more in-depth multimodal research and develop
better applications in the future. Furthermore, we release popular
video-language pre-training models, ALPRO and mPLUG-2, and our proposed
modularized decoder-only model mPLUG-video pre-trained on Youku-mPLUG.
Experiments show that models pre-trained on Youku-mPLUG gain up to 23.1%
improvement in video category classification. Besides, mPLUG-video achieves a
new state-of-the-art result on these benchmarks with 80.5% top-1 accuracy in
video category classification and 68.9 CIDEr score in video captioning,
respectively. Finally, we scale up mPLUG-video based on the frozen Bloomz with
only 1.7% trainable parameters as Chinese multimodal LLM, and demonstrate
impressive instruction and video understanding ability. The zero-shot
instruction understanding experiment indicates that pretraining with
Youku-mPLUG can enhance the ability to comprehend overall and detailed visual
semantics, recognize scene text, and leverage open-domain knowledge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Working in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConTextual Masked Auto-Encoder for Retrieval-based Dialogue Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04357v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04357v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenpeng Su, Xing Wu, Wei Zhou, Guangyuan Ma,  Songlin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue response selection aims to select an appropriate response from
several candidates based on a given user and system utterance history. Recent
studies have been improving the accuracy of dialogue response selection through
post-training, mostly relying on naive masked language modeling methods.
However, the recently developed generative methods have shown promising text
representation capabilities in IR community, which could potentially lead to
better dialogue semantics modeling. Thus, in this paper, we propose Dial-MAE
(Dialogue Contextual Masking Auto-encoder), a straightforward yet effective
post-training technique tailored for dialogue response selection. Dial-MAE uses
an asymmetric encoder-decoder architecture that learns to better compress the
semantics of the dialogue into dialogue-dense vectors. The process of Dial-MAE
involves a deep encoder creating a dialogue embedding with the masked dialogue
context, followed by a shallow decoder that uses this embedding along with the
highly masked response to restore the original response. Our experiments have
demonstrated that Dial-MAE is highly effective, achieving state-of-the-art
performance on two commonly evaluated benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">GPT</span> Self-Supervision for a Better Data Annotator 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohuan Pei, Yanxi Li, Chang Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of annotating data into concise summaries poses a significant
challenge across various domains, frequently requiring the allocation of
significant time and specialized knowledge by human experts. Despite existing
efforts to use large language models for annotation tasks, significant problems
such as limited applicability to unlabeled data, the absence of self-supervised
methods, and the lack of focus on complex structured data still persist. In
this work, we propose a GPT self-supervision annotation method. This method
embodies a generating-recovering paradigm that leverages the capabilities of
one-shot learning capabilities in Generative Pretrained Transformer (GPT). The
proposed approach comprises a one-shot tuning phase followed by a generation
phase. In the one-shot tuning phase, we sample a data from the support set as
part of the prompt for GPT to generate a textual summary, which is then used to
recover the original data. The alignment score between the recovered and
original data serves as a self-supervision navigator to refine the process. In
the generation stage, the optimally selected one-shot sample serves as a
template in the prompt and is applied to generating summaries from challenging
datasets. The annotation performance is evaluated by tuning several human
feedback reward networks and by calculating alignment scores between original
and recovered data at both sentence and structure levels. Our self-supervised
annotation method consistently achieves competitive scores, convincingly
demonstrating its robust strength in various data-to-summary annotation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ World Models for Math Story Problems <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04347v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04347v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andreas Opedal, Niklas Stoehr, Abulhair Saparov, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving math story problems is a complex task for students and NLP models
alike, requiring them to understand the world as described in the story and
reason over it to compute an answer. Recent years have seen impressive
performance on automatically solving these problems with large pre-trained
language models and innovative techniques to prompt them. However, it remains
unclear if these models possess accurate representations of mathematical
concepts. This leads to lack of interpretability and trustworthiness which
impedes their usefulness in various applications. In this paper, we consolidate
previous work on categorizing and representing math story problems and develop
MathWorld, which is a graph-based semantic formalism specific for the domain of
math story problems. With MathWorld, we can assign world models to math story
problems which represent the situations and actions introduced in the text and
their mathematical relationships. We combine math story problems from several
existing datasets and annotate a corpus of 1,019 problems and 3,204 logical
forms with MathWorld. Using this data, we demonstrate the following use cases
of MathWorld: (1) prompting language models with synthetically generated
question-answer pairs to probe their reasoning and world modeling abilities,
and (2) generating new problems by using the world models as a design space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL Findings 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Co-evolving Graph Reasoning Network for Emotion-Cause Pair Extraction <span class="chip">ECML-PKDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04340v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04340v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Xing, Ivor W. Tsang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion-Cause Pair Extraction (ECPE) aims to extract all emotion clauses and
their corresponding cause clauses from a document. Existing approaches tackle
this task through multi-task learning (MTL) framework in which the two subtasks
provide indicative clues for ECPE. However, the previous MTL framework
considers only one round of multi-task reasoning and ignores the reverse
feedbacks from ECPE to the subtasks. Besides, its multi-task reasoning only
relies on semantics-level interactions, which cannot capture the explicit
dependencies, and both the encoder sharing and multi-task hidden states
concatenations can hardly capture the causalities. To solve these issues, we
first put forward a new MTL framework based on Co-evolving Reasoning. It (1)
models the bidirectional feedbacks between ECPE and its subtasks; (2) allows
the three tasks to evolve together and prompt each other recurrently; (3)
integrates prediction-level interactions to capture explicit dependencies. Then
we propose a novel multi-task relational graph (MRG) to sufficiently exploit
the causal relations. Finally, we propose a Co-evolving Graph Reasoning Network
(CGR-Net) that implements our MTL framework and conducts Co-evolving Reasoning
on MRG. Experimental results show that our model achieves new state-of-the-art
performance, and further analysis confirms the advantages of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECML-PKDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study on the Reliability of Automatic Dysarthric Speech Assessments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04337v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04337v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier F. Cadet, Ranya Aloufi, Sara Ahmadi-Abhari, Hamed Haddadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automating dysarthria assessments offers the opportunity to develop
effective, low-cost tools that address the current limitations of manual and
subjective assessments. Nonetheless, it is unclear whether current approaches
rely on dysarthria-related speech patterns or external factors. We aim toward
obtaining a clearer understanding of dysarthria patterns. To this extent, we
study the effects of noise in recordings, both through addition and reduction.
We design and implement a new method for visualizing and comparing feature
extractors and models, at a patient level, in a more interpretable way. We use
the UA-Speech dataset with a speaker-based split of the dataset. Results
reported in the literature appear to have been done irrespective of such split,
leading to models that may be overconfident due to data-leakage. We hope that
these results raise awareness in the research community regarding the
requirements for establishing reliable automatic dysarthria assessment systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Echoes from Alexandria: A Large Resource for Multilingual Book
  Summarization <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Scirè, Simone Conia, Simone Ciciliano, Roberto Navigli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, research in text summarization has mainly focused on the
news domain, where texts are typically short and have strong layout features.
The task of full-book summarization presents additional challenges which are
hard to tackle with current resources, due to their limited size and
availability in English only. To overcome these limitations, we present "Echoes
from Alexandria", or in shortened form, "Echoes", a large resource for
multilingual book summarization. Echoes features three novel datasets: i)
Echo-Wiki, for multilingual book summarization, ii) Echo-XSum, for
extremely-compressive multilingual book summarization, and iii) Echo-FairySum,
for extractive book summarization. To the best of our knowledge, Echoes, with
its thousands of books and summaries, is the largest resource, and the first to
be multilingual, featuring 5 languages and 25 language pairs. In addition to
Echoes, we also introduce a new extractive-then-abstractive baseline, and,
supported by our experimental results and manual analysis of the summaries
generated, we argue that this baseline is more suitable for book summarization
than purely-abstractive approaches. We release our resource and software at
https://github.com/Babelscape/echoes-from-alexandria in the hope of fostering
innovative research in multilingual book summarization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, long paper at ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ IUTEAM1 at MEDIQA-Chat 2023: Is simple fine tuning effective for
  multilayer summarization of clinical conversations? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04328v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04328v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhananjay Srivastava
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Clinical conversation summarization has become an important application of
Natural language Processing. In this work, we intend to analyze summarization
model ensembling approaches, that can be utilized to improve the overall
accuracy of the generated medical report called chart note. The work starts
with a single summarization model creating the baseline. Then leads to an
ensemble of summarization models trained on a separate section of the chart
note. This leads to the final approach of passing the generated results to
another summarization model in a multi-layer/stage fashion for better coherency
of the generated text. Our results indicate that although an ensemble of models
specialized in each section produces better results, the multi-layer/stage
approach does not improve accuracy. The code for the above paper is available
at https://github.com/dhananjay-srivastava/MEDIQA-Chat-2023-iuteam1.git
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Genre Argument Mining: Can Language Models Automatically Fill in
  Missing Discourse Markers? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04314v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04314v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gil Rocha, Henrique Lopes Cardoso, Jonas Belouadi, Steffen Eger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Available corpora for Argument Mining differ along several axes, and one of
the key differences is the presence (or absence) of discourse markers to signal
argumentative content. Exploring effective ways to use discourse markers has
received wide attention in various discourse parsing tasks, from which it is
well-known that discourse markers are strong indicators of discourse relations.
To improve the robustness of Argument Mining systems across different genres,
we propose to automatically augment a given text with discourse markers such
that all relations are explicitly signaled. Our analysis unveils that popular
language models taken out-of-the-box fail on this task; however, when
fine-tuned on a new heterogeneous dataset that we construct (including
synthetic and real examples), they perform considerably better. We demonstrate
the impact of our approach on an Argument Mining downstream task, evaluated on
different corpora, showing that language models can be trained to automatically
fill in discourse markers across different corpora, improving the performance
of a downstream model in some, but not all, cases. Our proposed approach can
further be employed as an assistive tool for better discourse understanding.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personality testing of <span class="highlight-title">GPT</span>-3: Limited temporal reliability, but
  highlighted social desirability of <span class="highlight-title">GPT</span>-3's personality instruments results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04308v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04308v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bojana Bodroza, Bojana M. Dinic, Ljubisa Bojic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To assess the potential applications and limitations of chatbot GPT-3
Davinci-003, this study explored the temporal reliability of personality
questionnaires applied to the chatbot and its personality profile.
Psychological questionnaires were administered to the chatbot on two separate
occasions, followed by a comparison of the responses to human normative data.
The findings revealed varying levels of agreement in the chatbot's responses
over time, with some scales displaying excellent while others demonstrated poor
agreement. Overall, Davinci-003 displayed a socially desirable and pro-social
personality profile, particularly in the domain of communion. However, the
underlying basis of the chatbot's responses, whether driven by conscious
self-reflection or predetermined algorithms, remains uncertain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 1 table</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Allophant: Cross-lingual Phoneme Recognition with Articulatory
  Attributes <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04306v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04306v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Glocker, Aaricia Herygers, Munir Georges
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes Allophant, a multilingual phoneme recognizer. It requires
only a phoneme inventory for cross-lingual transfer to a target language,
allowing for low-resource recognition. The architecture combines a
compositional phone embedding approach with individually supervised phonetic
attribute classifiers in a multi-task architecture. We also introduce
Allophoible, an extension of the PHOIBLE database. When combined with a
distance based mapping approach for grapheme-to-phoneme outputs, it allows us
to train on PHOIBLE inventories directly. By training and evaluating on 34
languages, we found that the addition of multi-task learning improves the
model's capability of being applied to unseen phonemes and phoneme inventories.
On supervised languages we achieve phoneme error rate improvements of 11
percentage points (pp.) compared to a baseline without multi-task learning.
Evaluation of zero-shot transfer on 84 languages yielded a decrease in PER of
2.63 pp. over the baseline.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 2 figures, 2 tables, accepted to INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Phrase Retrieval for Open-Domain Conversational Question Answering with
  Conversational Dependency Modeling via Contrastive Learning <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soyeong Jeong, Jinheon Baek, Sung Ju Hwang, Jong C. Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-Domain Conversational Question Answering (ODConvQA) aims at answering
questions through a multi-turn conversation based on a retriever-reader
pipeline, which retrieves passages and then predicts answers with them.
However, such a pipeline approach not only makes the reader vulnerable to the
errors propagated from the retriever, but also demands additional effort to
develop both the retriever and the reader, which further makes it slower since
they are not runnable in parallel. In this work, we propose a method to
directly predict answers with a phrase retrieval scheme for a sequence of
words, reducing the conventional two distinct subtasks into a single one. Also,
for the first time, we study its capability for ODConvQA tasks. However, simply
adopting it is largely problematic, due to the dependencies between previous
and current turns in a conversation. To address this problem, we further
introduce a novel contrastive learning strategy, making sure to reflect
previous turns when retrieving the phrase for the current context, by
maximizing representational similarities of consecutive turns in a conversation
while minimizing irrelevant conversational contexts. We validate our model on
two ODConvQA datasets, whose experimental results show that it substantially
outperforms the relevant baselines with the retriever-reader. Code is available
at: https://github.com/starsuzi/PRO-ConvQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analysis of the Fed's communication by using textual entailment model of
  Zero-Shot classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04277v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04277v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yasuhiro Nakayama, Tomochika Sawaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we analyze documents published by central banks using text
mining techniques and propose a method to evaluate the policy tone of central
banks. Since the monetary policies of major central banks have a broad impact
on financial market trends, the pricing of risky assets, and the real economy,
market participants are attempting to more accurately capture changes in the
outlook for central banks' future monetary policies. Since the published
documents are also an important tool for the central bank to communicate with
the market, they are meticulously elaborated on grammatical syntax and wording,
and investors are urged to read more accurately about the central bank's policy
stance. Sentiment analysis on central bank documents has long been carried out,
but it has been difficult to interpret the meaning of the documents accurately
and to explicitly capture even the intentional change in nuance. This study
attempts to evaluate the implication of the zero-shot text classification
method for an unknown economic environment using the same model. We compare the
tone of the statements, minutes, press conference transcripts of FOMC meetings,
and the Fed officials' (chair, vice chair, and Governors) speeches. In
addition, the minutes of the FOMC meetings were subjected to a phase analysis
of changes in each policy stance since 1971.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 4 figures, 2 Tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-microphone Automatic Speech Segmentation in Meetings Based on
  Circular Harmonics Features <span class="chip">ISCA</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Théo Mariotte, Anthony Larcher, Silvio Montrésor, Jean-Hugh Thomas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speaker diarization is the task of answering Who spoke and when? in an audio
stream. Pipeline systems rely on speech segmentation to extract speakers'
segments and achieve robust speaker diarization. This paper proposes a common
framework to solve three segmentation tasks in the distant speech scenario:
Voice Activity Detection (VAD), Overlapped Speech Detection (OSD), and Speaker
Change Detection (SCD). In the literature, a few studies investigate the
multi-microphone distant speech scenario. In this work, we propose a new set of
spatial features based on direction-of-arrival estimations in the circular
harmonic domain (CH-DOA). These spatial features are extracted from
multi-microphone audio data and combined with standard acoustic features.
Experiments on the AMI meeting corpus show that CH-DOA can improve the
segmentation while being robust in the case of deactivated microphones.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Interspeech 2023, international Speech Communication Association
  (ISCA), Aug 2023, Dublin, Ireland</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transfer Learning from <span class="highlight-title">Pre-train</span>ed Language Models Improves End-to-End
  Speech Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04233v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04233v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kohei Matsuura, Takanori Ashihara, Takafumi Moriya, Tomohiro Tanaka, Takatomo Kano, Atsunori Ogawa, Marc Delcroix
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end speech summarization (E2E SSum) directly summarizes input speech
into easy-to-read short sentences with a single model. This approach is
promising because it, in contrast to the conventional cascade approach, can
utilize full acoustical information and mitigate to the propagation of
transcription errors. However, due to the high cost of collecting
speech-summary pairs, an E2E SSum model tends to suffer from training data
scarcity and output unnatural sentences. To overcome this drawback, we propose
for the first time to integrate a pre-trained language model (LM), which is
highly capable of generating natural sentences, into the E2E SSum decoder via
transfer learning. In addition, to reduce the gap between the independently
pre-trained encoder and decoder, we also propose to transfer the baseline E2E
SSum encoder instead of the commonly used automatic speech recognition encoder.
Experimental results show that the proposed model outperforms baseline and data
augmented models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Effective Neural Topic Modeling with Embedding Clustering Regularization <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04217v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04217v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaobao Wu, Xinshuai Dong, Thong Nguyen, Anh Tuan Luu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic models have been prevalent for decades with various applications.
However, existing topic models commonly suffer from the notorious topic
collapsing: discovered topics semantically collapse towards each other, leading
to highly repetitive topics, insufficient topic discovery, and damaged model
interpretability. In this paper, we propose a new neural topic model, Embedding
Clustering Regularization Topic Model (ECRTM). Besides the existing
reconstruction error, we propose a novel Embedding Clustering Regularization
(ECR), which forces each topic embedding to be the center of a separately
aggregated word embedding cluster in the semantic space. This enables each
produced topic to contain distinct word semantics, which alleviates topic
collapsing. Regularized by ECR, our ECRTM generates diverse and coherent topics
together with high-quality topic distributions of documents. Extensive
experiments on benchmark datasets demonstrate that ECRTM effectively addresses
the topic collapsing issue and consistently surpasses state-of-the-art
baselines in terms of topic quality, topic distributions of documents, and
downstream classification tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2023 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Knowledge Graph Embeddings to Enhance Contextual
  Representations for Relation Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fréjus A. A. Laleye, Loïc Rakotoson, Sylvain Massip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction task is a crucial and challenging aspect of Natural
Language Processing. Several methods have surfaced as of late, exhibiting
notable performance in addressing the task; however, most of these approaches
rely on vast amounts of data from large-scale knowledge graphs or language
models pretrained on voluminous corpora. In this paper, we hone in on the
effective utilization of solely the knowledge supplied by a corpus to create a
high-performing model. Our objective is to showcase that by leveraging the
hierarchical structure and relational distribution of entities within a corpus
without introducing external knowledge, a relation extraction model can achieve
significantly enhanced performance. We therefore proposed a relation extraction
approach based on the incorporation of pretrained knowledge graph embeddings at
the corpus scale into the sentence-level contextual representation. We
conducted a series of experiments which revealed promising and very interesting
results for our proposed approach.The obtained results demonstrated an
outperformance of our method compared to context-based relation extraction
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 1 figures, The 17th International Conference on Document
  Analysis and Recognition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An ASR-Based Tutor for Learning to Read: How to Optimize Feedback to
  First Graders <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04190v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04190v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Bai, Cristian Tejedor-Garcia, Ferdy Hubers, Catia Cucchiarini, Helmer Strik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The interest in employing automatic speech recognition (ASR) in applications
for reading practice has been growing in recent years. In a previous study, we
presented an ASR-based Dutch reading tutor application that was developed to
provide instantaneous feedback to first-graders learning to read. We saw that
ASR has potential at this stage of the reading process, as the results
suggested that pupils made progress in reading accuracy and fluency by using
the software. In the current study, we used children's speech from an existing
corpus (JASMIN) to develop two new ASR systems, and compared the results to
those of the previous study. We analyze correct/incorrect classification of the
ASR systems using human transcripts at word level, by means of evaluation
measures such as Cohen's Kappa, Matthews Correlation Coefficient (MCC),
precision, recall and F-measures. We observe improvements for the newly
developed ASR systems regarding the agreement with human-based judgment and
correct rejection (CR). The accuracy of the ASR systems varies for different
reading tasks and word types. Our results suggest that, in the current
configuration, it is difficult to classify isolated words. We discuss these
results, possible ways to improve our systems and avenues for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published (double-blind peer-reviewed) on SPECOM 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A New <span class="highlight-title">Dataset</span> and Empirical Study for Sentence Simplification in Chinese <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04188v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04188v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiping Yang, Renliang Sun, Xiaojun Wan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentence Simplification is a valuable technique that can benefit language
learners and children a lot. However, current research focuses more on English
sentence simplification. The development of Chinese sentence simplification is
relatively slow due to the lack of data. To alleviate this limitation, this
paper introduces CSS, a new dataset for assessing sentence simplification in
Chinese. We collect manual simplifications from human annotators and perform
data analysis to show the difference between English and Chinese sentence
simplifications. Furthermore, we test several unsupervised and zero/few-shot
learning methods on CSS and analyze the automatic evaluation and human
evaluation results. In the end, we explore whether Large Language Models can
serve as high-quality Chinese sentence simplification systems by evaluating
them on CSS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL2023 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowing-how & Knowing-that: A New Task for Machine Reading Comprehension
  of User Manuals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04187v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04187v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongru Liang, Jia Liu, Weihong Du, dingnan jin, Wenqiang Lei, Zujie Wen, Jiancheng Lv
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The machine reading comprehension (MRC) of user manuals has huge potential in
customer service. However,current methods have trouble answering complex
questions. Therefore, we introduce the Knowing-how & Knowing-that task that
requires the model to answer factoid-style, procedure-style, and inconsistent
questions about user manuals. We resolve this task by jointly representing the
steps and facts in a graph (TARA), which supports a unified inference of
various questions. Towards a systematical benchmarking study, we design a
heuristic method to automatically parse user manuals into TARAs and build an
annotated dataset to test the model's ability in answering real-world
questions. Empirical results demonstrate that representing user manuals as
TARAs is a desired solution for the MRC of user manuals. An in-depth
investigation of TARA further sheds light on the issues and broader impacts of
future representations of user manuals. We hope our work can move the MRC of
user manuals to a more complex and realistic stage.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Foundation Models with Language-Model-as-an-Examiner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, Lei Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous benchmarks have been established to assess the performance of
foundation models on open-ended question answering, which serves as a
comprehensive test of a model's ability to understand and generate language in
a manner similar to humans. Most of these works focus on proposing new
datasets, however, we see two main issues within previous benchmarking
pipelines, namely testing leakage and evaluation automation. In this paper, we
propose a novel benchmarking framework, Language-Model-as-an-Examiner, where
the LM serves as a knowledgeable examiner that formulates questions based on
its knowledge and evaluates responses in a reference-free manner. Our framework
allows for effortless extensibility as various LMs can be adopted as the
examiner, and the questions can be constantly updated given more diverse
trigger topics. For a more comprehensive and equitable evaluation, we devise
three strategies: (1) We instruct the LM examiner to generate questions across
a multitude of domains to probe for a broad acquisition, and raise follow-up
questions to engage in a more in-depth assessment. (2) Upon evaluation, the
examiner combines both scoring and ranking measurements, providing a reliable
result as it aligns closely with human annotations. (3) We additionally propose
a decentralized Peer-examination method to address the biases in a single
examiner. Our data and benchmarking results are available at:
https://lmexam.com.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When to Read Documents or QA History: On Unified and Selective
  Open-domain QA <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04176v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04176v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyungjae Lee, Sang-eun Han, Seung-won Hwang, Moontae Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the problem of open-domain question answering, with the
aim of answering a diverse range of questions leveraging knowledge resources.
Two types of sources, QA-pair and document corpora, have been actively
leveraged with the following complementary strength. The former is highly
precise when the paraphrase of given question $q$ was seen and answered during
training, often posed as a retrieval problem, while the latter generalizes
better for unseen questions. A natural follow-up is thus leveraging both
models, while a naive pipelining or integration approaches have failed to bring
additional gains over either model alone. Our distinction is interpreting the
problem as calibration, which estimates the confidence of predicted answers as
an indicator to decide when to use a document or QA-pair corpus. The
effectiveness of our method was validated on widely adopted benchmarks such as
Natural Questions and TriviaQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL 2023 camera ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From the One, Judge of the Whole: Typed Entailment Graph Construction
  with Predicate Generation <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04170v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04170v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhibin Chen, Yansong Feng, Dongyan Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entailment Graphs (EGs) have been constructed based on extracted corpora as a
strong and explainable form to indicate context-independent entailment
relations in natural languages. However, EGs built by previous methods often
suffer from the severe sparsity issues, due to limited corpora available and
the long-tail phenomenon of predicate distributions. In this paper, we propose
a multi-stage method, Typed Predicate-Entailment Graph Generator (TP-EGG), to
tackle this problem. Given several seed predicates, TP-EGG builds the graphs by
generating new predicates and detecting entailment relations among them. The
generative nature of TP-EGG helps us leverage the recent advances from large
pretrained language models (PLMs), while avoiding the reliance on carefully
prepared corpora. Experiments on benchmark datasets show that TP-EGG can
generate high-quality and scale-controllable entailment graphs, achieving
significant in-domain improvement over state-of-the-art EGs and boosting the
performance of down-stream inference tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures, accepted to ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Increasing Diversity While Maintaining Accuracy: Text Data Generation
  with Large Language Models and Human Interventions <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04140v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04140v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Joon Young Chung, Ece Kamar, Saleema Amershi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can be used to generate text data for training
and evaluating other models. However, creating high-quality datasets with LLMs
can be challenging. In this work, we explore human-AI partnerships to
facilitate high diversity and accuracy in LLM-based text data generation. We
first examine two approaches to diversify text generation: 1) logit
suppression, which minimizes the generation of languages that have already been
frequently generated, and 2) temperature sampling, which flattens the token
sampling probability. We found that diversification approaches can increase
data diversity but often at the cost of data accuracy (i.e., text and labels
being appropriate for the target domain). To address this issue, we examined
two human interventions, 1) label replacement (LR), correcting misaligned
labels, and 2) out-of-scope filtering (OOSF), removing instances that are out
of the user's domain of interest or to which no considered label applies. With
oracle studies, we found that LR increases the absolute accuracy of models
trained with diversified datasets by 14.4%. Moreover, we found that some models
trained with data generated with LR interventions outperformed LLM-based
few-shot classification. In contrast, OOSF was not effective in increasing
model accuracy, implying the need for future work in human-in-the-loop text
data generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a long paper at ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Knowledge-Augmented Language Model <span class="highlight-title">Prompt</span>ing for Zero-Shot Knowledge
  Graph Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04136v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04136v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinheon Baek, Alham Fikri Aji, Amir Saffari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are capable of performing zero-shot closed-book
question answering tasks, based on their internal knowledge stored in
parameters during pre-training. However, such internalized knowledge might be
insufficient and incorrect, which could lead LLMs to generate factually wrong
answers. Furthermore, fine-tuning LLMs to update their knowledge is expensive.
To this end, we propose to augment the knowledge directly in the input of LLMs.
Specifically, we first retrieve the relevant facts to the input question from
the knowledge graph based on semantic similarities between the question and its
associated facts. After that, we prepend the retrieved facts to the input
question in the form of the prompt, which is then forwarded to LLMs to generate
the answer. Our framework, Knowledge-Augmented language model PromptING
(KAPING), requires no model training, thus completely zero-shot. We validate
the performance of our KAPING framework on the knowledge graph question
answering task, that aims to answer the user's question based on facts over a
knowledge graph, on which ours outperforms relevant zero-shot baselines by up
to 48% in average, across multiple LLMs of various sizes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Fusion Interactions: A Study of Human and Automatic
  Quantification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04125v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04125v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Pu Liang, Yun Cheng, Ruslan Salakhutdinov, Louis-Philippe Morency
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal fusion of multiple heterogeneous and interconnected signals is a
fundamental challenge in almost all multimodal problems and applications. In
order to perform multimodal fusion, we need to understand the types of
interactions that modalities can exhibit: how each modality individually
provides information useful for a task and how this information changes in the
presence of other modalities. In this paper, we perform a comparative study of
how human annotators can be leveraged to annotate two categorizations of
multimodal interactions: (1) partial labels, where different randomly assigned
annotators annotate the label given the first, second, and both modalities, and
(2) counterfactual labels, where the same annotator is tasked to annotate the
label given the first modality before giving them the second modality and
asking them to explicitly reason about how their answer changes, before
proposing an alternative taxonomy based on (3) information decomposition, where
annotators annotate the degrees of redundancy: the extent to which modalities
individually and together give the same predictions on the task, uniqueness:
the extent to which one modality enables a task prediction that the other does
not, and synergy: the extent to which only both modalities enable one to make a
prediction about the task that one would not otherwise make using either
modality individually. Through extensive experiments and annotations, we
highlight several opportunities and limitations of each approach and propose a
method to automatically convert annotations of partial and counterfactual
labels to information decomposition, yielding an accurate and efficient method
for quantifying interactions in multimodal datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unbalanced Optimal Transport for Unbalanced Word Alignment <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04116v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04116v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuki Arase, Han Bao, Sho Yokoi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Monolingual word alignment is crucial to model semantic interactions between
sentences. In particular, null alignment, a phenomenon in which words have no
corresponding counterparts, is pervasive and critical in handling semantically
divergent sentences. Identification of null alignment is useful on its own to
reason about the semantic similarity of sentences by indicating there exists
information inequality. To achieve unbalanced word alignment that values both
alignment and null alignment, this study shows that the family of optimal
transport (OT), i.e., balanced, partial, and unbalanced OT, are natural and
powerful approaches even without tailor-made techniques. Our extensive
experiments covering unsupervised and supervised settings indicate that our
generic OT-based alignment methods are competitive against the
state-of-the-arts specially designed for word alignment, remarkably on
challenging datasets with high null alignment frequencies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for the Annual Meeting of the Association for Computational
  Linguistics (ACL 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gotta: Generative Few-shot Question Answering by <span class="highlight-title">Prompt</span>-based Cloze Data
  Augmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04101v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04101v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiusi Chen, Yu Zhang, Jinliang Deng, Jyun-Yu Jiang, Wei Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot question answering (QA) aims at precisely discovering answers to a
set of questions from context passages while only a few training samples are
available. Although existing studies have made some progress and can usually
achieve proper results, they suffer from understanding deep semantics for
reasoning out the questions. In this paper, we develop Gotta, a Generative
prOmpT-based daTa Augmentation framework to mitigate the challenge above.
Inspired by the human reasoning process, we propose to integrate the cloze task
to enhance few-shot QA learning. Following the recent success of prompt-tuning,
we present the cloze task in the same format as the main QA task, allowing the
model to learn both tasks seamlessly together to fully take advantage of the
power of prompt-tuning. Extensive experiments on widely used benchmarks
demonstrate that Gotta consistently outperforms competitive baselines,
validating the effectiveness of our proposed prompt-tuning-based cloze task,
which not only fine-tunes language models but also learns to guide reasoning in
QA tasks. Further analysis shows that the prompt-based loss incorporates the
auxiliary task better than the multi-task loss, highlighting the strength of
prompt-tuning on the few-shot QA task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ XSemPLR: Cross-Lingual Semantic Parsing in Multiple Natural Languages
  and Meaning Representations <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04085v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04085v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusen Zhang, Jun Wang, Zhiguo Wang, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cross-Lingual Semantic Parsing (CLSP) aims to translate queries in multiple
natural languages (NLs) into meaning representations (MRs) such as SQL, lambda
calculus, and logic forms. However, existing CLSP models are separately
proposed and evaluated on datasets of limited tasks and applications, impeding
a comprehensive and unified evaluation of CLSP on a diverse range of NLs and
MRs. To this end, we present XSemPLR, a unified benchmark for cross-lingual
semantic parsing featured with 22 natural languages and 8 meaning
representations by examining and selecting 9 existing datasets to cover 5 tasks
and 164 domains. We use XSemPLR to conduct a comprehensive benchmark study on a
wide range of multilingual language models including encoder-based models
(mBERT, XLM-R), encoder-decoder models (mBART, mT5), and decoder-based models
(Codex, BLOOM). We design 6 experiment settings covering various lingual
combinations (monolingual, multilingual, cross-lingual) and numbers of learning
samples (full dataset, few-shot, and zero-shot). Our experiments show that
encoder-decoder models (mT5) achieve the highest performance compared with
other popular models, and multilingual training can further improve the average
performance. Notably, multilingual large language models (e.g., BLOOM) are
still inadequate to perform CLSP tasks. We also find that the performance gap
between monolingual training and cross-lingual transfer learning is still
significant for multilingual models, though it can be mitigated by
cross-lingual few-shot training. Our dataset and code are available at
https://github.com/psunlpgroup/XSemPLR.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-only Domain Adaptation using Unified Speech-Text Representation in
  Transducer 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Huang, Boyu Li, Jun Zhang, Lu Lu, Zejun Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain adaptation using text-only corpus is challenging in end-to-end(E2E)
speech recognition. Adaptation by synthesizing audio from text through TTS is
resource-consuming. We present a method to learn Unified Speech-Text
Representation in Conformer Transducer(USTR-CT) to enable fast domain
adaptation using the text-only corpus. Different from the previous textogram
method, an extra text encoder is introduced in our work to learn text
representation and is removed during inference, so there is no modification for
online deployment. To improve the efficiency of adaptation, single-step and
multi-step adaptations are also explored. The experiments on adapting
LibriSpeech to SPGISpeech show the proposed method reduces the word error
rate(WER) by relatively 44% on the target domain, which is better than those of
TTS method and textogram method. Also, it is shown the proposed method can be
combined with internal language model estimation(ILME) to further improve the
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Augmentation for Improving Tail-traffic Robustness in Skill-routing
  for Dialogue Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04823v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04823v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ting-Wei Wu, Fatemeh Sheikholeslami, Mohammad Kachuee, Jaeyoung Do, Sungjin Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale conversational systems typically rely on a skill-routing
component to route a user request to an appropriate skill and interpretation to
serve the request. In such system, the agent is responsible for serving
thousands of skills and interpretations which create a long-tail distribution
due to the natural frequency of requests. For example, the samples related to
play music might be a thousand times more frequent than those asking for
theatre show times. Moreover, inputs used for ML-based skill routing are often
a heterogeneous mix of strings, embedding vectors, categorical and scalar
features which makes employing augmentation-based long-tail learning approaches
challenging. To improve the skill-routing robustness, we propose an
augmentation of heterogeneous skill-routing data and training targeted for
robust operation in long-tail data regimes. We explore a variety of conditional
encoder-decoder generative frameworks to perturb original data fields and
create synthetic training data. To demonstrate the effectiveness of the
proposed method, we conduct extensive experiments using real-world data from a
commercial conversational system. Based on the experiment results, the proposed
approach improves more than 80% (51 out of 63) of intents with less than 10K of
traffic instances in the skill-routing replication task.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Good Data, Large Data, or No Data? Comparing Three Approaches in
  Developing Research Aspect Classifiers for Biomedical Papers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04820v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04820v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shreya Chandrasekhar, Chieh-Yang Huang, Ting-Hao 'Kenneth' Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The rapid growth of scientific publications, particularly during the COVID-19
pandemic, emphasizes the need for tools to help researchers efficiently
comprehend the latest advancements. One essential part of understanding
scientific literature is research aspect classification, which categorizes
sentences in abstracts to Background, Purpose, Method, and Finding. In this
study, we investigate the impact of different datasets on model performance for
the crowd-annotated CODA-19 research aspect classification task. Specifically,
we explore the potential benefits of using the large, automatically curated
PubMed 200K RCT dataset and evaluate the effectiveness of large language models
(LLMs), such as LLaMA, GPT-3, ChatGPT, and GPT-4. Our results indicate that
using the PubMed 200K RCT dataset does not improve performance for the CODA-19
task. We also observe that while GPT-4 performs well, it does not outperform
the SciBERT model fine-tuned on the CODA-19 dataset, emphasizing the importance
of a dedicated and task-aligned datasets dataset for the target task. Our code
is available at https://github.com/Crowd-AI-Lab/CODA-19-exp.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>BioNLP workshop 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Privately generating tabular data using language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04803v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04803v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Sablayrolles, Yue Wang, Brian Karrer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Privately generating synthetic data from a table is an important brick of a
privacy-first world. We propose and investigate a simple approach of treating
each row in a table as a sentence and training a language model with
differential privacy. We show this approach obtains competitive results in
modelling tabular data across multiple datasets, even at small scales that
favor alternative methods based on marginal distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> on Knowledge Graphs for Healthcare: Resources, Applications,
  and Promises 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04802v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04802v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hejie Cui, Jiaying Lu, Shiyu Wang, Ran Xu, Wenjing Ma, Shaojun Yu, Yue Yu, Xuan Kan, Chen Ling, Joyce Ho, Fei Wang, Carl Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Healthcare knowledge graphs (HKGs) have emerged as a promising tool for
organizing medical knowledge in a structured and interpretable way, which
provides a comprehensive view of medical concepts and their relationships.
However, challenges such as data heterogeneity and limited coverage remain,
emphasizing the need for further research in the field of HKGs. This survey
paper serves as the first comprehensive overview of HKGs. We summarize the
pipeline and key techniques for HKG construction (i.e., from scratch and
through integration), as well as the common utilization approaches (i.e.,
model-free and model-based). To provide researchers with valuable resources, we
organize existing HKGs (The resource is available at
https://github.com/lujiaying/Awesome-HealthCare-KnowledgeBase) based on the
data types they capture and application domains, supplemented with pertinent
statistical information. In the application section, we delve into the
transformative impact of HKGs across various healthcare domains, spanning from
fine-grained basic science research to high-level clinical decision support.
Lastly, we shed light on the opportunities for creating comprehensive and
accurate HKGs in the era of large language models, presenting the potential to
revolutionize healthcare delivery and enhance the interpretability and
reliability of clinical prediction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Absformer: <span class="highlight-title">Transformer</span>-based Model for Unsupervised Multi-Document
  Abstractive Summarization <span class="chip">ICDAR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04787v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04787v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamed Trabelsi, Huseyin Uzunalioglu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-document summarization (MDS) refers to the task of summarizing the text
in multiple documents into a concise summary. The generated summary can save
the time of reading many documents by providing the important content in the
form of a few sentences. Abstractive MDS aims to generate a coherent and fluent
summary for multiple documents using natural language generation techniques. In
this paper, we consider the unsupervised abstractive MDS setting where there
are only documents with no groundtruh summaries provided, and we propose
Absformer, a new Transformer-based method for unsupervised abstractive summary
generation. Our method consists of a first step where we pretrain a
Transformer-based encoder using the masked language modeling (MLM) objective as
the pretraining task in order to cluster the documents into semantically
similar groups; and a second step where we train a Transformer-based decoder to
generate abstractive summaries for the clusters of documents. To our knowledge,
we are the first to successfully incorporate a Transformer-based model to solve
the unsupervised abstractive MDS task. We evaluate our approach using three
real-world datasets from different domains, and we demonstrate both substantial
improvements in terms of evaluation metrics over state-of-the-art
abstractive-based methods, and generalization to datasets from different
domains.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICDAR 2023 International Workshop on Machine Learning (WML)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The HCI Aspects of Public Deployment of Research Chatbots: A User Study,
  Design Recommendations, and Open Challenges 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04765v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04765v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Morteza Behrooz, William Ngan, Joshua Lane, Giuliano Morse, Benjamin Babcock, Kurt Shuster, Mojtaba Komeili, Moya Chen, Melanie Kambadur, Y-Lan Boureau, Jason Weston
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Publicly deploying research chatbots is a nuanced topic involving necessary
risk-benefit analyses. While there have recently been frequent discussions on
whether it is responsible to deploy such models, there has been far less focus
on the interaction paradigms and design approaches that the resulting
interfaces should adopt, in order to achieve their goals more effectively. We
aim to pose, ground, and attempt to answer HCI questions involved in this
scope, by reporting on a mixed-methods user study conducted on a recent
research chatbot. We find that abstract anthropomorphic representation for the
agent has a significant effect on user's perception, that offering AI
explainability may have an impact on feedback rates, and that two (diegetic and
extradiegetic) levels of the chat experience should be intentionally designed.
We offer design recommendations and areas of further focus for the research
community.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ INSTRUCTEVAL: Towards Holistic Evaluation of Instruction-Tuned Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yew Ken Chia, Pengfei Hong, Lidong Bing, Soujanya Poria
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-tuned large language models have revolutionized natural language
processing and have shown great potential in applications such as
conversational agents. These models, such as GPT-4, can not only master
language but also solve complex tasks in areas like mathematics, coding,
medicine, and law. Despite their impressive capabilities, there is still a lack
of comprehensive understanding regarding their full potential, primarily due to
the black-box nature of many models and the absence of holistic evaluation
studies. To address these challenges, we present INSTRUCTEVAL, a more
comprehensive evaluation suite designed specifically for instruction-tuned
large language models. Unlike previous works, our evaluation involves a
rigorous assessment of models based on problem-solving, writing ability, and
alignment to human values. We take a holistic approach to analyze various
factors affecting model performance, including the pretraining foundation,
instruction-tuning data, and training methods. Our findings reveal that the
quality of instruction data is the most crucial factor in scaling model
performance. While open-source models demonstrate impressive writing abilities,
there is substantial room for improvement in problem-solving and alignment. We
are encouraged by the rapid development of models by the open-source community,
but we also highlight the need for rigorous evaluation to support claims made
about these models. Through INSTRUCTEVAL, we aim to foster a deeper
understanding of instruction-tuned models and advancements in their
capabilities. INSTRUCTEVAL is publicly available at
https://github.com/declare-lab/instruct-eval.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>https://github.com/declare-lab/instruct-eval</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How Far Can Camels Go? Exploring the State of Instruction Tuning on Open
  Resources <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04751v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04751v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhong Wang, Hamish Ivison, Pradeep Dasigi, Jack Hessel, Tushar Khot, Khyathi Raghavi Chandu, David Wadden, Kelsey MacMillan, Noah A. Smith, Iz Beltagy, Hannaneh Hajishirzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work we explore recent advances in instruction-tuning language models
on a range of open instruction-following datasets. Despite recent claims that
open models can be on par with state-of-the-art proprietary models, these
claims are often accompanied by limited evaluation, making it difficult to
compare models across the board and determine the utility of various resources.
We provide a large set of instruction-tuned models from 6.7B to 65B parameters
in size, trained on 12 instruction datasets ranging from manually curated
(e.g., OpenAssistant) to synthetic and distilled (e.g., Alpaca) and
systematically evaluate them on their factual knowledge, reasoning,
multilinguality, coding, and open-ended instruction following abilities through
a collection of automatic, model-based, and human-based metrics. We further
introduce T\"ulu, our best performing instruction-tuned model suite finetuned
on a combination of high-quality open resources.
  Our experiments show that different instruction-tuning datasets can uncover
or enhance specific skills, while no single dataset (or combination) provides
the best performance across all evaluations. Interestingly, we find that model
and human preference-based evaluations fail to reflect differences in model
capabilities exposed by benchmark-based evaluations, suggesting the need for
the type of systemic evaluation performed in this work. Our evaluations show
that the best model in any given evaluation reaches on average 83% of ChatGPT
performance, and 68% of GPT-4 performance, suggesting that further investment
in building better base models and instruction-tuning data is required to close
the gap. We release our instruction-tuned models, including a fully finetuned
65B T\"ulu, along with our code, data, and evaluation framework at
https://github.com/allenai/open-instruct to facilitate future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 5 figure, 7 tables. Under the review of NeurIPS 2023
  Datasets and Benchmarks Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Large Language Model Annotations for Valid Downstream Statistical
  Inference in Social Science: Design-Based Semi-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Egami, Musashi Jacobs-Harukawa, Brandon M. Stewart, Hanying Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In computational social science (CSS), researchers analyze documents to
explain social and political phenomena. In most scenarios, CSS researchers
first obtain labels for documents and then explain labels using interpretable
regression analyses in the second step. The recent advancements in large
language models (LLMs) can lower costs for CSS research by annotating documents
cheaply at scale, but such surrogate labels are often imperfect and biased. We
present a new algorithm for using outputs from LLMs for downstream statistical
analyses while guaranteeing statistical properties -- like asymptotic
unbiasedness and proper uncertainty quantification -- which are fundamental to
CSS research. We show that direct use of LLM-predicted surrogate labels in
downstream statistical analyses leads to substantial bias and invalid
confidence intervals, even with high surrogate accuracy of 80--90\%. To address
this, we build on debiased machine learning to propose the design-based
semi-supervised learning (DSL) estimator. DSL employs a doubly-robust procedure
to combine surrogate labels with a smaller number of gold-standard labels. Our
approach guarantees valid inference for downstream statistical analyses, even
when surrogates are arbitrarily biased, without requiring stringent
assumptions, by controlling the probability of sampling documents for
gold-standard labeling. Both our theoretical analysis and experimental results
show that DSL provides valid statistical inference while achieving root mean
squared errors comparable to existing alternatives that focus only on
prediction without statistical guarantees.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ScienceBenchmark: A Complex Real-World Benchmark for Evaluating Natural
  Language to SQL Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04743v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04743v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Zhang, Jan Deriu, George Katsogiannis-Meimarakis, Catherine Kosten, Georgia Koutrika, Kurt Stockinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural Language to SQL systems (NL-to-SQL) have recently shown a significant
increase in accuracy for natural language to SQL query translation. This
improvement is due to the emergence of transformer-based language models, and
the popularity of the Spider benchmark - the de-facto standard for evaluating
NL-to-SQL systems. The top NL-to-SQL systems reach accuracies of up to 85\%.
However, Spider mainly contains simple databases with few tables, columns, and
entries, which does not reflect a realistic setting. Moreover, complex
real-world databases with domain-specific content have little to no training
data available in the form of NL/SQL-pairs leading to poor performance of
existing NL-to-SQL systems.
  In this paper, we introduce ScienceBenchmark, a new complex NL-to-SQL
benchmark for three real-world, highly domain-specific databases. For this new
benchmark, SQL experts and domain experts created high-quality NL/SQL-pairs for
each domain. To garner more data, we extended the small amount of
human-generated data with synthetic data generated using GPT-3. We show that
our benchmark is highly challenging, as the top performing systems on Spider
achieve a very low performance on our benchmark. Thus, the challenge is
many-fold: creating NL-to-SQL systems for highly complex domains with a small
amount of hand-made training data augmented with synthetic data. To our
knowledge, ScienceBenchmark is the first NL-to-SQL benchmark designed with
complex real-world scientific databases, containing challenging training and
test data carefully validated by domain experts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 2 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soft-<span class="highlight-title">prompt</span> Tuning for Large Language Models to Evaluate Bias 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04735v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04735v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jacob-Junqi Tian, David Emerson, Sevil Zanjani Miyandoab, Deval Pandya, Laleh Seyyed-Kalantari, Faiza Khan Khattak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompting large language models has gained immense popularity in recent years
due to the advantage of producing good results even without the need for
labelled data. However, this requires prompt tuning to get optimal prompts that
lead to better model performances. In this paper, we explore the use of
soft-prompt tuning on sentiment classification task to quantify the biases of
large language models (LLMs) such as Open Pre-trained Transformers (OPT) and
Galactica language model. Since these models are trained on real-world data
that could be prone to bias toward certain groups of populations, it is
important to identify these underlying issues. Using soft-prompts to evaluate
bias gives us the extra advantage of avoiding the human-bias injection that can
be caused by manually designed prompts. We check the model biases on different
sensitive attributes using the group fairness (bias) and find interesting bias
patterns. Since LLMs have been used in the industry in various applications, it
is crucial to identify the biases before deploying these models in practice. We
open-source our pipeline and encourage industry researchers to adapt our work
to their use cases.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>er: Zero-shot Adaptive Prefixes for Dialogue State Tracking Domain
  Adaptation <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04724v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04724v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Taha Aksu, Min-Yen Kan, Nancy F. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A challenge in the Dialogue State Tracking (DST) field is adapting models to
new domains without using any supervised data, zero-shot domain adaptation.
Parameter-Efficient Transfer Learning (PETL) has the potential to address this
problem due to its robustness. However, it has yet to be applied to the
zero-shot scenarios, as it is not clear how to apply it unsupervisedly.
  Our method, Prompter, uses descriptions of target domain slots to generate
dynamic prefixes that are concatenated to the key and values at each layer's
self-attention mechanism. This allows for the use of prefix-tuning in
zero-shot. Prompter outperforms previous methods on both the MultiWOZ and SGD
benchmarks. In generating prefixes, our analyses find that Prompter not only
utilizes the semantics of slot descriptions but also how often the slots appear
together in conversation. Moreover, Prompter's gains are due to its improved
ability to distinguish "none"-valued dialogue slots, compared against
baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Intrinsic Dimension Estimation for Robust Detection of AI-Generated
  Texts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva, Daniil Cherniavskii, Serguei Barannikov, Irina Piontkovskaya, Sergey Nikolenko, Evgeny Burnaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rapidly increasing quality of AI-generated content makes it difficult to
distinguish between human and AI-generated texts, which may lead to undesirable
consequences for society. Therefore, it becomes increasingly important to study
the properties of human texts that are invariant over text domains and various
proficiency of human writers, can be easily calculated for any language, and
can robustly separate natural and AI-generated texts regardless of the
generation model and sampling method. In this work, we propose such an
invariant of human texts, namely the intrinsic dimensionality of the manifold
underlying the set of embeddings of a given text sample. We show that the
average intrinsic dimensionality of fluent texts in natural language is
hovering around the value $9$ for several alphabet-based languages and around
$7$ for Chinese, while the average intrinsic dimensionality of AI-generated
texts for each language is $\approx 1.5$ lower, with a clear statistical
separation between human-generated and AI-generated distributions. This
property allows us to build a score-based artificial text detector. The
proposed detector's accuracy is stable over text domains, generator models, and
human writer proficiency levels, outperforming SOTA detectors in model-agnostic
and cross-domain scenarios by a significant margin.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Open Language Models by Learning from Organic Interactions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Xu, Da Ju, Joshua Lane, Mojtaba Komeili, Eric Michael Smith, Megan Ung, Morteza Behrooz, William Ngan, Rashel Moritz, Sainbayar Sukhbaatar, Y-Lan Boureau, Jason Weston, Kurt Shuster
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present BlenderBot 3x, an update on the conversational model BlenderBot 3,
which is now trained using organic conversation and feedback data from
participating users of the system in order to improve both its skills and
safety. We are publicly releasing the participating de-identified interaction
data for use by the research community, in order to spur further progress.
Training models with organic data is challenging because interactions with
people "in the wild" include both high quality conversations and feedback, as
well as adversarial and toxic behavior. We study techniques that enable
learning from helpful teachers while avoiding learning from people who are
trying to trick the model into unhelpful or toxic responses. BlenderBot 3x is
both preferred in conversation to BlenderBot 3, and is shown to produce safer
responses in challenging situations. While our current models are still far
from perfect, we believe further improvement can be achieved by continued use
of the techniques explored in this work.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ConceptBed: Evaluating Concept Learning Abilities of Text-to-Image
  Diffusion Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04695v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04695v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maitreya Patel, Tejas Gokhale, Chitta Baral, Yezhou Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ability to understand visual concepts and replicate and compose these
concepts from images is a central goal for computer vision. Recent advances in
text-to-image (T2I) models have lead to high definition and realistic image
quality generation by learning from large databases of images and their
descriptions. However, the evaluation of T2I models has focused on photorealism
and limited qualitative measures of visual understanding. To quantify the
ability of T2I models in learning and synthesizing novel visual concepts, we
introduce ConceptBed, a large-scale dataset that consists of 284 unique visual
concepts, 5K unique concept compositions, and 33K composite text prompts. Along
with the dataset, we propose an evaluation metric, Concept Confidence Deviation
(CCD), that uses the confidence of oracle concept classifiers to measure the
alignment between concepts generated by T2I generators and concepts contained
in ground truth images. We evaluate visual concepts that are either objects,
attributes, or styles, and also evaluate four dimensions of compositionality:
counting, attributes, relations, and actions. Our human study shows that CCD is
highly correlated with human understanding of concepts. Our results point to a
trade-off between learning the concepts and preserving the compositionality
which existing approaches struggle to overcome.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://conceptbed.github.io</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>Bench: Towards Evaluating the Robustness of Large Language Models
  on Adversarial <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, Xing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing reliance on Large Language Models (LLMs) across academia and
industry necessitates a comprehensive understanding of their robustness to
prompts. In response to this vital need, we introduce PromptBench, a robustness
benchmark designed to measure LLMs' resilience to adversarial prompts. This
study uses a plethora of adversarial textual attacks targeting prompts across
multiple levels: character, word, sentence, and semantic. These prompts are
then employed in diverse tasks, such as sentiment analysis, natural language
inference, reading comprehension, machine translation, and math
problem-solving. Our study generates 4,032 adversarial prompts, meticulously
evaluated over 8 tasks and 13 datasets, with 567,084 test samples in total. Our
findings demonstrate that contemporary LLMs are vulnerable to adversarial
prompts. Furthermore, we present comprehensive analysis to understand the
mystery behind prompt robustness and its transferability. We then offer
insightful robustness analysis and pragmatic recommendations for prompt
composition, beneficial to both researchers and everyday users. We make our
code, prompts, and methodologies to generate adversarial prompts publicly
accessible, thereby enabling and encouraging collaborative exploration in this
pivotal field: https://github.com/microsoft/promptbench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report; 23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PALR: Personalization Aware LLMs for Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07622v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07622v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fan Yang, Zheng Chen, Ziyan Jiang, Eunah Cho, Xiaojiang Huang, Yanbin Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have recently received significant attention for
their exceptional capabilities. Despite extensive efforts in developing
general-purpose LLMs that can be utilized in various natural language
processing (NLP) tasks, there has been less research exploring their potential
in recommender systems. In this paper, we propose a novel framework, named
PALR, which aiming to combine user history behaviors (such as clicks,
purchases, ratings, etc.) with LLMs to generate user preferred items.
Specifically, we first use user/item interactions as guidance for candidate
retrieval. Then we adopt a LLM-based ranking model to generate recommended
items. Unlike existing approaches that typically adopt general-purpose LLMs for
zero/few-shot recommendation testing or training on small-sized language models
(with less than 1 billion parameters), which cannot fully elicit LLMs'
reasoning abilities and leverage rich item side parametric knowledge, we
fine-tune a 7 billion parameters LLM for the ranking purpose. This model takes
retrieval candidates in natural language format as input, with instruction
which explicitly asking to select results from input candidates during
inference. Our experimental results demonstrate that our solution outperforms
state-of-the-art models on various sequential recommendation tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Models can Solve Computer Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17491v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17491v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geunwoo Kim, Pierre Baldi, Stephen McAleer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents capable of carrying out general tasks on a computer can improve
efficiency and productivity by automating repetitive tasks and assisting in
complex problem-solving. Ideally, such agents should be able to solve new
computer tasks presented to them through natural language commands. However,
previous approaches to this problem require large amounts of expert
demonstrations and task-specific reward functions, both of which are
impractical for new tasks. In this work, we show that a pre-trained large
language model (LLM) agent can execute computer tasks guided by natural
language using a simple prompting scheme where the agent Recursively Criticizes
and Improves its output (RCI). The RCI approach significantly outperforms
existing LLM methods for automating computer tasks and surpasses supervised
learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++
benchmark. We compare multiple LLMs and find that RCI with the
InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful
of demonstrations per task rather than tens of thousands, and without a
task-specific reward function. Furthermore, we demonstrate RCI prompting's
effectiveness in enhancing LLMs' reasoning abilities on a suite of natural
language reasoning tasks, outperforming chain of thought (CoT) prompting. We
find that RCI combined with CoT performs better than either separately. Our
code can be found here: https://github.com/posgnu/rci-agent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) with memory are computationally universal.
However, mainstream LLMs are not taking full advantage of memory, and the
designs are heavily influenced by biological brains. Due to their approximate
nature and proneness to the accumulation of errors, conventional neural memory
mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we
seek inspiration from modern computer architectures to augment LLMs with
symbolic memory for complex multi-hop reasoning. Such a symbolic memory
framework is instantiated as an LLM and a set of SQL databases, where the LLM
generates SQL instructions to manipulate the SQL databases. We validate the
effectiveness of the proposed memory framework on a synthetic dataset requiring
complex reasoning. The project website is available at
https://chatdatabase.github.io/ .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Z-Code++: A <span class="highlight-title">Pre-train</span>ed Language Model Optimized for Abstractive
  Summarization <span class="chip">ACL
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.09770v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.09770v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pengcheng He, Baolin Peng, Liyang Lu, Song Wang, Jie Mei, Yang Liu, Ruochen Xu, Hany Hassan Awadalla, Yu Shi, Chenguang Zhu, Wayne Xiong, Michael Zeng, Jianfeng Gao, Xuedong Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents Z-Code++, a new pre-trained language model optimized for
abstractive text summarization. The model extends the state of the art
encoder-decoder model using three techniques. First, we use a two-phase
pre-training process to improve model's performance on low-resource
summarization tasks. The model is first pre-trained using text corpora for
language understanding, and then is continually pre-trained on summarization
corpora for grounded text generation. Second, we replace self-attention layers
in the encoder with disentangled attention layers, where each word is
represented using two vectors that encode its content and position,
respectively. Third, we use fusion-in-encoder, a simple yet effective method of
encoding long sequences in a hierarchical manner. Z-Code++ creates new state of
the art on 9 out of 13 text summarization tasks across 5 languages. Our model
is parameter-efficient in that it outperforms the 600x larger PaLM-540B on
XSum, and the finetuned 200x larger GPT3-175B on SAMSum. In zero-shot and
few-shot settings, our model substantially outperforms the competing models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 3 figures. Accepted as long paper in main conference of ACL
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Easily Accessible Text-to-Image Generation Amplifies Demographic
  Stereotypes at Large Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.03759v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.03759v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Bianchi, Pratyusha Kalluri, Esin Durmus, Faisal Ladhak, Myra Cheng, Debora Nozza, Tatsunori Hashimoto, Dan Jurafsky, James Zou, Aylin Caliskan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning models that convert user-written text descriptions into
images are now widely available online and used by millions of users to
generate millions of images a day. We investigate the potential for these
models to amplify dangerous and complex stereotypes. We find a broad range of
ordinary prompts produce stereotypes, including prompts simply mentioning
traits, descriptors, occupations, or objects. For example, we find cases of
prompting for basic traits or social roles resulting in images reinforcing
whiteness as ideal, prompting for occupations resulting in amplification of
racial and gender disparities, and prompting for objects resulting in
reification of American norms. Stereotypes are present regardless of whether
prompts explicitly mention identity and demographic language or avoid such
language. Moreover, stereotypes persist despite mitigation strategies; neither
user attempts to counter stereotypes by requesting images with specific
counter-stereotypes nor institutional attempts to add system ``guardrails''
have prevented the perpetuation of stereotypes. Our analysis justifies concerns
regarding the impacts of today's models, presenting striking exemplars, and
connecting these findings with deep insights into harms drawn from social
scientific and humanist disciplines. This work contributes to the effort to
shed light on the uniquely complex biases in language-vision models and
demonstrates the ways that the mass deployment of text-to-image generation
models results in mass dissemination of stereotypes and resulting harms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>FAccT 2023 paper. The published version is available at
  10.1145/3593013.3594095</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SpokenWOZ: A Large-Scale Speech-Text <span class="highlight-title">Dataset</span> for Spoken Task-Oriented
  Dialogue in Multiple Domains 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13040v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13040v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuzheng Si, Wentao Ma, Haoyu Gao, Yuchuan Wu, Ting-En Lin, Yinpei Dai, Hangyu Li, Rui Yan, Fei Huang, Yongbin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented dialogue (TOD) models have made significant progress in recent
years. However, previous studies primarily focus on datasets written by
annotators, which has resulted in a gap between academic research and
real-world spoken conversation scenarios. While several small-scale spoken TOD
datasets are proposed to address robustness issues such as ASR errors, they
ignore the unique challenges in spoken conversation. To tackle the limitations,
we introduce SpokenWOZ, a large-scale speech-text dataset for spoken TOD,
containing 8 domains, 203k turns, 5.7k dialogues and 249 hours of audios from
human-to-human spoken conversations. SpokenWOZ further incorporates common
spoken characteristics such as word-by-word processing and reasoning in spoken
language. Based on these characteristics, we present cross-turn slot and
reasoning slot detection as new challenges. We conduct experiments on various
baselines, including text-modal models, newly proposed dual-modal models, and
LLMs, e.g., ChatGPT. The results show that the current models still have
substantial room for improvement in spoken conversation, where the most
advanced dialogue state tracker only achieves 25.65% in joint goal accuracy and
the SOTA end-to-end model only correctly completes the user request in 52.1% of
dialogues. The dataset, code, and leaderboard are available:
https://spokenwoz.github.io/SpokenWOZ-github.io/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extrapolative Controlled Sequence Generation via Iterative Refinement <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04562v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04562v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishakh Padmakumar, Richard Yuanzhe Pang, He He, Ankur P. Parikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of extrapolative controlled generation, i.e., generating
sequences with attribute values beyond the range seen in training. This task is
of significant importance in automated design, especially drug discovery, where
the goal is to design novel proteins that are \textit{better} (e.g., more
stable) than existing sequences. Thus, by definition, the target sequences and
their attribute values are out of the training distribution, posing challenges
to existing methods that aim to directly generate the target sequence. Instead,
in this work, we propose Iterative Controlled Extrapolation (ICE) which
iteratively makes local edits to a sequence to enable extrapolation. We train
the model on synthetically generated sequence pairs that demonstrate small
improvement in the attribute value. Results on one natural language task
(sentiment analysis) and two protein engineering tasks (ACE2 stability and AAV
fitness) show that ICE considerably outperforms state-of-the-art approaches
despite its simplicity. Our code and models are available at:
https://github.com/vishakhpk/iter-extrapolation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023 - Camera Ready Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Handling the Alignment for Wake Word Detection: A Comparison Between
  Alignment-Based, Alignment-Free and Hybrid Approaches 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08950v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08950v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vinicius Ribeiro, Yiteng Huang, Yuan Shangguan, Zhaojun Yang, Li Wan, Ming Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wake word detection exists in most intelligent homes and portable devices. It
offers these devices the ability to "wake up" when summoned at a low cost of
power and computing. This paper focuses on understanding alignment's role in
developing a wake-word system that answers a generic phrase. We discuss three
approaches. The first is alignment-based, where the model is trained with
frame-wise cross-entropy. The second is alignment-free, where the model is
trained with CTC. The third, proposed by us, is a hybrid solution in which the
model is trained with a small set of aligned data and then tuned with a
sizeable unaligned dataset. We compare the three approaches and evaluate the
impact of the different aligned-to-unaligned ratios for hybrid training. Our
results show that the alignment-free system performs better than the
alignment-based for the target operating point, and with a small fraction of
the data (20%), we can train a model that complies with our initial
constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Party Chat: Conversational Agents in Group Settings with Humans
  and Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.13835v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.13835v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jimmy Wei, Kurt Shuster, Arthur Szlam, Jason Weston, Jack Urbanek, Mojtaba Komeili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current dialogue research primarily studies pairwise (two-party)
conversations, and does not address the everyday setting where more than two
speakers converse together. In this work, we both collect and evaluate
multi-party conversations to study this more general case. We use the LIGHT
environment to construct grounded conversations, where each participant has an
assigned character to role-play. We thus evaluate the ability of language
models to act as one or more characters in such conversations. Models require
two skills that pairwise-trained models appear to lack: (1) being able to
decide when to talk; (2) producing coherent utterances grounded on multiple
characters. We compare models trained on our new dataset to existing
pairwise-trained dialogue models, as well as large language models with
few-shot prompting. We find that our new dataset, MultiLIGHT, which we will
publicly release, can help bring significant improvements in the group setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chat<span class="highlight-title">GPT</span> an ENFJ, Bard an ISTJ: Empirical Study on Personalities of Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19926v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19926v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jen-tse Huang, Wenxuan Wang, Man Ho Lam, Eric John Li, Wenxiang Jiao, Michael R. Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have made remarkable advancements in the field
of artificial intelligence, significantly reshaping the human-computer
interaction. We not only focus on the performance of LLMs, but also explore
their features from a psychological perspective, acknowledging the importance
of understanding their behavioral characteristics. Our study examines the
behavioral patterns displayed by LLMs by employing trait theory, a
psychological framework. We first focus on evaluating the consistency of
personality types exhibited by ChatGPT. Furthermore, experiments include
cross-lingual effects on seven additional languages, and the investigation of
six other LLMs. Moreover, the study investigates whether ChatGPT can exhibit
personality changes in response to instructions or contextual cues. The
findings show that ChatGPT consistently maintains its ENFJ personality
regardless of instructions or contexts. By shedding light on the
personalization of LLMs, we anticipate that our study will serve as a catalyst
for further research in this field.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added robustness analysis against fine-tuning (results of
  text-davinci-003); Added results of ChatGLM; Added limitations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Uncovering and Categorizing Social Biases in Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16253v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16253v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yan Liu, Yan Gao, Zhe Su, Xiaokang Chen, Elliott Ash, Jian-Guang Lou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content Warning: This work contains examples that potentially implicate
stereotypes, associations, and other harms that could be offensive to
individuals in certain social groups.} Large pre-trained language models are
acknowledged to carry social biases towards different demographics, which can
further amplify existing stereotypes in our society and cause even more harm.
Text-to-SQL is an important task, models of which are mainly adopted by
administrative industries, where unfair decisions may lead to catastrophic
consequences. However, existing Text-to-SQL models are trained on clean,
neutral datasets, such as Spider and WikiSQL. This, to some extent, cover up
social bias in models under ideal conditions, which nevertheless may emerge in
real application scenarios. In this work, we aim to uncover and categorize
social biases in Text-to-SQL models. We summarize the categories of social
biases that may occur in structured data for Text-to-SQL models. We build test
benchmarks and reveal that models with similar task accuracy can contain social
biases at very different rates. We show how to take advantage of our
methodology to uncover and assess social biases in the downstream Text-to-SQL
task. We will release our code and data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal interventions expose implicit situation models for commonsense
  language understanding <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03882v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03882v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takateru Yamakoshi, James L. McClelland, Adele E. Goldberg, Robert D. Hawkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accounts of human language processing have long appealed to implicit
``situation models'' that enrich comprehension with relevant but unstated world
knowledge. Here, we apply causal intervention techniques to recent transformer
models to analyze performance on the Winograd Schema Challenge (WSC), where a
single context cue shifts interpretation of an ambiguous pronoun. We identify a
relatively small circuit of attention heads that are responsible for
propagating information from the context word that guides which of the
candidate noun phrases the pronoun ultimately attends to. We then compare how
this circuit behaves in a closely matched ``syntactic'' control where the
situation model is not strictly necessary. These analyses suggest distinct
pathways through which implicit situation models are constructed to guide
pronoun resolution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring Anisotropy and Outliers in Multilingual Language Models for
  Cross-Lingual Semantic Sentence Similarity <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00458v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00458v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Katharina Hämmerl, Alina Fastowski, Jindřich Libovický, Alexander Fraser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous work has shown that the representations output by contextual
language models are more anisotropic than static type embeddings, and typically
display outlier dimensions. This seems to be true for both monolingual and
multilingual models, although much less work has been done on the multilingual
context. Why these outliers occur and how they affect the representations is
still an active area of research. We investigate outlier dimensions and their
relationship to anisotropy in multiple pre-trained multilingual language
models. We focus on cross-lingual semantic similarity tasks, as these are
natural tasks for evaluating multilingual representations. Specifically, we
examine sentence representations. Sentence transformers which are fine-tuned on
parallel resources (that are not always available) perform better on this task,
and we show that their representations are more isotropic. However, we aim to
improve multilingual representations in general. We investigate how much of the
performance difference can be made up by only transforming the embedding space
without fine-tuning, and visualise the resulting spaces. We test different
operations: Removing individual outlier dimensions, cluster-based isotropy
enhancement, and ZCA whitening. We publish our code for reproducibility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ACL Findings 2023. Fixed a citation in this version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Context-Sensitive Word Embedding Approach for The Detection of Troll
  Tweets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.08230v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.08230v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyhmus Yilmaz, Sultan Zavrak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we aimed to address the growing concern of trolling behavior
on social media by developing and evaluating a set of model architectures for
the automatic detection of troll tweets. Utilizing deep learning techniques and
pre-trained word embedding methods such as BERT, ELMo, and GloVe, we evaluated
the performance of each architecture using metrics such as classification
accuracy, F1 score, AUC, and precision. Our results indicate that BERT and ELMo
embedding methods performed better than the GloVe method, likely due to their
ability to provide contextualized word embeddings that better capture the
nuances and subtleties of language use in online social media. Additionally, we
found that CNN and GRU encoders performed similarly in terms of F1 score and
AUC, suggesting their effectiveness in extracting relevant information from
input text. The best-performing method was found to be an ELMo-based
architecture that employed a GRU classifier, with an AUC score of 0.929. This
research highlights the importance of utilizing contextualized word embeddings
and appropriate encoder methods in the task of troll tweet detection, which can
assist social-based systems in improving their performance in identifying and
addressing trolling behavior on their platforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Cancer Hallmark Classification with <span class="highlight-title">BERT</span>-based Deep Learning
  Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03501v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03501v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sultan Zavrak, Seyhmus Yilmaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a novel approach to accurately classify the hallmarks of
cancer, which is a crucial task in cancer research. Our proposed method
utilizes the Bidirectional Encoder Representations from Transformers (BERT)
architecture, which has shown exceptional performance in various downstream
applications. By applying transfer learning, we fine-tuned the pre-trained BERT
model on a small corpus of biomedical text documents related to cancer. The
outcomes of our experimental investigations demonstrate that our approach
attains a noteworthy accuracy of 94.45%, surpassing almost all prior findings
with a substantial increase of at least 8.04% as reported in the literature.
These findings highlight the effectiveness of our proposed model in accurately
classifying and comprehending text documents for cancer research, thus
contributing significantly to the field. As cancer remains one of the top ten
leading causes of death globally, our approach holds great promise in advancing
cancer research and improving patient outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Howk<span class="highlight-title">GPT</span>: Investigating the Detection of Chat<span class="highlight-title">GPT</span>-generated University
  Student Homework through Context-Aware Perplexity Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18226v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18226v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoforos Vasilatos, Manaar Alam, Talal Rahwan, Yasir Zaki, Michail Maniatakos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  As the use of Large Language Models (LLMs) in text generation tasks
proliferates, concerns arise over their potential to compromise academic
integrity. The education sector currently tussles with distinguishing
student-authored homework assignments from AI-generated ones. This paper
addresses the challenge by introducing HowkGPT, designed to identify homework
assignments generated by AI. HowkGPT is built upon a dataset of academic
assignments and accompanying metadata [17] and employs a pretrained LLM to
compute perplexity scores for student-authored and ChatGPT-generated responses.
These scores then assist in establishing a threshold for discerning the origin
of a submitted assignment. Given the specificity and contextual nature of
academic work, HowkGPT further refines its analysis by defining
category-specific thresholds derived from the metadata, enhancing the precision
of the detection. This study emphasizes the critical need for effective
strategies to uphold academic integrity amidst the growing influence of LLMs
and provides an approach to ensuring fair and accurate grading in educational
institutions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using Bottleneck Adapters to Identify Cancer in Clinical Notes under
  Low-Resource Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.09440v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.09440v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omid Rohanian, Hannah Jauncey, Mohammadmahdi Nouriborji, Vinod Kumar Chauhan, Bronner P. Gonçalves, Christiana Kartsonaki, ISARIC Clinical Characterisation Group, Laura Merson, David Clifton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Processing information locked within clinical health records is a challenging
task that remains an active area of research in biomedical NLP. In this work,
we evaluate a broad set of machine learning techniques ranging from simple RNNs
to specialised transformers such as BioBERT on a dataset containing clinical
notes along with a set of annotations indicating whether a sample is
cancer-related or not.
  Furthermore, we specifically employ efficient fine-tuning methods from NLP,
namely, bottleneck adapters and prompt tuning, to adapt the models to our
specialised task. Our evaluations suggest that fine-tuning a frozen BERT model
pre-trained on natural language and with bottleneck adapters outperforms all
other strategies, including full fine-tuning of the specialised BioBERT model.
Based on our findings, we suggest that using bottleneck adapters in
low-resource situations with limited access to labelled data or processing
capacity could be a viable strategy in biomedical text mining. The code used in
the experiments are going to be made available at
https://github.com/omidrohanian/bottleneck-adapters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Saliency Map Verbalization: Comparing Feature Importance Representations
  from Model-free and Instruction-based Methods <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07222v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07222v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nils Feldhus, Leonhard Hennig, Maximilian Dustin Nasert, Christopher Ebert, Robert Schwarzenberg, Sebastian Möller
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Saliency maps can explain a neural model's predictions by identifying
important input features. They are difficult to interpret for laypeople,
especially for instances with many features. In order to make them more
accessible, we formalize the underexplored task of translating saliency maps
into natural language and compare methods that address two key challenges of
this approach -- what and how to verbalize. In both automatic and human
evaluation setups, using token-level attributions from text classification
tasks, we compare two novel methods (search-based and instruction-based
verbalizations) against conventional feature importance representations
(heatmap visualizations and extractive rationales), measuring simulatability,
faithfulness, helpfulness and ease of understanding. Instructing GPT-3.5 to
generate saliency map verbalizations yields plausible explanations which
include associations, abstractive summarization and commonsense reasoning,
achieving by far the highest human ratings, but they are not faithfully
capturing numeric information and are inconsistent in their interpretation of
the task. In comparison, our search-based, model-free verbalization approach
efficiently completes templated verbalizations, is faithful by design, but
falls short in helpfulness and simulatability. Our results suggest that
saliency map verbalization makes feature attribution explanations more
comprehensible and less cognitively challenging to humans than conventional
representations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 Workshop on Natural Language Reasoning and Structured
  Explanations (NLRSE)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing Linguistic Generalisation in Language Models: A <span class="highlight-title">Dataset</span> for
  Brazilian Portuguese 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.14070v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.14070v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rodrigo Wilkens, Leonardo Zilio, Aline Villavicencio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Much recent effort has been devoted to creating large-scale language models.
Nowadays, the most prominent approaches are based on deep neural networks, such
as BERT. However, they lack transparency and interpretability, and are often
seen as black boxes. This affects not only their applicability in downstream
tasks but also the comparability of different architectures or even of the same
model trained using different corpora or hyperparameters. In this paper, we
propose a set of intrinsic evaluation tasks that inspect the linguistic
information encoded in models developed for Brazilian Portuguese. These tasks
are designed to evaluate how different language models generalise information
related to grammatical structures and multiword expressions (MWEs), thus
allowing for an assessment of whether the model has learned different
linguistic phenomena. The dataset that was developed for these tasks is
composed of a series of sentences with a single masked word and a cue phrase
that helps in narrowing down the context. This dataset is divided into MWEs and
grammatical structures, and the latter is subdivided into 6 tasks: impersonal
verbs, subject agreement, verb agreement, nominal agreement, passive and
connectors. The subset for MWEs was used to test BERTimbau Large, BERTimbau
Base and mBERT. For the grammatical structures, we used only BERTimbau Large,
because it yielded the best results in the MWE task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This is the original manuscript that was submitted to LREV. The final
  version was published recently and can be found at: https://rdcu.be/ddEa6.
  Language Resources and Evaluation, https://doi.org/10.1007/s10579-023-09664-1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tractable Control for Autoregressive Language Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07438v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07438v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Honghua Zhang, Meihua Dang, Nanyun Peng, Guy Van den Broeck
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the success of autoregressive large language models in text
generation, it remains a major challenge to generate text that satisfies
complex constraints: sampling from the conditional distribution
${\Pr}(\text{text} | \alpha)$ is intractable for even the simplest lexical
constraints $\alpha$. To overcome this challenge, we propose to use tractable
probabilistic models (TPMs) to impose lexical constraints in autoregressive
text generation models, which we refer to as GeLaTo (Generating Language with
Tractable Constraints). To demonstrate the effectiveness of this framework, we
use distilled hidden Markov models, where we can efficiently compute
${\Pr}(\text{text} | \alpha)$, to guide autoregressive generation from GPT2.
GeLaTo achieves state-of-the-art performance on challenging benchmarks for
constrained text generation (e.g., CommonGen), beating various strong baselines
by a large margin. Our work not only opens up new avenues for controlling large
language models but also motivates the development of more expressive TPMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SQL-PaLM: Improved Large Language Model Adaptation for Text-to-SQL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00739v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00739v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruoxi Sun, Sercan O. Arik, Hootan Nakhost, Hanjun Dai, Rajarishi Sinha, Pengcheng Yin, Tomas Pfister
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One impressive emergent capability of large language models (LLMs) is
generation of code, including Structured Query Language (SQL) for databases.
For the task of converting natural language text to SQL queries, Text-to-SQL,
adaptation of LLMs is of paramount importance, both in in-context learning and
fine-tuning settings, depending on the amount of adaptation data used. In this
paper, we propose an LLM-based Text-to-SQL model SQL-PaLM, leveraging on
PaLM-2, that pushes the state-of-the-art in both settings. Few-shot SQL-PaLM is
based on an execution-based self-consistency prompting approach designed for
Text-to-SQL, and achieves 77.3% in test-suite accuracy on Spider, which to our
best knowledge is the first to outperform previous state-of-the-art with
fine-tuning by a significant margin, 4%. Furthermore, we demonstrate that the
fine-tuned SQL-PALM outperforms it further by another 1%. Towards applying
SQL-PaLM to real-world scenarios we further evaluate its robustness on other
challenging variants of Spider and demonstrate the superior generalization
capability of SQL-PaLM. In addition, via extensive case studies, we demonstrate
the impressive intelligent capabilities and various success enablers of
LLM-based Text-to-SQL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Test-Time Training on Nearest Neighbors for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18466v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18466v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Hardt, Yu Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent efforts aim to augment language models with relevant information
retrieved from a database at test time. We avoid the need for prompt
engineering by directly fine-tuning the model on data retrieved at test time
using its standard training setup. For this purpose, we build a large-scale
distributed nearest neighbor index based on text embeddings of the Pile
dataset. Given a query to a language model, our system retrieves the neighbors
of the query and fine-tunes the model on the text data corresponding to those
neighbors. Surprisingly, retrieving and training on as few as 20 neighbors,
each for only one gradient iteration, drastically improves performance across
more than twenty language modeling tasks in the Pile benchmark. For example,
test-time training significantly narrows the performance gap between a small
GPT2 model and a GPTNeo model, more than ten times larger, that was
specifically trained to convergence on the Pile. Sufficient index quality and
size, however, are important. Our work establishes a valuable first baseline
for implementing test-time training in the context of large language models,
opening the door to numerous promising research avenues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Corrected Figure 8. Code repository here:
  https://github.com/socialfoundations/tttlm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Natural Language Processing for Long Texts: A <span class="highlight-title">Survey</span> of the
  State-of-the-Art 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16259v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16259v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dimitrios Tsirmpas, Ioannis Gkionis, Ioannis Mademlis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The adoption of Deep Neural Networks (DNNs) has greatly benefited Natural
Language Processing (NLP) during the past decade. However, the demands of long
document analysis are quite different from those of shorter texts, while the
ever increasing size of documents uploaded on-line renders automated
understanding of long texts a critical area of research. This article has two
goals: a) it overviews the relevant neural building blocks, thus serving as a
short tutorial, and b) it surveys the state-of-the-art in long document NLP,
mainly focusing on two central tasks: document classification and document
summarization. Sentiment analysis for long texts is also covered, since it is
typically treated as a particular case of document classification. Thus, this
article concerns document-level analysis. It discusses the main challenges and
issues of long document NLP, along with the current solutions. Finally, the
relevant, publicly available, annotated datasets are presented, in order to
facilitate further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>51 pages, 2 figures, 168 citations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CADGE: Context-Aware Dialogue Generation Enhanced with Graph-Structured
  Knowledge Aggregation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06294v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06294v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongbo Zhang, Chen Tang, Tyler Loakman, Chenghua Lin, Stefan Goetze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Commonsense knowledge is crucial to many natural language processing tasks.
Existing works usually incorporate graph knowledge with conventional graph
neural networks (GNNs), leading to the text and graph knowledge encoding
processes being separated in a serial pipeline. We argue that these separate
representation learning stages may be suboptimal for neural networks to learn
the overall context contained in both types of input knowledge. In this paper,
we propose a novel context-aware graph-attention model (Context-aware GAT),
which can effectively incorporate global features of relevant knowledge graphs
based on a context-enhanced knowledge aggregation process. Specifically, our
framework leverages a novel representation learning approach to process
heterogeneous features - combining flattened graph knowledge with text. To the
best of our knowledge, this is the first attempt at hierarchically applying
graph knowledge aggregation on a connected subgraph in addition to contextual
information to support commonsense dialogue generation. This framework shows
superior performance compared to conventional GNN-based language frameworks.
Both automatic and human evaluation demonstrates that our proposed model has
significant performance uplifts over state-of-the-art baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to KBS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ TwistList: Resources and Baselines for Tongue Twister Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03457v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03457v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Loakman, Chen Tang, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous work in phonetically-grounded language generation has mainly focused
on domains such as lyrics and poetry. In this paper, we present work on the
generation of tongue twisters - a form of language that is required to be
phonetically conditioned to maximise sound overlap, whilst maintaining semantic
consistency with an input topic, and still being grammatically correct. We
present \textbf{TwistList}, a large annotated dataset of tongue twisters,
consisting of 2.1K+ human-authored examples. We additionally present several
benchmark systems (referred to as TwisterMisters) for the proposed task of
tongue twister generation, including models that both do and do not require
training on in-domain data. We present the results of automatic and human
evaluation to demonstrate the performance of existing mainstream pre-trained
models in this task with limited (or no) task specific training and data, and
no explicit phonetic knowledge. We find that the task of tongue twister
generation is challenging for models under these conditions, yet some models
are still capable of generating acceptable examples of this language type.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Early Discovery of Emerging Entities in Persian Twitter with Semantic
  Similarity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.02434v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.02434v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shahin Yousefi, Mohsen Hooshmand, Mohsen Afsharchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering emerging entities (EEs) is the problem of finding entities before
their establishment. These entities can be critical for individuals, companies,
and governments. Many of these entities can be discovered on social media
platforms, e.g. Twitter. These identities have been the spot of research in
academia and industry in recent years. Similar to any machine learning problem,
data availability is one of the major challenges in this problem. This paper
proposes EEPT. That is an online clustering method able to discover EEs without
any need for training on a dataset. Additionally, due to the lack of a proper
evaluation metric, this paper uses a new metric to evaluate the results. The
results show that EEPT is promising and finds significant entities before their
establishment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global Contrastive Batch Sampling via Optimization on Sample
  Permutations <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.12874v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.12874v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vin Sachidananda, Ziyi Yang, Chenguang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive Learning has recently achieved state-of-the-art performance in a
wide range of tasks. Many contrastive learning approaches use mined hard
negatives to make batches more informative during training but these approaches
are inefficient as they increase epoch length proportional to the number of
mined negatives and require frequent updates of nearest neighbor indices or
mining from recent batches. In this work, we provide an alternative to hard
negative mining, Global Contrastive Batch Sampling (GCBS), an efficient
approximation to the batch assignment problem that upper bounds the gap between
the global and training losses, $\mathcal{L}^{Global} - \mathcal{L}^{Train}$,
in contrastive learning settings. Through experimentation we find GCBS improves
state-of-the-art performance in sentence embedding and code-search tasks.
Additionally, GCBS is easy to implement as it requires only a few additional
lines of code, does not maintain external data structures such as nearest
neighbor indices, is more computationally efficient than the most minimal hard
negative mining approaches, and makes no changes to the model being trained.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023; 21 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02349v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02349v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Momchil Hardalov, Pepa Atanasova, Todor Mihaylov, Galia Angelova, Kiril Simov, Petya Osenova, Ves Stoyanov, Ivan Koychev, Preslav Nakov, Dragomir Radev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present bgGLUE(Bulgarian General Language Understanding Evaluation), a
benchmark for evaluating language models on Natural Language Understanding
(NLU) tasks in Bulgarian. Our benchmark includes NLU tasks targeting a variety
of NLP problems (e.g., natural language inference, fact-checking, named entity
recognition, sentiment analysis, question answering, etc.) and machine learning
tasks (sequence labeling, document-level classification, and regression). We
run the first systematic evaluation of pre-trained language models for
Bulgarian, comparing and contrasting results across the nine tasks in the
benchmark. The evaluation results show strong performance on sequence labeling
tasks, but there is a lot of room for improvement for tasks that require more
complex reasoning. We make bgGLUE publicly available together with the
fine-tuning and the evaluation code, as well as a public leaderboard at
https://bgglue.github.io/, and we hope that it will enable further advancements
in developing NLU models for Bulgarian.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2023 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Multi-Step Reasoning by Solving Arithmetic Tasks <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01707v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01707v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianduo Wang, Wei Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical reasoning is regarded as a necessary ability for Language Models
(LMs). Recent works demonstrate large LMs' impressive performance in solving
math problems. The success is attributed to their Chain-of-Thought (CoT)
reasoning abilities, i.e., the ability to decompose complex questions into
step-by-step reasoning chains, but such ability seems only to emerge from
models with abundant parameters. This work investigates how to incorporate
relatively small LMs with the capabilities of multi-step reasoning. We propose
to inject such abilities by continually pre-training LMs on a synthetic dataset
MsAT which is composed of Multi-step Arithmetic Tasks. Our experiments on four
math word problem datasets show the effectiveness of the proposed method in
enhancing LMs' math reasoning abilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023. Code and data are available at
  https://github.com/TianduoWang/MsAT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DataFinder: Scientific <span class="highlight-title">Dataset</span> Recommendation from Natural Language
  Descriptions <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16636v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16636v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vijay Viswanathan, Luyu Gao, Tongshuang Wu, Pengfei Liu, Graham Neubig
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern machine learning relies on datasets to develop and validate research
ideas. Given the growth of publicly available data, finding the right dataset
to use is increasingly difficult. Any research question imposes explicit and
implicit constraints on how well a given dataset will enable researchers to
answer this question, such as dataset size, modality, and domain. We
operationalize the task of recommending datasets given a short natural language
description of a research idea, to help people find relevant datasets for their
needs. Dataset recommendation poses unique challenges as an information
retrieval problem; datasets are hard to directly index for search and there are
no corpora readily available for this task. To facilitate this task, we build
the DataFinder Dataset which consists of a larger automatically-constructed
training set (17.5K queries) and a smaller expert-annotated evaluation set (392
queries). Using this data, we compare various information retrieval algorithms
on our test set and present a superior bi-encoder retriever for text-based
dataset recommendation. This system, trained on the DataFinder Dataset, finds
more relevant search results than existing third-party dataset search engines.
To encourage progress on dataset recommendation, we release our dataset and
models to the public.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ACL 2023. Code published at
  https://github.com/viswavi/datafinder</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MACSum: Controllable Summarization with Mixed Attributes <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05041v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05041v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusen Zhang, Yang Liu, Ziyi Yang, Yuwei Fang, Yulong Chen, Dragomir Radev, Chenguang Zhu, Michael Zeng, Rui Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Controllable summarization allows users to generate customized summaries with
specified attributes. However, due to the lack of designated annotations of
controlled summaries, existing works have to craft pseudo datasets by adapting
generic summarization benchmarks. Furthermore, most research focuses on
controlling single attributes individually (e.g., a short summary or a highly
abstractive summary) rather than controlling a mix of attributes together
(e.g., a short and highly abstractive summary). In this paper, we propose
MACSum, the first human-annotated summarization dataset for controlling mixed
attributes. It contains source texts from two domains, news articles and
dialogues, with human-annotated summaries controlled by five designed
attributes (Length, Extractiveness, Specificity, Topic, and Speaker). We
propose two simple and effective parameter-efficient approaches for the new
task of mixed controllable summarization based on hard prompt tuning and soft
prefix tuning. Results and analysis demonstrate that hard prompt models yield
the best performance on all metrics and human evaluations. However,
mixed-attribute control is still challenging for summarization tasks. Our
dataset and code are available at https://github.com/psunlpgroup/MACSum.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chat<span class="highlight-title">GPT</span> Informed Graph Neural Network for Stock Movement Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03763v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03763v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zihan Chen, Lei Nico Zheng, Cheng Lu, Jialu Yuan, Di Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT has demonstrated remarkable capabilities across various natural
language processing (NLP) tasks. However, its potential for inferring dynamic
network structures from temporal textual data, specifically financial news,
remains an unexplored frontier. In this research, we introduce a novel
framework that leverages ChatGPT's graph inference capabilities to enhance
Graph Neural Networks (GNN). Our framework adeptly extracts evolving network
structures from textual data, and incorporates these networks into graph neural
networks for subsequent predictive tasks. The experimental results from stock
movement forecasting indicate our model has consistently outperformed the
state-of-the-art Deep Learning-based benchmarks. Furthermore, the portfolios
constructed based on our model's outputs demonstrate higher annualized
cumulative returns, alongside reduced volatility and maximum drawdown. This
superior performance highlights the potential of ChatGPT for text-based network
inferences and underscores its promising implications for the financial sector.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under Review. 10 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deductive Verification of Chain-of-Thought Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03872v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03872v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) significantly benefit from Chain-of-Thought
(CoT) prompting in performing various reasoning tasks. While CoT allows models
to produce more comprehensive reasoning processes, its emphasis on intermediate
reasoning steps can inadvertently introduce hallucinations and accumulated
errors, thereby limiting models' ability to solve complex reasoning tasks.
Inspired by how humans engage in careful and meticulous deductive logical
reasoning processes to solve tasks, we seek to enable language models to
perform explicit and rigorous deductive reasoning, and also ensure the
trustworthiness of their reasoning process through self-verification. However,
directly verifying the validity of an entire deductive reasoning process is
challenging, even with advanced models like ChatGPT. In light of this, we
propose to decompose a reasoning verification process into a series of
step-by-step subprocesses, each only receiving their necessary context and
premises. To facilitate this procedure, we propose Natural Program, a natural
language-based deductive reasoning format. Our approach enables models to
generate precise reasoning steps where subsequent steps are more rigorously
grounded on prior steps. It also empowers language models to carry out
reasoning self-verification in a step-by-step manner. By integrating this
verification process into each deductive reasoning stage, we significantly
enhance the rigor and trustfulness of generated reasoning steps. Along this
process, we also improve the answer correctness on complex reasoning tasks.
Code will be released at https://github.com/lz1oceani/verify_cot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Inference-Time Intervention: Eliciting Truthful Answers from a Language
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Inference-Time Intervention (ITI), a technique designed to
enhance the truthfulness of large language models (LLMs). ITI operates by
shifting model activations during inference, following a set of directions
across a limited number of attention heads. This intervention significantly
improves the performance of LLaMA models on the TruthfulQA benchmark. On an
instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from
32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and
demonstrate how to balance it by tuning the intervention strength. ITI is
minimally invasive and computationally inexpensive. Moreover, the technique is
data efficient: while approaches like RLHF require extensive annotations, ITI
locates truthful directions using only few hundred examples. Our findings
suggest that LLMs may have an internal representation of the likelihood of
something being true, even as they produce falsehoods on the surface.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>code: https://github.com/likenneth/honest_llama</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Small Character Models Match Large Word Models for Autocomplete Under
  Memory Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.03251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.03251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganesh Jawahar, Subhabrata Mukherjee, Debadeepta Dey, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan, Caio Cesar Teodoro Mendes, Gustavo Henrique de Rosa, Shital Shah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autocomplete is a task where the user inputs a piece of text, termed prompt,
which is conditioned by the model to generate semantically coherent
continuation. Existing works for this task have primarily focused on datasets
(e.g., email, chat) with high frequency user prompt patterns (or focused
prompts) where word-based language models have been quite effective. In this
work, we study the more challenging open-domain setting consisting of low
frequency user prompt patterns (or broad prompts, e.g., prompt about 93rd
academy awards) and demonstrate the effectiveness of character-based language
models. We study this problem under memory-constrained settings (e.g., edge
devices and smartphones), where character-based representation is effective in
reducing the overall model size (in terms of parameters). We use WikiText-103
benchmark to simulate broad prompts and demonstrate that character models rival
word models in exact match accuracy for the autocomplete task, when controlled
for the model size. For instance, we show that a 20M parameter character model
performs similar to an 80M parameter word model in the vanilla setting. We
further propose novel methods to improve character models by incorporating
inductive bias in the form of compositional information and representation
transfer from large word models. Datasets and code used in this work are
available at https://github.com/UBC-NLP/char_autocomplete.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SustaiNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UNIDECOR: A Unified Deception Corpus for Cross-Corpus Deception
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02827v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02827v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aswathy Velutharambath, Roman Klinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verbal deception has been studied in psychology, forensics, and computational
linguistics for a variety of reasons, like understanding behaviour patterns,
identifying false testimonies, and detecting deception in online communication.
Varying motivations across research fields lead to differences in the domain
choices to study and in the conceptualization of deception, making it hard to
compare models and build robust deception detection systems for a given
language. With this paper, we improve this situation by surveying available
English deception datasets which include domains like social media reviews,
court testimonials, opinion statements on specific topics, and deceptive
dialogues from online strategy games. We consolidate these datasets into a
single unified corpus. Based on this resource, we conduct a correlation
analysis of linguistic cues of deception across datasets to understand the
differences and perform cross-corpus modeling experiments which show that a
cross-domain generalization is challenging to achieve. The unified deception
corpus (UNIDECOR) can be obtained from
https://www.ims.uni-stuttgart.de/data/unidecor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MultiInstruct: Improving Multi-Modal Zero-Shot Learning via Instruction
  Tuning <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10773v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10773v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyang Xu, Ying Shen, Lifu Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction tuning, a new learning paradigm that fine-tunes pre-trained
language models on tasks specified through instructions, has shown promising
zero-shot performance on various natural language processing tasks. However,
it's still not explored for vision and multimodal tasks. In this work, we
introduce MultiInstruct, the first multimodal instruction tuning benchmark
dataset that consists of 47 diverse multimodal tasks covering 11 broad
categories. Each task is designed at least with 5,000 instances (input-out
pairs) from existing open-source datasets and 5 expert-written instructions. We
take OFA as the base pre-trained model for multimodal instruction tuning, and
to improve its performance, we explore multiple transfer learning strategies to
leverage the large-scale Natural Instructions dataset. Experimental results
demonstrate its strong zero-shot performance on various unseen multimodal tasks
and the benefit of transfer learning from text-only instructions. We also
design a new evaluation metric: Sensitivity, to evaluate how sensitive the
model is to the variety of instructions. Our results indicate that the model is
less sensitive to the varying instructions after finetuning on a diverse set of
tasks and instructions for each task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoMoE: Heterogeneous Mixture-of-Experts with Adaptive Computation for
  Efficient Neural Machine Translation <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07535v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07535v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ganesh Jawahar, Subhabrata Mukherjee, Xiaodong Liu, Young Jin Kim, Muhammad Abdul-Mageed, Laks V. S. Lakshmanan, Ahmed Hassan Awadallah, Sebastien Bubeck, Jianfeng Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mixture-of-Expert (MoE) models have obtained state-of-the-art performance in
Neural Machine Translation (NMT) tasks. Existing works in MoE mostly consider a
homogeneous design where the same number of experts of the same size are placed
uniformly throughout the network. Furthermore, existing MoE works do not
consider computational constraints (e.g., FLOPs, latency) to guide their
design. To this end, we develop AutoMoE -- a framework for designing
heterogeneous MoE's under computational constraints. AutoMoE leverages Neural
Architecture Search (NAS) to obtain efficient sparse MoE sub-transformers with
4x inference speedup (CPU) and FLOPs reduction over manually designed
Transformers, with parity in BLEU score over dense Transformer and within 1
BLEU point of MoE SwitchTransformer, on aggregate over benchmark datasets for
NMT. Heterogeneous search space with dense and sparsely activated Transformer
modules (e.g., how many experts? where to place them? what should be their
sizes?) allows for adaptive compute -- where different amounts of computations
are used for different tokens in the input. Adaptivity comes naturally from
routing decisions which send tokens to experts of different sizes. AutoMoE
code, data, and trained models are available at https://aka.ms/AutoMoE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Causal Framework to Quantify the Robustness of Mathematical Reasoning
  with Language Models <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.12023v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.12023v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Stolfo, Zhijing Jin, Kumar Shridhar, Bernhard Schölkopf, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We have recently witnessed a number of impressive results on hard
mathematical reasoning problems with language models. At the same time, the
robustness of these models has also been called into question; recent works
have shown that models can rely on shallow patterns in the problem description
when generating a solution. Building on the idea of behavioral testing, we
propose a novel framework, which pins down the causal effect of various factors
in the input, e.g., the surface form of the problem text, the operands, and
math operators on the output solution. By grounding the behavioral analysis in
a causal graph describing an intuitive reasoning process, we study the behavior
of language models in terms of robustness and sensitivity to direct
interventions in the input space. We apply our framework on a test bed of math
word problems. Our analysis shows that robustness does not appear to
continuously improve as a function of size, but the GPT-3 Davinci models (175B)
achieve a dramatic improvement in both robustness and sensitivity compared to
all other GPT variants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023. A shorter version of the paper was accepted at the MATH-AI
  Workshop at NeurIPS 2022. 15 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BUCA: A Binary Classification Approach to Unsupervised Commonsense
  Question Answering <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15932v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15932v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie He, Simon Chi Lok U, Víctor Gutiérrez-Basulto, Jeff Z. Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Unsupervised commonsense reasoning (UCR) is becoming increasingly popular as
the construction of commonsense reasoning datasets is expensive, and they are
inevitably limited in their scope. A popular approach to UCR is to fine-tune
language models with external knowledge (e.g., knowledge graphs), but this
usually requires a large number of training examples. In this paper, we propose
to transform the downstream multiple choice question answering task into a
simpler binary classification task by ranking all candidate answers according
to their reasonableness. To this end, for training the model, we convert the
knowledge graph triples into reasonable and unreasonable texts. Extensive
experimental results show the effectiveness of our approach on various multiple
choice question answering benchmarks. Furthermore, compared with existing UCR
approaches using KGs, ours is less data hungry. Our code is available at
https://github.com/probe2/BUCA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explicit Knowledge Transfer for Weakly-Supervised Code Generation <span class="chip">ICLR</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.16740v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.16740v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhangir Azerbayev, Ansong Ni, Hailey Schoelkopf, Dragomir Radev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) can acquire strong code-generation capabilities
through few-shot learning. In contrast, supervised fine-tuning is still needed
for smaller models to achieve good performance. Such fine-tuning demands a
large number of task-specific NL-code pairs, which are expensive to obtain. In
this paper, we attempt to transfer the code generation ability of an LLM to a
smaller model with the aid of weakly-supervised data. More specifically, we
propose explicit knowledge transfer (EKT), which uses the few-shot capabilities
of a teacher LLM to create NL-code pairs that we then filter for correctness
and fine-tune the student on. We evaluate EKT on the task of generating code
solutions to math word problems from the GSM8k dataset. We find that EKT not
only yields better performance than training with expert iteration, but also
outperforms knowledge distillation, another form of knowledge transfer. A
GPT-Neo 1.3B model trained using EKT with a GPT-J teacher achieves a 12.4%
pass@100 on GSM8k, while the same student and teacher trained with knowledge
distillation yield only a 3.7% pass@100. We also show that it is possible for a
student model to outperform the teacher using EKT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated on Jun 7. 2023 with ICLR workshop header</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Optimization and Control <span class="chip" style="font-size: 60%">37</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The lifted functional approach to mean field games with common noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04560v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04560v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mark Cerenzia, Aaron Palmer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new path-by-path approach to mean field games with common
noise that recovers duality at the pathwise level. We verify this perspective
by explicitly solving some difficult examples with linear-quadratic data,
including control in the volatility coefficient of the common noise as well as
the constraint of partial information. As an application, we establish the
celebrated separation principle in the latter context. In pursuing this
program, we believe we have made a crucial contribution to clarifying the
notion of regular solution in the path dependent PDE literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparison of SeDuMi and SDPT3 Solvers for Stability of Continuous-time
  Linear System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangda Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  SeDuMi and SDPT3 are two solvers for solving Semi-definite Programming (SDP)
or Linear Matrix Inequality (LMI) problems. A computational performance
comparison of these two are undertaken in this paper regarding the Stability of
Continuous-time Linear Systems. The comparison mainly focuses on computational
times and memory requirements for different scales of problems. To implement
and compare the two solvers on a set of well-posed problems, we employ YALMIP,
a widely used toolbox for modeling and optimization in MATLAB. The primary goal
of this study is to provide an empirical assessment of the relative
computational efficiency of SeDuMi and SDPT3 under varying problem conditions.
Our evaluation indicates that SDPT3 performs much better in large-scale,
high-precision calculations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Mirror Descent Perspective on Classical and Quantum Blahut-Arimoto
  Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kerry He, James Saunderson, Hamza Fawzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Blahut-Arimoto algorithm is a well known method to compute classical
channel capacities and rate-distortion functions. Recent works have extended
this algorithm to compute various quantum analogs of these quantities. In this
paper, we show how these Blahut-Arimoto algorithms are special instances of
mirror descent, which is a well-studied generalization of gradient descent for
constrained convex optimization. Using new convex analysis tools, we show how
relative smoothness and strong convexity analysis recovers known sublinear and
linear convergence rates for Blahut-Arimoto algorithms. This mirror descent
viewpoint allows us to derive related algorithms with similar convergence
guarantees to solve problems in information theory for which
Blahut-Arimoto-type algorithms are not directly applicable. We apply this
framework to compute energy-constrained classical and quantum channel
capacities, classical and quantum rate-distortion functions, and approximations
of the relative entropy of entanglement, all with provable convergence
guarantees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Hierarchical OPF Algorithm with Improved Gradient Evaluation in
  Three-Phase Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Liang, Xinyang Zhou, Changhong Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Linear approximation commonly used in solving alternating-current optimal
power flow (AC-OPF) simplifies the system models but incurs accumulated voltage
errors in large power networks. Such errors will make the primal-dual type
gradient algorithms converge to the solutions at which the power networks may
be exposed to the risk of voltage violation. In this paper, we improve a recent
hierarchical OPF algorithm that rested on primal-dual gradients evaluated with
a linearized distribution power flow model. Specifically, we propose a more
accurate gradient evaluation method based on a three-phase unbalanced nonlinear
distribution power flow model to mitigate the errors arising from model
linearization. The resultant gradients feature a blocked structure that enables
us to further develop an improved hierarchical primal-dual algorithm to solve
the OPF problem. Numerical results on the IEEE $123$-bus test feeder and a
$4,518$-node test feeder show that the proposed method can enhance the overall
voltage safety while achieving comparable computational efficiency with the
linearized algorithm.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic optimal transport and Hamilton-Jacobi-Bellman equations on
  the set of probability measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04283v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04283v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Charles Bertucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a stochastic version of the optimal transport problem. We
provide an analysis by means of the study of the associated
Hamilton-Jacobi-Bellman equation, which is set on the set of probability
measures. We introduce a new definition of viscosity solutions of this
equation, which yields general comparison principles, in particular for cases
involving terms modeling stochasticity in the optimal control problem. We are
then able to establish results of existence and uniqueness of viscosity
solutions of the Hamilton-Jacobi-Bellman equation. These results rely on
controllability results for stochastic optimal transport that we also
establish.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integer Carathéodory results with bounded multiplicity 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stefan Kuhlmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integer Carath\'eodory rank of a pointed rational cone $C$ is the
smallest number $k$ such that every integer vector contained in $C$ is an
integral non-negative combination of at most $k$ Hilbert basis elements. We
investigate the integer Carath\'eodory rank of simplicial cones with respect to
their multiplicity, i.e., the determinant of the integral generators of the
cone. One of the main results states that simplicial cones with multiplicity
bounded by five have the integral Carath\'eodory property, that is, the integer
Carath\'eodory rank equals the dimension. Furthermore, we present a novel upper
bound on the integer Carath\'eodory rank which depends on the dimension and the
multiplicity. This bound improves upon the best known upper bound on the
integer Carath\'eodory rank if the dimension exceeds the multiplicity. At last,
we present special cones which have the integral Carath\'eodory property such
as certain dual cones of Gorenstein cones.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Distributed accelerated proximal conjugate gradient methods for
  multi-agent constrained optimization problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04230v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04230v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anteneh Getachew Gebrie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The purpose of this paper is to introduce two new classes of accelerated
distributed proximal conjugate gradient algorithms for multi-agent constrained
optimization problems; given as minimization of a function decomposed as a sum
of M number of smooth and M number of nonsmooth functions over the common fixed
points of M number of nonlinear mappings. Exploiting the special properties of
the cost component function of the objective function and the nonlinear mapping
of the constraint problem of each agent, a new inertial accelerated incremental
and parallel computing distributed algorithms will be presented based on the
combinations of computations of proximal, conjugate gradient and Halpern
methods. Some numerical experiments and comparisons are given to illustrate our
results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Decomposition Approach to Last Mile Delivery Using Public
  Transportation Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04219v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04219v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minakshi Punam Mandal, Claudia Archetti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study explores the potential of using public transportation systems for
freight delivery, where we intend to utilize the spare capacities of public
vehicles like buses, trams, metros, and trains, particularly during off-peak
hours, to transport packages within the city instead of using dedicated
delivery vehicles. The study contributes {to the growing} literature on
innovative strategies for performing sustainable last mile deliveries. We study
an operational level problem called the Three-Tier Delivery Problem on Public
Transportation, where packages are first transported from the Consolidation and
Distribution Center (CDC) to nearby public vehicle stations by delivery trucks.
From there, public vehicles transport them into the city area. The last leg of
the delivery is performed to deliver the packages to their respective customers
using green vehicles or eco-friendly systems. We propose mixed-integer linear
programming formulations to study the transport of packages from the CDC to the
customers, use decomposition approaches to solve them, and provide numerical
experiments to demonstrate the efficiency and effectiveness of the system. Our
results show that this system has the potential to drastically reduce the
length of trips performed by dedicated delivery vehicles, thereby reducing the
negative social and environmental impacts of existing last mile delivery
systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Input Rate Control in Stochastic Road Traffic Networks: Effective
  Bandwidths 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04218v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04218v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikki Levering, Rudesindo Núñez-Queija
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In road traffic networks, large traffic volumes may lead to extreme delays.
These severe delays are caused by the fact that, whenever the maximum capacity
of a road is approached, speeds drop rapidly. Therefore, the focus in this
paper is on real-time control of traffic input rates, thereby aiming to prevent
such detrimental capacity drops. To account for the fact that, by the
heterogeneity within and between traffic streams, the available capacity of a
road suffers from randomness, we introduce a stochastic flow model that
describes the impact of traffic input streams on the available road capacities.
Then, exploiting similarities with traffic control of telecommunication
networks, in which the available bandwidth is a stochastic function of the
input rate, and in which the use of effective bandwidths have proven an
effective input rate control framework, we propose a similar traffic rate
control policy based on the concept of effective bandwidths. This policy allows
for increased waiting times at the access boundaries of the network, so as to
limit the probability of large delays within the network. Numerical examples
show that, by applying such a control policy capacity violations are indeed
rare, and that the increased waiting at the boundaries of the network is of
much smaller scale, compared to uncontrolled delays in the network.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Two-step inertial Bregman alternating structure-adapted proximal
  gradient descent algorithm for nonconvex and nonsmooth problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenzheng Guo, Jing Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the paper, we introduce several accelerate iterative algorithms for
solving the multiple-set split common fixed-point problem of quasi-nonexpansive
operators in real Hilbert space. Based on primal-dual method, we construct
several iterative algorithms in a way that combines inertial technology and the
self-adaptive stepsize such that the implementation of the algorithms doesn't
need any prior information about bounded linear operator norm. Under suitable
assumptions, weak convergence of the proposed algorithms is established. As
applications, we obtain relative iterative algorithms to solve the multiple-set
split feasibility problem. Finally, the performance of the proposed algorithms
is illustrated by numerical experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Learning for Stochastic Optimization: A Bayesian Perspective <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yves Rychener, Daniel Kuhn Tobias Sutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a principled approach to end-to-end learning in stochastic
optimization. First, we show that the standard end-to-end learning algorithm
admits a Bayesian interpretation and trains a posterior Bayes action map.
Building on the insights of this analysis, we then propose new end-to-end
learning algorithms for training decision maps that output solutions of
empirical risk minimization and distributionally robust optimization problems,
two dominant modeling paradigms in optimization under uncertainty. Numerical
results for a synthetic newsvendor problem illustrate the key differences
between alternative training schemes. We also investigate an economic dispatch
problem based on real data to showcase the impact of the neural network
architecture of the decision maps on their test performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accelerating 128-bit Floating-Point Matrix Multiplication on FPGAs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04087v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04087v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fumiya Kono, Naohito Nakasato, Maho Nakata
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  General Matrix Multiplication (GEMM) is a fundamental operation widely used
in scientific computations. Its performance and accuracy significantly impact
the performance and accuracy of applications that depend on it. One such
application is semidefinite programming (SDP), and it often requires binary128
or higher precision arithmetic to solve problems involving SDP stably. However,
only some processors support binary128 arithmetic, which makes SDP solvers
generally slow. In this study, we focused on accelerating GEMM with binary128
arithmetic on field-programmable gate arrays (FPGAs) to enable the flexible
design of accelerators for the desired computations. Our binary128 GEMM designs
on a recent high-performance FPGA achieved approximately 90GFlops, 147x faster
than the computation executed on a recent CPU with 20 threads for large
matrices. Using our binary128 GEMM design on the FPGA, we successfully
accelerated two numerical applications: LU decomposition and SDP problems, for
the first time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modelling the discretization error of initial value problems using the
  Wishart distribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04084v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04084v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Marumo, Takeru Matsuda, Yuto Miyatake
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new discretization error quantification method for the
numerical integration of ordinary differential equations. The error is modelled
by using the Wishart distribution, which enables us to capture the correlation
between variables. Error quantification is achieved by solving an optimization
problem under the order constraints for the covariance matrices. An algorithm
for the optimization problem is also established in a slightly broader context.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $K$-Nearest-Neighbor Resampling for Off-Policy Evaluation in Stochastic
  Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04836v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04836v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Giegrich, Roel Oomen, Christoph Reisinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel $K$-nearest neighbor resampling procedure for estimating
the performance of a policy from historical data containing realized episodes
of a decision process generated under a different policy. We focus on feedback
policies that depend deterministically on the current state in environments
with continuous state-action spaces and system-inherent stochasticity effected
by chosen actions. Such settings are common in a wide range of high-stake
applications and are actively investigated in the context of stochastic
control. Our procedure exploits that similar state/action pairs (in a metric
sense) are associated with similar rewards and state transitions. This enables
our resampling procedure to tackle the counterfactual estimation problem
underlying off-policy evaluation (OPE) by simulating trajectories similarly to
Monte Carlo methods. Compared to other OPE methods, our algorithm does not
require optimization, can be efficiently implemented via tree-based nearest
neighbor search and parallelization and does not explicitly assume a parametric
model for the environment's dynamics. These properties make the proposed
resampling algorithm particularly useful for stochastic control environments.
We prove that our method is statistically consistent in estimating the
performance of a policy in the OPE setting under weak assumptions and for data
sets containing entire episodes rather than independent transitions. To
establish the consistency, we generalize Stone's Theorem, a well-known result
in nonparametric statistics on local averaging, to include episodic data and
the counterfactual estimation underlying OPE. Numerical experiments demonstrate
the effectiveness of the algorithm in a variety of stochastic control settings
including a linear quadratic regulator, trade execution in limit order books
and online stochastic bin packing.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Learning for Population-Dependent Controls in Mean Field Control
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04788v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04788v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gokce Dayanikli, Mathieu Lauriere, Jiacheng Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose several approaches to learn optimal
population-dependent controls, in order to solve mean field control problems
(MFC). Such policies enable us to solve MFC problems with generic common noise.
We analyze the convergence of the proposed approximation algorithms,
particularly the N-particle approximation. The effectiveness of our algorithms
is supported by three different experiments, including systemic risk, price
impact and crowd motion. We first show that our algorithms converge to the
correct solution in an explicitly solvable MFC problem. Then, we conclude by
showing that population-dependent controls outperform state-dependent controls.
Along the way, we show that specific neural network architectures can improve
the learning further.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Achieving Consensus over Compact Submanifolds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04769v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04769v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Hu, Jiaojiao Zhang, Kangkang Deng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the consensus problem in a decentralized network, focusing on a
compact submanifold that acts as a nonconvex constraint set. By leveraging the
proximal smoothness of the compact submanifold, which encompasses the local
singleton property and the local Lipschitz continuity of the projection
operator on the manifold, and establishing the connection between the
projection operator and general retraction, we show that the Riemannian
gradient descent with a unit step size has locally linear convergence if the
network has a satisfactory level of connectivity. Moreover, based on the
geometry of the compact submanifold, we prove that a convexity-like regularity
condition, referred to as the restricted secant inequality, always holds in an
explicitly characterized neighborhood around the solution set of the nonconvex
consensus problem. By leveraging this restricted secant inequality and imposing
a weaker connectivity requirement on the decentralized network, we present a
comprehensive analysis of the linear convergence of the Riemannian gradient
descent, taking into consideration appropriate initialization and step size.
Furthermore, if the network is well connected, we demonstrate that the local
Lipschitz continuity endowed by proximal smoothness is a sufficient condition
for the restricted secant inequality, thus contributing to the local error
bound. We believe that our established results will find more application in
the consensus problems over a more general proximally smooth set. Numerical
experiments are conducted to validate our theoretical findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Natural Thresholding Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04730v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04730v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rachel Grotheer, Shuang Li, Anna Ma, Deanna Needell, Jing Qin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse signal recovery is one of the most fundamental problems in various
applications, including medical imaging and remote sensing. Many greedy
algorithms based on the family of hard thresholding operators have been
developed to solve the sparse signal recovery problem. More recently, Natural
Thresholding (NT) has been proposed with improved computational efficiency.
This paper proposes and discusses convergence guarantees for stochastic natural
thresholding algorithms by extending the NT from the deterministic version with
linear measurements to the stochastic version with a general objective
function. We also conduct various numerical experiments on linear and nonlinear
measurements to demonstrate the performance of StoNT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Alternating Minimization with Applications to Weighted Low
  Rank Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao Song, Mingquan Ye, Junze Yin, Lichen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weighted low rank approximation is a fundamental problem in numerical linear
algebra, and it has many applications in machine learning. Given a matrix $M
\in \mathbb{R}^{n \times n}$, a weight matrix $W \in \mathbb{R}_{\geq 0}^{n
\times n}$, a parameter $k$, the goal is to output two matrices $U, V \in
\mathbb{R}^{n \times k}$ such that $\| W \circ (M - U V) \|_F$ is minimized,
where $\circ$ denotes the Hadamard product. Such a problem is known to be
NP-hard and even hard to approximate [RSW16]. Meanwhile, alternating
minimization is a good heuristic solution for approximating weighted low rank
approximation. The work [LLR16] shows that, under mild assumptions, alternating
minimization does provide provable guarantees. In this work, we develop an
efficient and robust framework for alternating minimization. For weighted low
rank approximation, this improves the runtime of [LLR16] from $n^2 k^2$ to
$n^2k$. At the heart of our work framework is a high-accuracy multiple response
regression solver together with a robust analysis of alternating minimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2302.11068</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smooth Non-Stationary Bandits <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12366v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12366v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Su Jia, Qian Xie, Nathan Kallus, Peter I. Frazier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many applications of online decision making, the environment is
non-stationary and it is therefore crucial to use bandit algorithms that handle
changes. Most existing approaches are designed to protect against non-smooth
changes, constrained only by total variation or Lipschitzness over time, where
they guarantee $\tilde \Theta(T^{2/3})$ regret. However, in practice
environments are often changing {\bf smoothly}, so such algorithms may incur
higher-than-necessary regret in these settings and do not leverage information
on the rate of change. We study a non-stationary two-armed bandits problem
where we assume that an arm's mean reward is a $\beta$-H\"older function over
(normalized) time, meaning it is $(\beta-1)$-times Lipschitz-continuously
differentiable. We show the first separation between the smooth and non-smooth
regimes by presenting a policy with $\tilde O(T^{3/5})$ regret for $\beta=2$.
We complement this result by an $\Omg(T^{(\beta+1)/(2\beta+1)})$ lower bound
for any integer $\beta\ge 1$, which matches our upper bound for $\beta=2$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Unregularized Third Order Newton Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.10051v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.10051v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Olha Silina, Jeffrey Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a third-order Newton's method which in each
iteration solves a semidefinite program as a subproblem. Our approach is based
on moving to the local minimum of the third-order Taylor expansion at each
iteration, rather than that of the second order. We show that this scheme has
local cubic convergence. We then provide numerical experiments comparing this
scheme to some standard algorithms.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient boosting for convex cone predict and optimize problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.06895v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.06895v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Butler, Roy H. Kwon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prediction models are typically optimized independently from decision
optimization. A smart predict then optimize (SPO) framework optimizes
prediction models to minimize downstream decision regret. In this paper we
present dboost, the first general purpose implementation of smart gradient
boosting for `predict, then optimize' problems. The framework supports convex
quadratic cone programming and gradient boosting is performed by implicit
differentiation of a custom fixed-point mapping. Experiments comparing with
state-of-the-art SPO methods show that dboost can further reduce out-of-sample
decision regret.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Perfect Copositive Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17310v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17310v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Valentin Dannenberg, Achill Schürmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we give a first study of perfect copositive $n \times n$
matrices. They can be used to find rational certificates for completely
positive matrices. We describe similarities and differences to classical
perfect, positive definite matrices. Most of the differences occur only for $n
\geq 3$, where we find for instance lower rank and indefinite perfect matrices.
Nevertheless, we find for all $n$ that for every classical perfect matrix there
is an arithmetically equivalent one which is also perfect copositive.
Furthermore we study the neighborhood graph and polyhedral structure of perfect
copositive matrices. As an application we obtain a new characterization of the
cone of completely positive matrices: It is equal to the set of nonnegative
matrices having a nonnegative inner product with all perfect copositive
matrices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Augmenting Bi-objective Branch and Bound by Scalarization-Based
  Information 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11974v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11974v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julius Bauß, Michael Stiglmayr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Branch and Bound based algorithms are a standard approach to solve
single-objective (mixed-)integer optimization problems, multi-objective Branch
and Bound methods are only rarely applied compared to the predominant objective
space methods. In this paper we propose modifications to increase the
performance of multi-objective Branch and Bound algorithms by utilizing
scalarization-based information. We use the hypervolume indicator as a measure
for the gap between lower and upper bound set to implement a multi-objective
best-first strategy. By adaptively solving scalarizations in the root node to
integer optimality we improve both, upper and lower bound set. The obtained
lower bound can then be integrated into the lower bounds of all active nodes,
while the determined solution is added to the upper bound set. Numerical
experiments show that the number of investigated nodes can be significantly
reduced by up to 83% and the total computation time can be reduced by up to
80%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Myopic Quantal Response Policy: Thompson Sampling Meets Behavioral
  Economics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.01028v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.01028v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingying Ding, Yifan Feng, Ying Rong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a novel family of behavioral policies for the multi-armed bandit
(MAB) problem, which we have termed Myopic Quantal Response (MQR). MQR
prescribes a simple way to randomize over arms according to historical rewards
and a "coefficient of exploitation," which explicitly manages the
exploration-exploitation trade-off. MQR is a dynamic adaptation of quantal
response models where the anticipated utilities are directly derived from past
rewards. Furthermore, it can be viewed as a generalization of the Thompson
Sampling (TS) algorithm. We develop an asymptotic theory for MQR and show how
it can help understand not only asymptotically optimal policies like TS, but
also those that are suboptimal due to "under" or "over" exploring. In the
non-asymptotic setup, we demonstrate how MQR can be used as a structural
estimation tool: Given observed data (i.e., realized actions and rewards), we
can estimate the implied coefficient of exploitation of any given policy
(either generated by human beings or algorithms). This allows us to diagnose
whether and to what extent the policy underexplores or overexplores.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Difference Learning with Continuous Time and State in the
  Stochastic Setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.07960v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.07960v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziad Kobeissi, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of continuous-time policy evaluation. This consists
in learning through observations the value function associated with an
uncontrolled continuous-time stochastic dynamic and a reward function. We
propose two original variants of the well-known TD(0) method using vanishing
time steps. One is model-free and the other is model-based. For both methods,
we prove theoretical convergence rates that we subsequently verify through
numerical simulations. Alternatively, those methods can be interpreted as novel
reinforcement learning approaches for approximating solutions of linear PDEs
(partial differential equations) or linear BSDEs (backward stochastic
differential equations).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two Losses Are Better Than One: Faster Optimization Using a Cheaper
  Proxy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03542v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03542v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blake Woodworth, Konstantin Mishchenko, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an algorithm for minimizing an objective with hard-to-compute
gradients by using a related, easier-to-access function as a proxy. Our
algorithm is based on approximate proximal point iterations on the proxy
combined with relatively few stochastic gradients from the objective. When the
difference between the objective and the proxy is $\delta$-smooth, our
algorithm guarantees convergence at a rate matching stochastic gradient descent
on a $\delta$-smooth objective, which can lead to substantially better sample
efficiency. Our algorithm has many potential applications in machine learning,
and provides a principled means of leveraging synthetic data, physics
simulators, mixed public and private data, and more.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Complexity of a Projected Newton-CG Method for Optimization with Bounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.15989v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.15989v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yue Xie, Stephen J. Wright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper describes a method for solving smooth nonconvex minimization
problems subject to bound constraints with good worst-case complexity
guarantees and practical performance. The method contains elements of two
existing methods: the classical gradient projection approach for
bound-constrained optimization and a recently proposed Newton-conjugate
gradient algorithm for unconstrained nonconvex optimization. Using a new
definition of approximate second-order optimality parametrized by some
tolerance $\epsilon$ (which is compared with related definitions from previous
works), we derive complexity bounds in terms of $\epsilon$ for both the number
of iterations required and the total amount of computation. The latter is
measured by the number of gradient evaluations or Hessian-vector products. We
also describe illustrative computational results on several test problems from
low-rank matrix optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimal control for sampling the transition path process and estimating
  rates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17112v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17112v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaxin Yuan, Amar Shah, Channing Bentz, Maria Cameron
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many processes in nature such as conformal changes in biomolecules and
clusters of interacting particles, genetic switches, noisy mechanical or
electromechanical oscillators, and many others are modeled using stochastic
differential equations with small white noise. The study of rare transitions
between metastable states in such systems is of great interest and importance,
but direct simulations are difficult due to long waiting times. Transition path
theory is a mathematical framework for the quantitative description of rare
events. Its direct implementation the key component of which is the solution of
the committor problem, a boundary value problem for the backward Kolmogorov
equation, is often challenging due to high dimensionality or other numerical
issues. This work exploits the key fact that the optimal controller constructed
from the committor leads to generation of transition trajectories exclusively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A posteriori error estimates for fully coupled McKean-Vlasov
  forward-backward SDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.07731v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.07731v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christoph Reisinger, Wolfgang Stockinger, Yufei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fully coupled McKean-Vlasov forward-backward stochastic differential
equations (MV-FBSDEs) arise naturally from large population optimization
problems. Judging the quality of given numerical solutions for MV-FBSDEs, which
usually require Picard iterations and approximations of nested conditional
expectations, is typically difficult. This paper proposes an a posteriori error
estimator to quantify the $L^2$-approximation error of an arbitrarily generated
approximation on a time grid. We establish that the error estimator is
equivalent to the global approximation error between the given numerical
solution and the solution of a forward Euler discretized MV-FBSDE. A crucial
and challenging step in the analysis is the proof of stability of this Euler
approximation to the MV-FBSDE, which is of independent interest. We further
demonstrate that, for sufficiently fine time grids, the accuracy of numerical
solutions for solving the continuous MV-FBSDE can also be measured by the error
estimator. The error estimates justify the use of residual-based algorithms for
solving MV-FBSDEs. Numerical experiments for MV-FBSDEs arising from mean field
control and games confirm the effectiveness and practical applicability of the
error estimator.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The effectiveness of the error estimator is demonstrated in
  high-dimensional and nonlinear examples</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explicit representations for Banach subspaces of Lizorkin distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.05312v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.05312v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Neumayer, Michael Unser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Lizorkin space is well-suited for studying various operators; e.g.,
fractional Laplacians and the Radon transform. In this paper, we show that the
space is unfortunately not complemented in the Schwartz space. However, we can
show that it is dense in $C_0(\mathbb R^d)$, a property that is shared by the
larger Schwartz space and that turns out to be useful for applications. Based
on this result, we investigate subspaces of Lizorkin distributions that are
Banach spaces and for which a continuous representation operator exists. Then,
we introduce a variational framework involving these spaces and that makes use
of the constructed operator. By investigating two particular cases of this
framework, we are able to strengthen existing results for fractional splines
and 2-layer ReLU networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Reinforcement Learning in Finite-Horizon to Explore the Most
  Probable Transition Pathway 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.12994v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.12994v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin Guo, Ting Gao, Peng Zhang, Jiequn Han, Jinqiao Duan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many scientific and engineering problems, noise and nonlinearity are
unavoidable, which could induce interesting mathematical problem such as
transition phenomena. This paper focuses on efficiently discovering the most
probable transition pathway for stochastic dynamical systems employing
reinforcement learning. With the Onsager-Machlup action functional theory to
quantify rare events in stochastic dynamical systems, finding the most probable
pathway is equivalent to solving a variational problem on the action
functional. When the action function cannot be explicitly expressed by paths
near the reference orbit, the variational problem needs to be converted into an
optimal control problem. First, by integrating terminal prediction into the
reinforcement learning framework, we develop a Terminal Prediction Deep
Deterministic Policy Gradient (TP-DDPG) algorithm to deal with the
finite-horizon optimal control issue in a forward way. Next, we present the
convergence analysis of our algorithm for the value function in terms of the
neural network's approximation error and estimation error. Finally, we conduct
various experiments in different dimensions for the transition problems in
applications to illustrate the effectiveness of our algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 24 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving NP-hard Min-max Routing Problems as Sequential Generation with
  Equity Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwoo Son, Minsu Kim, Sanghyeok Choi, Jinkyoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Min-max routing problems aim to minimize the maximum tour length among agents
as they collaboratively visit all cities, i.e., the completion time. These
problems include impactful real-world applications but are known as NP-hard.
Existing methods are facing challenges, particularly in large-scale problems
that require the coordination of numerous agents to cover thousands of cities.
This paper proposes a new deep-learning framework to solve large-scale min-max
routing problems. We model the simultaneous decision-making of multiple agents
as a sequential generation process, allowing the utilization of scalable
deep-learning models for sequential decision-making. In the sequentially
approximated problem, we propose a scalable contextual Transformer model,
Equity-Transformer, which generates sequential actions considering an equitable
workload among other agents. The effectiveness of Equity-Transformer is
demonstrated through its superior performance in two representative min-max
routing tasks: the min-max multiple traveling salesman problem (min-max mTSP)
and the min-max multiple pick-up and delivery problem (min-max mPDP). Notably,
our method achieves significant reductions of runtime, approximately 335 times,
and cost values of about 53% compared to a competitive heuristic (LKH3) in the
case of 100 vehicles with 1,000 cities of mTSP. We provide reproducible source
code: https://github.com/kaist-silab/equity-transformer
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blessings and Curses of Covariate Shifts: Adversarial Learning Dynamics,
  Directional Convergence, and Equilibria 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02457v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02457v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tengyuan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Covariate distribution shifts and adversarial perturbations present
robustness challenges to the conventional statistical learning framework: mild
shifts in the test covariate distribution can significantly affect the
performance of the statistical model learned based on the training
distribution. The model performance typically deteriorates when extrapolation
happens: namely, covariates shift to a region where the training distribution
is scarce, and naturally, the learned model has little information. For
robustness and regularization considerations, adversarial perturbation
techniques are proposed as a remedy; however, careful study needs to be carried
out about what extrapolation region adversarial covariate shift will focus on,
given a learned model. This paper precisely characterizes the extrapolation
region, examining both regression and classification in an infinite-dimensional
setting. We study the implications of adversarial covariate shifts to
subsequent learning of the equilibrium -- the Bayes optimal model -- in a
sequential game framework. We exploit the dynamics of the adversarial learning
game and reveal the curious effects of the covariate shift to equilibrium
learning and experimental design. In particular, we establish two directional
convergence results that exhibit distinctive phenomena: (1) a blessing in
regression, the adversarial covariate shifts in an exponential rate to an
optimal experimental design for rapid subsequent learning, (2) a curse in
classification, the adversarial covariate shifts in a subquadratic rate fast to
the hardest experimental design trapping subsequent learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global and Preference-based Optimization with Mixed Variables using
  Piecewise Affine Surrogates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04686v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04686v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mengjia Zhu, Alberto Bemporad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimization problems involving mixed variables, i.e., variables of numerical
and categorical nature, can be challenging to solve, especially in the presence
of complex constraints. Moreover, when the objective function is the result of
a complicated simulation or experiment, it may be expensive to evaluate. This
paper proposes a novel surrogate-based global optimization algorithm to solve
linearly constrained mixed-variable problems up to medium-large size (around
100 variables after encoding and 20 constraints) based on constructing a
piecewise affine surrogate of the objective function over feasible samples. We
introduce two types of exploration functions to efficiently search the feasible
domain via mixed-integer linear programming solvers. We also provide a
preference-based version of the algorithm, which can be used when only pairwise
comparisons between samples can be acquired while the underlying objective
function to minimize remains unquantified. The two algorithms are tested on
mixed-variable benchmark problems with and without constraints. The results
show that, within a small number of acquisitions, the proposed algorithms can
often achieve better or comparable results than other existing methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>code available at https://github.com/mjzhu-p/PWAS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On modeling NP-Complete problems as polynomial-sized linear programs:
  Escaping/Side-stepping the "barriers" 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.07716v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.07716v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moustapha Diaby, Mark Karwan, Lei Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In view of the extended formulations (EFs) developments (e.g. "Fiorini, S.,
S. Massar, S. Pokutta, H.R. Tiwary, and R. de Wolf [2015]. Exponential Lower
Bounds for Polytopes in Combinatorial Optimization. Journal of the ACM 62:2"),
we focus in this paper on the question of whether it is possible to model an
NP-Complete problem as a polynomial-sized linear program. For the sake of
simplicity of exposition, the discussions are focused on the TSP. We show that
a finding that there exists no polynomial-sized extended formulation of "the
TSP polytope" does not (necessarily) imply that it is "impossible" for a
polynomial-sized linear program to solve the TSP optimization problem. We show
that under appropriate conditions the TSP optimization problem can be solved
without recourse to the traditional city-to-city ("travel leg") variables,
thereby side-stepping/"escaping from" "the TSP polytope" and hence, the
barriers. Some illustrative examples are discussed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages; 4 figures; 2 tables; Version 2: Minor typos corrected;
  Version 3: Discussion/editorial clarification in section 2.2.1 (last
  paragraph); This version (4): Further clarification in last paragraph of
  section 2.2.1</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Designing System Level Synthesis Controllers for Nonlinear Systems with
  Stability Guarantees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.03923v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.03923v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lauren Conger, Syndey Vernon, Eric Mazumdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a method for controlling systems with nonlinear dynamics and
full actuation by approximating the dynamics with polynomials and applying a
system level synthesis controller. We show how to optimize over this class of
controllers using a neural network while maintaining stability guarantees,
without requiring a Lyapunov function. We give bounds for the domain over which
the use of the class of controllers preserves stability and gives bounds on the
control costs incurred by optimized controllers. We then numerically validate
our approach and show improved performance compared with feedback linearization
-- suggesting that the SLS controllers are able to take advantage of
nonlinearities in the dynamics while guaranteeing stability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stabilizing Queuing Networks with Model Data-Independent Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2011.11788v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2011.11788v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Xie, Li Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Classical queuing network control strategies typically rely on accurate
knowledge of model data, i.e., arrival and service rates. However, such data
are not always available and may be time-variant. To address this challenge, we
consider a class of model data-independent (MDI) control policies that only
rely on traffic state observation and network topology. Specifically, we focus
on the MDI control policies that can stabilize multi-class Markovian queuing
networks under centralized and decentralized policies. Control actions include
routing, sequencing, and holding. By expanding the routes and constructing
piecewise-linear test functions, we derive an easy-to-use criterion to check
the stability of a multi-class network under a given MDI policy. For
stabilizable multi-class networks, we show that a centralized, stabilizing MDI
policy exists. For stabilizable single-class networks, we further show that a
decentralized, stabilizing MDI policy exists. In addition, for both settings,
we construct explicit policies that attain maximal throughput and present
numerical examples to illustrate the results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Control of Network Systems</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ModuleFormer: Learning Modular Large Language Models From Uncurated Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04640v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04640v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yikang Shen, Zheyu Zhang, Tianyou Cao, Shawn Tan, Zhenfang Chen, Chuang Gan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have achieved remarkable results. But existing
models are expensive to train and deploy, and it is also difficult to expand
their knowledge beyond pre-training data without forgetting previous knowledge.
This paper proposes a new neural network architecture, ModuleFormer, that
leverages modularity to improve the efficiency and flexibility of large
language models. ModuleFormer is based on the Sparse Mixture of Experts (SMoE).
Unlike the previous SMoE-based modular language model [Gururangan et al.,
2021], which requires domain-labeled data to learn domain-specific experts,
ModuleFormer can induce modularity from uncurated data with its new load
balancing and load concentration losses. ModuleFormer is a modular architecture
that includes two different types of modules, new stick-breaking attention
heads, and feedforward experts. Different modules are sparsely activated
conditions on the input token during training and inference. In our experiment,
we found that the modular architecture enables three important abilities for
large pre-trained language models: 1) Efficiency, since ModuleFormer only
activates a subset of its modules for each input token, thus it could achieve
the same performance as dense LLMs with more than two times throughput; 2)
Extendability, ModuleFormer is more immune to catastrophic forgetting than
dense LLMs and can be easily extended with new modules to learn new knowledge
that is not included in the training data; 3) Specialisation, finetuning
ModuleFormer could specialize a subset of modules to the finetuning task, and
the task-unrelated modules could be easily pruned for a lightweight deployment.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>s as Statisticians: Provable In-Context Learning with
  In-Context Algorithm Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04637v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04637v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, Song Mei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural sequence models based on the transformer architecture have
demonstrated remarkable \emph{in-context learning} (ICL) abilities, where they
can perform new tasks when prompted with training and test examples, without
any parameter update to the model. This work first provides a comprehensive
statistical theory for transformers to perform ICL. Concretely, we show that
transformers can implement a broad class of standard machine learning
algorithms in context, such as least squares, ridge regression, Lasso, learning
generalized linear models, and gradient descent on two-layer neural networks,
with near-optimal predictive power on various in-context data distributions.
Using an efficient implementation of in-context gradient descent as the
underlying mechanism, our transformer constructions admit mild size bounds, and
can be learned with polynomially many pretraining sequences.
  Building on these ``base'' ICL algorithms, intriguingly, we show that
transformers can implement more complex ICL procedures involving
\emph{in-context algorithm selection}, akin to what a statistician can do in
real life -- A \emph{single} transformer can adaptively select different base
ICL algorithms -- or even perform qualitatively different tasks -- on different
input sequences, without any explicit prompting of the right algorithm or task.
We both establish this in theory by explicit constructions, and also observe
this phenomenon experimentally. In theory, we construct two general mechanisms
for algorithm selection with concrete examples: pre-ICL testing, and post-ICL
validation. As an example, we use the post-ICL validation mechanism to
construct a transformer that can perform nearly Bayes-optimal ICL on a
challenging task -- noisy linear models with mixed noise levels.
Experimentally, we demonstrate the strong in-context algorithm selection
capabilities of standard transformer architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GP-UNIT: Generative Prior for Versatile Unsupervised Image-to-Image
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04636v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04636v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuai Yang, Liming Jiang, Ziwei Liu, Chen Change Loy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in deep learning have witnessed many successful unsupervised
image-to-image translation models that learn correspondences between two visual
domains without paired data. However, it is still a great challenge to build
robust mappings between various domains especially for those with drastic
visual discrepancies. In this paper, we introduce a novel versatile framework,
Generative Prior-guided UNsupervised Image-to-image Translation (GP-UNIT), that
improves the quality, applicability and controllability of the existing
translation models. The key idea of GP-UNIT is to distill the generative prior
from pre-trained class-conditional GANs to build coarse-level cross-domain
correspondences, and to apply the learned prior to adversarial translations to
excavate fine-level correspondences. With the learned multi-level content
correspondences, GP-UNIT is able to perform valid translations between both
close domains and distant domains. For close domains, GP-UNIT can be
conditioned on a parameter to determine the intensity of the content
correspondences during translation, allowing users to balance between content
and style consistency. For distant domains, semi-supervised learning is
explored to guide GP-UNIT to discover accurate semantic correspondences that
are hard to learn solely from the appearance. We validate the superiority of
GP-UNIT over state-of-the-art translation models in robust, high-quality and
diversified translations between various domains through extensive experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE Transactions on Pattern Analysis and Machine
  Intelligence (TPAMI). Code: https://github.com/williamyang1991/GP-UNIT
  Project page: https://www.mmlab-ntu.com/project/gpunit/. arXiv admin note:
  substantial text overlap with arXiv:2204.03641</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Reliability of Watermarks for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04634v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04634v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Kirchenbauer, Jonas Geiping, Yuxin Wen, Manli Shu, Khalid Saifullah, Kezhi Kong, Kasun Fernando, Aniruddha Saha, Micah Goldblum, Tom Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) are now deployed to everyday use and positioned
to produce large quantities of text in the coming decade. Machine-generated
text may displace human-written text on the internet and has the potential to
be used for malicious purposes, such as spearphishing attacks and social media
bots. Watermarking is a simple and effective strategy for mitigating such harms
by enabling the detection and documentation of LLM-generated text. Yet, a
crucial question remains: How reliable is watermarking in realistic settings in
the wild? There, watermarked text might be mixed with other text sources,
paraphrased by human writers or other language models, and used for
applications in a broad number of domains, both social and technical. In this
paper, we explore different detection schemes, quantify their power at
detecting watermarks, and determine how much machine-generated text needs to be
observed in each scenario to reliably detect the watermark. We especially
highlight our human study, where we investigate the reliability of watermarking
when faced with human paraphrasing. We compare watermark-based detection to
other detection strategies, finding overall that watermarking is a reliable
solution, especially because of its sample complexity - for all attacks we
consider, the watermark evidence compounds the more examples are given, and the
watermark is eventually detected.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages in the main body. Code is available at
  https://github.com/jwkirchenbauer/lm-watermarking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Contrastive Lift: 3D Object Instance Segmentation by Slow-Fast
  Contrastive Fusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04633v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04633v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yash Bhalgat, Iro Laina, João F. Henriques, Andrew Zisserman, Andrea Vedaldi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instance segmentation in 3D is a challenging task due to the lack of
large-scale annotated datasets. In this paper, we show that this task can be
addressed effectively by leveraging instead 2D pre-trained models for instance
segmentation. We propose a novel approach to lift 2D segments to 3D and fuse
them by means of a neural field representation, which encourages multi-view
consistency across frames. The core of our approach is a slow-fast clustering
objective function, which is scalable and well-suited for scenes with a large
number of objects. Unlike previous approaches, our method does not require an
upper bound on the number of objects or object tracking across frames. To
demonstrate the scalability of the slow-fast clustering, we create a new
semi-realistic dataset called the Messy Rooms dataset, which features scenes
with up to 500 objects per scene. Our approach outperforms the state-of-the-art
on challenging scenes from the ScanNet, Hypersim, and Replica datasets, as well
as on our newly created Messy Rooms dataset, demonstrating the effectiveness
and scalability of our slow-fast clustering method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Yet Another Algorithm for Supervised Principal Component Analysis:
  Supervised Linear Centroid-Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomojit Ghosh, Michael Kirby
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new supervised dimensionality reduction technique called
Supervised Linear Centroid-Encoder (SLCE), a linear counterpart of the
nonlinear Centroid-Encoder (CE) \citep{ghosh2022supervised}. SLCE works by
mapping the samples of a class to its class centroid using a linear
transformation. The transformation is a projection that reconstructs a point
such that its distance from the corresponding class centroid, i.e.,
centroid-reconstruction loss, is minimized in the ambient space. We derive a
closed-form solution using an eigendecomposition of a symmetric matrix. We did
a detailed analysis and presented some crucial mathematical properties of the
proposed approach. %We also provide an iterative solution approach based
solving the optimization problem using a descent method. We establish a
connection between the eigenvalues and the centroid-reconstruction loss. In
contrast to Principal Component Analysis (PCA) which reconstructs a sample in
the ambient space, the transformation of SLCE uses the instances of a class to
rebuild the corresponding class centroid. Therefore the proposed method can be
considered a form of supervised PCA. Experimental results show the performance
advantage of SLCE over other supervised methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A novel algorithm for supervised PCA. 22 pages (including 2 reference
  pages), 8 figures and mathematical analysis of the proposed algorithm. The
  article is under review now</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Align, Distill, and Augment Everything All at Once for Imbalanced
  Semi-Supervised Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04621v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04621v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuel Sanchez Aimar, Hannah Helgesen, Michael Felsberg, Marco Kuhlmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Addressing the class imbalance in long-tailed semi-supervised learning (SSL)
poses a few significant challenges stemming from differences between the
marginal distributions of unlabeled data and the labeled data, as the former is
often unknown and potentially distinct from the latter. The first challenge is
to avoid biasing the pseudo-labels towards an incorrect distribution, such as
that of the labeled data or a balanced distribution, during training. However,
we still wish to ensure a balanced unlabeled distribution during inference,
which is the second challenge. To address both of these challenges, we propose
a three-faceted solution: a flexible distribution alignment that progressively
aligns the classifier from a dynamically estimated unlabeled prior towards a
balanced distribution, a soft consistency regularization that exploits
underconfident pseudo-labels discarded by threshold-based methods, and a schema
for expanding the unlabeled set with input data from the labeled partition.
This last facet comes in as a response to the commonly-overlooked fact that
disjoint partitions of labeled and unlabeled data prevent the benefits of
strong data augmentation on the labeled set. Our overall framework requires no
additional training cycles, so it will align, distill, and augment everything
all at once (ADALLO). Our extensive evaluations of ADALLO on imbalanced SSL
benchmark datasets, including CIFAR10-LT, CIFAR100-LT, and STL10-LT with
varying degrees of class imbalance, amount of labeled data, and distribution
mismatch, demonstrate significant improvements in the performance of imbalanced
SSL under large distribution mismatch, as well as competitiveness with
state-of-the-art methods when the labeled and unlabeled data follow the same
marginal distribution. Our code will be released upon paper acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review, 12 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Goal-conditioned GFlowNets for Controllable Multi-Objective Molecular
  Design 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04620v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04620v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julien Roy, Pierre-Luc Bacon, Christopher Pal, Emmanuel Bengio
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, in-silico molecular design has received much attention from
the machine learning community. When designing a new compound for
pharmaceutical applications, there are usually multiple properties of such
molecules that need to be optimised: binding energy to the target,
synthesizability, toxicity, EC50, and so on. While previous approaches have
employed a scalarization scheme to turn the multi-objective problem into a
preference-conditioned single objective, it has been established that this kind
of reduction may produce solutions that tend to slide towards the extreme
points of the objective space when presented with a problem that exhibits a
concave Pareto front. In this work we experiment with an alternative
formulation of goal-conditioned molecular generation to obtain a more
controllable conditional model that can uniformly explore solutions along the
entire Pareto front.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Out-of-distribution Robustness in NLP: Benchmark, Analysis,
  and LLMs Evaluations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04618v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04618v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lifan Yuan, Yangyi Chen, Ganqu Cui, Hongcheng Gao, Fangyuan Zou, Xingyi Cheng, Heng Ji, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper reexamines the research on out-of-distribution (OOD) robustness in
the field of NLP. We find that the distribution shift settings in previous
studies commonly lack adequate challenges, hindering the accurate evaluation of
OOD robustness. To address these issues, we propose a benchmark construction
protocol that ensures clear differentiation and challenging distribution
shifts. Then we introduce BOSS, a Benchmark suite for Out-of-distribution
robustneSS evaluation covering 5 tasks and 20 datasets. Based on BOSS, we
conduct a series of experiments on pre-trained language models for analysis and
evaluation of OOD robustness. First, for vanilla fine-tuning, we examine the
relationship between in-distribution (ID) and OOD performance. We identify
three typical types that unveil the inner learning mechanism, which could
potentially facilitate the forecasting of OOD robustness, correlating with the
advancements on ID datasets. Then, we evaluate 5 classic methods on BOSS and
find that, despite exhibiting some effectiveness in specific cases, they do not
offer significant improvement compared to vanilla fine-tuning. Further, we
evaluate 5 LLMs with various adaptation paradigms and find that when sufficient
ID data is available, fine-tuning domain-specific models outperform LLMs on ID
examples significantly. However, in the case of OOD instances, prioritizing
LLMs with in-context learning yields better results. We identify that both
fine-tuned small models and LLMs face challenges in effectively addressing
downstream tasks. The code is public at
\url{https://github.com/lifan-yuan/OOD_NLP}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code is available at \url{https://github.com/lifan-yuan/OOD_NLP}</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Uncovering solutions from data corrupted by systematic errors: A
  physics-constrained convolutional neural network approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Kelshaw, Luca Magri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Information on natural phenomena and engineering systems is typically
contained in data. Data can be corrupted by systematic errors in models and
experiments. In this paper, we propose a tool to uncover the spatiotemporal
solution of the underlying physical system by removing the systematic errors
from data. The tool is the physics-constrained convolutional neural network
(PC-CNN), which combines information from both the systems governing equations
and data. We focus on fundamental phenomena that are modelled by partial
differential equations, such as linear convection, Burgers equation, and
two-dimensional turbulence. First, we formulate the problem, describe the
physics-constrained convolutional neural network, and parameterise the
systematic error. Second, we uncover the solutions from data corrupted by large
multimodal systematic errors. Third, we perform a parametric study for
different systematic errors. We show that the method is robust. Fourth, we
analyse the physical properties of the uncovered solutions. We show that the
solutions inferred from the PC-CNN are physical, in contrast to the data
corrupted by systematic errors that does not fulfil the governing equations.
This work opens opportunities for removing epistemic errors from models, and
systematic errors from measurements.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language Models Get a Gender Makeover: Mitigating Gender Bias with
  Few-Shot Data Interventions <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Himanshu Thakur, Atishay Jain, Praneetha Vaddamanu, Paul Pu Liang, Louis-Philippe Morency
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Societal biases present in pre-trained large language models are a critical
issue as these models have been shown to propagate biases in countless
downstream applications, rendering them unfair towards specific groups of
people. Since large-scale retraining of these models from scratch is both time
and compute-expensive, a variety of approaches have been previously proposed
that de-bias a pre-trained model. While the majority of current
state-of-the-art debiasing methods focus on changes to the training regime, in
this paper, we propose data intervention strategies as a powerful yet simple
technique to reduce gender bias in pre-trained models. Specifically, we
empirically show that by fine-tuning a pre-trained model on only 10 de-biased
(intervened) training examples, the tendency to favor any gender is
significantly reduced. Since our proposed method only needs a few training
examples, our few-shot debiasing approach is highly feasible and practical.
Through extensive experimentation, we show that our debiasing technique
performs better than competitive state-of-the-art baselines with minimal loss
in language modeling ability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2023 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalization Across Observation Shifts in Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04595v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04595v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anuj Mahajan, Amy Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning policies which are robust to changes in the environment are critical
for real world deployment of Reinforcement Learning agents. They are also
necessary for achieving good generalization across environment shifts. We focus
on bisimulation metrics, which provide a powerful means for abstracting task
relevant components of the observation and learning a succinct representation
space for training the agent using reinforcement learning. In this work, we
extend the bisimulation framework to also account for context dependent
observation shifts. Specifically, we focus on the simulator based learning
setting and use alternate observations to learn a representation space which is
invariant to observation shifts using a novel bisimulation based objective.
This allows us to deploy the agent to varying observation settings during test
time and generalize to unseen scenarios. We further provide novel theoretical
bounds for simulator fidelity and performance transfer guarantees for using a
learnt policy to unseen shifts. Empirical analysis on the high-dimensional
image based control domains demonstrates the efficacy of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proximity-Informed Calibration for Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04590v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04590v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miao Xiong, Ailin Deng, Pang Wei Koh, Jiaying Wu, Shen Li, Jianqing Xu, Bryan Hooi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Confidence calibration is central to providing accurate and interpretable
uncertainty estimates, especially under safety-critical scenarios. However, we
find that existing calibration algorithms often overlook the issue of proximity
bias, a phenomenon where models tend to be more overconfident in low proximity
data (i.e., lying in the sparse region of the data distribution) compared to
high proximity samples, and thus suffer from inconsistent miscalibration across
different proximity samples. We examine the problem over pretrained ImageNet
models and observe that: 1) Proximity bias exists across a wide variety of
model architectures and sizes; 2) Transformer-based models are more susceptible
to proximity bias than CNN-based models; 3) Proximity bias persists even after
performing popular calibration algorithms like temperature scaling; 4) Models
tend to overfit more heavily on low proximity samples than on high proximity
samples. Motivated by the empirical findings, we propose ProCal, a
plug-and-play algorithm with a theoretical guarantee to adjust sample
confidence based on proximity. To further quantify the effectiveness of
calibration algorithms in mitigating proximity bias, we introduce
proximity-informed expected calibration error (PIECE) with theoretical
analysis. We show that ProCal is effective in addressing proximity bias and
improving calibration on balanced, long-tail, and distribution-shift settings
under four metrics over various model architectures.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Divide and Repair: Using Options to Improve Performance of Imitation
  Learning Against Adversarial Demonstrations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04581v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04581v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prithviraj Dasgupta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of learning to perform a task from demonstrations
given by teachers or experts, when some of the experts' demonstrations might be
adversarial and demonstrate an incorrect way to perform the task. We propose a
novel technique that can identify parts of demonstrated trajectories that have
not been significantly modified by the adversary and utilize them for learning,
using temporally extended policies or options. We first define a trajectory
divergence measure based on the spatial and temporal features of demonstrated
trajectories to detect and discard parts of the trajectories that have been
significantly modified by an adversarial expert, and, could degrade the
learner's performance, if used for learning, We then use an options-based
algorithm that partitions trajectories and learns only from the parts of
trajectories that have been determined as admissible. We provide theoretical
results of our technique to show that repairing partial trajectories improves
the sample efficiency of the demonstrations without degrading the learner's
performance. We then evaluate the proposed algorithm for learning to play an
Atari-like, computer-based game called LunarLander in the presence of different
types and degrees of adversarial attacks of demonstrated trajectories. Our
experimental results show that our technique can identify adversarially
modified parts of the demonstrated trajectories and successfully prevent the
learning performance from degrading due to adversarial demonstrations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages, 4 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recent applications of machine learning, remote sensing, and iot
  approaches in yield prediction: a critical <span class="highlight-title">review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatima Zahra Bassine, Terence Epule Epule, Ayoub Kechchour, Abdelghani Chehbouni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The integration of remote sensing and machine learning in agriculture is
transforming the industry by providing insights and predictions through data
analysis. This combination leads to improved yield prediction and water
management, resulting in increased efficiency, better yields, and more
sustainable agricultural practices. Achieving the United Nations' Sustainable
Development Goals, especially "zero hunger," requires the investigation of crop
yield and precipitation gaps, which can be accomplished through, the usage of
artificial intelligence (AI), machine learning (ML), remote sensing (RS), and
the internet of things (IoT). By integrating these technologies, a robust
agricultural mobile or web application can be developed, providing farmers and
decision-makers with valuable information and tools for improving crop
management and increasing efficiency. Several studies have investigated these
new technologies and their potential for diverse tasks such as crop monitoring,
yield prediction, irrigation management, etc. Through a critical review, this
paper reviews relevant articles that have used RS, ML, cloud computing, and IoT
in crop yield prediction. It reviews the current state-of-the-art in this field
by critically evaluating different machine-learning approaches proposed in the
literature for crop yield prediction and water management. It provides insights
into how these methods can improve decision-making in agricultural production
systems. This work will serve as a compendium for those interested in yield
prediction in terms of primary literature but, most importantly, what
approaches can be used for real-time and robust prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>35 pages, 12 figures, 14 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Chat<span class="highlight-title">GPT</span> is fun, but it is not funny! Humor is still challenging Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sophie Jentzsch, Kristian Kersting
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humor is a central aspect of human communication that has not been solved for
artificial agents so far. Large language models (LLMs) are increasingly able to
capture implicit and contextual information. Especially, OpenAI's ChatGPT
recently gained immense public attention. The GPT3-based model almost seems to
communicate on a human level and can even tell jokes. Humor is an essential
component of human communication. But is ChatGPT really funny? We put ChatGPT's
sense of humor to the test. In a series of exploratory experiments around
jokes, i.e., generation, explanation, and detection, we seek to understand
ChatGPT's capability to grasp and reproduce human humor. Since the model itself
is not accessible, we applied prompt-based experiments. Our empirical evidence
indicates that jokes are not hard-coded but mostly also not newly generated by
the model. Over 90% of 1008 generated jokes were the same 25 Jokes. The system
accurately explains valid jokes but also comes up with fictional explanations
for invalid jokes. Joke-typical characteristics can mislead ChatGPT in the
classification of jokes. ChatGPT has not solved computational humor yet but it
can be a big leap toward "funny" machines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ StudentEval: A Benchmark of Student-Written <span class="highlight-title">Prompt</span>s for Large Language
  Models of Code 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04556v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04556v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah McLean Babe, Sydney Nguyen, Yangtian Zi, Arjun Guha, Molly Q Feldman, Carolyn Jane Anderson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Code LLMs are being rapidly deployed and there is evidence that they can make
professional programmers more productive. Current benchmarks for code
generation measure whether models generate correct programs given an expert
prompt. In this paper, we present a new benchmark containing multiple prompts
per problem, written by a specific population of non-expert prompters:
beginning programmers. StudentEval contains 1,749 prompts for 48 problems,
written by 80 students who have only completed one semester of Python
programming. Our students wrote these prompts while working interactively with
a Code LLM, and we observed very mixed success rates. We use StudentEval to
evaluate 5 Code LLMs and find that StudentEval is a better discriminator of
model performance than existing benchmarks. We analyze the prompts and find
significant variation in students' prompting techniques. We also find that
nondeterministic LLM sampling could mislead students into thinking that their
prompts are more (or less) effective than they actually are, which has
implications for how to teach with Code LLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-Task Training with In-Domain Language Models for Diagnostic
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brihat Sharma, Yanjun Gao, Timothy Miller, Matthew M. Churpek, Majid Afshar, Dmitriy Dligach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative artificial intelligence (AI) is a promising direction for
augmenting clinical diagnostic decision support and reducing diagnostic errors,
a leading contributor to medical errors. To further the development of clinical
AI systems, the Diagnostic Reasoning Benchmark (DR.BENCH) was introduced as a
comprehensive generative AI framework, comprised of six tasks representing key
components in clinical reasoning. We present a comparative analysis of
in-domain versus out-of-domain language models as well as multi-task versus
single task training with a focus on the problem summarization task in DR.BENCH
(Gao et al., 2023). We demonstrate that a multi-task, clinically trained
language model outperforms its general domain counterpart by a large margin,
establishing a new state-of-the-art performance, with a ROUGE-L score of 28.55.
This research underscores the value of domain-specific training for optimizing
clinical diagnostic reasoning tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 2023 ClinicalNLP Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence of SARSA with linear function approximation: The random
  horizon case 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lina Palmborg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The reinforcement learning algorithm SARSA combined with linear function
approximation has been shown to converge for infinite horizon discounted Markov
decision problems (MDPs). In this paper, we investigate the convergence of the
algorithm for random horizon MDPs, which has not previously been shown. We
show, similar to earlier results for infinite horizon discounted MDPs, that if
the behaviour policy is $\varepsilon$-soft and Lipschitz continuous with
respect to the weight vector of the linear function approximation, with small
enough Lipschitz constant, then the algorithm will converge with probability
one when considering a random horizon MDP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Design Fundamentals of Diffusion Models: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyi Chang, George A. Koulieris, Hubert P. H. Shum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models are generative models, which gradually add and remove noise
to learn the underlying distribution of training data for data generation. The
components of diffusion models have gained significant attention with many
design choices proposed. Existing reviews have primarily focused on
higher-level solutions, thereby covering less on the design fundamentals of
components. This study seeks to address this gap by providing a comprehensive
and coherent review on component-wise design choices in diffusion models.
Specifically, we organize this review according to their three key components,
namely the forward process, the reverse process, and the sampling procedure.
This allows us to provide a fine-grained perspective of diffusion models,
benefiting future studies in the analysis of individual components, the
applicability of design choices, and the implementation of diffusion models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multimodal Learning Without Labeled Multimodal Data: Guarantees and
  Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04539v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04539v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Pu Liang, Chun Kai Ling, Yun Cheng, Alex Obolenskiy, Yudong Liu, Rohan Pandey, Alex Wilf, Louis-Philippe Morency, Ruslan Salakhutdinov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many machine learning systems that jointly learn from multiple modalities,
a core research question is to understand the nature of multimodal
interactions: the emergence of new task-relevant information during learning
from both modalities that was not present in either alone. We study this
challenge of interaction quantification in a semi-supervised setting with only
labeled unimodal data and naturally co-occurring multimodal data (e.g.,
unlabeled images and captions, video and corresponding audio) but when labeling
them is time-consuming. Using a precise information-theoretic definition of
interactions, our key contributions are the derivations of lower and upper
bounds to quantify the amount of multimodal interactions in this
semi-supervised setting. We propose two lower bounds based on the amount of
shared information between modalities and the disagreement between separately
trained unimodal classifiers, and derive an upper bound through connections to
approximate algorithms for min-entropy couplings. We validate these estimated
bounds and show how they accurately track true interactions. Finally, two
semi-supervised multimodal applications are explored based on these theoretical
results: (1) analyzing the relationship between multimodal performance and
estimated interactions, and (2) self-supervised learning that embraces
disagreement between modalities beyond agreement as is typically done.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code available at: https://github.com/pliang279/PID</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Git-Theta: A Git Extension for Collaborative Development of Machine
  Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04529v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04529v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikhil Kandpal, Brian Lester, Mohammed Muqeeth, Anisha Mascarenhas, Monty Evans, Vishal Baskaran, Tenghao Huang, Haokun Liu, Colin Raffel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Currently, most machine learning models are trained by centralized teams and
are rarely updated. In contrast, open-source software development involves the
iterative development of a shared artifact through distributed collaboration
using a version control system. In the interest of enabling collaborative and
continual improvement of machine learning models, we introduce Git-Theta, a
version control system for machine learning models. Git-Theta is an extension
to Git, the most widely used version control software, that allows fine-grained
tracking of changes to model parameters alongside code and other artifacts.
Unlike existing version control systems that treat a model checkpoint as a blob
of data, Git-Theta leverages the structure of checkpoints to support
communication-efficient updates, automatic model merges, and meaningful
reporting about the difference between two versions of a model. In addition,
Git-Theta includes a plug-in system that enables users to easily add support
for new functionality. In this paper, we introduce Git-Theta's design and
features and include an example use-case of Git-Theta where a pre-trained model
is continually adapted and modified. We publicly release Git-Theta in hopes of
kickstarting a new era of collaborative model development.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span>Bench: Towards Evaluating the Robustness of Large Language Models
  on Adversarial <span class="highlight-title">Prompt</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04528v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04528v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaijie Zhu, Jindong Wang, Jiaheng Zhou, Zichen Wang, Hao Chen, Yidong Wang, Linyi Yang, Wei Ye, Neil Zhenqiang Gong, Yue Zhang, Xing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing reliance on Large Language Models (LLMs) across academia and
industry necessitates a comprehensive understanding of their robustness to
prompts. In response to this vital need, we introduce PromptBench, a robustness
benchmark designed to measure LLMs' resilience to adversarial prompts. This
study uses a plethora of adversarial textual attacks targeting prompts across
multiple levels: character, word, sentence, and semantic. These prompts are
then employed in diverse tasks, such as sentiment analysis, natural language
inference, reading comprehension, machine translation, and math
problem-solving. Our study generates 4,032 adversarial prompts, meticulously
evaluated over 8 tasks and 13 datasets, with 567,084 test samples in total. Our
findings demonstrate that contemporary LLMs are vulnerable to adversarial
prompts. Furthermore, we present comprehensive analysis to understand the
mystery behind prompt robustness and its transferability. We then offer
insightful robustness analysis and pragmatic recommendations for prompt
composition, beneficial to both researchers and everyday users. We make our
code, prompts, and methodologies to generate adversarial prompts publicly
accessible, thereby enabling and encouraging collaborative exploration in this
pivotal field: https://github.com/microsoft/promptbench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical report; 23 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ContriMix: Unsupervised disentanglement of content and attribute for
  domain generalization in microscopy image analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tan H. Nguyen, Dinkar Juyal, Jin Li, Aaditya Prakash, Shima Nofallah, Chintan Shah, Sai Chowdary Gullapally, Michael Griffin, Anand Sampat, John Abel, Justin Lee, Amaro Taylor-Weiner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Domain generalization is critical for real-world applications of machine
learning models to microscopy images, including histopathology and fluorescence
imaging. Artifacts in histopathology arise through a complex combination of
factors relating to tissue collection and laboratory processing, as well as
factors intrinsic to patient samples. In fluorescence imaging, these artifacts
stem from variations across experimental batches. The complexity and subtlety
of these artifacts make the enumeration of data domains intractable. Therefore,
augmentation-based methods of domain generalization that require domain
identifiers and manual fine-tuning are inadequate in this setting. To overcome
this challenge, we introduce ContriMix, a domain generalization technique that
learns to generate synthetic images by disentangling and permuting the
biological content ("content") and technical variations ("attributes") in
microscopy images. ContriMix does not rely on domain identifiers or handcrafted
augmentations and makes no assumptions about the input characteristics of
images. We assess the performance of ContriMix on two pathology datasets
(Camelyon17-WILDS and a prostate cell classification dataset) and one
fluorescence microscopy dataset (RxRx1-WILDS). ContriMix outperforms current
state-of-the-art methods in all datasets, motivating its usage for microscopy
image analysis in real-world settings where domain information is hard to come
by.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Estimating Koopman operators with sketching to provably learn large
  scale dynamical systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giacomo Meanti, Antoine Chatalic, Vladimir R. Kostic, Pietro Novelli, Massimiliano Pontil, Lorenzo Rosasco
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The theory of Koopman operators allows to deploy non-parametric machine
learning algorithms to predict and analyze complex dynamical systems.
Estimators such as principal component regression (PCR) or reduced rank
regression (RRR) in kernel spaces can be shown to provably learn Koopman
operators from finite empirical observations of the system's time evolution.
Scaling these approaches to very long trajectories is a challenge and requires
introducing suitable approximations to make computations feasible. In this
paper, we boost the efficiency of different kernel-based Koopman operator
estimators using random projections (sketching). We derive, implement and test
the new "sketched" estimators with extensive experiments on synthetic and
large-scale molecular dynamics datasets. Further, we establish non asymptotic
error bounds giving a sharp characterization of the trade-offs between
statistical learning rates and computational efficiency. Our empirical and
theoretical analysis shows that the proposed estimators provide a sound and
efficient way to learn large scale dynamical systems. In particular our
experiments indicate that the proposed estimators retain the same accuracy of
PCR or RRR, while being much faster.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sample-Level Weighting for Multi-Task Learning with Auxiliary Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04519v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04519v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emilie Grégoire, Hafeez Chaudhary, Sam Verboven
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-task learning (MTL) can improve the generalization performance of
neural networks by sharing representations with related tasks. Nonetheless, MTL
can also degrade performance through harmful interference between tasks. Recent
work has pursued task-specific loss weighting as a solution for this
interference. However, existing algorithms treat tasks as atomic, lacking the
ability to explicitly separate harmful and helpful signals beyond the task
level. To this end, we propose SLGrad, a sample-level weighting algorithm for
multi-task learning with auxiliary tasks. Through sample-specific task weights,
SLGrad reshapes the task distributions during training to eliminate harmful
auxiliary signals and augment useful task signals. Substantial generalization
performance gains are observed on (semi-) synthetic datasets and common
supervised multi-task problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal sensor placement for reconstructing wind pressure field around
  buildings using compressed sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04518v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04518v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xihaier Luo, Ahsan Kareem, Shinjae Yoo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deciding how to optimally deploy sensors in a large, complex, and spatially
extended structure is critical to ensure that the surface pressure field is
accurately captured for subsequent analysis and design. In some cases,
reconstruction of missing data is required in downstream tasks such as the
development of digital twins. This paper presents a data-driven sparse sensor
selection algorithm, aiming to provide the most information contents for
reconstructing aerodynamic characteristics of wind pressures over tall building
structures parsimoniously. The algorithm first fits a set of basis functions to
the training data, then applies a computationally efficient QR algorithm that
ranks existing pressure sensors in order of importance based on the state
reconstruction to this tailored basis. The findings of this study show that the
proposed algorithm successfully reconstructs the aerodynamic characteristics of
tall buildings from sparse measurement locations, generating stable and optimal
solutions across a range of conditions. As a result, this study serves as a
promising first step toward leveraging the success of data-driven and machine
learning algorithms to supplement traditional genetic algorithms currently used
in wind engineering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>31 pages, 19 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving neural network representations using human similarity
  judgments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Muttenthaler, Lorenz Linhardt, Jonas Dippel, Robert A. Vandermeulen, Katherine Hermann, Andrew K. Lampinen, Simon Kornblith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks have reached human-level performance on many computer
vision tasks. However, the objectives used to train these networks enforce only
that similar images are embedded at similar locations in the representation
space, and do not directly constrain the global structure of the resulting
space. Here, we explore the impact of supervising this global structure by
linearly aligning it with human similarity judgments. We find that a naive
approach leads to large changes in local representational structure that harm
downstream performance. Thus, we propose a novel method that aligns the global
structure of representations while preserving their local structure. This
global-local transform considerably improves accuracy across a variety of
few-shot learning and anomaly detection tasks. Our results indicate that human
visual representations are globally organized in a way that facilitates
learning from few examples, and incorporating this global structure into neural
network representations improves performance on downstream tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hardness of Deceptive Certificate Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04505v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04505v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Stephan Wäldchen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress towards theoretical interpretability guarantees for AI has
been made with classifiers that are based on interactive proof systems. A
prover selects a certificate from the datapoint and sends it to a verifier who
decides the class. In the context of machine learning, such a certificate can
be a feature that is informative of the class. For a setup with high soundness
and completeness, the exchanged certificates must have a high mutual
information with the true class of the datapoint. However, this guarantee
relies on a bound on the Asymmetric Feature Correlation of the dataset, a
property that so far is difficult to estimate for high-dimensional data. It was
conjectured in W\"aldchen et al. that it is computationally hard to exploit the
AFC, which is what we prove here.
  We consider a malicious prover-verifier duo that aims to exploit the AFC to
achieve high completeness and soundness while using uninformative certificates.
We show that this task is $\mathsf{NP}$-hard and cannot be approximated better
than $\mathcal{O}(m^{1/8 - \epsilon})$, where $m$ is the number of possible
certificates, for $\epsilon>0$ under the Dense-vs-Random conjecture. This is
some evidence that AFC should not prevent the use of interactive classification
for real-world tasks, as it is computationally hard to be exploited.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of Chat<span class="highlight-title">GPT</span> on Biomedical Tasks: A Zero-Shot Comparison with
  Fine-Tuned Generative <span class="highlight-title">Transformer</span>s <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04504v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04504v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Israt Jahan, Md Tahmid Rahman Laskar, Chun Peng, Jimmy Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  ChatGPT is a large language model developed by OpenAI. Despite its impressive
performance across various tasks, no prior work has investigated its capability
in the biomedical domain yet. To this end, this paper aims to evaluate the
performance of ChatGPT on various benchmark biomedical tasks, such as relation
extraction, document classification, question answering, and summarization. To
the best of our knowledge, this is the first work that conducts an extensive
evaluation of ChatGPT in the biomedical domain. Interestingly, we find based on
our evaluation that in biomedical datasets that have smaller training sets,
zero-shot ChatGPT even outperforms the state-of-the-art fine-tuned generative
transformer models, such as BioGPT and BioBART. This suggests that ChatGPT's
pre-training on large text corpora makes it quite specialized even in the
biomedical domain. Our findings demonstrate that ChatGPT has the potential to
be a valuable tool for various tasks in the biomedical domain that lack large
annotated data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by BioNLP@ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning with Noisy Labels by Adaptive Gradient-Based Outlier Removal <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasiia Sedova, Lena Zellinger, Benjamin Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An accurate and substantial dataset is necessary to train a reliable and
well-performing model. However, even manually labeled datasets contain errors,
not to mention automatically labeled ones. The problem of data denoising was
addressed in different existing research, most of which focuses on the
detection of outliers and their permanent removal - a process that is likely to
over- or underfilter the dataset. In this work, we propose AGRA: a new method
for Adaptive GRAdient-based outlier removal. Instead of cleaning the dataset
prior to model training, the dataset is adjusted during the training process.
By comparing the aggregated gradient of a batch of samples and an individual
example gradient, our method dynamically decides whether a corresponding
example is helpful for the model at this point or is counter-productive and
should be left out for the current update. Extensive evaluation on several
datasets demonstrates the AGRA effectiveness, while comprehensive results
analysis supports our initial hypothesis: permanent hard outlier removal is not
always what model benefits the most from.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for ECML PKDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Fair Multi-Agent Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04498v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04498v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amir Leshem
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the problem of fair multi-agent multi-arm bandit
learning when agents do not communicate with each other, except collision
information, provided to agents accessing the same arm simultaneously. We
provide an algorithm with regret $O\left(N^3 \log N \log T \right)$ (assuming
bounded rewards, with unknown bound). This significantly improves previous
results which had regret of order $O(\log T \log\log T)$ and exponential
dependence on the number of agents. The result is attained by using a
distributed auction algorithm to learn the sample-optimal matching, a new type
of exploitation phase whose length is derived from the observed samples, and a
novel order-statistics-based regret analysis. Simulation results present the
dependence of the regret on $\log T$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Limits, approximation and size transferability for GNNs on sparse graphs
  via graphops <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04495v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04495v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thien Le, Stefanie Jegelka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can graph neural networks generalize to graphs that are different from the
graphs they were trained on, e.g., in size? In this work, we study this
question from a theoretical perspective. While recent work established such
transferability and approximation results via graph limits, e.g., via graphons,
these only apply non-trivially to dense graphs. To include frequently
encountered sparse graphs such as bounded-degree or power law graphs, we take a
perspective of taking limits of operators derived from graphs, such as the
aggregation operation that makes up GNNs. This leads to the recently introduced
limit notion of graphops (Backhausz and Szegedy, 2022). We demonstrate how the
operator perspective allows us to develop quantitative bounds on the distance
between a finite GNN and its limit on an infinite graph, as well as the
distance between the GNN on graphs of different sizes that share structural
properties, under a regularity assumption verified for various graph sequences.
Our results hold for dense and sparse graphs, and various notions of graph
limits.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NeurIPS 2023 submission, 34 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair Column Subset Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04489v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04489v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonis Matakos, Bruno Ordozgoiti, Suhas Thejaswi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of fair column subset selection. In particular, we
assume that two groups are present in the data, and the chosen column subset
must provide a good approximation for both, relative to their respective best
rank-k approximations. We show that this fair setting introduces significant
challenges: in order to extend known results, one cannot do better than the
trivial solution of simply picking twice as many columns as the original
methods. We adopt a known approach based on deterministic leverage-score
sampling, and show that merely sampling a subset of appropriate size becomes
NP-hard in the presence of two groups. Whereas finding a subset of two times
the desired size is trivial, we provide an efficient algorithm that achieves
the same guarantees with essentially 1.5 times that size. We validate our
methods through an extensive set of experiments on real-world data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rewarded soups: towards Pareto-optimal alignment by interpolating
  weights fine-tuned on diverse rewards 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04488v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04488v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Rame, Guillaume Couairon, Mustafa Shukor, Corentin Dancette, Jean-Baptiste Gaya, Laure Soulier, Matthieu Cord
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Foundation models are first pre-trained on vast unsupervised datasets and
then fine-tuned on labeled data. Reinforcement learning, notably from human
feedback (RLHF), can further align the network with the intended usage. Yet the
imperfections in the proxy reward may hinder the training and lead to
suboptimal results; the diversity of objectives in real-world tasks and human
opinions exacerbate the issue. This paper proposes embracing the heterogeneity
of diverse rewards by following a multi-policy strategy. Rather than focusing
on a single a priori reward, we aim for Pareto-optimal generalization across
the entire space of preferences. To this end, we propose rewarded soup, first
specializing multiple networks independently (one for each proxy reward) and
then interpolating their weights linearly. This succeeds empirically because we
show that the weights remain linearly connected when fine-tuned on diverse
rewards from a shared pre-trained initialization. We demonstrate the
effectiveness of our approach for text-to-text (summarization, Q&A, helpful
assistant, review), text-image (image captioning, text-to-image generation,
visual grounding, VQA), and control (locomotion) tasks. We hope to enhance the
alignment of deep models, and how they interact with the world in all its
diversity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Training-Free Neural Active Learning with Initialization-Robustness
  Guarantees <span class="chip">ICML
  2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04454v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04454v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Apivich Hemachandra, Zhongxiang Dai, Jasraj Singh, See-Kiong Ng, Bryan Kian Hsiang Low
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing neural active learning algorithms have aimed to optimize the
predictive performance of neural networks (NNs) by selecting data for
labelling. However, other than a good predictive performance, being robust
against random parameter initializations is also a crucial requirement in
safety-critical applications. To this end, we introduce our expected variance
with Gaussian processes (EV-GP) criterion for neural active learning, which is
theoretically guaranteed to select data points which lead to trained NNs with
both (a) good predictive performances and (b) initialization robustness.
Importantly, our EV-GP criterion is training-free, i.e., it does not require
any training of the NN during data selection, which makes it computationally
efficient. We empirically demonstrate that our EV-GP criterion is highly
correlated with both initialization robustness and generalization performance,
and show that it consistently outperforms baseline methods in terms of both
desiderata, especially in situations with limited initial data or large batch
sizes.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to 40th International Conference on Machine Learning (ICML
  2023), 41 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multi-modal Latent Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04445v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04445v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mustapha Bounoua, Giulio Franzese, Pietro Michiardi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multi-modal data-sets are ubiquitous in modern applications, and multi-modal
Variational Autoencoders are a popular family of models that aim to learn a
joint representation of the different modalities. However, existing approaches
suffer from a coherence-quality tradeoff, where models with good generation
quality lack generative coherence across modalities, and vice versa. We discuss
the limitations underlying the unsatisfactory performance of existing methods,
to motivate the need for a different approach. We propose a novel method that
uses a set of independently trained, uni-modal, deterministic autoencoders.
Individual latent variables are concatenated into a common latent space, which
is fed to a masked diffusion model to enable generative modeling. We also
introduce a new multi-time training method to learn the conditional score
network for multi-modal diffusion. Our methodology substantially outperforms
competitors in both generation quality and coherence, as shown through an
extensive experimental campaign.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Optimal Locally Private Mean Estimation via Random Projections 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hilal Asi, Vitaly Feldman, Jelani Nelson, Huy L. Nguyen, Kunal Talwar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of locally private mean estimation of high-dimensional
vectors in the Euclidean ball. Existing algorithms for this problem either
incur sub-optimal error or have high communication and/or run-time complexity.
We propose a new algorithmic framework, ProjUnit, for private mean estimation
that yields algorithms that are computationally efficient, have low
communication complexity, and incur optimal error up to a $1+o(1)$-factor. Our
framework is deceptively simple: each randomizer projects its input to a random
low-dimensional subspace, normalizes the result, and then runs an optimal
algorithm such as PrivUnitG in the lower-dimensional space. In addition, we
show that, by appropriately correlating the random projection matrices across
devices, we can achieve fast server run-time. We mathematically analyze the
error of the algorithm in terms of properties of the random projections, and
study two instantiations. Lastly, our experiments for private mean estimation
and private federated learning demonstrate that our algorithms empirically
obtain nearly the same utility as optimal ones while having significantly lower
communication and computational cost.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dual policy as self-model for planning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04440v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04440v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaesung Yoo, Fernanda de la Torre, Robert Guangyu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Planning is a data efficient decision-making strategy where an agent selects
candidate actions by exploring possible future states. To simulate future
states when there is a high-dimensional action space, the knowledge of one's
decision making strategy must be used to limit the number of actions to be
explored. We refer to the model used to simulate one's decisions as the agent's
self-model. While self-models are implicitly used widely in conjunction with
world models to plan actions, it remains unclear how self-models should be
designed. Inspired by current reinforcement learning approaches and
neuroscience, we explore the benefits and limitations of using a distilled
policy network as the self-model. In such dual-policy agents, a model-free
policy and a distilled policy are used for model-free actions and planned
actions, respectively. Our results on a ecologically relevant, parametric
environment indicate that distilled policy network for self-model stabilizes
training, has faster inference than using model-free policy, promotes better
exploration, and could learn a comprehensive understanding of its own
behaviors, at the cost of distilling a new network apart from the model-free
policy.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Faithful Knowledge Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04431v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04431v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom A. Lamb, Rudy Brunel,  Krishnamurthy,  Dvijotham, M. Pawan Kumar, Philip H. S. Torr, Francisco Eiras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge distillation (KD) has received much attention due to its success in
compressing networks to allow for their deployment in resource-constrained
systems. While the problem of adversarial robustness has been studied before in
the KD setting, previous works overlook what we term the relative calibration
of the student network with respect to its teacher in terms of soft
confidences. In particular, we focus on two crucial questions with regard to a
teacher-student pair: (i) do the teacher and student disagree at points close
to correctly classified dataset examples, and (ii) is the distilled student as
confident as the teacher around dataset examples? These are critical questions
when considering the deployment of a smaller student network trained from a
robust teacher within a safety-critical setting. To address these questions, we
introduce a faithful imitation framework to discuss the relative calibration of
confidences, as well as provide empirical and certified methods to evaluate the
relative calibration of a student w.r.t. its teacher. Further, to verifiably
align the relative calibration incentives of the student to those of its
teacher, we introduce faithful distillation. Our experiments on the MNIST and
Fashion-MNIST datasets demonstrate the need for such an analysis and the
advantages of the increased verifiability of faithful distillation over
alternative adversarial distillation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12pgs (main content), 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Balancing of competitive two-player Game Levels with Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Rupp, Manuel Eberhardinger, Kai Eckert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The balancing process for game levels in a competitive two-player context
involves a lot of manual work and testing, particularly in non-symmetrical game
levels. In this paper, we propose an architecture for automated balancing of
tile-based levels within the recently introduced PCGRL framework (procedural
content generation via reinforcement learning). Our architecture is divided
into three parts: (1) a level generator, (2) a balancing agent and, (3) a
reward modeling simulation. By playing the level in a simulation repeatedly,
the balancing agent is rewarded for modifying it towards the same win rates for
all players. To this end, we introduce a novel family of swap-based
representations to increase robustness towards playability. We show that this
approach is capable to teach an agent how to alter a level for balancing better
and faster than plain PCGRL. In addition, by analyzing the agent's swapping
behavior, we can draw conclusions about which tile types influence the
balancing most. We test and show our results using the Neural MMO (NMMO)
environment in a competitive two-player setting.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 8 figures, 1 table. Accepted at IEEE Conference on Games
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards High-Performance Exploratory Data Analysis (EDA) Via Stable
  Equilibrium Point 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04425v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04425v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxuan Song, Yongyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploratory data analysis (EDA) is a vital procedure for data science
projects. In this work, we introduce a stable equilibrium point (SEP) - based
framework for improving the efficiency and solution quality of EDA. By
exploiting the SEPs to be the representative points, our approach aims to
generate high-quality clustering and data visualization for large-scale data
sets. A very unique property of the proposed method is that the SEPs will
directly encode the clustering properties of data sets. Compared with prior
state-of-the-art clustering and data visualization methods, the proposed
methods allow substantially improving computing efficiency and solution quality
for large-scale data analysis tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Computing Optimal Tree Ensembles <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04423v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04423v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christian Komusiewicz, Pascal Kunz, Frank Sommer, Manuel Sorge
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Random forests and, more generally, (decision\nobreakdash-)tree ensembles are
widely used methods for classification and regression. Recent algorithmic
advances allow to compute decision trees that are optimal for various measures
such as their size or depth. We are not aware of such research for tree
ensembles and aim to contribute to this area. Mainly, we provide two novel
algorithms and corresponding lower bounds. First, we are able to carry over and
substantially improve on tractability results for decision trees, obtaining a
$(6\delta D S)^S \cdot poly$-time algorithm, where $S$ is the number of cuts in
the tree ensemble, $D$ the largest domain size, and $\delta$ is the largest
number of features in which two examples differ. To achieve this, we introduce
the witness-tree technique which also seems promising for practice. Second, we
show that dynamic programming, which has been successful for decision trees,
may also be viable for tree ensembles, providing an $\ell^n \cdot poly$-time
algorithm, where $\ell$ is the number of trees and $n$ the number of examples.
Finally, we compare the number of cuts necessary to classify training data sets
for decision trees and tree ensembles, showing that ensembles may need
exponentially fewer cuts for increasing number of trees.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generalized Teacher Forcing for Learning Chaotic Dynamics <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04406v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04406v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Hess, Zahra Monfared, Manuel Brenner, Daniel Durstewitz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chaotic dynamical systems (DS) are ubiquitous in nature and society. Often we
are interested in reconstructing such systems from observed time series for
prediction or mechanistic insight, where by reconstruction we mean learning
geometrical and invariant temporal properties of the system in question (like
attractors). However, training reconstruction algorithms like recurrent neural
networks (RNNs) on such systems by gradient-descent based techniques faces
severe challenges. This is mainly due to exploding gradients caused by the
exponential divergence of trajectories in chaotic systems. Moreover, for
(scientific) interpretability we wish to have as low dimensional
reconstructions as possible, preferably in a model which is mathematically
tractable. Here we report that a surprisingly simple modification of teacher
forcing leads to provably strictly all-time bounded gradients in training on
chaotic systems, and, when paired with a simple architectural rearrangement of
a tractable RNN design, piecewise-linear RNNs (PLRNNs), allows for faithful
reconstruction in spaces of at most the dimensionality of the observed system.
We show on several DS that with these amendments we can reconstruct DS better
than current SOTA algorithms, in much lower dimensions. Performance differences
were particularly compelling on real world data with which most other methods
severely struggled. This work thus led to a simple yet powerful DS
reconstruction algorithm which is highly interpretable at the same time.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the Proceedings of the 40th International
  Conference on Machine Learning (ICML 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Policy-Based Self-Competition for Planning Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04403v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04403v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonathan Pirnay, Quirin Göttl, Jakob Burger, Dominik Gerhard Grimm
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  AlphaZero-type algorithms may stop improving on single-player tasks in case
the value network guiding the tree search is unable to approximate the outcome
of an episode sufficiently well. One technique to address this problem is
transforming the single-player task through self-competition. The main idea is
to compute a scalar baseline from the agent's historical performances and to
reshape an episode's reward into a binary output, indicating whether the
baseline has been exceeded or not. However, this baseline only carries limited
information for the agent about strategies how to improve. We leverage the idea
of self-competition and directly incorporate a historical policy into the
planning process instead of its scalar performance. Based on the recently
introduced Gumbel AlphaZero (GAZ), we propose our algorithm GAZ 'Play-to-Plan'
(GAZ PTP), in which the agent learns to find strong trajectories by planning
against possible strategies of its past self. We show the effectiveness of our
approach in two well-known combinatorial optimization problems, the Traveling
Salesman Problem and the Job-Shop Scheduling Problem. With only half of the
simulation budget for search, GAZ PTP consistently outperforms all selected
single-player variants of GAZ.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Fair Classifier Embracing Triplet Collapse 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04400v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04400v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Martzloff, N. Posocco, Q. Ferré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the behaviour of the triplet loss and show that it
can be exploited to limit the biases created and perpetuated by machine
learning models. Our fair classifier uses the collapse of the triplet loss when
its margin is greater than the maximum distance between two points in the
latent space, in the case of stochastic triplet selection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 7 figures, CAp2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Diffusion-based Image Translation using Asymmetric Gradient
  Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04396v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04396v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gihyun Kwon, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have shown significant progress in image translation tasks
recently. However, due to their stochastic nature, there's often a trade-off
between style transformation and content preservation. Current strategies aim
to disentangle style and content, preserving the source image's structure while
successfully transitioning from a source to a target domain under text or
one-shot image conditions. Yet, these methods often require computationally
intense fine-tuning of diffusion models or additional neural networks. To
address these challenges, here we present an approach that guides the reverse
process of diffusion sampling by applying asymmetric gradient guidance. This
results in quicker and more stable image manipulation for both text-guided and
image-guided image translation. Our model's adaptability allows it to be
implemented with both image- and latent-diffusion models. Experiments show that
our method outperforms various state-of-the-art models in image translation
tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multilingual Clinical NER: Translation or Cross-lingual Transfer? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04384v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04384v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xavier Fontaine, Félix Gaschi, Parisa Rastin, Yannick Toussaint
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language tasks like Named Entity Recognition (NER) in the clinical
domain on non-English texts can be very time-consuming and expensive due to the
lack of annotated data. Cross-lingual transfer (CLT) is a way to circumvent
this issue thanks to the ability of multilingual large language models to be
fine-tuned on a specific task in one language and to provide high accuracy for
the same task in another language. However, other methods leveraging
translation models can be used to perform NER without annotated data in the
target language, by either translating the training set or test set. This paper
compares cross-lingual transfer with these two alternative methods, to perform
clinical NER in French and in German without any training data in those
languages. To this end, we release MedNERF a medical NER test set extracted
from French drug prescriptions and annotated with the same guidelines as an
English dataset. Through extensive experiments on this dataset and on a German
medical dataset (Frei and Kramer, 2021), we show that translation-based methods
can achieve similar performance to CLT but require more care in their design.
And while they can take advantage of monolingual clinical language models,
those do not guarantee better results than large general-purpose multilingual
models, whether with cross-lingual transfer or translation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, Proceedings of the 5th Clinical Natural Language Processing
  Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Get More for Less in Decentralized Learning Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akash Dhasade, Anne-Marie Kermarrec, Rafael Pires, Rishi Sharma, Milos Vujasinovic, Jeffrey Wigger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized learning (DL) systems have been gaining popularity because they
avoid raw data sharing by communicating only model parameters, hence preserving
data confidentiality. However, the large size of deep neural networks poses a
significant challenge for decentralized training, since each node needs to
exchange gigabytes of data, overloading the network. In this paper, we address
this challenge with JWINS, a communication-efficient and fully decentralized
learning system that shares only a subset of parameters through sparsification.
JWINS uses wavelet transform to limit the information loss due to
sparsification and a randomized communication cut-off that reduces
communication usage without damaging the performance of trained models. We
demonstrate empirically with 96 DL nodes on non-IID datasets that JWINS can
achieve similar accuracies to full-sharing DL while sending up to 64% fewer
bytes. Additionally, on low communication budgets, JWINS outperforms the
state-of-the-art communication-efficient DL algorithm CHOCO-SGD by up to 4x in
terms of network savings and time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Label Shift Quantification with Robustness Guarantees via Distribution
  Feature Matching <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04376v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04376v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bastien Dussap, Gilles Blanchard, Badr-Eddine Chérief-Abdellatif
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantification learning deals with the task of estimating the target label
distribution under label shift. In this paper, we first present a unifying
framework, distribution feature matching (DFM), that recovers as particular
instances various estimators introduced in previous literature. We derive a
general performance bound for DFM procedures, improving in several key aspects
upon previous bounds derived in particular cases. We then extend this analysis
to study robustness of DFM procedures in the misspecified setting under
departure from the exact label shift hypothesis, in particular in the case of
contamination of the target by an unknown distribution. These theoretical
findings are confirmed by a detailed numerical study on simulated and
real-world datasets. We also introduce an efficient, scalable and robust
version of kernel-based DFM using the Random Fourier Feature principle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the European Conference on Machine Learning and
  Principles and Practice of Knowledge Discovery in Databases (ECML) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning via Wasserstein-Based High Probability Generalisation Bounds 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04375v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04375v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul Viallard, Maxime Haddouche, Umut Simsekli, Benjamin Guedj
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Minimising upper bounds on the population risk or the generalisation gap has
been widely used in structural risk minimisation (SRM) - this is in particular
at the core of PAC-Bayesian learning. Despite its successes and unfailing surge
of interest in recent years, a limitation of the PAC-Bayesian framework is that
most bounds involve a Kullback-Leibler (KL) divergence term (or its
variations), which might exhibit erratic behavior and fail to capture the
underlying geometric structure of the learning problem - hence restricting its
use in practical applications. As a remedy, recent studies have attempted to
replace the KL divergence in the PAC-Bayesian bounds with the Wasserstein
distance. Even though these bounds alleviated the aforementioned issues to a
certain extent, they either hold in expectation, are for bounded losses, or are
nontrivial to minimize in an SRM framework. In this work, we contribute to this
line of research and prove novel Wasserstein distance-based PAC-Bayesian
generalisation bounds for both batch learning with independent and identically
distributed (i.i.d.) data, and online learning with potentially non-i.i.d.
data. Contrary to previous art, our bounds are stronger in the sense that (i)
they hold with high probability, (ii) they apply to unbounded (potentially
heavy-tailed) losses, and (iii) they lead to optimizable training objectives
that can be used in SRM. As a result we derive novel Wasserstein-based
PAC-Bayesian learning algorithms and we illustrate their empirical advantage on
a variety of experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Label Aware Speech Representation Learning For Language Identification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04374v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04374v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shikhar Vashishth, Shikhar Bharadwaj, Sriram Ganapathy, Ankur Bapna, Min Ma, Wei Han, Vera Axelrod, Partha Talukdar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech representation learning approaches for non-semantic tasks such as
language recognition have either explored supervised embedding extraction
methods using a classifier model or self-supervised representation learning
approaches using raw data. In this paper, we propose a novel framework of
combining self-supervised representation learning with the language label
information for the pre-training task. This framework, termed as Label Aware
Speech Representation (LASR) learning, uses a triplet based objective function
to incorporate language labels along with the self-supervised loss function.
The speech representations are further fine-tuned for the downstream task. The
language recognition experiments are performed on two public datasets - FLEURS
and Dhwani. In these experiments, we illustrate that the proposed LASR
framework improves over the state-of-the-art systems on language
identification. We also report an analysis of the robustness of LASR approach
to noisy/missing labels as well as its application to multi-lingual speech
recognition tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Recruitment Strategy for Collaborative Mobile Crowd Sensing
  Based on GCN Trustworthiness Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04366v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04366v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongwei Zhan, Yingjie Wang, Peiyong Duan, Akshita Maradapu Vera Venkata Sai, Zhaowei Liu, Chaocan Xiang, Xiangrong Tong, Weilong Wang, Zhipeng Cai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Collaborative Mobile Crowd Sensing (CMCS) enhances data quality and coverage
by promoting teamwork in task sensing, with worker recruitment representing a
complex multi-objective optimization problem. Existing strategies mainly focus
on the characteristics of workers themselves, neglecting the asymmetric trust
relationships between them, which affects the rationality of task utility
evaluation. To address this, this paper first employs the Mini-Batch K-Means
clustering algorithm and deploys edge servers to enable efficient distributed
worker recruitment. Historical data and task requirements are utilized to
obtain workers' ability types and distances. A trust-directed graph in the
worker's social network is input into the Graph Convolutional Network (GCN)
framework for training, capturing asymmetric trustworthiness between worker
pairs. Privacy leakage is prevented in CMCS scenarios through high trust values
between workers. Ultimately, an undirected recruitment graph is constructed
using workers' abilities, trust values, and distance weights, transforming the
worker recruitment problem into a Maximum Weight Average Subgraph Problem
(MWASP). A Tabu Search Recruitment (TSR) algorithm is proposed to rationally
recruit a balanced multi-objective optimal task utility worker set for each
task. Extensive simulation experiments on four real-world datasets demonstrate
the effectiveness of the proposed strategy, outperforming other strategies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Edge conductivity in PtSe$_2$ nanostructures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04365v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04365v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roman Kempt, Agnieszka Kuc, Thomas Brumme, Thomas Heine
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  PtSe$_2$ is a promising 2D material for nanoelectromechanical sensing and
photodetection in the infrared regime. One of its most compelling features is
the facile synthesis at temperatures below 500 {\deg}C, which is compatible
with current back-end-of-line semiconductor processing. However, this process
generates polycrystalline thin films with nanoflake-like domains of 5 to 100 nm
size. To investigate the lateral quantum confinement effect in this size
regime, we train a deep neural network to obtain an interatomic potential at
DFT accuracy and use that to model ribbons, surfaces, nanoflakes, and
nanoplatelets of PtSe$_2$ with lateral widths between 5 to 15 nm. We determine
which edge terminations are the most stable and find evidence that the
electrical conductivity is localized on the edges for lateral sizes below 10
nm. This suggests that the transport channels in thin films of PtSe$_2$ might
be dominated by networks of edges, instead of transport through the layers
themselves.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian Optimisation Against Climate Change: Applications and
  Benchmarks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04343v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04343v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sigrid Passano Hellan, Christopher G. Lucas, Nigel H. Goddard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian optimisation is a powerful method for optimising black-box
functions, popular in settings where the true function is expensive to evaluate
and no gradient information is available. Bayesian optimisation can improve
responses to many optimisation problems within climate change for which
simulator models are unavailable or expensive to sample from. While there have
been several feasibility demonstrations of Bayesian optimisation in
climate-related applications, there has been no unifying review of applications
and benchmarks. We provide such a review here, to encourage the use of Bayesian
optimisation in important and well-suited application domains. We identify four
main application domains: material discovery, wind farm layout, optimal
renewable control and environmental monitoring. For each domain we identify a
public benchmark or data set that is easy to use and evaluate systems against,
while being representative of real-world problems. Due to the lack of a
suitable benchmark for environmental monitoring, we propose LAQN-BO, based on
air pollution data. Our contributions are: a) identifying a representative
range of benchmarks, providing example code where necessary; b) introducing a
new benchmark, LAQN-BO; and c) promoting a wider use of climate change
applications among Bayesian optimisation practitioners.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unpaired Deep Learning for Pharmacokinetic Parameter Estimation from
  Dynamic Contrast-Enhanced MRI 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04339v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04339v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gyutaek Oh, Won-Jin Moon, Jong Chul Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  DCE-MRI provides information about vascular permeability and tissue perfusion
through the acquisition of pharmacokinetic parameters. However, traditional
methods for estimating these pharmacokinetic parameters involve fitting tracer
kinetic models, which often suffer from computational complexity and low
accuracy due to noisy arterial input function (AIF) measurements. Although some
deep learning approaches have been proposed to tackle these challenges, most
existing methods rely on supervised learning that requires paired input DCE-MRI
and labeled pharmacokinetic parameter maps. This dependency on labeled data
introduces significant time and resource constraints, as well as potential
noise in the labels, making supervised learning methods often impractical. To
address these limitations, here we present a novel unpaired deep learning
method for estimating both pharmacokinetic parameters and the AIF using a
physics-driven CycleGAN approach. Our proposed CycleGAN framework is designed
based on the underlying physics model, resulting in a simpler architecture with
a single generator and discriminator pair. Crucially, our experimental results
indicate that our method, which does not necessitate separate AIF measurements,
produces more reliable pharmacokinetic parameters than other techniques.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Changing Data Sources in the Age of Machine Learning for Official
  Statistics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04338v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04338v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cedric De Boom, Michael Reusens
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Data science has become increasingly essential for the production of official
statistics, as it enables the automated collection, processing, and analysis of
large amounts of data. With such data science practices in place, it enables
more timely, more insightful and more flexible reporting. However, the quality
and integrity of data-science-driven statistics rely on the accuracy and
reliability of the data sources and the machine learning techniques that
support them. In particular, changes in data sources are inevitable to occur
and pose significant risks that are crucial to address in the context of
machine learning for official statistics.
  This paper gives an overview of the main risks, liabilities, and
uncertainties associated with changing data sources in the context of machine
learning for official statistics. We provide a checklist of the most prevalent
origins and causes of changing data sources; not only on a technical level but
also regarding ownership, ethics, regulation, and public perception. Next, we
highlight the repercussions of changing data sources on statistical reporting.
These include technical effects such as concept drift, bias, availability,
validity, accuracy and completeness, but also the neutrality and potential
discontinuation of the statistical offering. We offer a few important
precautionary measures, such as enhancing robustness in both data sourcing and
statistical techniques, and thorough monitoring. In doing so, machine
learning-based official statistics can maintain integrity, reliability,
consistency, and relevance in policy-making, decision-making, and public
discourse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Presented at UNECE Machine Learning for Official Statistics Workshop
  2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CaptAinGlove: Capacitive and Inertial Fusion-Based Glove for Real-Time
  on Edge Hand Gesture Recognition for Drone Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04319v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04319v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hymalai Bello, Sungho Suh, Daniel Geißler, Lala Ray, Bo Zhou, Paul Lukowicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present CaptAinGlove, a textile-based, low-power (1.15Watts),
privacy-conscious, real-time on-the-edge (RTE) glove-based solution with a tiny
memory footprint (2MB), designed to recognize hand gestures used for drone
control. We employ lightweight convolutional neural networks as the backbone
models and a hierarchical multimodal fusion to reduce power consumption and
improve accuracy. The system yields an F1-score of 80% for the offline
evaluation of nine classes; eight hand gesture commands and null activity. For
the RTE, we obtained an F1-score of 67% (one user).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Timing Process Interventions with Causal Inference and Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04299v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04299v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hans Weytjens, Wouter Verbeke, Jochen De Weerdt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The shift from the understanding and prediction of processes to their
optimization offers great benefits to businesses and other organizations.
Precisely timed process interventions are the cornerstones of effective
optimization. Prescriptive process monitoring (PresPM) is the sub-field of
process mining that concentrates on process optimization. The emerging PresPM
literature identifies state-of-the-art methods, causal inference (CI) and
reinforcement learning (RL), without presenting a quantitative comparison. Most
experiments are carried out using historical data, causing problems with the
accuracy of the methods' evaluations and preempting online RL. Our contribution
consists of experiments on timed process interventions with synthetic data that
renders genuine online RL and the comparison to CI possible, and allows for an
accurate evaluation of the results. Our experiments reveal that RL's policies
outperform those from CI and are more robust at the same time. Indeed, the RL
policies approach perfect policies. Unlike CI, the unaltered online RL approach
can be applied to other, more generic PresPM problems such as next best
activity recommendations. Nonetheless, CI has its merits in settings where
online learning is not an option.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Phrase Retrieval for Open-Domain Conversational Question Answering with
  Conversational Dependency Modeling via Contrastive Learning <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04293v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04293v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soyeong Jeong, Jinheon Baek, Sung Ju Hwang, Jong C. Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Open-Domain Conversational Question Answering (ODConvQA) aims at answering
questions through a multi-turn conversation based on a retriever-reader
pipeline, which retrieves passages and then predicts answers with them.
However, such a pipeline approach not only makes the reader vulnerable to the
errors propagated from the retriever, but also demands additional effort to
develop both the retriever and the reader, which further makes it slower since
they are not runnable in parallel. In this work, we propose a method to
directly predict answers with a phrase retrieval scheme for a sequence of
words, reducing the conventional two distinct subtasks into a single one. Also,
for the first time, we study its capability for ODConvQA tasks. However, simply
adopting it is largely problematic, due to the dependencies between previous
and current turns in a conversation. To address this problem, we further
introduce a novel contrastive learning strategy, making sure to reflect
previous turns when retrieving the phrase for the current context, by
maximizing representational similarities of consecutive turns in a conversation
while minimizing irrelevant conversational contexts. We validate our model on
two ODConvQA datasets, whose experimental results show that it substantially
outperforms the relevant baselines with the retriever-reader. Code is available
at: https://github.com/starsuzi/PRO-ConvQA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revising deep learning methods in parking lot occupancy detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04288v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04288v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasia Martynova, Mikhail Kuznetsov, Vadim Porvatov, Vladislav Tishin, Andrey Kuznetsov, Natalia Semenova, Ksenia Kuznetsova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parking guidance systems have recently become a popular trend as a part of
the smart cities' paradigm of development. The crucial part of such systems is
the algorithm allowing drivers to search for available parking lots across
regions of interest. The classic approach to this task is based on the
application of neural network classifiers to camera records. However, existing
systems demonstrate a lack of generalization ability and appropriate testing
regarding specific visual conditions. In this study, we extensively evaluate
state-of-the-art parking lot occupancy detection algorithms, compare their
prediction quality with the recently emerged vision transformers, and propose a
new pipeline based on EfficientNet architecture. Performed computational
experiments have demonstrated the performance increase in the case of our
model, which was evaluated on 5 different datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ColNav: Real-Time Colon Navigation for Colonoscopy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04269v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04269v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Netanel Frank, Erez Posner, Emmanuelle Muhlethaler, Adi Zholkover, Moshe Bouhnik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Colorectal cancer screening through colonoscopy continues to be the dominant
global standard, as it allows identifying pre-cancerous or adenomatous lesions
and provides the ability to remove them during the procedure itself.
Nevertheless, failure by the endoscopist to identify such lesions increases the
likelihood of lesion progression to subsequent colorectal cancer. Ultimately,
colonoscopy remains operator-dependent, and the wide range of quality in
colonoscopy examinations among endoscopists is influenced by variations in
their technique, training, and diligence. This paper presents a novel real-time
navigation guidance system for Optical Colonoscopy (OC). Our proposed system
employs a real-time approach that displays both an unfolded representation of
the colon and a local indicator directing to un-inspected areas. These
visualizations are presented to the physician during the procedure, providing
actionable and comprehensible guidance to un-surveyed areas in real-time, while
seamlessly integrating into the physician's workflow. Through coverage
experimental evaluation, we demonstrated that our system resulted in a higher
polyp recall (PR) and high inter-rater reliability with physicians for coverage
prediction. These results suggest that our real-time navigation guidance system
has the potential to improve the quality and effectiveness of Optical
Colonoscopy and ultimately benefit patient outcomes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Permutaion Equivariant Graph Framelets for Heterophilous Semi-supervised
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04265v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04265v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianfei Li, Ruigang Zheng, Han Feng, Xiaosheng Zhuang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The nature of heterophilous graphs is significantly different with that of
homophilous graphs, which suggests aggregations beyond 1-hop neighborhood and
causes difficulties in early graph neural network models. In this paper, we
develop a new way to implement multi-scale extraction via constructing
Haar-type graph framelets with desired properties of permutation equivariance,
efficiency, and sparsity, for deep learning tasks on graphs. We further deisgn
a graph framelet neural network model PEGFAN using our constructed graph
framelets. The experiments are conducted on a synthetic dataset and 9 benchmark
datasets to compare performance with other state-of-the-art models. The result
shows that our model can achieve best performance on certain datasets of
heterophilous graphs (including the majority of heterophilous datasets with
relatively larger sizes and denser connections) and competitive performance on
the remaining.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Self-Adjusting Weighted Expected Improvement for Bayesian Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04262v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04262v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Carolin Benjamins, Elena Raponi, Anja Jankovic, Carola Doerr, Marius Lindauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian Optimization (BO) is a class of surrogate-based, sample-efficient
algorithms for optimizing black-box problems with small evaluation budgets. The
BO pipeline itself is highly configurable with many different design choices
regarding the initial design, surrogate model, and acquisition function (AF).
Unfortunately, our understanding of how to select suitable components for a
problem at hand is very limited. In this work, we focus on the definition of
the AF, whose main purpose is to balance the trade-off between exploring
regions with high uncertainty and those with high promise for good solutions.
We propose Self-Adjusting Weighted Expected Improvement (SAWEI), where we let
the exploration-exploitation trade-off self-adjust in a data-driven manner,
based on a convergence criterion for BO. On the noise-free black-box BBOB
functions of the COCO benchmarking platform, our method exhibits a favorable
any-time performance compared to handcrafted baselines and serves as a robust
default choice for any problem structure. The suitability of our method also
transfers to HPOBench. With SAWEI, we are a step closer to on-the-fly,
data-driven, and robust BO designs that automatically adjust their sampling
behavior to the problem at hand.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AutoML Conference 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Accounting For Informative Sampling When Learning to Forecast Treatment
  Outcomes Over Time 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04255v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04255v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Toon Vanderschueren, Alicia Curth, Wouter Verbeke, Mihaela van der Schaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning (ML) holds great potential for accurately forecasting
treatment outcomes over time, which could ultimately enable the adoption of
more individualized treatment strategies in many practical applications.
However, a significant challenge that has been largely overlooked by the ML
literature on this topic is the presence of informative sampling in
observational data. When instances are observed irregularly over time, sampling
times are typically not random, but rather informative -- depending on the
instance's characteristics, past outcomes, and administered treatments. In this
work, we formalize informative sampling as a covariate shift problem and show
that it can prohibit accurate estimation of treatment outcomes if not properly
accounted for. To overcome this challenge, we present a general framework for
learning treatment outcomes in the presence of informative sampling using
inverse intensity-weighting, and propose a novel method, TESAR-CDE, that
instantiates this framework using Neural CDEs. Using a simulation environment
based on a clinical use case, we demonstrate the effectiveness of our approach
in learning under informative sampling.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Proceedings of the 40th International Conference on
  Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adversarial Sample Detection Through Neural Network Transport Dynamics <span class="chip">ECML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04252v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04252v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Skander Karkar, Patrick Gallinari, Alain Rakotomamonjy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a detector of adversarial samples that is based on the view of
neural networks as discrete dynamic systems. The detector tells clean inputs
from abnormal ones by comparing the discrete vector fields they follow through
the layers. We also show that regularizing this vector field during training
makes the network more regular on the data distribution's support, thus making
the activations of clean inputs more distinguishable from those of abnormal
ones. Experimentally, we compare our detector favorably to other detectors on
seen and unseen attacks, and show that the regularization of the network's
dynamics improves the performance of adversarial detectors that use the
internal embeddings as inputs, while also improving test accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ECML PKDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stochastic Collapse: How Gradient Noise Attracts SGD Dynamics Towards
  Simpler Subnetworks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04251v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04251v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Feng Chen, Daniel Kunin, Atsushi Yamamura, Surya Ganguli
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we reveal a strong implicit bias of stochastic gradient descent
(SGD) that drives overly expressive networks to much simpler subnetworks,
thereby dramatically reducing the number of independent parameters, and
improving generalization. To reveal this bias, we identify invariant sets, or
subsets of parameter space that remain unmodified by SGD. We focus on two
classes of invariant sets that correspond to simpler subnetworks and commonly
appear in modern architectures. Our analysis uncovers that SGD exhibits a
property of stochastic attractivity towards these simpler invariant sets. We
establish a sufficient condition for stochastic attractivity based on a
competition between the loss landscape's curvature around the invariant set and
the noise introduced by stochastic gradients. Remarkably, we find that an
increased level of noise strengthens attractivity, leading to the emergence of
attractive invariant sets associated with saddle-points or local maxima of the
train loss. We observe empirically the existence of attractive invariant sets
in trained deep neural networks, implying that SGD dynamics often collapses to
simple subnetworks with either vanishing or redundant neurons. We further
demonstrate how this simplifying process of stochastic collapse benefits
generalization in a linear teacher-student framework. Finally, through this
analysis, we mechanistically explain why early training with large learning
rates for extended periods benefits subsequent generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>30 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data Mining for Faster, Interpretable Solutions to Inverse Problems: A
  Case Study Using Additive Manufacturing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04228v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04228v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chandrika Kamath, Juliette Franzman, Ravi Ponmalai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Solving inverse problems, where we find the input values that result in
desired values of outputs, can be challenging. The solution process is often
computationally expensive and it can be difficult to interpret the solution in
high-dimensional input spaces. In this paper, we use a problem from additive
manufacturing to address these two issues with the intent of making it easier
to solve inverse problems and exploit their results. First, focusing on
Gaussian process surrogates that are used to solve inverse problems, we
describe how a simple modification to the idea of tapering can substantially
speed up the surrogate without losing accuracy in prediction. Second, we
demonstrate that Kohonen self-organizing maps can be used to visualize and
interpret the solution to the inverse problem in the high-dimensional input
space. For our data set, as not all input dimensions are equally important, we
show that using weighted distances results in a better organized map that makes
the relationships among the inputs obvious.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 figures and 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Normalization Layers Are All That Sharpness-Aware Minimization Needs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04226v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04226v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian Mueller, Tiffany Vlaar, David Rolnick, Matthias Hein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sharpness-aware minimization (SAM) was proposed to reduce sharpness of minima
and has been shown to enhance generalization performance in various settings.
In this work we show that perturbing only the affine normalization parameters
(comprising less than 0.1% of the total parameters) in the adversarial step of
SAM outperforms perturbing all of the parameters. This finding generalizes to
different SAM variants and both ResNet (Batch Normalization) and Vision
Transformer (Layer Normalization) architectures. We consider alternative sparse
perturbation approaches and find that these do not achieve similar performance
enhancement at such extreme sparsity levels, showing that this behaviour is
unique to the normalization layers. Although our findings reaffirm the
effectiveness of SAM in improving generalization performance, they cast doubt
on whether this is solely caused by reduced sharpness. The code for our
experiments is publicly available at https://github.com/mueller-mp/SAM-ON.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Vision <span class="highlight-title">Transformer</span> for Human Pose Estimation via Patch
  Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04225v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04225v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaleab A. Kinfu, René Vidal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Convolutional Neural Networks (CNNs) have been widely successful in 2D
human pose estimation, Vision Transformers (ViTs) have emerged as a promising
alternative to CNNs, boosting state-of-the-art performance. However, the
quadratic computational complexity of ViTs has limited their applicability for
processing high-resolution images and long videos. To address this challenge,
we propose a simple method for reducing ViT's computational complexity based on
selecting and processing a small number of most informative patches while
disregarding others. We leverage a lightweight pose estimation network to guide
the patch selection process, ensuring that the selected patches contain the
most important information. Our experimental results on three widely used 2D
pose estimation benchmarks, namely COCO, MPII and OCHuman, demonstrate the
effectiveness of our proposed methods in significantly improving speed and
reducing computational complexity with a slight drop in performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causally Learning an Optimal Rework Policy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oliver Schacht, Sven Klaassen, Philipp Schwarz, Martin Spindler, Daniel Grünbaum, Sebastian Imhof
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In manufacturing, rework refers to an optional step of a production process
which aims to eliminate errors or remedy products that do not meet the desired
quality standards. Reworking a production lot involves repeating a previous
production stage with adjustments to ensure that the final product meets the
required specifications. While offering the chance to improve the yield and
thus increase the revenue of a production lot, a rework step also incurs
additional costs. Additionally, the rework of parts that already meet the
target specifications may damage them and decrease the yield. In this paper, we
apply double/debiased machine learning (DML) to estimate the conditional
treatment effect of a rework step during the color conversion process in
opto-electronic semiconductor manufacturing on the final product yield. We
utilize the implementation DoubleML to develop policies for the rework of
components and estimate their value empirically. From our causal machine
learning analysis we derive implications for the coating of monochromatic LEDs
with conversion layers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Look Beneath the Surface: Exploiting Fundamental Symmetry for
  Sample-Efficient Offline RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04220v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04220v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Cheng, Xianyuan Zhan, Zhihao Wu, Wenjia Zhang, Shoucheng Song, Han Wang, Youfang Lin, Li Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL) offers an appealing approach to
real-world tasks by learning policies from pre-collected datasets without
interacting with the environment. However, the performance of existing offline
RL algorithms heavily depends on the scale and state-action space coverage of
datasets. Real-world data collection is often expensive and uncontrollable,
leading to small and narrowly covered datasets and posing significant
challenges for practical deployments of offline RL. In this paper, we provide a
new insight that leveraging the fundamental symmetry of system dynamics can
substantially enhance offline RL performance under small datasets.
Specifically, we propose a Time-reversal symmetry (T-symmetry) enforced
Dynamics Model (TDM), which establishes consistency between a pair of forward
and reverse latent dynamics. TDM provides both well-behaved representations for
small datasets and a new reliability measure for OOD samples based on
compliance with the T-symmetry. These can be readily used to construct a new
offline RL algorithm (TSRL) with less conservative policy constraints and a
reliable latent space data augmentation procedure. Based on extensive
experiments, we find TSRL achieves great performance on small benchmark
datasets with as few as 1% of the original samples, which significantly
outperforms the recent offline RL algorithms in terms of data efficiency and
generalizability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The first two authors contributed equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DualHGNN: A Dual Hypergraph Neural Network for Semi-Supervised Node
  Classification based on Multi-View Learning and Density Awareness <span class="chip">IJCNN 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04214v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04214v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianpeng Liao, Jun Yan, Qian Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph-based semi-supervised node classification has been shown to become a
state-of-the-art approach in many applications with high research value and
significance. Most existing methods are only based on the original intrinsic or
artificially established graph structure which may not accurately reflect the
"true" correlation among data and are not optimal for semi-supervised node
classification in the downstream graph neural networks. Besides, while existing
graph-based methods mostly utilize the explicit graph structure, some implicit
information, for example, the density information, can also provide latent
information that can be further exploited. To address these limitations, this
paper proposes the Dual Hypergraph Neural Network (DualHGNN), a new dual
connection model integrating both hypergraph structure learning and hypergraph
representation learning simultaneously in a unified architecture. The DualHGNN
first leverages a multi-view hypergraph learning network to explore the optimal
hypergraph structure from multiple views, constrained by a consistency loss
proposed to improve its generalization. Then, DualHGNN employs a density-aware
hypergraph attention network to explore the high-order semantic correlation
among data points based on the density-aware attention mechanism. Extensive
experiments are conducted in various benchmark datasets, and the results
demonstrate the effectiveness of the proposed approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been accepted by 2023 International Joint Conference on
  Neural Networks (IJCNN 2023). arXiv admin note: text overlap with
  arXiv:2201.11511</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Migrate Demographic Group For Fair GNNs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04212v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04212v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        YanMing Hu, TianChi Liao, JiaLong Chen, Chuan Chen, Jing Bian, ZiBin Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural networks (GNNs) have been applied in many scenarios due to the
superior performance of graph learning. However, fairness is always ignored
when designing GNNs. As a consequence, biased information in training data can
easily affect vanilla GNNs, causing biased results toward particular
demographic groups (divided by sensitive attributes, such as race and age).
There have been efforts to address the fairness issue. However, existing fair
techniques generally divide the demographic groups by raw sensitive attributes
and assume that are fixed. The biased information correlated with raw sensitive
attributes will run through the training process regardless of the implemented
fair techniques. It is urgent to resolve this problem for training fair GNNs.
To tackle this problem, we propose a brand new framework, FairMigration, which
can dynamically migrate the demographic groups instead of keeping that fixed
with raw sensitive attributes. FairMigration is composed of two training
stages. In the first stage, the GNNs are initially optimized by personalized
self-supervised learning, and the demographic groups are adjusted dynamically.
In the second stage, the new demographic groups are frozen and supervised
learning is carried out under the constraints of new demographic groups and
adversarial training. Extensive experiments reveal that FairMigration balances
model performance and fairness well.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Knowledge Graph Embeddings to Enhance Contextual
  Representations for Relation Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fréjus A. A. Laleye, Loïc Rakotoson, Sylvain Massip
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction task is a crucial and challenging aspect of Natural
Language Processing. Several methods have surfaced as of late, exhibiting
notable performance in addressing the task; however, most of these approaches
rely on vast amounts of data from large-scale knowledge graphs or language
models pretrained on voluminous corpora. In this paper, we hone in on the
effective utilization of solely the knowledge supplied by a corpus to create a
high-performing model. Our objective is to showcase that by leveraging the
hierarchical structure and relational distribution of entities within a corpus
without introducing external knowledge, a relation extraction model can achieve
significantly enhanced performance. We therefore proposed a relation extraction
approach based on the incorporation of pretrained knowledge graph embeddings at
the corpus scale into the sentence-level contextual representation. We
conducted a series of experiments which revealed promising and very interesting
results for our proposed approach.The obtained results demonstrated an
outperformance of our method compared to context-based relation extraction
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 1 figures, The 17th International Conference on Document
  Analysis and Recognition</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Hyperparameter Learning under Approximate Inference in
  Gaussian Process Models <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04201v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04201v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Li, ST John, Arno Solin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Approximate inference in Gaussian process (GP) models with non-conjugate
likelihoods gets entangled with the learning of the model hyperparameters. We
improve hyperparameter learning in GP models and focus on the interplay between
variational inference (VI) and the learning target. While VI's lower bound to
the marginal likelihood is a suitable objective for inferring the approximate
posterior, we show that a direct approximation of the marginal likelihood as in
Expectation Propagation (EP) is a better learning objective for hyperparameter
optimization. We design a hybrid training procedure to bring the best of both
worlds: it leverages conjugate-computation VI for inference and uses an EP-like
marginal likelihood approximation for hyperparameter learning. We compare VI,
EP, Laplace approximation, and our proposed training procedure and empirically
demonstrate the effectiveness of our proposal across a wide range of data sets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Machine Learning (ICML) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An ASR-Based Tutor for Learning to Read: How to Optimize Feedback to
  First Graders <span class="chip">SP</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04190v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04190v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu Bai, Cristian Tejedor-Garcia, Ferdy Hubers, Catia Cucchiarini, Helmer Strik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The interest in employing automatic speech recognition (ASR) in applications
for reading practice has been growing in recent years. In a previous study, we
presented an ASR-based Dutch reading tutor application that was developed to
provide instantaneous feedback to first-graders learning to read. We saw that
ASR has potential at this stage of the reading process, as the results
suggested that pupils made progress in reading accuracy and fluency by using
the software. In the current study, we used children's speech from an existing
corpus (JASMIN) to develop two new ASR systems, and compared the results to
those of the previous study. We analyze correct/incorrect classification of the
ASR systems using human transcripts at word level, by means of evaluation
measures such as Cohen's Kappa, Matthews Correlation Coefficient (MCC),
precision, recall and F-measures. We observe improvements for the newly
developed ASR systems regarding the agreement with human-based judgment and
correct rejection (CR). The accuracy of the ASR systems varies for different
reading tasks and word types. Our results suggest that, in the current
configuration, it is difficult to classify isolated words. We discuss these
results, possible ways to improve our systems and avenues for future research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published (double-blind peer-reviewed) on SPECOM 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Self-supervised</span> Audio Teacher-Student <span class="highlight-title">Transformer</span> for Both Clip-level
  and Frame-level Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04186v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04186v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xian Li, Nian Shao, Xiaofei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, self-supervised learning (SSL) has emerged as a popular
approach for learning audio representations. The ultimate goal of audio
self-supervised pre-training is to transfer knowledge to downstream audio
tasks, generally including clip-level and frame-level tasks. Clip-level tasks
classify the scene or sound of an entire audio clip, e.g. audio tagging,
instrument recognition, etc. While frame-level tasks detect event-level
timestamps from an audio clip, e.g. sound event detection, speaker diarization,
etc. Prior studies primarily evaluate on clip-level downstream tasks.
Frame-level tasks are important for fine-grained acoustic scene/event
understanding, and are generally more challenging than clip-level tasks. In
order to tackle both clip-level and frame-level tasks, this paper proposes two
self-supervised audio representation learning methods: ATST-Clip and
ATST-Frame, responsible for learning clip-level and frame-level
representations, respectively. ATST stands for Audio Teacher-Student
Transformer, which means both methods use a transformer encoder and a
teacher-student training scheme.Experimental results show that our ATST-Frame
model obtains state-of-the-art (SOTA) performance on most of the clip-level and
frame-level downstream tasks. Especially, it outperforms other models by a
large margin on the frame-level sound event detection task. In addition, the
performance can be further improved by combining the two models through
knowledge distillation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE TASLP. arXiv admin note: text overlap with
  arXiv:2204.12076</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Foundation Models with Language-Model-as-an-Examiner 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04181v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04181v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushi Bai, Jiahao Ying, Yixin Cao, Xin Lv, Yuze He, Xiaozhi Wang, Jifan Yu, Kaisheng Zeng, Yijia Xiao, Haozhe Lyu, Jiayin Zhang, Juanzi Li, Lei Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous benchmarks have been established to assess the performance of
foundation models on open-ended question answering, which serves as a
comprehensive test of a model's ability to understand and generate language in
a manner similar to humans. Most of these works focus on proposing new
datasets, however, we see two main issues within previous benchmarking
pipelines, namely testing leakage and evaluation automation. In this paper, we
propose a novel benchmarking framework, Language-Model-as-an-Examiner, where
the LM serves as a knowledgeable examiner that formulates questions based on
its knowledge and evaluates responses in a reference-free manner. Our framework
allows for effortless extensibility as various LMs can be adopted as the
examiner, and the questions can be constantly updated given more diverse
trigger topics. For a more comprehensive and equitable evaluation, we devise
three strategies: (1) We instruct the LM examiner to generate questions across
a multitude of domains to probe for a broad acquisition, and raise follow-up
questions to engage in a more in-depth assessment. (2) Upon evaluation, the
examiner combines both scoring and ranking measurements, providing a reliable
result as it aligns closely with human annotations. (3) We additionally propose
a decentralized Peer-examination method to address the biases in a single
examiner. Our data and benchmarking results are available at:
https://lmexam.com.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimal Transport Model Distributional Robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04178v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04178v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Van-Anh Nguyen, Trung Le, Anh Tuan Bui, Thanh-Toan Do, Dinh Phung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Distributional robustness is a promising framework for training deep learning
models that are less vulnerable to adversarial examples and data distribution
shifts. Previous works have mainly focused on exploiting distributional
robustness in data space. In this work, we explore an optimal transport-based
distributional robustness framework on model spaces. Specifically, we examine a
model distribution in a Wasserstein ball of a given center model distribution
that maximizes the loss. We have developed theories that allow us to learn the
optimal robust center model distribution. Interestingly, through our developed
theories, we can flexibly incorporate the concept of sharpness awareness into
training a single model, ensemble models, and Bayesian Neural Networks by
considering specific forms of the center model distribution, such as a Dirac
delta distribution over a single model, a uniform distribution over several
models, and a general Bayesian Neural Network. Furthermore, we demonstrate that
sharpness-aware minimization (SAM) is a specific case of our framework when
using a Dirac delta distribution over a single model, while our framework can
be viewed as a probabilistic extension of SAM. We conduct extensive experiments
to demonstrate the usefulness of our framework in the aforementioned settings,
and the results show remarkable improvements in our approaches to the
baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Learning for Stochastic Optimization: A Bayesian Perspective <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04174v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04174v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yves Rychener, Daniel Kuhn Tobias Sutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We develop a principled approach to end-to-end learning in stochastic
optimization. First, we show that the standard end-to-end learning algorithm
admits a Bayesian interpretation and trains a posterior Bayes action map.
Building on the insights of this analysis, we then propose new end-to-end
learning algorithms for training decision maps that output solutions of
empirical risk minimization and distributionally robust optimization problems,
two dominant modeling paradigms in optimization under uncertainty. Numerical
results for a synthetic newsvendor problem illustrate the key differences
between alternative training schemes. We also investigate an economic dispatch
problem based on real data to showcase the impact of the neural network
architecture of the decision maps on their test performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient Alternating Minimization with Applications to Weighted Low
  Rank Approximation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04169v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04169v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhao Song, Mingquan Ye, Junze Yin, Lichen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weighted low rank approximation is a fundamental problem in numerical linear
algebra, and it has many applications in machine learning. Given a matrix $M
\in \mathbb{R}^{n \times n}$, a weight matrix $W \in \mathbb{R}_{\geq 0}^{n
\times n}$, a parameter $k$, the goal is to output two matrices $U, V \in
\mathbb{R}^{n \times k}$ such that $\| W \circ (M - U V) \|_F$ is minimized,
where $\circ$ denotes the Hadamard product. Such a problem is known to be
NP-hard and even hard to approximate [RSW16]. Meanwhile, alternating
minimization is a good heuristic solution for approximating weighted low rank
approximation. The work [LLR16] shows that, under mild assumptions, alternating
minimization does provide provable guarantees. In this work, we develop an
efficient and robust framework for alternating minimization. For weighted low
rank approximation, this improves the runtime of [LLR16] from $n^2 k^2$ to
$n^2k$. At the heart of our work framework is a high-accuracy multiple response
regression solver together with a robust analysis of alternating minimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2302.11068</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rethinking Weak Supervision in Helping Contrastive Learning <span class="chip">ICML2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04160v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04160v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyi Cui, Weiran Huang, Yifei Wang, Yisen Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has shown outstanding performances in both supervised
and unsupervised learning, and has recently been introduced to solve weakly
supervised learning problems such as semi-supervised learning and noisy label
learning. Despite the empirical evidence showing that semi-supervised labels
improve the representations of contrastive learning, it remains unknown if
noisy supervised information can be directly used in training instead of after
manual denoising. Therefore, to explore the mechanical differences between
semi-supervised and noisy-labeled information in helping contrastive learning,
we establish a unified theoretical framework of contrastive learning under weak
supervision. Specifically, we investigate the most intuitive paradigm of
jointly training supervised and unsupervised contrastive losses. By translating
the weakly supervised information into a similarity graph under the framework
of spectral clustering based on the posterior probability of weak labels, we
establish the downstream classification error bound. We prove that
semi-supervised labels improve the downstream error bound whereas noisy labels
have limited effects under such a paradigm. Our theoretical findings here
provide new insights for the community to rethink the role of weak supervision
in helping contrastive learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Balanced Product of Calibrated Experts for Long-Tailed Recognition <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.05260v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.05260v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emanuel Sanchez Aimar, Arvi Jonnarth, Michael Felsberg, Marco Kuhlmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real-world recognition problems are characterized by long-tailed label
distributions. These distributions make representation learning highly
challenging due to limited generalization over the tail classes. If the test
distribution differs from the training distribution, e.g. uniform versus
long-tailed, the problem of the distribution shift needs to be addressed. A
recent line of work proposes learning multiple diverse experts to tackle this
issue. Ensemble diversity is encouraged by various techniques, e.g. by
specializing different experts in the head and the tail classes. In this work,
we take an analytical approach and extend the notion of logit adjustment to
ensembles to form a Balanced Product of Experts (BalPoE). BalPoE combines a
family of experts with different test-time target distributions, generalizing
several previous approaches. We show how to properly define these distributions
and combine the experts in order to achieve unbiased predictions, by proving
that the ensemble is Fisher-consistent for minimizing the balanced error. Our
theoretical analysis shows that our balanced ensemble requires calibrated
experts, which we achieve in practice using mixup. We conduct extensive
experiments and our method obtains new state-of-the-art results on three
long-tailed datasets: CIFAR-100-LT, ImageNet-LT, and iNaturalist-2018. Our code
is available at https://github.com/emasa/BalPoE-CalibratedLT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at CVPR 2023, 19 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Models can Solve Computer Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17491v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17491v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Geunwoo Kim, Pierre Baldi, Stephen McAleer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Agents capable of carrying out general tasks on a computer can improve
efficiency and productivity by automating repetitive tasks and assisting in
complex problem-solving. Ideally, such agents should be able to solve new
computer tasks presented to them through natural language commands. However,
previous approaches to this problem require large amounts of expert
demonstrations and task-specific reward functions, both of which are
impractical for new tasks. In this work, we show that a pre-trained large
language model (LLM) agent can execute computer tasks guided by natural
language using a simple prompting scheme where the agent Recursively Criticizes
and Improves its output (RCI). The RCI approach significantly outperforms
existing LLM methods for automating computer tasks and surpasses supervised
learning (SL) and reinforcement learning (RL) approaches on the MiniWoB++
benchmark. We compare multiple LLMs and find that RCI with the
InstructGPT-3+RLHF LLM is state-of-the-art on MiniWoB++, using only a handful
of demonstrations per task rather than tens of thousands, and without a
task-specific reward function. Furthermore, we demonstrate RCI prompting's
effectiveness in enhancing LLMs' reasoning abilities on a suite of natural
language reasoning tasks, outperforming chain of thought (CoT) prompting. We
find that RCI combined with CoT performs better than either separately. Our
code can be found here: https://github.com/posgnu/rci-agent.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Smooth Non-Stationary Bandits <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12366v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12366v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Su Jia, Qian Xie, Nathan Kallus, Peter I. Frazier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many applications of online decision making, the environment is
non-stationary and it is therefore crucial to use bandit algorithms that handle
changes. Most existing approaches are designed to protect against non-smooth
changes, constrained only by total variation or Lipschitzness over time, where
they guarantee $\tilde \Theta(T^{2/3})$ regret. However, in practice
environments are often changing {\bf smoothly}, so such algorithms may incur
higher-than-necessary regret in these settings and do not leverage information
on the rate of change. We study a non-stationary two-armed bandits problem
where we assume that an arm's mean reward is a $\beta$-H\"older function over
(normalized) time, meaning it is $(\beta-1)$-times Lipschitz-continuously
differentiable. We show the first separation between the smooth and non-smooth
regimes by presenting a policy with $\tilde O(T^{3/5})$ regret for $\beta=2$.
We complement this result by an $\Omg(T^{(\beta+1)/(2\beta+1)})$ lower bound
for any integer $\beta\ge 1$, which matches our upper bound for $\beta=2$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mixed Autoencoder for <span class="highlight-title">Self-supervised</span> Visual Representation Learning <span class="chip">CVPR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17152v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17152v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kai Chen, Zhili Liu, Lanqing Hong, Hang Xu, Zhenguo Li, Dit-Yan Yeung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked Autoencoder (MAE) has demonstrated superior performance on various
vision tasks via randomly masking image patches and reconstruction. However,
effective data augmentation strategies for MAE still remain open questions,
different from those in contrastive learning that serve as the most important
part. This paper studies the prevailing mixing augmentation for MAE. We first
demonstrate that naive mixing will in contrast degenerate model performance due
to the increase of mutual information (MI). To address, we propose homologous
recognition, an auxiliary pretext task, not only to alleviate the MI
increasement by explicitly requiring each patch to recognize homologous
patches, but also to perform object-aware self-supervised pre-training for
better downstream dense perception performance. With extensive experiments, we
demonstrate that our proposed Mixed Autoencoder (MixedAE) achieves the
state-of-the-art transfer results among masked image modeling (MIM)
augmentations on different downstream tasks with significant efficiency.
Specifically, our MixedAE outperforms MAE by +0.3% accuracy, +1.7 mIoU and +0.9
AP on ImageNet-1K, ADE20K and COCO respectively with a standard ViT-Base.
Moreover, MixedAE surpasses iBOT, a strong MIM method combined with instance
discrimination, while accelerating training by 2x. To our best knowledge, this
is the very first work to consider mixing for MIM from the perspective of
pretext task design. Code will be made available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by CVPR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) with memory are computationally universal.
However, mainstream LLMs are not taking full advantage of memory, and the
designs are heavily influenced by biological brains. Due to their approximate
nature and proneness to the accumulation of errors, conventional neural memory
mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we
seek inspiration from modern computer architectures to augment LLMs with
symbolic memory for complex multi-hop reasoning. Such a symbolic memory
framework is instantiated as an LLM and a set of SQL databases, where the LLM
generates SQL instructions to manipulate the SQL databases. We validate the
effectiveness of the proposed memory framework on a synthetic dataset requiring
complex reasoning. The project website is available at
https://chatdatabase.github.io/ .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gradient boosting for convex cone predict and optimize problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.06895v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.06895v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrew Butler, Roy H. Kwon
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prediction models are typically optimized independently from decision
optimization. A smart predict then optimize (SPO) framework optimizes
prediction models to minimize downstream decision regret. In this paper we
present dboost, the first general purpose implementation of smart gradient
boosting for `predict, then optimize' problems. The framework supports convex
quadratic cone programming and gradient boosting is performed by implicit
differentiation of a custom fixed-point mapping. Experiments comparing with
state-of-the-art SPO methods show that dboost can further reduce out-of-sample
decision regret.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GAD-NR: Graph Anomaly Detection via Neighborhood Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01951v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01951v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amit Roy, Juan Shu, Jia Li, Carl Yang, Olivier Elshocht, Jeroen Smeets, Pan Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Anomaly Detection (GAD) is a technique used to identify abnormal nodes
within graphs, finding applications in network security, fraud detection,
social media spam detection, and various other domains. A common method for GAD
is Graph Auto-Encoders (GAEs), which encode graph data into node
representations and identify anomalies by assessing the reconstruction quality
of the graphs based on these representations. However, existing GAE models are
primarily optimized for direct link reconstruction, resulting in nodes
connected in the graph being clustered in the latent space. As a result, they
excel at detecting cluster-type structural anomalies but struggle with more
complex structural anomalies that do not conform to clusters. To address this
limitation, we propose a novel solution called GAD-NR, a new variant of GAE
that incorporates neighborhood reconstruction for graph anomaly detection.
GAD-NR aims to reconstruct the entire neighborhood of a node, encompassing the
local structure, self-attributes, and neighbor attributes, based on the
corresponding node representation. By comparing the neighborhood reconstruction
loss between anomalous nodes and normal nodes, GAD-NR can effectively detect
any anomalies. Extensive experimentation conducted on six real-world datasets
validates the effectiveness of GAD-NR, showcasing significant improvements (by
up to 30% in AUC) over state-of-the-art competitors. The source code for GAD-NR
is openly available. Importantly, the comparative analysis reveals that the
existing methods perform well only in detecting one or two types of anomalies
out of the three types studied. In contrast, GAD-NR excels at detecting all
three types of anomalies across the datasets, demonstrating its comprehensive
anomaly detection capabilities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Suggest Breaks: Sustainable Optimization of Long-Term User
  Engagement <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.13585v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.13585v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eden Saig, Nir Rosenfeld
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimizing user engagement is a key goal for modern recommendation systems,
but blindly pushing users towards increased consumption risks burn-out, churn,
or even addictive habits. To promote digital well-being, most platforms now
offer a service that periodically prompts users to take breaks. These, however,
must be set up manually, and so may be suboptimal for both users and the
system. In this paper, we study the role of breaks in recommendation, and
propose a framework for learning optimal breaking policies that promote and
sustain long-term engagement. Based on the notion that recommendation dynamics
are susceptible to both positive and negative feedback, we cast recommendation
as a Lotka-Volterra dynamical system, where breaking reduces to a problem of
optimal control. We then give an efficient learning algorithm, provide
theoretical guarantees, and empirically demonstrate the utility of our approach
on semi-synthetic data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta-learning Control Variates: Variance Reduction with Limited Data <span class="chip">UAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04756v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04756v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuo Sun, Chris J. Oates, François-Xavier Briol
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Control variates can be a powerful tool to reduce the variance of Monte Carlo
estimators, but constructing effective control variates can be challenging when
the number of samples is small. In this paper, we show that when a large number
of related integrals need to be computed, it is possible to leverage the
similarity between these integration tasks to improve performance even when the
number of samples per task is very small. Our approach, called meta learning
CVs (Meta-CVs), can be used for up to hundreds or thousands of tasks. Our
empirical assessment indicates that Meta-CVs can lead to significant variance
reduction in such settings, and our theoretical analysis establishes general
conditions under which Meta-CVs can be successfully trained.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication (with an oral presentation) at UAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-Conditioned Generative Modeling of Object-Centric Representations
  for Video Decomposition and Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08951v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08951v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chengmin Gao, Bin Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When perceiving the world from multiple viewpoints, humans have the ability
to reason about the complete objects in a compositional manner even when an
object is completely occluded from certain viewpoints. Meanwhile, humans are
able to imagine novel views after observing multiple viewpoints. Recent
remarkable advances in multi-view object-centric learning still leaves some
unresolved problems: 1) The shapes of partially or completely occluded objects
can not be well reconstructed. 2) The novel viewpoint prediction depends on
expensive viewpoint annotations rather than implicit rules in view
representations. In this paper, we introduce a time-conditioned generative
model for videos. To reconstruct the complete shape of an object accurately, we
enhance the disentanglement between the latent representations of objects and
views, where the latent representations of time-conditioned views are jointly
inferred with a Transformer and then are input to a sequential extension of
Slot Attention to learn object-centric representations. In addition, Gaussian
processes are employed as priors of view latent variables for video generation
and novel-view prediction without viewpoint annotations. Experiments on
multiple datasets demonstrate that the proposed model can make object-centric
video decomposition, reconstruct the complete shapes of occluded objects, and
make novel-view predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Theory of Link Prediction via Relational Weisfeiler-Leman 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02209v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02209v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyue Huang, Miguel Romero Orth, İsmail İlkan Ceylan, Pablo Barceló
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks are prominent models for representation learning over
graph-structured data. While the capabilities and limitations of these models
are well-understood for simple graphs, our understanding remains incomplete in
the context of knowledge graphs. Our goal is to provide a systematic
understanding of the landscape of graph neural networks for knowledge graphs
pertaining to the prominent task of link prediction. Our analysis entails a
unifying perspective on seemingly unrelated models and unlocks a series of
other models. The expressive power of various models is characterized via a
corresponding relational Weisfeiler-Leman algorithm. This analysis is extended
to provide a precise logical characterization of the class of functions
captured by a class of graph neural networks. The theoretical findings
presented in this paper explain the benefits of some widely employed practical
design choices, which are validated empirically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Extrapolative Controlled Sequence Generation via Iterative Refinement <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04562v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04562v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vishakh Padmakumar, Richard Yuanzhe Pang, He He, Ankur P. Parikh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of extrapolative controlled generation, i.e., generating
sequences with attribute values beyond the range seen in training. This task is
of significant importance in automated design, especially drug discovery, where
the goal is to design novel proteins that are \textit{better} (e.g., more
stable) than existing sequences. Thus, by definition, the target sequences and
their attribute values are out of the training distribution, posing challenges
to existing methods that aim to directly generate the target sequence. Instead,
in this work, we propose Iterative Controlled Extrapolation (ICE) which
iteratively makes local edits to a sequence to enable extrapolation. We train
the model on synthetically generated sequence pairs that demonstrate small
improvement in the attribute value. Results on one natural language task
(sentiment analysis) and two protein engineering tasks (ACE2 stability and AAV
fitness) show that ICE considerably outperforms state-of-the-art approaches
despite its simplicity. Our code and models are available at:
https://github.com/vishakhpk/iter-extrapolation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023 - Camera Ready Version</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Filtering-based General Approach to Learning Rational Constraints of
  Epistemic Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02918v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02918v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Chi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Epistemic graphs are a generalization of the epistemic approach to
probabilistic argumentation. Hunter proposed a 2-way generalization framework
to learn epistemic constraints from crowd-sourcing data. However, the learnt
epistemic constraints only reflect users' beliefs from data, without
considering the rationality encoded in epistemic graphs. Meanwhile, the current
framework can only generate epistemic constraints that reflect whether an agent
believes an argument, but not the degree to which it believes in it. The major
challenge to achieving this effect is that the computational complexity will
increase sharply when expanding the variety of constraints, which may lead to
unacceptable time performance. To address these problems, we propose a
filtering-based approach using a multiple-way generalization step to generate a
set of rational rules which are consistent with their epistemic graphs from a
dataset. This approach is able to learn a wider variety of rational rules that
reflect information in both the domain model and the user model. Moreover, to
improve computational efficiency, we introduce a new function to exclude
meaningless rules. The empirical results show that our approach significantly
outperforms the existing framework when expanding the variety of rules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 6 figures, submitted to CLAR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Fast, Well-Founded Approximation to the Empirical Neural Tangent
  Kernel 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.12543v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.12543v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohamad Amin Mohamadi, Wonho Bae, Danica J. Sutherland
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Empirical neural tangent kernels (eNTKs) can provide a good understanding of
a given network's representation: they are often far less expensive to compute
and applicable more broadly than infinite width NTKs. For networks with O
output units (e.g. an O-class classifier), however, the eNTK on N inputs is of
size $NO \times NO$, taking $O((NO)^2)$ memory and up to $O((NO)^3)$
computation. Most existing applications have therefore used one of a handful of
approximations yielding $N \times N$ kernel matrices, saving orders of
magnitude of computation, but with limited to no justification. We prove that
one such approximation, which we call "sum of logits", converges to the true
eNTK at initialization for any network with a wide final "readout" layer. Our
experiments demonstrate the quality of this approximation for various uses
across a range of settings.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FPUS23: An Ultrasound Fetus Phantom <span class="highlight-title">Dataset</span> with Deep Neural Network
  Evaluations for Fetus Orientations, Fetal Planes, and Anatomical Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07852v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07852v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bharath Srinivas Prabakaran, Paul Hamelmann, Erik Ostrowski, Muhammad Shafique
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Ultrasound imaging is one of the most prominent technologies to evaluate the
growth, progression, and overall health of a fetus during its gestation.
However, the interpretation of the data obtained from such studies is best left
to expert physicians and technicians who are trained and well-versed in
analyzing such images. To improve the clinical workflow and potentially develop
an at-home ultrasound-based fetal monitoring platform, we present a novel fetus
phantom ultrasound dataset, FPUS23, which can be used to identify (1) the
correct diagnostic planes for estimating fetal biometric values, (2) fetus
orientation, (3) their anatomical features, and (4) bounding boxes of the fetus
phantom anatomies at 23 weeks gestation. The entire dataset is composed of
15,728 images, which are used to train four different Deep Neural Network
models, built upon a ResNet34 backbone, for detecting aforementioned fetus
features and use-cases. We have also evaluated the models trained using our
FPUS23 dataset, to show that the information learned by these models can be
used to substantially increase the accuracy on real-world ultrasound fetus
datasets. We make the FPUS23 dataset and the pre-trained models publicly
accessible at https://github.com/bharathprabakaran/FPUS23, which will further
facilitate future research on fetal ultrasound imaging and analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for Publication at IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-Party Chat: Conversational Agents in Group Settings with Humans
  and Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.13835v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.13835v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jimmy Wei, Kurt Shuster, Arthur Szlam, Jason Weston, Jack Urbanek, Mojtaba Komeili
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Current dialogue research primarily studies pairwise (two-party)
conversations, and does not address the everyday setting where more than two
speakers converse together. In this work, we both collect and evaluate
multi-party conversations to study this more general case. We use the LIGHT
environment to construct grounded conversations, where each participant has an
assigned character to role-play. We thus evaluate the ability of language
models to act as one or more characters in such conversations. Models require
two skills that pairwise-trained models appear to lack: (1) being able to
decide when to talk; (2) producing coherent utterances grounded on multiple
characters. We compare models trained on our new dataset to existing
pairwise-trained dialogue models, as well as large language models with
few-shot prompting. We find that our new dataset, MultiLIGHT, which we will
publicly release, can help bring significant improvements in the group setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Introduction to Medical Imaging Informatics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00421v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00421v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md. Zihad Bin Jahangir, Ruksat Hossain, Riadul Islam, MD Abdullah Al Nasim, Md. Mahim Anjum Haque, Md Jahangir Alam, Sajedul Talukder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical imaging informatics is a rapidly growing field that combines the
principles of medical imaging and informatics to improve the acquisition,
management, and interpretation of medical images. This chapter introduces the
basic concepts of medical imaging informatics, including image processing,
feature engineering, and machine learning. It also discusses the recent
advancements in computer vision and deep learning technologies and how they are
used to develop new quantitative image markers and prediction models for
disease detection, diagnosis, and prognosis prediction. By covering the basic
knowledge of medical imaging informatics, this chapter provides a foundation
for understanding the role of informatics in medicine and its potential impact
on patient care.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 11 figures, 2 tables; Acceptance of the chapter for the
  Springer book "Data-driven approaches to medical imaging"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Patient Dropout Prediction in Virtual Health: A Multimodal Dynamic
  Knowledge Graph and Text Mining Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03833v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03833v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Geng, Wenli Zhang, Jiaheng Xie, Gemin Liang, Ben Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual health has been acclaimed as a transformative force in healthcare
delivery. Yet, its dropout issue is critical that leads to poor health
outcomes, increased health, societal, and economic costs. Timely prediction of
patient dropout enables stakeholders to take proactive steps to address
patients' concerns, potentially improving retention rates. In virtual health,
the information asymmetries inherent in its delivery format, between different
stakeholders, and across different healthcare delivery systems hinder the
performance of existing predictive methods. To resolve those information
asymmetries, we propose a Multimodal Dynamic Knowledge-driven Dropout
Prediction (MDKDP) framework that learns implicit and explicit knowledge from
doctor-patient dialogues and the dynamic and complex networks of various
stakeholders in both online and offline healthcare delivery systems. We
evaluate MDKDP by partnering with one of the largest virtual health platforms
in China. MDKDP improves the F1-score by 3.26 percentage points relative to the
best benchmark. Comprehensive robustness analyses show that integrating
stakeholder attributes, knowledge dynamics, and compact bilinear pooling
significantly improves the performance. Our work provides significant
implications for healthcare IT by revealing the value of mining relations and
knowledge across different service modalities. Practically, MDKDP offers a
novel design artifact for virtual health platforms in patient dropout
management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PyTorch Hyperparameter Tuning - A Tutorial for spotPython 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11930v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11930v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Bartz-Beielstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of hyperparameter tuning (or hyperparameter optimization) is to
optimize the hyperparameters to improve the performance of the machine or deep
learning model. spotPython (``Sequential Parameter Optimization Toolbox in
Python'') is the Python version of the well-known hyperparameter tuner SPOT,
which has been developed in the R programming environment for statistical
analysis for over a decade. PyTorch is an optimized tensor library for deep
learning using GPUs and CPUs. This document shows how to integrate the
spotPython hyperparameter tuner into the PyTorch training workflow. As an
example, the results of the CIFAR10 image classifier are used. In addition to
an introduction to spotPython, this tutorial also includes a brief comparison
with Ray Tune, a Python library for running experiments and tuning
hyperparameters. This comparison is based on the PyTorch hyperparameter tuning
tutorial. The advantages and disadvantages of both approaches are discussed. We
show that spotPython achieves similar or even better results while being more
flexible and transparent than Ray Tune.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Refers to spotPython version 0.2.15</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence Analysis of Sequencial Split Learning on Heterogeneous Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01633v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01633v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yipeng Li, Xinchen Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) and Split Learning (SL) are two popular paradigms of
distributed machine learning. By offloading the computation-intensive portions
to the server, SL is promising for deep model training on resource-constrained
devices, yet still lacking of rigorous convergence analysis. In this paper, we
derive the convergence guarantees of Sequential SL (SSL, the vanilla case of SL
that conducts the model training in sequence) for strongly/general/non-convex
objectives on heterogeneous data. Notably, the derived guarantees suggest that
SSL is better than Federated Averaging (FedAvg, the most popular algorithm in
FL) on heterogeneous data. We validate the counterintuitive analysis result
empirically on extremely heterogeneous data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ HeterPS: Distributed Deep Learning With Reinforcement Learning Based
  Scheduling in Heterogeneous Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.10635v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.10635v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ji Liu, Zhihua Wu, Dianhai Yu, Yanjun Ma, Danlei Feng, Minxu Zhang, Xinxuan Wu, Xuefeng Yao, Dejing Dou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNNs) exploit many layers and a large number of
parameters to achieve excellent performance. The training process of DNN models
generally handles large-scale input data with many sparse features, which
incurs high Input/Output (IO) cost, while some layers are compute-intensive.
The training process generally exploits distributed computing resources to
reduce training time. In addition, heterogeneous computing resources, e.g.,
CPUs, GPUs of multiple types, are available for the distributed training
process. Thus, the scheduling of multiple layers to diverse computing resources
is critical for the training process. To efficiently train a DNN model using
the heterogeneous computing resources, we propose a distributed framework,
i.e., Paddle-Heterogeneous Parameter Server (Paddle-HeterPS), composed of a
distributed architecture and a Reinforcement Learning (RL)-based scheduling
method. The advantages of Paddle-HeterPS are three-fold compared with existing
frameworks. First, Paddle-HeterPS enables efficient training process of diverse
workloads with heterogeneous computing resources. Second, Paddle-HeterPS
exploits an RL-based method to efficiently schedule the workload of each layer
to appropriate computing resources to minimize the cost while satisfying
throughput constraints. Third, Paddle-HeterPS manages data storage and data
communication among distributed computing resources. We carry out extensive
experiments to show that Paddle-HeterPS significantly outperforms
state-of-the-art approaches in terms of throughput (14.5 times higher) and
monetary cost (312.3% smaller). The codes of the framework are publicly
available at: https://github.com/PaddlePaddle/Paddle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 11 figures, 2 tables; To appear in Future Generation
  Computer Systems (FGCS)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adversarially Robust PAC Learnability of Real-Valued Functions <span class="chip">ICML2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.12977v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.12977v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Idan Attias, Steve Hanneke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study robustness to test-time adversarial attacks in the regression
setting with $\ell_p$ losses and arbitrary perturbation sets. We address the
question of which function classes are PAC learnable in this setting. We show
that classes of finite fat-shattering dimension are learnable in both
realizable and agnostic settings. Moreover, for convex function classes, they
are even properly learnable. In contrast, some non-convex function classes
provably require improper learning algorithms. Our main technique is based on a
construction of an adversarially robust sample compression scheme of a size
determined by the fat-shattering dimension. Along the way, we introduce a novel
agnostic sample compression scheme for real-valued functions, which may be of
independent interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to ICML2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tracr: Compiled <span class="highlight-title">Transformer</span>s as a Laboratory for Interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.05062v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.05062v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Lindner, János Kramár, Sebastian Farquhar, Matthew Rahtz, Thomas McGrath, Vladimir Mikulik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show how to "compile" human-readable programs into standard decoder-only
transformer models. Our compiler, Tracr, generates models with known structure.
This structure can be used to design experiments. For example, we use it to
study "superposition" in transformers that execute multi-step algorithms.
Additionally, the known structure of Tracr-compiled models can serve as
ground-truth for evaluating interpretability methods. Commonly, because the
"programs" learned by transformers are unknown it is unclear whether an
interpretation succeeded. We demonstrate our approach by implementing and
examining programs including computing token frequencies, sorting, and
parenthesis checking. We provide an open-source implementation of Tracr at
https://github.com/deepmind/tracr.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GAN-MPC: Training Model Predictive Controllers with Parameterized Cost
  Functions using Demonstrations from Non-identical Experts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19111v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19111v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Returaj Burnwal, Anirban Santara, Nirav P. Bhatt, Balaraman Ravindran, Gaurav Aggarwal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model predictive control (MPC) is a popular approach for trajectory
optimization in practical robotics applications. MPC policies can optimize
trajectory parameters under kinodynamic and safety constraints and provide
guarantees on safety, optimality, generalizability, interpretability, and
explainability. However, some behaviors are complex and it is difficult to
hand-craft an MPC objective function. A special class of MPC policies called
Learnable-MPC addresses this difficulty using imitation learning from expert
demonstrations. However, they require the demonstrator and the imitator agents
to be identical which is hard to satisfy in many real world applications of
robotics. In this paper, we address the practical problem of training
Learnable-MPC policies when the demonstrator and the imitator do not share the
same dynamics and their state spaces may have a partial overlap. We propose a
novel approach that uses a generative adversarial network (GAN) to minimize the
Jensen-Shannon divergence between the state-trajectory distributions of the
demonstrator and the imitator. We evaluate our approach on a variety of
simulated robotics tasks of DeepMind Control suite and demonstrate the efficacy
of our approach at learning the demonstrator's behavior without having to copy
their actions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Recipient of the best paper award at RBCDSAI-DAI 2023, IIT Madras
  (https://rbcdsai.iitm.ac.in/DAI-2023/)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Role of Randomization in Adversarially Robust Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07221v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07221v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucas Gnecco-Heredia, Yann Chevaleyre, Benjamin Negrevergne, Laurent Meunier, Muni Sreenivas Pydi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks are known to be vulnerable to small adversarial
perturbations in test data. To defend against adversarial attacks,
probabilistic classifiers have been proposed as an alternative to deterministic
ones. However, literature has conflicting findings on the effectiveness of
probabilistic classifiers in comparison to deterministic ones. In this paper,
we clarify the role of randomization in building adversarially robust
classifiers. Given a base hypothesis set of deterministic classifiers, we show
the conditions under which a randomized ensemble outperforms the hypothesis set
in adversarial risk, extending previous results. Additionally, we show that for
any probabilistic classifier (including randomized ensembles), there exists a
deterministic classifier that outperforms it. Finally, we give an explicit
description of the deterministic hypothesis set that contains such a
deterministic classifier for many types of commonly used probabilistic
classifiers, i.e. randomized ensembles and parametric/input noise injection.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages + bibliography and appendix, 2 figures. This is a replacement
  with important changes, including a refinement of the main result in the last
  paper and a new section on passing from deterministic to randomized</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Expressivity of GNNs with Subgraph-specific Factor Embedded
  Normalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19903v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19903v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaixuan Chen, Shunyu Liu, Tongtian Zhu, Tongya Zheng, Haofei Zhang, Zunlei Feng, Jingwen Ye, Mingli Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks~(GNNs) have emerged as a powerful category of learning
architecture for handling graph-structured data. However, existing GNNs
typically ignore crucial structural characteristics in node-induced subgraphs,
which thus limits their expressiveness for various downstream tasks. In this
paper, we strive to strengthen the representative capabilities of GNNs by
devising a dedicated plug-and-play normalization scheme, termed as
SUbgraph-sPEcific FactoR Embedded Normalization (SuperNorm), that explicitly
considers the intra-connection information within each node-induced subgraph.
To this end, we embed the subgraph-specific factor at the beginning and the end
of the standard BatchNorm, as well as incorporate graph instance-specific
statistics for improved distinguishable capabilities. In the meantime, we
provide theoretical analysis to support that, with the elaborated SuperNorm, an
arbitrary GNN is at least as powerful as the 1-WL test in distinguishing
non-isomorphism graphs. Furthermore, the proposed SuperNorm scheme is also
demonstrated to alleviate the over-smoothing phenomenon. Experimental results
related to predictions of graph, node, and link properties on the eight popular
datasets demonstrate the effectiveness of the proposed method. The code is
available at \url{https://github.com/chenchkx/SuperNorm}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Gaussian Mixture Representations for Tensor Time Series
  Forecasting <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00390v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00390v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiewen Deng, Jinliang Deng, Renhe Jiang, Xuan Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tensor time series (TTS) data, a generalization of one-dimensional time
series on a high-dimensional space, is ubiquitous in real-world scenarios,
especially in monitoring systems involving multi-source spatio-temporal data
(e.g., transportation demands and air pollutants). Compared to modeling time
series or multivariate time series, which has received much attention and
achieved tremendous progress in recent years, tensor time series has been paid
less effort. Properly coping with the tensor time series is a much more
challenging task, due to its high-dimensional and complex inner structure. In
this paper, we develop a novel TTS forecasting framework, which seeks to
individually model each heterogeneity component implied in the time, the
location, and the source variables. We name this framework as GMRL, short for
Gaussian Mixture Representation Learning. Experiment results on two real-world
TTS datasets verify the superiority of our approach compared with the
state-of-the-art baselines. Code and data are published on
https://github.com/beginner-sketch/GMRL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCAI 2023 Main Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Group Fairness with Uncertainty in Sensitive Attributes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08077v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08077v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abhin Shah, Maohao Shen, Jongha Jon Ryu, Subhro Das, Prasanna Sattigeri, Yuheng Bu, Gregory W. Wornell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning a fair predictive model is crucial to mitigate biased decisions
against minority groups in high-stakes applications. A common approach to learn
such a model involves solving an optimization problem that maximizes the
predictive power of the model under an appropriate group fairness constraint.
However, in practice, sensitive attributes are often missing or noisy resulting
in uncertainty. We demonstrate that solely enforcing fairness constraints on
uncertain sensitive attributes can fall significantly short in achieving the
level of fairness of models trained without uncertainty. To overcome this
limitation, we propose a bootstrap-based algorithm that achieves the target
level of fairness despite the uncertainty in sensitive attributes. The
algorithm is guided by a Gaussian analysis for the independence notion of
fairness where we propose a robust quadratically constrained quadratic problem
to ensure a strict fairness guarantee with uncertain sensitive attributes. Our
algorithm is applicable to both discrete and continuous sensitive attributes
and is effective in real-world classification and regression tasks for various
group fairness notions, e.g., independence and separation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Open-TransMind: A New Baseline and Benchmark for 1st Foundation Model
  Challenge of Intelligent Transportation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifeng Shi, Feng Lv, Xinliang Wang, Chunlong Xia, Shaojie Li, Shujie Yang, Teng Xi, Gang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the continuous improvement of computing power and deep learning
algorithms in recent years, the foundation model has grown in popularity.
Because of its powerful capabilities and excellent performance, this technology
is being adopted and applied by an increasing number of industries. In the
intelligent transportation industry, artificial intelligence faces the
following typical challenges: few shots, poor generalization, and a lack of
multi-modal techniques. Foundation model technology can significantly alleviate
the aforementioned issues. To address these, we designed the 1st Foundation
Model Challenge, with the goal of increasing the popularity of foundation model
technology in traffic scenarios and promoting the rapid development of the
intelligent transportation industry. The challenge is divided into two tracks:
all-in-one and cross-modal image retrieval. Furthermore, we provide a new
baseline and benchmark for the two tracks, called Open-TransMind. According to
our knowledge, Open-TransMind is the first open-source transportation
foundation model with multi-task and multi-modal capabilities. Simultaneously,
Open-TransMind can achieve state-of-the-art performance on detection,
classification, and segmentation datasets of traffic scenarios. Our source code
is available at https://github.com/Traffic-X/Open-TransMind.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Policy Gradient in Robust MDPs with Global Convergence Guarantee 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10439v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10439v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiuhao Wang, Chin Pang Ho, Marek Petrik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Robust Markov decision processes (RMDPs) provide a promising framework for
computing reliable policies in the face of model errors. Many successful
reinforcement learning algorithms build on variations of policy-gradient
methods, but adapting these methods to RMDPs has been challenging. As a result,
the applicability of RMDPs to large, practical domains remains limited. This
paper proposes a new Double-Loop Robust Policy Gradient (DRPG), the first
generic policy gradient method for RMDPs. In contrast with prior robust policy
gradient algorithms, DRPG monotonically reduces approximation errors to
guarantee convergence to a globally optimal policy in tabular RMDPs. We
introduce a novel parametric transition kernel and solve the inner loop robust
policy via a gradient-based method. Finally, our numerical results demonstrate
the utility of our new algorithm and confirm its global convergence properties.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Temporal Difference Learning with Continuous Time and State in the
  Stochastic Setting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.07960v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.07960v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziad Kobeissi, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the problem of continuous-time policy evaluation. This consists
in learning through observations the value function associated with an
uncontrolled continuous-time stochastic dynamic and a reward function. We
propose two original variants of the well-known TD(0) method using vanishing
time steps. One is model-free and the other is model-based. For both methods,
we prove theoretical convergence rates that we subsequently verify through
numerical simulations. Alternatively, those methods can be interpreted as novel
reinforcement learning approaches for approximating solutions of linear PDEs
(partial differential equations) or linear BSDEs (backward stochastic
differential equations).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Two Losses Are Better Than One: Faster Optimization Using a Cheaper
  Proxy 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03542v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03542v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Blake Woodworth, Konstantin Mishchenko, Francis Bach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present an algorithm for minimizing an objective with hard-to-compute
gradients by using a related, easier-to-access function as a proxy. Our
algorithm is based on approximate proximal point iterations on the proxy
combined with relatively few stochastic gradients from the objective. When the
difference between the objective and the proxy is $\delta$-smooth, our
algorithm guarantees convergence at a rate matching stochastic gradient descent
on a $\delta$-smooth objective, which can lead to substantially better sample
efficiency. Our algorithm has many potential applications in machine learning,
and provides a principled means of leveraging synthetic data, physics
simulators, mixed public and private data, and more.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vid2Act: Activate Offline Videos for Visual RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03360v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03360v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minting Pan, Yitao Zheng, Wendong Zhang, Yunbo Wang, Xiaokang Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pretraining RL models on offline video datasets is a promising way to improve
their training efficiency in online tasks, but challenging due to the inherent
mismatch in tasks, dynamics, and behaviors across domains. A recent model, APV,
sidesteps the accompanied action records in offline datasets and instead
focuses on pretraining a task-irrelevant, action-free world model within the
source domains. We present Vid2Act, a model-based RL method that learns to
transfer valuable action-conditioned dynamics and potentially useful action
demonstrations from offline to online settings. The main idea is to use the
world models not only as simulators for behavior learning but also as tools to
measure the domain relevance for both dynamics representation transfer and
policy transfer. Specifically, we train the world models to generate a set of
time-varying task similarities using a domain-selective knowledge distillation
loss. These similarities serve two purposes: (i) adaptively transferring the
most useful source knowledge to facilitate dynamics learning, and (ii) learning
to replay the most relevant source actions to guide the target policy. We
demonstrate the advantages of Vid2Act over the action-free visual RL
pretraining method in both Meta-World and DeepMind Control Suite.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ROIPCA: An online memory-restricted PCA algorithm based on rank-one
  updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1911.11049v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1911.11049v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roy Mitz, Yoel Shkolnisky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Principal components analysis (PCA) is a fundamental algorithm in data
analysis. Its memory-restricted online versions are useful in many modern
applications, where the data are too large to fit in memory, or when data
arrive as a stream of items. In this paper, we propose ROIPCA and fROIPCA, two
online PCA algorithms that are based on rank-one updates. While ROIPCA is
typically more accurate, fROIPCA is faster and has comparable accuracy. We show
the relation between fROIPCA and an existing popular gradient algorithm for
online PCA, and in particular, prove that fROIPCA is in fact a gradient
algorithm with an optimal learning rate. We demonstrate numerically the
advantages of our algorithms over existing state-of-the-art algorithms in terms
of accuracy and runtime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Planning Multiple Epidemic Interventions with Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12802v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12802v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anh Mai, Nikunj Gupta, Azza Abouzied, Dennis Shasha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Combating an epidemic entails finding a plan that describes when and how to
apply different interventions, such as mask-wearing mandates, vaccinations,
school or workplace closures. An optimal plan will curb an epidemic with
minimal loss of life, disease burden, and economic cost. Finding an optimal
plan is an intractable computational problem in realistic settings.
Policy-makers, however, would greatly benefit from tools that can efficiently
search for plans that minimize disease and economic costs especially when
considering multiple possible interventions over a continuous and complex
action space given a continuous and equally complex state space. We formulate
this problem as a Markov decision process. Our formulation is unique in its
ability to represent multiple continuous interventions over any disease model
defined by ordinary differential equations. We illustrate how to effectively
apply state-of-the-art actor-critic reinforcement learning algorithms (PPO and
SAC) to search for plans that minimize overall costs. We empirically evaluate
the learning performance of these algorithms and compare their performance to
hand-crafted baselines that mimic plans constructed by policy-makers. Our
method outperforms baselines. Our work confirms the viability of a
computational approach to support policy-makers
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Large-Scale Study of Probabilistic Calibration in Neural Network
  Regression <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02738v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02738v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Dheur, Souhaib Ben Taieb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate probabilistic predictions are essential for optimal decision making.
While neural network miscalibration has been studied primarily in
classification, we investigate this in the less-explored domain of regression.
We conduct the largest empirical study to date to assess the probabilistic
calibration of neural networks. We also analyze the performance of
recalibration, conformal, and regularization methods to enhance probabilistic
calibration. Additionally, we introduce novel differentiable recalibration and
regularization methods, uncovering new insights into their effectiveness. Our
findings reveal that regularization methods offer a favorable tradeoff between
calibration and sharpness. Post-hoc methods exhibit superior probabilistic
calibration, which we attribute to the finite-sample coverage guarantee of
conformal prediction. Furthermore, we demonstrate that quantile recalibration
can be considered as a specific case of conformal prediction. Our study is
fully reproducible and implemented in a common code base for fair comparisons.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 40th International Conference on Machine Learning
  (ICML 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Digital Audio Forensics: Blind Human Voice Mimicry Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.12573v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.12573v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahar Al Ajmi, Khizar Hayat, Alaa M. Al Obaidi, Naresh Kumar, Munaf Najmuldeen, Baptiste Magnier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Audio is one of the most used ways of human communication, but at the same
time it can be easily misused to trick people. With the revolution of AI, the
related technologies are now accessible to almost everyone thus making it
simple for the criminals to commit crimes and forgeries. In this work, we
introduce a deep learning method to develop a classifier that will blindly
classify an input audio as real or mimicked; the word 'blindly' refers to the
ability to detect mimicked audio without references or real sources. The
proposed model was trained on a set of important features extracted from a
large dataset of audios to get a classifier that was tested on the same set of
features from different audios. The data was extracted from two raw datasets,
especially composed for this work; an all English dataset and a mixed dataset
(Arabic plus English). These datasets have been made available, in raw form,
through GitHub for the use of the research community at
https://github.com/SaSs7/Dataset. For the purpose of comparison, the audios
were also classified through human inspection with the subjects being the
native speakers. The ensued results were interesting and exhibited formidable
accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures (6 if you count subfigures), 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Tail Neural Network for Realtime Custom Keyword Spotting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.12933v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.12933v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihao Xue, Qianyao Shen, Guoqing Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose a Boosting Tail Neural Network (BTNN) for improving
the performance of Realtime Custom Keyword Spotting (RCKS) that is still an
industrial challenge for demanding powerful classification ability with limited
computation resources. Inspired by Brain Science that a brain is only partly
activated for a nerve simulation and numerous machine learning algorithms are
developed to use a batch of weak classifiers to resolve arduous problems, which
are often proved to be effective. We show that this method is helpful to the
RCKS problem. The proposed approach achieve better performances in terms of
wakeup rate and false alarm.
  In our experiments compared with those traditional algorithms that use only
one strong classifier, it gets 18\% relative improvement. We also point out
that this approach may be promising in future ASR exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, 8 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Trustworthiness Score to Evaluate CNNs Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08839v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08839v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abanoub Ghobrial, Darryl Hond, Hamid Asgari, Kerstin Eder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the black box nature of Convolutional Neural Networks (CNNs), the
continuous validation of CNNs during operation is challenging with the absence
of a human monitor. As a result this makes it difficult for developers and
regulators to gain confidence in the deployment of autonomous systems employing
CNNs. It is critical for safety during operation to know when CNN's predictions
are trustworthy or suspicious. With the absence of a human monitor, the basic
approach is to use the model's output confidence score to assess if predictions
are trustworthy or suspicious. However, the model's confidence score is a
result of computations coming from a black box, therefore lacks transparency
and makes it challenging to automatedly credit trustworthiness to predictions.
We introduce the trustworthiness score (TS), a simple metric that provides a
more transparent and effective way of providing confidence in CNNs predictions
compared to model's confidence score. The metric quantifies the trustworthiness
in a prediction by checking for the existence of certain features in the
predictions made by the CNN. We also use the underlying idea of the TS metric,
to provide a suspiciousness score (SS) in the overall input frame to help in
the detection of suspicious frames where false negatives exist. We conduct a
case study using YOLOv5 on persons detection to demonstrate our method and
usage of TS and SS. The case study shows that using our method consistently
improves the precision of predictions compared to relying on model confidence
score alone, for both 1) approving of trustworthy predictions (~20%
improvement) and 2) detecting suspicious frames (~5% improvement).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Literature <span class="highlight-title">Review</span>: Computer Vision Applications in Transportation
  Logistics and Warehousing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06009v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06009v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Naumann, Felix Hertlein, Laura Dörr, Steffen Thoma, Kai Furmans
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Computer vision applications in transportation logistics and warehousing have
a huge potential for process automation. We present a structured literature
review on research in the field to help leverage this potential. The literature
is categorized w.r.t. the application, i.e. the task it tackles and w.r.t. the
computer vision techniques that are used. Regarding applications, we subdivide
the literature in two areas: Monitoring, i.e. observing and retrieving relevant
information from the environment, and manipulation, where approaches are used
to analyze and interact with the environment. Additionally, we point out
directions for future research and link to recent developments in computer
vision that are suitable for application in logistics. Finally, we present an
overview of existing datasets and industrial solutions. The results of our
analysis are also available online at https://a-nau.github.io/cv-in-logistics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SGD with Large Step Sizes Learns Sparse Features <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.05337v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.05337v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksym Andriushchenko, Aditya Varre, Loucas Pillaud-Vivien, Nicolas Flammarion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We showcase important features of the dynamics of the Stochastic Gradient
Descent (SGD) in the training of neural networks. We present empirical
observations that commonly used large step sizes (i) lead the iterates to jump
from one side of a valley to the other causing loss stabilization, and (ii)
this stabilization induces a hidden stochastic dynamics orthogonal to the
bouncing directions that biases it implicitly toward sparse predictors.
Furthermore, we show empirically that the longer large step sizes keep SGD high
in the loss landscape valleys, the better the implicit regularization can
operate and find sparse representations. Notably, no explicit regularization is
used so that the regularization effect comes solely from the SGD training
dynamics influenced by the step size schedule. Therefore, these observations
unveil how, through the step size schedules, both gradient and noise drive
together the SGD dynamics through the loss landscape of neural networks. We
justify these findings theoretically through the study of simple neural network
models as well as qualitative arguments inspired from stochastic processes.
Finally, this analysis allows us to shed a new light on some common practice
and observed phenomena when training neural networks. The code of our
experiments is available at https://github.com/tml-epfl/sgd-sparse-features.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The camera-ready version (ICML 2023): extended experiments on deep
  networks (DenseNets on CIFAR-10, CIFAR-100, and Tiny ImageNet), empirically
  validated the SDE modelling, improved the clarity of the paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Deep Learning Framework for Verilog Autocompletion Towards Design and
  Verification Automation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.13840v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.13840v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enrique Dehaerne, Bappaditya Dey, Sandip Halder, Stefan De Gendt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Innovative Electronic Design Automation (EDA) solutions are important to meet
the design requirements for increasingly complex electronic devices. Verilog, a
hardware description language, is widely used for the design and verification
of digital circuits and is synthesized using specific EDA tools. However,
writing code is a repetitive and time-intensive task. This paper proposes,
primarily, a novel deep learning framework for training a Verilog
autocompletion model and, secondarily, a Verilog dataset of files and snippets
obtained from open-source repositories. The framework involves integrating
models pretrained on general programming language data and finetuning them on a
dataset curated to be similar to a target downstream task. This is validated by
comparing different pretrained models trained on different subsets of the
proposed Verilog dataset using multiple evaluation metrics. These experiments
demonstrate that the proposed framework achieves better BLEU, ROUGE-L, and chrF
scores by 9.5%, 6.7%, and 6.9%, respectively, compared to a model trained from
scratch. Code and data are made available at:
https://github.com/99EnriqueD/verilog_autocompletion .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated text to correct language errors and added a link to
  supplementary code and data
  (https://github.com/99EnriqueD/verilog_autocompletion). 6 pages, 3 figures, 4
  tables. To be presented as a WIP poster at DAC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learning Testing in an ADAS Case Study Using
  Simulation-Integrated Bio-Inspired Search-Based Testing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.12026v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.12026v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahshid Helali Moghadam, Markus Borg, Mehrdad Saadatmand, Seyed Jalaleddin Mousavirad, Markus Bohlin, Björn Lisper
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an extended version of Deeper, a search-based
simulation-integrated test solution that generates failure-revealing test
scenarios for testing a deep neural network-based lane-keeping system. In the
newly proposed version, we utilize a new set of bio-inspired search algorithms,
genetic algorithm (GA), $({\mu}+{\lambda})$ and $({\mu},{\lambda})$ evolution
strategies (ES), and particle swarm optimization (PSO), that leverage a quality
population seed and domain-specific cross-over and mutation operations tailored
for the presentation model used for modeling the test scenarios. In order to
demonstrate the capabilities of the new test generators within Deeper, we carry
out an empirical evaluation and comparison with regard to the results of five
participating tools in the cyber-physical systems testing competition at SBST
2021. Our evaluation shows the newly proposed test generators in Deeper not
only represent a considerable improvement on the previous version but also
prove to be effective and efficient in provoking a considerable number of
diverse failure-revealing test scenarios for testing an ML-driven lane-keeping
system. They can trigger several failures while promoting test scenario
diversity, under a limited test time budget, high target failure severity, and
strict speed limit constraints.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in Journal Of Software: Evolution And
  Process</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Distributed Bayesian Linear Regression with MCMC 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13778v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13778v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Barış Alparslan, Sinan Yıldırım, Ş. İlker Birbil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel Bayesian inference framework for distributed
differentially private linear regression. We consider a distributed setting
where multiple parties hold parts of the data and share certain summary
statistics of their portions in privacy-preserving noise. We develop a novel
generative statistical model for privately shared statistics, which exploits a
useful distributional relation between the summary statistics of linear
regression. Bayesian estimation of the regression coefficients is conducted
mainly using Markov chain Monte Carlo algorithms, while we also provide a fast
version to perform Bayesian estimation in one iteration. The proposed methods
have computational advantages over their competitors. We provide numerical
results on both real and simulated data, which demonstrate that the proposed
algorithms provide well-rounded estimation and prediction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 4 figures, code available at:
  https://github.com/sinanyildirim/Bayesian_DP_dist_LR</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Modern Look at the Relationship between Sharpness and Generalization <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07011v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07011v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksym Andriushchenko, Francesco Croce, Maximilian Müller, Matthias Hein, Nicolas Flammarion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sharpness of minima is a promising quantity that can correlate with
generalization in deep networks and, when optimized during training, can
improve generalization. However, standard sharpness is not invariant under
reparametrizations of neural networks, and, to fix this,
reparametrization-invariant sharpness definitions have been proposed, most
prominently adaptive sharpness (Kwon et al., 2021). But does it really capture
generalization in modern practical settings? We comprehensively explore this
question in a detailed study of various definitions of adaptive sharpness in
settings ranging from training from scratch on ImageNet and CIFAR-10 to
fine-tuning CLIP on ImageNet and BERT on MNLI. We focus mostly on transformers
for which little is known in terms of sharpness despite their widespread usage.
Overall, we observe that sharpness does not correlate well with generalization
but rather with some training parameters like the learning rate that can be
positively or negatively correlated with generalization depending on the setup.
Interestingly, in multiple cases, we observe a consistent negative correlation
of sharpness with out-of-distribution error implying that sharper minima can
generalize better. Finally, we illustrate on a simple model that the right
sharpness measure is highly data-dependent, and that we do not understand well
this aspect for realistic data distributions. The code of our experiments is
available at https://github.com/tml-epfl/sharpness-vs-generalization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The camera-ready version (accepted at ICML 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rigid Body Flows for Sampling Molecular Crystal Structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11355v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11355v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Köhler, Michele Invernizzi, Pim de Haan, Frank Noé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Normalizing flows (NF) are a class of powerful generative models that have
gained popularity in recent years due to their ability to model complex
distributions with high flexibility and expressiveness. In this work, we
introduce a new type of normalizing flow that is tailored for modeling
positions and orientations of multiple objects in three-dimensional space, such
as molecules in a crystal. Our approach is based on two key ideas: first, we
define smooth and expressive flows on the group of unit quaternions, which
allows us to capture the continuous rotational motion of rigid bodies; second,
we use the double cover property of unit quaternions to define a proper density
on the rotation group. This ensures that our model can be trained using
standard likelihood-based methods or variational inference with respect to a
thermodynamic target density. We evaluate the method by training Boltzmann
generators for two molecular examples, namely the multi-modal density of a
tetrahedral system in an external field and the ice XI phase in the TIP4P water
model. Our flows can be combined with flows operating on the internal degrees
of freedom of molecules and constitute an important step towards the modeling
of distributions of many interacting molecules.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Machine Learning, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Gaussian Hierarchical Latent Dirichlet Allocation: Bringing Polysemy
  Back 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2002.10855v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2002.10855v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takahiro Yoshida, Ryohei Hisano, Takaaki Ohnishi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topic models are widely used to discover the latent representation of a set
of documents. The two canonical models are latent Dirichlet allocation, and
Gaussian latent Dirichlet allocation, where the former uses multinomial
distributions over words, and the latter uses multivariate Gaussian
distributions over pre-trained word embedding vectors as the latent topic
representations, respectively. Compared with latent Dirichlet allocation,
Gaussian latent Dirichlet allocation is limited in the sense that it does not
capture the polysemy of a word such as ``bank.'' In this paper, we show that
Gaussian latent Dirichlet allocation could recover the ability to capture
polysemy by introducing a hierarchical structure in the set of topics that the
model can use to represent a given document. Our Gaussian hierarchical latent
Dirichlet allocation significantly improves polysemy detection compared with
Gaussian-based models and provides more parsimonious topic representations
compared with hierarchical latent Dirichlet allocation. Our extensive
quantitative experiments show that our model also achieves better topic
coherence and held-out document predictive accuracy over a wide range of corpus
and word embedding vectors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Random Grid Neural Processes for Parametric Partial Differential
  Equations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11040v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11040v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnaud Vadeboncoeur, Ieva Kazlauskaite, Yanni Papandreou, Fehmi Cirak, Mark Girolami, Ömer Deniz Akyildiz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new class of spatially stochastic physics and data informed
deep latent models for parametric partial differential equations (PDEs) which
operate through scalable variational neural processes. We achieve this by
assigning probability measures to the spatial domain, which allows us to treat
collocation grids probabilistically as random variables to be marginalised out.
Adapting this spatial statistics view, we solve forward and inverse problems
for parametric PDEs in a way that leads to the construction of Gaussian process
models of solution fields. The implementation of these random grids poses a
unique set of challenges for inverse physics informed deep learning frameworks
and we propose a new architecture called Grid Invariant Convolutional Networks
(GICNets) to overcome these challenges. We further show how to incorporate
noisy data in a principled manner into our physics informed model to improve
predictions for problems where data may be available but whose measurement
location does not coincide with any fixed mesh or grid. The proposed method is
tested on a nonlinear Poisson problem, Burgers equation, and Navier-Stokes
equations, and we provide extensive numerical comparisons. We demonstrate
significant computational advantages over current physics informed neural
learning methods for parametric PDEs while improving the predictive
capabilities and flexibility of these models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dual Propagation: Accelerating Contrastive Hebbian Learning with Dyadic
  Neurons 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01228v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01228v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rasmus Høier, D. Staudt, Christopher Zach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Activity difference based learning algorithms-such as contrastive Hebbian
learning and equilibrium propagation-have been proposed as biologically
plausible alternatives to error back-propagation. However, on traditional
digital chips these algorithms suffer from having to solve a costly inference
problem twice, making these approaches more than two orders of magnitude slower
than back-propagation. In the analog realm equilibrium propagation may be
promising for fast and energy efficient learning, but states still need to be
inferred and stored twice. Inspired by lifted neural networks and compartmental
neuron models we propose a simple energy based compartmental neuron model,
termed dual propagation, in which each neuron is a dyad with two intrinsic
states. At inference time these intrinsic states encode the error/activity
duality through their difference and their mean respectively. The advantage of
this method is that only a single inference phase is needed and that inference
can be solved in layerwise closed-form. Experimentally we show on common
computer vision datasets, including Imagenet32x32, that dual propagation
performs equivalently to back-propagation both in terms of accuracy and
runtime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Added reflections on biological plausibility and results comparisons
  to state-of-the-art versions of equilibrium propagation and difference target
  propagation</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Federated Deep Learning for Intrusion Detection in IoT Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02715v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02715v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Othmane Belarbi, Theodoros Spyridopoulos, Eirini Anthi, Ioannis Mavromatis, Pietro Carnelli, Aftab Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vast increase of IoT technologies and the ever-evolving attack vectors
and threat actors have increased cyber-security risks dramatically. Novel
attacks can compromise IoT devices to gain access to sensitive data or control
them to deploy further malicious activities. The detection of novel attacks
often relies upon AI solutions. A common approach to implementing AI-based IDS
in distributed IoT systems is in a centralised manner. However, this approach
may violate data privacy and secrecy. In addition, centralised data collection
prohibits the scale-up of IDSs. Therefore, intrusion detection solutions in IoT
ecosystems need to move towards a decentralised direction. FL has attracted
significant interest in recent years due to its ability to perform
collaborative learning while preserving data confidentiality and locality.
Nevertheless, most FL-based IDS for IoT systems are designed under unrealistic
data distribution conditions. To that end, we design an experiment
representative of the real world and evaluate the performance of two FL IDS
implementations, one based on DNNs and another on our previous work on DBNs.
For our experiments, we rely on TON-IoT, a realistic IoT network traffic
dataset, associating each IP address with a single FL client. Additionally, we
explore pre-training and investigate various aggregation methods to mitigate
the impact of data heterogeneity. Lastly, we benchmark our approach against a
centralised solution. The comparison shows that the heterogeneous nature of the
data has a considerable negative impact on the model performance when trained
in a distributed manner. However, in the case of a pre-trained initial global
FL model, we demonstrate a performance improvement of over 20% (F1-score) when
compared against a randomly initiated global model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figues, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Neural Collapse in Deep Linear Networks: From Balanced to Imbalanced
  Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.00437v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.00437v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hien Dang, Tho Tran, Stanley Osher, Hung Tran-The, Nhat Ho, Tan Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern deep neural networks have achieved impressive performance on tasks
from image classification to natural language processing. Surprisingly, these
complex systems with massive amounts of parameters exhibit the same structural
properties in their last-layer features and classifiers across canonical
datasets when training until convergence. In particular, it has been observed
that the last-layer features collapse to their class-means, and those
class-means are the vertices of a simplex Equiangular Tight Frame (ETF). This
phenomenon is known as Neural Collapse ($\mathcal{NC}$). Recent papers have
theoretically shown that $\mathcal{NC}$ emerges in the global minimizers of
training problems with the simplified ``unconstrained feature model''. In this
context, we take a step further and prove the $\mathcal{NC}$ occurrences in
deep linear networks for the popular mean squared error (MSE) and cross entropy
(CE) losses, showing that global solutions exhibit $\mathcal{NC}$ properties
across the linear layers. Furthermore, we extend our study to imbalanced data
for MSE loss and present the first geometric analysis of $\mathcal{NC}$ under
bias-free setting. Our results demonstrate the convergence of the last-layer
features and classifiers to a geometry consisting of orthogonal vectors, whose
lengths depend on the amount of data in their corresponding classes. Finally,
we empirically validate our theoretical analyses on synthetic and practical
network architectures with both balanced and imbalanced scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>75 pages, 20 figures, 4 tables. Hien Dang and Tho Tran contributed
  equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Quantum Machine Learning for Malware Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09674v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09674v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Grégoire Barrué, Tony Quertier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In a context of malicious software detection, machine learning (ML) is widely
used to generalize to new malware. However, it has been demonstrated that ML
models can be fooled or may have generalization problems on malware that has
never been seen. We investigate the possible benefits of quantum algorithms for
classification tasks. We implement two models of Quantum Machine Learning
algorithms, and we compare them to classical models for the classification of a
dataset composed of malicious and benign executable files. We try to optimize
our algorithms based on methods found in the literature, and analyze our
results in an exploratory way, to identify the most interesting directions to
explore for the future.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PCT-CycleGAN: Paired Complementary Temporal Cycle-Consistent Adversarial
  Networks for Radar-Based Precipitation Nowcasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15046v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15046v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jaeho Choi, Yura Kim, Kwang-Ho Kim, Sung-Hwa Jung, Ikhyun Cho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The precipitation nowcasting methods have been elaborated over the centuries
because rain has a crucial impact on human life. Not only quantitative
precipitation forecast (QPF) models and convolutional long short-term memory
(ConvLSTM), but also various sophisticated methods such as the latest MetNet-2
are emerging. In this paper, we propose a paired complementary temporal
cycle-consistent adversarial networks (PCT-CycleGAN) for radar-based
precipitation nowcasting, inspired by cycle-consistent adversarial networks
(CycleGAN), which shows strong performance in image-to-image translation.
PCT-CycleGAN generates temporal causality using two generator networks with
forward and backward temporal dynamics in paired complementary cycles. Each
generator network learns a huge number of one-to-one mappings about
time-dependent radar-based precipitation data to approximate a mapping function
representing the temporal dynamics in each direction. To create robust temporal
causality between paired complementary cycles, novel connection loss is
proposed. And torrential loss to cover exceptional heavy rain events is also
proposed. The generator network learning forward temporal dynamics in
PCT-CycleGAN generates radar-based precipitation data 10 minutes from the
current time. Also, it provides a reliable prediction of up to 2 hours with
iterative forecasting. The superiority of PCT-CycleGAN is demonstrated through
qualitative and quantitative comparisons with several previous methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explaining the Explainers in Graph Neural Networks: a Comparative Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Antonio Longa, Steve Azzolin, Gabriele Santin, Giulia Cencetti, Pietro Liò, Bruno Lepri, Andrea Passerini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Following a fast initial breakthrough in graph based learning, Graph Neural
Networks (GNNs) have reached a widespread application in many science and
engineering fields, prompting the need for methods to understand their decision
process.
  GNN explainers have started to emerge in recent years, with a multitude of
methods both novel or adapted from other domains. To sort out this plethora of
alternative approaches, several studies have benchmarked the performance of
different explainers in terms of various explainability metrics. However, these
earlier works make no attempts at providing insights into why different GNN
architectures are more or less explainable, or which explainer should be
preferred in a given setting.
  In this survey, we fill these gaps by devising a systematic experimental
study, which tests ten explainers on eight representative architectures trained
on six carefully designed graph and node classification datasets. With our
results we provide key insights on the choice and applicability of GNN
explainers, we isolate key components that make them usable and successful and
provide recommendations on how to avoid common interpretation pitfalls. We
conclude by highlighting open questions and directions of possible future
research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DevFormer: A Symmetric <span class="highlight-title">Transformer</span> for Context-Aware Device Placement <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.13225v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.13225v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haeyeon Kim, Minsu Kim, Federico Berto, Joungho Kim, Jinkyoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present DevFormer, a novel transformer-based architecture
for addressing the complex and computationally demanding problem of hardware
design optimization. Despite the demonstrated efficacy of transformers in
domains including natural language processing and computer vision, their use in
hardware design has been limited by the scarcity of offline data. Our approach
addresses this limitation by introducing strong inductive biases such as
relative positional embeddings and action-permutation symmetricity that
effectively capture the hardware context and enable efficient design
optimization with limited offline data. We apply DevFoemer to the problem of
decoupling capacitor placement and show that it outperforms state-of-the-art
methods in both simulated and real hardware, leading to improved performances
while reducing the number of components by more than $30\%$. Finally, we show
that our approach achieves promising results in other offline contextual
learning-based combinatorial optimization tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Machine Learning (ICML) 2023. Extended
  version of NeurIPS 2022 Offline RL Workshop "Collaborative symmetricity
  exploitation for offline learning of hardware design solver"</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Answering Complex Logical Queries on Knowledge Graphs via Query
  Computation Tree Optimization <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09567v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09567v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushi Bai, Xin Lv, Juanzi Li, Lei Hou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Answering complex logical queries on incomplete knowledge graphs is a
challenging task, and has been widely studied. Embedding-based methods require
training on complex queries, and cannot generalize well to out-of-distribution
query structures. Recent work frames this task as an end-to-end optimization
problem, and it only requires a pretrained link predictor. However, due to the
exponentially large combinatorial search space, the optimal solution can only
be approximated, limiting the final accuracy. In this work, we propose QTO
(Query Computation Tree Optimization) that can efficiently find the exact
optimal solution. QTO finds the optimal solution by a forward-backward
propagation on the tree-like computation graph, i.e., query computation tree.
In particular, QTO utilizes the independence encoded in the query computation
tree to reduce the search space, where only local computations are involved
during the optimization procedure. Experiments on 3 datasets show that QTO
obtains state-of-the-art performance on complex query answering, outperforming
previous best results by an average of 22%. Moreover, QTO can interpret the
intermediate solutions for each of the one-hop atoms in the query with over 90%
accuracy. The code of our paper is at https://github.com/bys0318/QTO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Flat Seeking Bayesian Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02713v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02713v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Van-Anh Nguyen, Tung-Long Vuong, Hoang Phan, Thanh-Toan Do, Dinh Phung, Trung Le
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bayesian Neural Networks (BNNs) provide a probabilistic interpretation for
deep learning models by imposing a prior distribution over model parameters and
inferring a posterior distribution based on observed data. The model sampled
from the posterior distribution can be used for providing ensemble predictions
and quantifying prediction uncertainty. It is well-known that deep learning
models with lower sharpness have better generalization ability. However,
existing posterior inferences are not aware of sharpness/flatness in terms of
formulation, possibly leading to high sharpness for the models sampled from
them. In this paper, we develop theories, the Bayesian setting, and the
variational inference approach for the sharpness-aware posterior. Specifically,
the models sampled from our sharpness-aware posterior, and the optimal
approximate posterior estimating this sharpness-aware posterior, have better
flatness, hence possibly possessing higher generalization ability. We conduct
experiments by leveraging the sharpness-aware posterior with state-of-the-art
Bayesian Neural Networks, showing that the flat-seeking counterparts outperform
their baselines in all metrics of interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Weighted Strategy for Non-stationary Parametric Bandits <span class="chip">AISTATS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.02691v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.02691v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jing Wang, Peng Zhao, Zhi-Hua Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-stationary parametric bandits have attracted much attention recently.
There are three principled ways to deal with non-stationarity, including
sliding-window, weighted, and restart strategies. As many non-stationary
environments exhibit gradual drifting patterns, the weighted strategy is
commonly adopted in real-world applications. However, previous theoretical
studies show that its analysis is more involved and the algorithms are either
computationally less efficient or statistically suboptimal. This paper revisits
the weighted strategy for non-stationary parametric bandits. In linear bandits
(LB), we discover that this undesirable feature is due to an inadequate regret
analysis, which results in an overly complex algorithm design. We propose a
refined analysis framework, which simplifies the derivation and importantly
produces a simpler weight-based algorithm that is as efficient as
window/restart-based algorithms while retaining the same regret as previous
studies. Furthermore, our new framework can be used to improve regret bounds of
other parametric bandits, including Generalized Linear Bandits (GLB) and
Self-Concordant Bandits (SCB). For example, we develop a simple weighted GLB
algorithm with an $\widetilde{O}(k_\mu^{\frac{5}{4}} c_\mu^{-\frac{3}{4}}
d^{\frac{3}{4}} P_T^{\frac{1}{4}}T^{\frac{3}{4}})$ regret, improving the
$\widetilde{O}(k_\mu^{2} c_\mu^{-1}d^{\frac{9}{10}}
P_T^{\frac{1}{5}}T^{\frac{4}{5}})$ bound in prior work, where $k_\mu$ and
$c_\mu$ characterize the reward model's nonlinearity, $P_T$ measures the
non-stationarity, $d$ and $T$ denote the dimension and time horizon.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>AISTATS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Estimating 3D Dental Structures using Simulated Panoramic Radiographs
  and Neural Ray Tracing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04027v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04027v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sihwa Park, Seongjun Kim, Doeyoung Kwon, Yohan Jang, In-Seok Song, Seungjun Baek
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Panoramic radiography (Panoramic X-ray, PX) is a widely used imaging modality
for dental examination. Since PX only provides 2D flattened views of the oral
structure, its applicability is limited as compared to 3D Cone-beam computed
tomography (CBCT). In this paper, we propose a framework to estimate CBCT-like
3D structures from real-world PX. Our framework tackles full 3D reconstruction
for varying subjects (patients) where each reconstruction is based only on a
single panoramic image. We create an intermediate representation called
simulated PX (SimPX) from CBCT data which is based both on the Beer-Lambert law
of X-ray rendering and rotational principles of PX imaging. SimPX aims at not
only truthfully simulating PX, but also facilitates the reverting process back
to 3D data. We propose a novel neural model based on ray tracing which exploits
both global and local input features to convert SimPX to 3D output. At
inference, a real PX image is translated to a SimPX-style image with semantic
regularization, and the translated image is processed by generation/refinement
modules to produce high-quality outputs. Experiments show that our method
outperforms prior state-of-the-art in reconstruction tasks both quantitatively
and qualitatively. Our method does not require any prior information such as
the shape of dental arches, nor the matched PX-CBCT dataset for training, which
is difficult to obtain in clinical practice.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Test-Time Training on Nearest Neighbors for Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18466v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18466v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Moritz Hardt, Yu Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent efforts aim to augment language models with relevant information
retrieved from a database at test time. We avoid the need for prompt
engineering by directly fine-tuning the model on data retrieved at test time
using its standard training setup. For this purpose, we build a large-scale
distributed nearest neighbor index based on text embeddings of the Pile
dataset. Given a query to a language model, our system retrieves the neighbors
of the query and fine-tunes the model on the text data corresponding to those
neighbors. Surprisingly, retrieving and training on as few as 20 neighbors,
each for only one gradient iteration, drastically improves performance across
more than twenty language modeling tasks in the Pile benchmark. For example,
test-time training significantly narrows the performance gap between a small
GPT2 model and a GPTNeo model, more than ten times larger, that was
specifically trained to convergence on the Pile. Sufficient index quality and
size, however, are important. Our work establishes a valuable first baseline
for implementing test-time training in the context of large language models,
opening the door to numerous promising research avenues.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Corrected Figure 8. Code repository here:
  https://github.com/socialfoundations/tttlm</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Functional Equivalence and Path Connectivity of Reducible Hyperbolic
  Tangent Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05089v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05089v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Farrugia-Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the learning process of artificial neural networks requires
clarifying the structure of the parameter space within which learning takes
place. A neural network parameter's functional equivalence class is the set of
parameters implementing the same input--output function. For many
architectures, almost all parameters have a simple and well-documented
functional equivalence class. However, there is also a vanishing minority of
reducible parameters, with richer functional equivalence classes caused by
redundancies among the network's units.
  In this paper, we give an algorithmic characterisation of unit redundancies
and reducible functional equivalence classes for a single-hidden-layer
hyperbolic tangent architecture. We show that such functional equivalence
classes are piecewise-linear path-connected sets, and that for parameters with
a majority of redundant units, the sets have a diameter of at most 7 linear
segments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Solving NP-hard Min-max Routing Problems as Sequential Generation with
  Equity Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02689v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02689v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwoo Son, Minsu Kim, Sanghyeok Choi, Jinkyoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Min-max routing problems aim to minimize the maximum tour length among agents
as they collaboratively visit all cities, i.e., the completion time. These
problems include impactful real-world applications but are known as NP-hard.
Existing methods are facing challenges, particularly in large-scale problems
that require the coordination of numerous agents to cover thousands of cities.
This paper proposes a new deep-learning framework to solve large-scale min-max
routing problems. We model the simultaneous decision-making of multiple agents
as a sequential generation process, allowing the utilization of scalable
deep-learning models for sequential decision-making. In the sequentially
approximated problem, we propose a scalable contextual Transformer model,
Equity-Transformer, which generates sequential actions considering an equitable
workload among other agents. The effectiveness of Equity-Transformer is
demonstrated through its superior performance in two representative min-max
routing tasks: the min-max multiple traveling salesman problem (min-max mTSP)
and the min-max multiple pick-up and delivery problem (min-max mPDP). Notably,
our method achieves significant reductions of runtime, approximately 335 times,
and cost values of about 53% compared to a competitive heuristic (LKH3) in the
case of 100 vehicles with 1,000 cities of mTSP. We provide reproducible source
code: https://github.com/kaist-silab/equity-transformer
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Meta-SAGE: Scale Meta-Learning Scheduled Adaptation with Guided
  Exploration for Mitigating Scale Shift on Combinatorial Optimization <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02688v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02688v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwoo Son, Minsu Kim, Hyeonah Kim, Jinkyoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes Meta-SAGE, a novel approach for improving the scalability
of deep reinforcement learning models for combinatorial optimization (CO)
tasks. Our method adapts pre-trained models to larger-scale problems in test
time by suggesting two components: a scale meta-learner (SML) and scheduled
adaptation with guided exploration (SAGE). First, SML transforms the context
embedding for subsequent adaptation of SAGE based on scale information. Then,
SAGE adjusts the model parameters dedicated to the context embedding for a
specific instance. SAGE introduces locality bias, which encourages selecting
nearby locations to determine the next location. The locality bias gradually
decays as the model is adapted to the target instance. Results show that
Meta-SAGE outperforms previous adaptation methods and significantly improves
scalability in representative CO tasks. Our source code is available at
https://github.com/kaist-silab/meta-sage
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 9 figures, International Conference on Machine Learning
  (ICML) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Using Machine Teaching to Investigate Human Assumptions when Teaching
  Reinforcement Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2009.02476v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2009.02476v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yun-Shiuan Chuang, Xuezhou Zhang, Yuzhe Ma, Mark K. Ho, Joseph L. Austerweil, Xiaojin Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Successful teaching requires an assumption of how the learner learns - how
the learner uses experiences from the world to update their internal states. We
investigate what expectations people have about a learner when they teach them
in an online manner using rewards and punishment. We focus on a common
reinforcement learning method, Q-learning, and examine what assumptions people
have using a behavioral experiment. To do so, we first establish a normative
standard, by formulating the problem as a machine teaching optimization
problem. To solve the machine teaching optimization problem, we use a deep
learning approximation method which simulates learners in the environment and
learns to predict how feedback affects the learner's internal states. What do
people assume about a learner's learning and discount rates when they teach
them an idealized exploration-exploitation task? In a behavioral experiment, we
find that people can teach the task to Q-learners in a relatively efficient and
effective manner when the learner uses a small value for its discounting rate
and a large value for its learning rate. However, they still are suboptimal. We
also find that providing people with real-time updates of how possible feedback
would affect the Q-learner's internal states weakly helps them teach. Our
results reveal how people teach using evaluative feedback and provide guidance
for how engineers should design machine agents in a manner that is intuitive
for people.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ What Makes Data Suitable for a Locally Connected Neural Network? A
  Necessary and Sufficient Condition Based on Quantum Entanglement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11249v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11249v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yotam Alexander, Nimrod De La Vega, Noam Razin, Nadav Cohen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The question of what makes a data distribution suitable for deep learning is
a fundamental open problem. Focusing on locally connected neural networks (a
prevalent family of architectures that includes convolutional and recurrent
neural networks as well as local self-attention models), we address this
problem by adopting theoretical tools from quantum physics. Our main
theoretical result states that a certain locally connected neural network is
capable of accurate prediction over a data distribution if and only if the data
distribution admits low quantum entanglement under certain canonical partitions
of features. As a practical application of this result, we derive a
preprocessing method for enhancing the suitability of a data distribution to
locally connected neural networks. Experiments with widespread models over
various datasets demonstrate our findings. We hope that our use of quantum
entanglement will encourage further adoption of tools from physics for formally
reasoning about the relation between deep learning and real-world data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improved Privacy-Preserving PCA Using Space-optimized Homomorphic Matrix
  Multiplication 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17341v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17341v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xirong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Principal Component Analysis (PCA) is a pivotal technique in the fields of
machine learning and data analysis. It aims to reduce the dimensionality of a
dataset while minimizing the loss of information. In recent years, there have
been endeavors to utilize homomorphic encryption in privacy-preserving PCA
algorithms. These approaches commonly employ a PCA routine known as
PowerMethod, which takes the covariance matrix as input and generates an
approximate eigenvector corresponding to the primary component of the dataset.
However, their performance and accuracy are constrained by the incapability of
homomorphic covariance matrix computation and the absence of a universal vector
normalization strategy for the PowerMethod algorithm. In this study, we propose
a novel approach to privacy-preserving PCA that addresses these limitations,
resulting in superior efficiency, accuracy, and scalability compared to
previous approaches. We attain such efficiency and precision through the
following contributions: (i) We implement space optimization techniques for a
homomorphic matrix multiplication method (Jiang et al., SIGSAC 2018), making it
less prone to memory saturation in parallel computation scenarios. (ii)
Leveraging the benefits of this optimized matrix multiplication, we devise an
efficient homomorphic circuit for computing the covariance matrix
homomorphically. (iii) Utilizing the covariance matrix, we develop a novel and
efficient homomorphic circuit for the PowerMethod that incorporates a universal
homomorphic vector normalization strategy to enhance both its accuracy and
practicality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Blessings and Curses of Covariate Shifts: Adversarial Learning Dynamics,
  Directional Convergence, and Equilibria 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.02457v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.02457v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tengyuan Liang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Covariate distribution shifts and adversarial perturbations present
robustness challenges to the conventional statistical learning framework: mild
shifts in the test covariate distribution can significantly affect the
performance of the statistical model learned based on the training
distribution. The model performance typically deteriorates when extrapolation
happens: namely, covariates shift to a region where the training distribution
is scarce, and naturally, the learned model has little information. For
robustness and regularization considerations, adversarial perturbation
techniques are proposed as a remedy; however, careful study needs to be carried
out about what extrapolation region adversarial covariate shift will focus on,
given a learned model. This paper precisely characterizes the extrapolation
region, examining both regression and classification in an infinite-dimensional
setting. We study the implications of adversarial covariate shifts to
subsequent learning of the equilibrium -- the Bayes optimal model -- in a
sequential game framework. We exploit the dynamics of the adversarial learning
game and reveal the curious effects of the covariate shift to equilibrium
learning and experimental design. In particular, we establish two directional
convergence results that exhibit distinctive phenomena: (1) a blessing in
regression, the adversarial covariate shifts in an exponential rate to an
optimal experimental design for rapid subsequent learning, (2) a curse in
classification, the adversarial covariate shifts in a subquadratic rate fast to
the hardest experimental design trapping subsequent learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-06-06T00:00:00Z">2023-06-06</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">105</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CL-UZH at SemEval-2023 Task 10: Sexism Detection through Incremental
  Fine-Tuning and Multi-Task Learning with Label Descriptions <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janis Goldzycher
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The widespread popularity of social media has led to an increase in hateful,
abusive, and sexist language, motivating methods for the automatic detection of
such phenomena. The goal of the SemEval shared task \textit{Towards Explainable
Detection of Online Sexism} (EDOS 2023) is to detect sexism in English social
media posts (subtask A), and to categorize such posts into four coarse-grained
sexism categories (subtask B), and eleven fine-grained subcategories (subtask
C). In this paper, we present our submitted systems for all three subtasks,
based on a multi-task model that has been fine-tuned on a range of related
tasks and datasets before being fine-tuned on the specific EDOS subtasks. We
implement multi-task learning by formulating each task as binary pairwise text
classification, where the dataset and label descriptions are given along with
the input text. The results show clear improvements over a fine-tuned
DeBERTa-V3 serving as a baseline leading to $F_1$-scores of 85.9\% in subtask A
(rank 13/84), 64.8\% in subtask B (rank 19/69), and 44.9\% in subtask C
(26/63).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 4 figures, Accepted at The 17th International Workshop on
  Semantic Evaluation, ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Utterance Classification with Logical Neural Network: Explainable AI for
  Mental Disorder Diagnosis <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yeldar Toleubay, Don Joven Agravante, Daiki Kimura, Baihan Lin, Djallel Bouneffouf, Michiaki Tatsubori
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In response to the global challenge of mental health problems, we proposes a
Logical Neural Network (LNN) based Neuro-Symbolic AI method for the diagnosis
of mental disorders. Due to the lack of effective therapy coverage for mental
disorders, there is a need for an AI solution that can assist therapists with
the diagnosis. However, current Neural Network models lack explainability and
may not be trusted by therapists. The LNN is a Recurrent Neural Network
architecture that combines the learning capabilities of neural networks with
the reasoning capabilities of classical logic-based AI. The proposed system
uses input predicates from clinical interviews to output a mental disorder
class, and different predicate pruning techniques are used to achieve
scalability and higher scores. In addition, we provide an insight extraction
method to aid therapists with their diagnosis. The proposed system addresses
the lack of explainability of current Neural Network models and provides a more
trustworthy solution for mental disorder diagnosis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) with memory are computationally universal.
However, mainstream LLMs are not taking full advantage of memory, and the
designs are heavily influenced by biological brains. Due to their approximate
nature and proneness to the accumulation of errors, conventional neural memory
mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we
seek inspiration from modern computer architectures to augment LLMs with
symbolic memory for complex multi-hop reasoning. Such a symbolic memory
framework is instantiated as an LLM and a set of SQL databases, where the LLM
generates SQL instructions to manipulate the SQL databases. We validate the
effectiveness of the proposed memory framework on a synthetic dataset requiring
complex reasoning. The project website is available at
https://chatdatabase.github.io/ .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal interventions expose implicit situation models for commonsense
  language understanding <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03882v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03882v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takateru Yamakoshi, James L. McClelland, Adele E. Goldberg, Robert D. Hawkins
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accounts of human language processing have long appealed to implicit
``situation models'' that enrich comprehension with relevant but unstated world
knowledge. Here, we apply causal intervention techniques to recent transformer
models to analyze performance on the Winograd Schema Challenge (WSC), where a
single context cue shifts interpretation of an ambiguous pronoun. We identify a
relatively small circuit of attention heads that are responsible for
propagating information from the context word that guides which of the
candidate noun phrases the pronoun ultimately attends to. We then compare how
this circuit behaves in a closely matched ``syntactic'' control where the
situation model is not strictly necessary. These analyses suggest distinct
pathways through which implicit situation models are constructed to guide
pronoun resolution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deductive Verification of Chain-of-Thought Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) significantly benefit from Chain-of-Thought
(CoT) prompting in performing various reasoning tasks. While CoT allows models
to produce more comprehensive reasoning processes, its emphasis on intermediate
reasoning steps can inadvertently introduce hallucinations and accumulated
errors, thereby limiting models' ability to solve complex reasoning tasks.
Inspired by how humans engage in careful and meticulous deductive logical
reasoning processes to solve tasks, we seek to enable language models to
perform explicit and rigorous deductive reasoning, and also ensure the
trustworthiness of their reasoning process through self-verification. However,
directly verifying the validity of an entire deductive reasoning process is
challenging, even with advanced models like ChatGPT. In light of this, we
propose to decompose a reasoning verification process into a series of
step-by-step subprocesses, each only receiving their necessary context and
premises. To facilitate this procedure, we propose Natural Program, a natural
language-based deductive reasoning format. Our approach enables models to
generate precise reasoning steps where subsequent steps are more rigorously
grounded on prior steps. It also empowers language models to carry out
reasoning self-verification in a step-by-step manner. By integrating this
verification process into each deductive reasoning stage, we significantly
enhance the rigor and trustfulness of generated reasoning steps. Along this
process, we also improve the answer correctness on complex reasoning tasks.
Code will be released at https://github.com/lz1oceani/verify_cot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Correction of Errors in Preference Ratings from Automated Metrics for
  Text Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Deriu, Pius von Däniken, Don Tuggener, Mark Cieliebak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A major challenge in the field of Text Generation is evaluation: Human
evaluations are cost-intensive, and automated metrics often display
considerable disagreement with human judgments. In this paper, we propose a
statistical model of Text Generation evaluation that accounts for the
error-proneness of automated metrics when used to generate preference rankings
between system outputs. We show that existing automated metrics are generally
over-confident in assigning significant differences between systems in this
setting. However, our model enables an efficient combination of human and
automated ratings to remedy the error-proneness of the automated metrics. We
show that using this combination, we only require about 50% of the human
annotations typically used in evaluations to arrive at robust and statistically
significant results while yielding the same evaluation outcome as the pure
human evaluation in 95% of cases. We showcase the benefits of approach for
three text generation tasks: dialogue systems, machine translation, and text
summarization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Iterative Translation Refinement with Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03856v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03856v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pinzhen Chen, Zhicheng Guo, Barry Haddow, Kenneth Heafield
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have shown surprising performances in understanding
instructions and performing natural language tasks. In this paper, we propose
iterative translation refinement to leverage the power of large language models
for more natural translation and post-editing. We show that by simply involving
a large language model in an iterative process, the output quality improves
beyond mere translation. Extensive test scenarios with GPT-3.5 reveal that
although iterations reduce string-based metric scores, neural metrics indicate
comparable if not improved translation quality. Further, human evaluations
demonstrate that our method effectively reduces translationese compared to
initial GPT translations and even human references, especially for into-English
directions. Ablation studies underscore the importance of anchoring the
refinement process to the source input and a reasonable initial translation.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From Key Points to Key Point Hierarchy: Structured and Expressive
  Opinion Summarization <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03853v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03853v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arie Cattan, Lilach Eden, Yoav Kantor, Roy Bar-Haim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Key Point Analysis (KPA) has been recently proposed for deriving fine-grained
insights from collections of textual comments. KPA extracts the main points in
the data as a list of concise sentences or phrases, termed key points, and
quantifies their prevalence. While key points are more expressive than word
clouds and key phrases, making sense of a long, flat list of key points, which
often express related ideas in varying levels of granularity, may still be
challenging. To address this limitation of KPA, we introduce the task of
organizing a given set of key points into a hierarchy, according to their
specificity. Such hierarchies may be viewed as a novel type of Textual
Entailment Graph. We develop ThinkP, a high quality benchmark dataset of key
point hierarchies for business and product reviews, obtained by consolidating
multiple annotations. We compare different methods for predicting pairwise
relations between key points, and for inferring a hierarchy from these pairwise
predictions. In particular, for the task of computing pairwise key point
relations, we achieve significant gains over existing strong baselines by
applying directional distributional similarity methods to a novel
distributional representation of key points, and further boost performance via
weak supervision.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEACE: Perfect linear concept erasure in closed form 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, Stella Biderman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept erasure aims to remove specified features from a representation. It
can be used to improve fairness (e.g. preventing a classifier from using gender
or race) and interpretability (e.g. removing a concept to observe changes in
model behavior). In this paper, we introduce LEAst-squares Concept Erasure
(LEACE), a closed-form method which provably prevents all linear classifiers
from detecting a concept while inflicting the least possible damage to the
representation. We apply LEACE to large language models with a novel procedure
called "concept scrubbing," which erases target concept information from every
layer in the network. We demonstrate the usefulness of our method on two tasks:
measuring the reliance of language models on part-of-speech information, and
reducing gender bias in BERT embeddings. Code is available at
https://github.com/EleutherAI/concept-erasure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span> Space Optimizing Few-shot Reasoning Success with Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03799v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03799v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fobo Shi, Peijun Qing, Dong Yang, Nan Wang, Youbo Lei, Haonan Lu, Xiaodong Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt engineering is an essential technique for enhancing the abilities of
large language models (LLMs) by providing explicit and specific instructions.
It enables LLMs to excel in various tasks, such as arithmetic reasoning,
question answering, summarization, relation extraction, machine translation,
and sentiment analysis. Researchers have been actively exploring different
prompt engineering strategies, such as Chain of Thought (CoT), Zero-CoT, and
In-context learning. However, an unresolved problem arises from the fact that
current approaches lack a solid theoretical foundation for determining optimal
prompts. To address this issue in prompt engineering, we propose a new and
effective approach called Prompt Space. Our methodology utilizes text
embeddings to obtain basis vectors by matrix decomposition, and then constructs
a space for representing all prompts. Prompt Space significantly outperforms
state-of-the-art prompt paradigms on ten public reasoning benchmarks. Notably,
without the help of the CoT method and the prompt "Let's think step by step",
Prompt Space shows superior performance over the few-shot method. Overall, our
approach provides a robust and fundamental theoretical framework for selecting
simple and effective prompts. This advancement marks a significant step towards
improving prompt engineering for a wide variety of applications in LLMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Natural language processing (NLP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Linguistic Features for Turkish Text Readability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03774v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03774v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmet Yavuz Uluslu, Gerold Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents the first comprehensive study on automatic readability
assessment of Turkish texts. We combine state-of-the-art neural network models
with linguistic features at lexical, morphosyntactic, syntactic and discourse
levels to develop an advanced readability tool. We evaluate the effectiveness
of traditional readability formulas compared to modern automated methods and
identify key linguistic features that determine the readability of Turkish
texts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FinRED: A <span class="highlight-title">Dataset</span> for Relation Extraction in Financial Domain <span class="chip">WWW'22</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03736v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03736v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumya Sharma, Tapas Nayak, Arusarka Bose, Ajay Kumar Meena, Koustuv Dasgupta, Niloy Ganguly, Pawan Goyal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction models trained on a source domain cannot be applied on a
different target domain due to the mismatch between relation sets. In the
current literature, there is no extensive open-source relation extraction
dataset specific to the finance domain. In this paper, we release FinRED, a
relation extraction dataset curated from financial news and earning call
transcripts containing relations from the finance domain. FinRED has been
created by mapping Wikidata triplets using distance supervision method. We
manually annotate the test data to ensure proper evaluation. We also experiment
with various state-of-the-art relation extraction models on this dataset to
create the benchmark. We see a significant drop in their performance on FinRED
compared to the general relation extraction datasets which tells that we need
better models for financial relation extraction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at FinWeb at WWW'22</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Cross-Linguistic Pressure for Uniform Information Density in Word
  Order 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03734v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03734v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Hikaru Clark, Clara Meister, Tiago Pimentel, Michael Hahn, Ryan Cotterell, Richard Futrell, Roger Levy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While natural languages differ widely in both canonical word order and word
order flexibility, their word orders still follow shared cross-linguistic
statistical patterns, often attributed to functional pressures. In the effort
to identify these pressures, prior work has compared real and counterfactual
word orders. Yet one functional pressure has been overlooked in such
investigations: the uniform information density (UID) hypothesis, which holds
that information should be spread evenly throughout an utterance. Here, we ask
whether a pressure for UID may have influenced word order patterns
cross-linguistically. To this end, we use computational models to test whether
real orders lead to greater information uniformity than counterfactual orders.
In our empirical study of 10 typologically diverse languages, we find that: (i)
among SVO languages, real word orders consistently have greater uniformity than
reverse word orders, and (ii) only linguistically implausible counterfactual
orders consistently exceed the uniformity of real orders. These findings are
compatible with a pressure for information uniformity in the development and
usage of natural languages.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Approach To User Agent String Parsing For Vulnerability Analysis
  Using Mutli-Headed Attention <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dhruv Nandakumar, Sathvik Murli, Ankur Khosla, Kevin Choi, Abdul Rahman, Drew Walsh, Scott Riede, Eric Dull, Edward Bowen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasing reliance on the internet has led to the proliferation of a
diverse set of web-browsers and operating systems (OSs) capable of browsing the
web. User agent strings (UASs) are a component of web browsing that are
transmitted with every Hypertext Transfer Protocol (HTTP) request. They contain
information about the client device and software, which is used by web servers
for various purposes such as content negotiation and security. However, due to
the proliferation of various browsers and devices, parsing UASs is a
non-trivial task due to a lack of standardization of UAS formats. Current
rules-based approaches are often brittle and can fail when encountering such
non-standard formats. In this work, a novel methodology for parsing UASs using
Multi-Headed Attention Based transformers is proposed. The proposed methodology
exhibits strong performance in parsing a variety of UASs with differing
formats. Furthermore, a framework to utilize parsed UASs to estimate the
vulnerability scores for large sections of publicly visible IT networks or
regions is also discussed. The methodology present here can also be easily
extended or deployed for real-time parsing of logs in enterprise settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to the International Conference on Machine Learning and
  Cybernetics (ICMLC) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Financial Numeric Extreme Labelling: A <span class="highlight-title">Dataset</span> and Benchmarking for XBRL
  Tagging <span class="chip">ACL'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03723v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03723v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumya Sharma, Subhendu Khatuya, Manjunath Hegde, Afreen Shaikh. Koustuv Dasgupta, Pawan Goyal, Niloy Ganguly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The U.S. Securities and Exchange Commission (SEC) mandates all public
companies to file periodic financial statements that should contain numerals
annotated with a particular label from a taxonomy. In this paper, we formulate
the task of automating the assignment of a label to a particular numeral span
in a sentence from an extremely large label set. Towards this task, we release
a dataset, Financial Numeric Extreme Labelling (FNXL), annotated with 2,794
labels. We benchmark the performance of the FNXL dataset by formulating the
task as (a) a sequence labelling problem and (b) a pipeline with span
extraction followed by Extreme Classification. Although the two approaches
perform comparably, the pipeline solution provides a slight edge for the least
frequent labels.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL'23 Findings Paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating the Effectiveness of Natural Language Inference for Hate
  Speech Detection in Languages with Limited Labeled Data <span class="chip">WOAH</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03722v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03722v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Janis Goldzycher, Moritz Preisig, Chantal Amrhein, Gerold Schneider
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most research on hate speech detection has focused on English where a
sizeable amount of labeled training data is available. However, to expand hate
speech detection into more languages, approaches that require minimal training
data are needed. In this paper, we test whether natural language inference
(NLI) models which perform well in zero- and few-shot settings can benefit hate
speech detection performance in scenarios where only a limited amount of
labeled data is available in the target language. Our evaluation on five
languages demonstrates large performance improvements of NLI fine-tuning over
direct fine-tuning in the target language. However, the effectiveness of
previous work that proposed intermediate fine-tuning on English data is hard to
match. Only in settings where the English training data does not match the test
domain, can our customised NLI-formulation outperform intermediate fine-tuning
on English. Based on our extensive experiments, we propose a set of
recommendations for hate speech detection in languages where minimal labeled
training data is available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 7 figures, Accepted at the 7th Workshop on Online Abuse and
  Harms (WOAH), ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Difference of <span class="highlight-title">BERT</span>-style and CLIP-style Text Encoders <span class="chip">ACL-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03678v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03678v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhihong Chen, Guiming Hardy Chen, Shizhe Diao, Xiang Wan, Benyou Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Masked language modeling (MLM) has been one of the most popular pretraining
recipes in natural language processing, e.g., BERT, one of the representative
models. Recently, contrastive language-image pretraining (CLIP) has also
attracted attention, especially its vision models that achieve excellent
performance on a broad range of vision tasks. However, few studies are
dedicated to studying the text encoders learned by CLIP. In this paper, we
analyze the difference between BERT-style and CLIP-style text encoders from
three experiments: (i) general text understanding, (ii) vision-centric text
understanding, and (iii) text-to-image generation. Experimental analyses show
that although CLIP-style text encoders underperform BERT-style ones for general
text understanding tasks, they are equipped with a unique ability, i.e.,
synesthesia, for the cross-modal association, which is more similar to the
senses of humans.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Natural Language Processing. 10 pages, 1 figure. Findings of ACL-2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Injecting knowledge into language generation: a case study in
  auto-charting after-visit care instructions from medical dialogue <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03652v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03652v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maksim Eremeev, Ilya Valmianski, Xavier Amatriain, Anitha Kannan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Factual correctness is often the limiting factor in practical applications of
natural language generation in high-stakes domains such as healthcare. An
essential requirement for maintaining factuality is the ability to deal with
rare tokens. This paper focuses on rare tokens that appear in both the source
and the reference sequences, and which, when missed during generation, decrease
the factual correctness of the output text. For high-stake domains that are
also knowledge-rich, we show how to use knowledge to (a) identify which rare
tokens that appear in both source and reference are important and (b) uplift
their conditional probability. We introduce the ``utilization rate'' that
encodes knowledge and serves as a regularizer by maximizing the marginal
probability of selected tokens. We present a study in a knowledge-rich domain
of healthcare, where we tackle the problem of generating after-visit care
instructions based on patient-doctor dialogues. We verify that, in our dataset,
specific medical concepts with high utilization rates are underestimated by
conventionally trained sequence-to-sequence models. We observe that correcting
this with our approach to knowledge injection reduces the uncertainty of the
model as well as improves factuality and coherence without negatively impacting
fluency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Quantum Probability Driven Framework for Joint Multi-Modal Sarcasm,
  Sentiment and Emotion Analysis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03650v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03650v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaochen Liu, Yazhou Zhang, Dawei Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sarcasm, sentiment, and emotion are three typical kinds of spontaneous
affective responses of humans to external events and they are tightly
intertwined with each other. Such events may be expressed in multiple
modalities (e.g., linguistic, visual and acoustic), e.g., multi-modal
conversations. Joint analysis of humans' multi-modal sarcasm, sentiment, and
emotion is an important yet challenging topic, as it is a complex cognitive
process involving both cross-modality interaction and cross-affection
correlation. From the probability theory perspective, cross-affection
correlation also means that the judgments on sarcasm, sentiment, and emotion
are incompatible. However, this exposed phenomenon cannot be sufficiently
modelled by classical probability theory due to its assumption of
compatibility. Neither do the existing approaches take it into consideration.
In view of the recent success of quantum probability (QP) in modeling human
cognition, particularly contextual incompatible decision making, we take the
first step towards introducing QP into joint multi-modal sarcasm, sentiment,
and emotion analysis. Specifically, we propose a QUantum probabIlity driven
multi-modal sarcasm, sEntiment and emoTion analysis framework, termed QUIET.
Extensive experiments on two datasets and the results show that the
effectiveness and advantages of QUIET in comparison with a wide range of the
state-of-the-art baselines. We also show the great potential of QP in
multi-affect analysis.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergence and Diversity in the Control Hierarchy <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03628v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03628v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra Butoi, Ryan Cotterell, David Chiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Weir has defined a hierarchy of language classes whose second member
($\mathcal{L}_2$) is generated by tree-adjoining grammars (TAG), linear indexed
grammars (LIG), combinatory categorial grammars, and head grammars. The
hierarchy is obtained using the mechanism of control, and $\mathcal{L}_2$ is
obtained using a context-free grammar (CFG) whose derivations are controlled by
another CFG. We adapt Weir's definition of a controllable CFG to give a
definition of controllable pushdown automata (PDAs). This yields three new
characterizations of $\mathcal{L}_2$ as the class of languages generated by
PDAs controlling PDAs, PDAs controlling CFGs, and CFGs controlling PDAs. We
show that these four formalisms are not only weakly equivalent but equivalent
in a stricter sense that we call d-weak equivalence. Furthermore, using an even
stricter notion of equivalence called d-strong equivalence, we make precise the
intuition that a CFG controlling a CFG is a TAG, a PDA controlling a PDA is an
embedded PDA, and a PDA controlling a CFG is a LIG. The fourth member of this
family, a CFG controlling a PDA, does not correspond to any formalism we know
of, so we invent one and call it a Pushdown Adjoining Automaton.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures. Accepted at ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">Survey</span> of Quantum-Cognitively Inspired Sentiment Analysis Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03608v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03608v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaochen Liu, Qiuchi Li, Benyou Wang, Yazhou Zhang, Dawei Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quantum theory, originally proposed as a physical theory to describe the
motions of microscopic particles, has been applied to various non-physics
domains involving human cognition and decision-making that are inherently
uncertain and exhibit certain non-classical, quantum-like characteristics.
Sentiment analysis is a typical example of such domains. In the last few years,
by leveraging the modeling power of quantum probability (a non-classical
probability stemming from quantum mechanics methodology) and deep neural
networks, a range of novel quantum-cognitively inspired models for sentiment
analysis have emerged and performed well. This survey presents a timely
overview of the latest developments in this fascinating cross-disciplinary
area. We first provide a background of quantum probability and quantum
cognition at a theoretical level, analyzing their advantages over classical
theories in modeling the cognitive aspects of sentiment analysis. Then, recent
quantum-cognitively inspired models are introduced and discussed in detail,
focusing on how they approach the key challenges of the sentiment analysis
task. Finally, we discuss the limitations of the current research and highlight
future research directions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CUE: An Uncertainty Interpretation Framework for Text Classifiers Built
  on <span class="highlight-title">Pre-Train</span>ed Language Models <span class="chip">UAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03598v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03598v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiazheng Li, Zhaoyue Sun, Bin Liang, Lin Gui, Yulan He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text classifiers built on Pre-trained Language Models (PLMs) have achieved
remarkable progress in various tasks including sentiment analysis, natural
language inference, and question-answering. However, the occurrence of
uncertain predictions by these classifiers poses a challenge to their
reliability when deployed in practical applications. Much effort has been
devoted to designing various probes in order to understand what PLMs capture.
But few studies have delved into factors influencing PLM-based classifiers'
predictive uncertainty. In this paper, we propose a novel framework, called
CUE, which aims to interpret uncertainties inherent in the predictions of
PLM-based models. In particular, we first map PLM-encoded representations to a
latent space via a variational auto-encoder. We then generate text
representations by perturbing the latent space which causes fluctuation in
predictive uncertainty. By comparing the difference in predictive uncertainty
between the perturbed and the original text representations, we are able to
identify the latent dimensions responsible for uncertainty and subsequently
trace back to the input features that contribute to such uncertainty. Our
extensive experiments on four benchmark datasets encompassing linguistic
acceptability classification, emotion classification, and natural language
inference show the feasibility of our proposed framework. Our source code is
available at: https://github.com/lijiazheng99/CUE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to UAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Language acquisition: do children and language models follow similar
  learning stages? <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03586v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03586v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linnea Evanson, Yair Lakretz, Jean-Rémi King
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  During language acquisition, children follow a typical sequence of learning
stages, whereby they first learn to categorize phonemes before they develop
their lexicon and eventually master increasingly complex syntactic structures.
However, the computational principles that lead to this learning trajectory
remain largely unknown. To investigate this, we here compare the learning
trajectories of deep language models to those of children. Specifically, we
test whether, during its training, GPT-2 exhibits stages of language
acquisition comparable to those observed in children aged between 18 months and
6 years. For this, we train 48 GPT-2 models from scratch and evaluate their
syntactic and semantic abilities at each training step, using 96 probes curated
from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these
evaluations with the behavior of 54 children during language production. Our
analyses reveal three main findings. First, similarly to children, the language
models tend to learn linguistic skills in a systematic order. Second, this
learning scheme is parallel: the language tasks that are learned last improve
from the very first training steps. Third, some - but not all - learning stages
are shared between children and these language models. Overall, these results
shed new light on the principles of language acquisition, and highlight
important divergences in how humans and modern algorithms learn to process
natural language.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2023. *Equal Contribution</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Take the Hint: Improving Arabic Diacritization with
  Partially-Diacritized Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03557v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03557v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Parnia Bahar, Mattia Di Gangi, Nick Rossenbach, Mohammad Zeineldeen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic Arabic diacritization is useful in many applications, ranging from
reading support for language learners to accurate pronunciation predictor for
downstream tasks like speech synthesis. While most of the previous works
focused on models that operate on raw non-diacritized text, production systems
can gain accuracy by first letting humans partly annotate ambiguous words. In
this paper, we propose 2SDiac, a multi-source model that can effectively
support optional diacritics in input to inform all predictions. We also
introduce Guided Learning, a training scheme to leverage given diacritics in
input with different levels of random masking. We show that the provided hints
during test affect more output positions than those annotated. Moreover,
experiments on two common benchmarks show that our approach i) greatly
outperforms the baseline also when evaluated on non-diacritized text; and ii)
achieves state-of-the-art results while reducing the parameter count by over
60%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Arabic text diacritization, partially-diacritized text, Arabic
  natural language processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciLit: A Platform for Joint Scientific Literature Discovery,
  Summarization and Citation Generation <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03535v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03535v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nianlong Gu, Richard H. R. Hahnloser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Scientific writing involves retrieving, summarizing, and citing relevant
papers, which can be time-consuming processes in large and rapidly evolving
fields. By making these processes inter-operable, natural language processing
(NLP) provides opportunities for creating end-to-end assistive writing tools.
We propose SciLit, a pipeline that automatically recommends relevant papers,
extracts highlights, and suggests a reference sentence as a citation of a
paper, taking into consideration the user-provided context and keywords. SciLit
efficiently recommends papers from large databases of hundreds of millions of
papers using a two-stage pre-fetching and re-ranking literature search system
that flexibly deals with addition and removal of a paper database. We provide a
convenient user interface that displays the recommended papers as extractive
summaries and that offers abstractively-generated citing sentences which are
aligned with the provided context and which mention the chosen keyword(s). Our
assistive tool for literature discovery and scientific writing is available at
https://scilit.vercel.app
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2023 System Demonstration</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "A Little is Enough": Few-Shot Quality Estimation based Corpus Filtering
  improves Machine Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03507v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03507v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Akshay Batheja, Pushpak Bhattacharyya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Quality Estimation (QE) is the task of evaluating the quality of a
translation when reference translation is not available. The goal of QE aligns
with the task of corpus filtering, where we assign the quality score to the
sentence pairs present in the pseudo-parallel corpus. We propose a Quality
Estimation based Filtering approach to extract high-quality parallel data from
the pseudo-parallel corpus. To the best of our knowledge, this is a novel
adaptation of the QE framework to extract quality parallel corpus from the
pseudo-parallel corpus. By training with this filtered corpus, we observe an
improvement in the Machine Translation (MT) system's performance by up to 1.8
BLEU points, for English-Marathi, Chinese-English, and Hindi-Bengali language
pairs, over the baseline model. The baseline model is the one that is trained
on the whole pseudo-parallel corpus. Our Few-shot QE model transfer learned
from the English-Marathi QE model and fine-tuned on only 500 Hindi-Bengali
training instances, shows an improvement of up to 0.6 BLEU points for
Hindi-Bengali language pair, compared to the baseline model. This demonstrates
the promise of transfer learning in the setting under discussion. QE systems
typically require in the order of (7K-25K) of training data. Our Hindi-Bengali
QE is trained on only 500 instances of training that is 1/40th of the normal
requirement and achieves comparable performance. All the scripts and datasets
utilized in this study will be publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Applying Standards to Advance Upstream & Downstream Ethics in Large
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03503v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03503v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jose Berengueres, Marybeth Sandell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper explores how AI-owners can develop safeguards for AI-generated
content by drawing from established codes of conduct and ethical standards in
other content-creation industries. It delves into the current state of ethical
awareness on Large Language Models (LLMs). By dissecting the mechanism of
content generation by LLMs, four key areas (upstream/downstream and at user
prompt/answer), where safeguards could be effectively applied, are identified.
A comparative analysis of these four areas follows and includes an evaluation
of the existing ethical safeguards in terms of cost, effectiveness, and
alignment with established industry practices. The paper's key argument is that
existing IT-related ethical codes, while adequate for traditional IT
engineering, are inadequate for the challenges posed by LLM-based content
generation. Drawing from established practices within journalism, we propose
potential standards for businesses involved in distributing and selling
LLM-generated content. Finally, potential conflicts of interest between dataset
curation at upstream and ethical benchmarking downstream are highlighted to
underscore the need for a broader evaluation beyond mere output. This study
prompts a nuanced conversation around ethical implications in this rapidly
evolving field of content generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 4 tables, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Adaptable and Interactive Image Captioning with Data
  Augmentation and Episodic Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03500v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03500v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aliki Anagnostopoulou, Mareike Hartmann, Daniel Sonntag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive machine learning (IML) is a beneficial learning paradigm in cases
of limited data availability, as human feedback is incrementally integrated
into the training process. In this paper, we present an IML pipeline for image
captioning which allows us to incrementally adapt a pre-trained image
captioning model to a new data distribution based on user input. In order to
incorporate user input into the model, we explore the use of a combination of
simple data augmentation methods to obtain larger data batches for each newly
annotated data instance and implement continual learning methods to prevent
catastrophic forgetting from repeated updates. For our experiments, we split a
domain-specific image captioning dataset, namely VizWiz, into non-overlapping
parts to simulate an incremental input flow for continually adapting the model
to new data. We find that, while data augmentation worsens results, even when
relatively small amounts of data are available, episodic memory is an effective
strategy to retain knowledge from previously seen clusters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SciCap+: A Knowledge Augmented <span class="highlight-title">Dataset</span> to Study the Challenges of
  Scientific Figure Captioning <span class="chip">AAAI23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03491v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03491v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhishen Yang, Raj Dabre, Hideki Tanaka, Naoaki Okazaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In scholarly documents, figures provide a straightforward way of
communicating scientific findings to readers. Automating figure caption
generation helps move model understandings of scientific documents beyond text
and will help authors write informative captions that facilitate communicating
scientific findings. Unlike previous studies, we reframe scientific figure
captioning as a knowledge-augmented image captioning task that models need to
utilize knowledge embedded across modalities for caption generation. To this
end, we extended the large-scale SciCap
dataset~\cite{hsu-etal-2021-scicap-generating} to SciCap+ which includes
mention-paragraphs (paragraphs mentioning figures) and OCR tokens. Then, we
conduct experiments with the M4C-Captioner (a multimodal transformer-based
model with a pointer network) as a baseline for our study. Our results indicate
that mention-paragraphs serves as additional context knowledge, which
significantly boosts the automatic standard image caption evaluation scores
compared to the figure-only baselines. Human evaluations further reveal the
challenges of generating figure captions that are informative to readers. The
code and SciCap+ dataset will be publicly available at
https://github.com/ZhishenYang/scientific_figure_captioning_dataset
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in SDU workshop at AAAI23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Putting Humans in the Image Captioning Loop 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03476v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03476v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aliki Anagnostopoulou, Mareike Hartmann, Daniel Sonntag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Image Captioning (IC) models can highly benefit from human feedback in the
training process, especially in cases where data is limited. We present
work-in-progress on adapting an IC system to integrate human feedback, with the
goal to make it easily adaptable to user-specific data. Our approach builds on
a base IC model pre-trained on the MS COCO dataset, which generates captions
for unseen images. The user will then be able to offer feedback on the image
and the generated/predicted caption, which will be augmented to create
additional training instances for the adaptation of the model. The additional
instances are integrated into the model using step-wise updates, and a sparse
memory replay component is used to avoid catastrophic forgetting. We hope that
this approach, while leading to improved results, will also result in
customizable IC models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint Event Extraction via Structural Semantic Matching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03469v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03469v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochen Li, Tianhao Gao, Jingkun Wang, Weiping Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event Extraction (EE) is one of the essential tasks in information
extraction, which aims to detect event mentions from text and find the
corresponding argument roles. The EE task can be abstracted as a process of
matching the semantic definitions and argument structures of event types with
the target text. This paper encodes the semantic features of event types and
makes structural matching with target text. Specifically, Semantic Type
Embedding (STE) and Dynamic Structure Encoder (DSE) modules are proposed. Also,
the Joint Structural Semantic Matching (JSSM) model is built to jointly perform
event detection and argument extraction tasks through a bidirectional attention
layer. The experimental results on the ACE2005 dataset indicate that our model
achieves a significant performance improvement
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language Commanding via Program Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Apurva Gandhi, Thong Q. Nguyen, Huitian Jiao, Robert Steen, Ameya Bhatawdekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Semantic Interpreter, a natural language-friendly AI system for
productivity software such as Microsoft Office that leverages large language
models (LLMs) to execute user intent across application features. While LLMs
are excellent at understanding user intent expressed as natural language, they
are not sufficient for fulfilling application-specific user intent that
requires more than text-to-text transformations. We therefore introduce the
Office Domain Specific Language (ODSL), a concise, high-level language
specialized for performing actions in and interacting with entities in Office
applications. Semantic Interpreter leverages an Analysis-Retrieval prompt
construction method with LLMs for program synthesis, translating natural
language user utterances to ODSL programs that can be transpiled to application
APIs and then executed. We focus our discussion primarily on a research
exploration for Microsoft PowerPoint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Phonetically-Grounded Language Generation: The Case of Tongue Twisters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tyler Loakman, Chen Tang, Chenghua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous work in phonetically-grounded language generation has mainly focused
on domains such as lyrics and poetry. In this paper, we present work on the
generation of tongue twisters - a form of language that is required to be
phonetically conditioned to maximise sound overlap, whilst maintaining semantic
consistency with an input topic, and still being grammatically correct. We
present \textbf{TwistList}, a large annotated dataset of tongue twisters,
consisting of 2.1K+ human-authored examples. We additionally present several
benchmark systems (referred to as TwisterMisters) for the proposed task of
tongue twister generation, including models that both do and do not require
training on in-domain data. We present the results of automatic and human
evaluation to demonstrate the performance of existing mainstream pre-trained
models in this task with limited (or no) task specific training and data, and
no explicit phonetic knowledge. We find that the task of tongue twister
generation is challenging for models under these conditions, yet some models
are still capable of generating acceptable examples of this language type.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automatic Assessment of Oral Reading Accuracy for Reading Diagnostics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03444v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03444v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Molenaar, Cristian Tejedor-Garcia, Helmer Strik, Catia Cucchiarini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Automatic assessment of reading fluency using automatic speech recognition
(ASR) holds great potential for early detection of reading difficulties and
subsequent timely intervention. Precise assessment tools are required,
especially for languages other than English. In this study, we evaluate six
state-of-the-art ASR-based systems for automatically assessing Dutch oral
reading accuracy using Kaldi and Whisper. Results show our most successful
system reached substantial agreement with human evaluations (MCC = .63). The
same system reached the highest correlation between forced decoding confidence
scores and word correctness (r = .45). This system's language model (LM)
consisted of manual orthographic transcriptions and reading prompts of the test
data, which shows that including reading errors in the LM improves assessment
performance. We discuss the implications for developing automatic assessment
systems and identify possible avenues of future research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Alzheimer Disease Classification through ASR-based Transcriptions:
  Exploring the Impact of Punctuation and Pauses 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03443v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03443v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lucía Gómez-Zaragozá, Simone Wills, Cristian Tejedor-Garcia, Javier Marín-Morales, Mariano Alcañiz, Helmer Strik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Alzheimer's Disease (AD) is the world's leading neurodegenerative disease,
which often results in communication difficulties. Analysing speech can serve
as a diagnostic tool for identifying the condition. The recent ADReSS challenge
provided a dataset for AD classification and highlighted the utility of manual
transcriptions. In this study, we used the new state-of-the-art Automatic
Speech Recognition (ASR) model Whisper to obtain the transcriptions, which also
include automatic punctuation. The classification models achieved test accuracy
scores of 0.854 and 0.833 combining the pretrained FastText word embeddings and
recurrent neural networks on manual and ASR transcripts respectively.
Additionally, we explored the influence of including pause information and
punctuation in the transcriptions. We found that punctuation only yielded minor
improvements in some cases, whereas pause encoding aided AD classification for
both manual and ASR transcriptions across all approaches investigated.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Models of Code Fail at Completing Code with Potential
  Bugs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03438v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03438v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuan Dinh, Jinman Zhao, Samson Tan, Renato Negrinho, Leonard Lausen, Sheng Zha, George Karypis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models of code (Code-LLMs) have recently brought tremendous
advances to code completion, a fundamental feature of programming assistance
and code intelligence. However, most existing works ignore the possible
presence of bugs in the code context for generation, which are inevitable in
software development. Therefore, we introduce and study the buggy-code
completion problem, inspired by the realistic scenario of real-time code
suggestion where the code context contains potential bugs -- anti-patterns that
can become bugs in the completed program. To systematically study the task, we
introduce two datasets: one with synthetic bugs derived from semantics-altering
operator changes (buggy-HumanEval) and one with realistic bugs derived from
user submissions to coding problems (buggy-FixEval). We find that the presence
of potential bugs significantly degrades the generation performance of the
high-performing Code-LLMs. For instance, the passing rates of CodeGen-2B-mono
on test cases of buggy-HumanEval drop more than 50% given a single potential
bug in the context. Finally, we investigate several post-hoc methods for
mitigating the adverse effect of potential bugs and find that there remains a
large gap in post-mitigation performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Role of Attention in <span class="highlight-title">Prompt</span>-tuning <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03435v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03435v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samet Oymak, Ankit Singh Rawat, Mahdi Soltanolkotabi, Christos Thrampoulidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prompt-tuning is an emerging strategy to adapt large language models (LLM) to
downstream tasks by learning a (soft-)prompt parameter from data. Despite its
success in LLMs, there is limited theoretical understanding of the power of
prompt-tuning and the role of the attention mechanism in prompting. In this
work, we explore prompt-tuning for one-layer attention architectures and study
contextual mixture-models where each input token belongs to a context-relevant
or -irrelevant set. We isolate the role of prompt-tuning through a
self-contained prompt-attention model. Our contributions are as follows: (1) We
show that softmax-prompt-attention is provably more expressive than
softmax-self-attention and linear-prompt-attention under our contextual data
model. (2) We analyze the initial trajectory of gradient descent and show that
it learns the prompt and prediction head with near-optimal sample complexity
and demonstrate how prompt can provably attend to sparse context-relevant
tokens. (3) Assuming a known prompt but an unknown prediction head, we
characterize the exact finite sample performance of prompt-attention which
reveals the fundamental performance limits and the precise benefit of the
context information. We also provide experiments that verify our theoretical
insights on real datasets and demonstrate how prompt-tuning enables the model
to attend to context-relevant information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Efficient and Interpretable Compressive Text Summarisation with
  Unsupervised Dual-Agent Reinforcement Learning <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03415v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03415v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peggy Tang, Junbin Gao, Lei Zhang, Zhiyong Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, compressive text summarisation offers a balance between the
conciseness issue of extractive summarisation and the factual hallucination
issue of abstractive summarisation. However, most existing compressive
summarisation methods are supervised, relying on the expensive effort of
creating a new training dataset with corresponding compressive summaries. In
this paper, we propose an efficient and interpretable compressive summarisation
method that utilises unsupervised dual-agent reinforcement learning to optimise
a summary's semantic coverage and fluency by simulating human judgment on
summarisation quality. Our model consists of an extractor agent and a
compressor agent, and both agents have a multi-head attentional pointer-based
structure. The extractor agent first chooses salient sentences from a document,
and then the compressor agent compresses these extracted sentences by selecting
salient words to form a summary without using reference summaries to compute
the summary reward. To our best knowledge, this is the first work on
unsupervised compressive summarisation. Experimental results on three widely
used datasets (e.g., Newsroom, CNN/DM, and XSum) show that our model achieves
promising performance and a significant improvement on Newsroom in terms of the
ROUGE metric, as well as interpretability of semantic coverage of summarisation
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The 4th Workshop on Simple and Efficient Natural Language Processing
  (SustaiNLP 2023), co-located with ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Generate-then-Retrieve: Intent-Aware FAQ Retrieval in Product Search <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03411v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03411v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyu Chen, Jason Choi, Besnik Fetahu, Oleg Rokhlenko, Shervin Malmasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Customers interacting with product search engines are increasingly
formulating information-seeking queries. Frequently Asked Question (FAQ)
retrieval aims to retrieve common question-answer pairs for a user query with
question intent. Integrating FAQ retrieval in product search can not only
empower users to make more informed purchase decisions, but also enhance user
retention through efficient post-purchase support. Determining when an FAQ
entry can satisfy a user's information need within product search, without
disrupting their shopping experience, represents an important challenge. We
propose an intent-aware FAQ retrieval system consisting of (1) an intent
classifier that predicts when a user's information need can be answered by an
FAQ; (2) a reformulation model that rewrites a query into a natural question.
Offline evaluation demonstrates that our approach improves Hit@1 by 13% on
retrieving ground-truth FAQs, while reducing latency by 95% compared to
baseline systems. These improvements are further validated by real user
feedback, where 71% of displayed FAQs on top of product search results received
explicit positive user feedback. Overall, our findings show promising
directions for integrating FAQ retrieval into product search at scale.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TextFormer: A Query-based End-to-End Text Spotter with Mixed Supervision 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukun Zhai, Xiaoqiang Zhang, Xiameng Qin, Sanyuan Zhao, Xingping Dong, Jianbing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end text spotting is a vital computer vision task that aims to
integrate scene text detection and recognition into a unified framework.
Typical methods heavily rely on Region-of-Interest (RoI) operations to extract
local features and complex post-processing steps to produce final predictions.
To address these limitations, we propose TextFormer, a query-based end-to-end
text spotter with Transformer architecture. Specifically, using query embedding
per text instance, TextFormer builds upon an image encoder and a text decoder
to learn a joint semantic understanding for multi-task modeling. It allows for
mutual training and optimization of classification, segmentation, and
recognition branches, resulting in deeper feature sharing without sacrificing
flexibility or simplicity. Additionally, we design an Adaptive Global
aGgregation (AGG) module to transfer global features into sequential features
for reading arbitrarily-shaped texts, which overcomes the sub-optimization
problem of RoI operations. Furthermore, potential corpus information is
utilized from weak annotations to full labels through mixed supervision,
further improving text detection and end-to-end text spotting results.
Extensive experiments on various bilingual (i.e., English and Chinese)
benchmarks demonstrate the superiority of our method. Especially on TDA-ReCTS
dataset, TextFormer surpasses the state-of-the-art method in terms of 1-NED by
13.2%.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>MIR 2023, 15 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ $\textit{WHAT}$, $\textit{WHEN}$, and $\textit{HOW}$ to Ground:
  Designing User Persona-Aware Conversational Agents for Engaging Dialogue <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03361v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03361v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Deuksin Kwon, Sunwoo Lee, Ki Hyun Kim, Seojin Lee, Taeyoon Kim, Eric Davis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a method for building a personalized open-domain dialogue
system to address the $\textit{WWH}$ ($\textit{WHAT}$, $\textit{WHEN}$, and
$\textit{HOW}$) problem for natural response generation in a commercial
setting, where personalized dialogue responses are heavily interleaved with
casual response turns. The proposed approach involves weighted dataset
blending, negative persona information augmentation methods, and the design of
personalized conversation datasets to address the challenges of $\textit{WWH}$
in personalized, open-domain dialogue systems. Our work effectively balances
dialogue fluency and tendency to ground, while also introducing a response-type
label to improve the controllability and explainability of the grounded
responses. The combination of these methods leads to more fluent conversations,
as evidenced by subjective human evaluations as well as objective evaluations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ACL 2023 Industry Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BatchSampler: Sampling Mini-Batches for Contrastive Learning in Vision,
  Language, and Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03355v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03355v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhen Yang, Tinglin Huang, Ming Ding, Yuxiao Dong, Rex Ying, Yukuo Cen, Yangliao Geng, Jie Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-Batch contrastive learning is a state-of-the-art self-supervised method
that brings semantically-similar instances close while pushing dissimilar
instances apart within a mini-batch. Its key to success is the negative sharing
strategy, in which every instance serves as a negative for the others within
the mini-batch. Recent studies aim to improve performance by sampling hard
negatives \textit{within the current mini-batch}, whose quality is bounded by
the mini-batch itself. In this work, we propose to improve contrastive learning
by sampling mini-batches from the input data. We present
BatchSampler\footnote{The code is available at
\url{https://github.com/THUDM/BatchSampler}} to sample mini-batches of
hard-to-distinguish (i.e., hard and true negatives to each other) instances. To
make each mini-batch have fewer false negatives, we design the proximity graph
of randomly-selected instances. To form the mini-batch, we leverage random walk
with restart on the proximity graph to help sample hard-to-distinguish
instances. BatchSampler is a simple and general technique that can be directly
plugged into existing contrastive learning models in vision, language, and
graphs. Extensive experiments on datasets of three modalities show that
BatchSampler can consistently improve the performance of powerful contrastive
models, as shown by significant improvements of SimCLR on ImageNet-100, SimCSE
on STS (language), and GraphCL and MVGRL on graph datasets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Click: Controllable Text Generation with Sequence Likelihood Contrastive
  Learning <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03350v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03350v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chujie Zheng, Pei Ke, Zheng Zhang, Minlie Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has always been an important yet challenging problem to control language
models to avoid generating texts with undesirable attributes, such as toxic
language and unnatural repetition. We introduce Click for controllable text
generation, which needs no modification to the model architecture and
facilitates out-of-the-box use of trained models. It employs a contrastive loss
on sequence likelihood, which fundamentally decreases the generation
probability of negative samples (i.e., generations with undesirable
attributes). It also adopts a novel likelihood ranking-based strategy to
construct contrastive samples from model generations. On the tasks of language
detoxification, sentiment steering, and repetition reduction, we show that
Click outperforms strong baselines of controllable text generation and
demonstrate the superiority of Click's sample construction strategy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inference-Time Intervention: Eliciting Truthful Answers from a Language
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03341v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03341v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, Martin Wattenberg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Inference-Time Intervention (ITI), a technique designed to
enhance the truthfulness of large language models (LLMs). ITI operates by
shifting model activations during inference, following a set of directions
across a limited number of attention heads. This intervention significantly
improves the performance of LLaMA models on the TruthfulQA benchmark. On an
instruction-finetuned LLaMA called Alpaca, ITI improves its truthfulness from
32.5% to 65.1%. We identify a tradeoff between truthfulness and helpfulness and
demonstrate how to balance it by tuning the intervention strength. ITI is
minimally invasive and computationally inexpensive. Moreover, the technique is
data efficient: while approaches like RLHF require extensive annotations, ITI
locates truthful directions using only few hundred examples. Our findings
suggest that LLMs may have an internal representation of the likelihood of
something being true, even as they produce falsehoods on the surface.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>code: https://github.com/likenneth/honest_llama</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Empirical Analysis of Parameter-Efficient Methods for Debiasing
  <span class="highlight-title">Pre-Train</span>ed Language Models <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhongbin Xie, Thomas Lukasiewicz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The increasingly large size of modern pretrained language models not only
makes them inherit more human-like biases from the training corpora, but also
makes it computationally expensive to mitigate such biases. In this paper, we
investigate recent parameter-efficient methods in combination with
counterfactual data augmentation (CDA) for bias mitigation. We conduct
extensive experiments with prefix tuning, prompt tuning, and adapter tuning on
different language models and bias types to evaluate their debiasing
performance and abilities to preserve the internal knowledge of a pre-trained
model. We find that the parameter-efficient methods (i) are effective in
mitigating gender bias, where adapter tuning is consistently the most effective
one and prompt tuning is more suitable for GPT-2 than BERT, (ii) are less
effective when it comes to racial and religious bias, which may be attributed
to the limitations of CDA, and (iii) can perform similarly to or sometimes
better than full fine-tuning with improved time and memory efficiency, as well
as maintain the internal knowledge in BERT and GPT-2, evaluated via fact
retrieval and downstream fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted to ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Augmenting Reddit Posts to Determine Wellness Dimensions impacting
  Mental Health 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04059v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04059v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chandreen Liyanage, Muskan Garg, Vijay Mago, Sunghwan Sohn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Amid ongoing health crisis, there is a growing necessity to discern possible
signs of Wellness Dimensions (WD) manifested in self-narrated text. As the
distribution of WD on social media data is intrinsically imbalanced, we
experiment the generative NLP models for data augmentation to enable further
improvement in the pre-screening task of classifying WD. To this end, we
propose a simple yet effective data augmentation approach through prompt-based
Generative NLP models, and evaluate the ROUGE scores and syntactic/semantic
similarity among existing interpretations and augmented data. Our approach with
ChatGPT model surpasses all the other methods and achieves improvement over
baselines such as Easy-Data Augmentation and Backtranslation. Introducing data
augmentation to generate more training samples and balanced dataset, results in
the improved F-score and the Matthew's Correlation Coefficient for upto 13.11%
and 15.95%, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Active Sparse Conversations for Improved Audio-Visual Embodied
  Navigation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04047v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04047v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiulong Liu, Sudipta Paul, Moitreya Chatterjee, Anoop Cherian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Efficient navigation towards an audio-goal necessitates an embodied agent to
not only possess the ability to use audio-visual cues effectively, but also be
equipped to actively (but occasionally) seek human/oracle assistance without
sacrificing autonomy, e.g., when it is uncertain of where to navigate towards
locating a noisy or sporadic audio goal. To this end, we present CAVEN -- a
conversational audio-visual embodied navigation agent that is capable of posing
navigation questions to a human/oracle and processing the oracle responses;
both in free-form natural language. At the core of CAVEN is a multimodal
hierarchical reinforcement learning (RL) setup that is equipped with a
high-level policy that is trained to choose from one of three low-level
policies (at every step), namely: (i) to navigate using audio-visual cues, or
(ii) to frame a question to the oracle and receive a short or detailed
response, or (iii) ask generic questions (when unsure of what to ask) and
receive instructions. Key to generating the agent's questions is our novel
TrajectoryNet that forecasts the most likely next steps to the goal and a
QuestionNet that uses these steps to produce a question. All the policies are
learned end-to-end via the RL setup, with penalties to enforce sparsity in
receiving navigation instructions from the oracle. To evaluate the performance
of CAVEN, we present extensive experiments on the SoundSpaces framework for the
task of semantic audio-visual navigation. Our results show that CAVEN achieves
upto 12% gain in performance over competing methods, especially in localizing
new sound sources, even in the presence of auditory distractions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Analysis of Reader Engagement in Literary Fiction through Eye
  Tracking and Linguistic Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04043v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04043v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rose Neis, Karin de Langis, Zae Myung Kim, Dongyeop Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Capturing readers' engagement in fiction is a challenging but important
aspect of narrative understanding. In this study, we collected 23 readers'
reactions to 2 short stories through eye tracking, sentence-level annotations,
and an overall engagement scale survey. We analyzed the significance of various
qualities of the text in predicting how engaging a reader is likely to find it.
As enjoyment of fiction is highly contextual, we also investigated individual
differences in our data. Furthering our understanding of what captivates
readers in fiction will help better inform models used in creative narrative
generation and collaborative writing tools.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Triggering Multi-Hop Reasoning for Question Answering in Language Models
  using Soft <span class="highlight-title">Prompt</span>s and Random Walks <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kanishka Misra, Cicero Nogueira dos Santos, Siamak Shakeri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite readily memorizing world knowledge about entities, pre-trained
language models (LMs) struggle to compose together two or more facts to perform
multi-hop reasoning in question-answering tasks. In this work, we propose
techniques that improve upon this limitation by relying on random walks over
structured knowledge graphs. Specifically, we use soft prompts to guide LMs to
chain together their encoded knowledge by learning to map multi-hop questions
to random walk paths that lead to the answer. Applying our methods on two T5
LMs shows substantial improvements over standard tuning approaches in answering
questions that require 2-hop reasoning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Findings of ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sentiment Analysis in Finance: From <span class="highlight-title">Transformer</span>s Back to eXplainable
  Lexicons (XLex) 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03997v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03997v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryan Rizinski, Hristijan Peshov, Kostadin Mishev, Milos Jovanovik, Dimitar Trajanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Lexicon-based sentiment analysis (SA) in finance leverages specialized,
manually annotated lexicons created by human experts to extract sentiment from
financial texts. Although lexicon-based methods are simple to implement and
fast to operate on textual data, they require considerable manual annotation
efforts to create, maintain, and update the lexicons. These methods are also
considered inferior to the deep learning-based approaches, such as transformer
models, which have become dominant in various NLP tasks due to their remarkable
performance. However, transformers require extensive data and computational
resources for both training and testing. Additionally, they involve significant
prediction times, making them unsuitable for real-time production environments
or systems with limited processing capabilities. In this paper, we introduce a
novel methodology named eXplainable Lexicons (XLex) that combines the
advantages of both lexicon-based methods and transformer models. We propose an
approach that utilizes transformers and SHapley Additive exPlanations (SHAP)
for explainability to learn financial lexicons. Our study presents four main
contributions. Firstly, we demonstrate that transformer-aided explainable
lexicons can enhance the vocabulary coverage of the benchmark Loughran-McDonald
(LM) lexicon, reducing the human involvement in annotating, maintaining, and
updating the lexicons. Secondly, we show that the resulting lexicon outperforms
the standard LM lexicon in SA of financial datasets. Thirdly, we illustrate
that the lexicon-based approach is significantly more efficient in terms of
model speed and size compared to transformers. Lastly, the XLex approach is
inherently more interpretable than transformer models as lexicon models rely on
predefined rules, allowing for better insights into the results of SA and
making the XLex approach a viable tool for financial decision-making.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Access</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Toward More Accurate and Generalizable Evaluation Metrics for
  Task-Oriented Dialogs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abishek Komma, Nagesh Panyam Chandrasekarasastry, Timothy Leffel Anuj Goyal, Angeliki Metallinou, Spyros Matsoukas, Aram Galstyan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Measurement of interaction quality is a critical task for the improvement of
spoken dialog systems. Existing approaches to dialog quality estimation either
focus on evaluating the quality of individual turns, or collect dialog-level
quality measurements from end users immediately following an interaction. In
contrast to these approaches, we introduce a new dialog-level annotation
workflow called Dialog Quality Annotation (DQA). DQA expert annotators evaluate
the quality of dialogs as a whole, and also label dialogs for attributes such
as goal completion and user sentiment. In this contribution, we show that: (i)
while dialog quality cannot be completely decomposed into dialog-level
attributes, there is a strong relationship between some objective dialog
attributes and judgments of dialog quality; (ii) for the task of dialog-level
quality estimation, a supervised model trained on dialog-level annotations
outperforms methods based purely on aggregating turn-level features; and (iii)
the proposed evaluation model shows better domain generalization ability
compared to the baselines. On the basis of these results, we argue that having
high-quality human-annotated data is an important component of evaluating
interaction quality for large industrial-scale voice assistant platforms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Büyük dil modellerinin Türkçe verisetleri ile
  eğitilmesi ve ince ayarlanması 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        A. Taha Arslan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have advanced enormously, gained vast attraction and
are having a phase of intensed research. Some of the developed models and
training datasets have been made open-accessible. Hence these may be further
fine-tuned with some techniques to obtain specialized models for specific
tasks. When it comes to Turkish language, open-access models do not provide
satisfactory coverage. This is also observed over published datasets. In this
work, we propose some ideas to mitigate this issue: creating large Turkish
datasets, training LLMs with these and fine-tuning pre-trained models with
Turkish inputs. We report our findings on Turkish-based trainings with the
problems encountered along the way. We conclude with outcomes of these
experiments and propose ideas for further works.
  --
  B\"uy\"uk dil modelleri inan{\i}lmaz \"ol\c{c}\"ude geli\c{s}mekte, b\"uy\"uk
ilgi toplayarak ve \"uzerlerinde yo\u{g}un ara\c{s}tirmalarin yapildi\u{g}i bir
d\"onemdedirler. Geli\c{s}tirilen modeller ve e\u{g}itimde kullanilan
verisetlerinden bazilari a\c{c}ik eri\c{s}imli olarak sunulmaktadir. B\"oylece
ince ayarlama teknikleri uygulayarak \"ozelle\c{s}mi\c{s} g\"orevler i\c{c}in
\c{c}ali\c{s}abilir modeller elde edilmektedir. T\"urk\c{c}e s\"oz konusu
oldu\u{g}unda bu modellerinin kapsayicili\u{g}i yeterli d\"uzeyde de\u{g}ildir.
Bu durum, yayimlanan verisetlerinde de g\"ozlemlenebilir. Bunu a\c{s}manin
yollari T\"urk\c{c}e i\c{c}erikli b\"uy\"uk verisetlerinin olu\c{s}turulmasi,
b\"uy\"uk dil modellerinin bunlarla e\u{g}itilmesi ve \"onceden
e\u{g}itilmi\c{s} modellerin T\"urk\c{c}e girdilerle ince ayarlanmalari
olabilir. Bu \c{c}ali\c{s}mada a\c{c}ik eri\c{s}imli dil modelleri ve
verisetleri \"uzerinde durulmakta ve T\"urk\c{c}e temelli bazi deneyler,
kar\c{s}ila\c{s}ilan sorunlar ve sonu\c{c}lar irdelenmektedir.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Revisiting Conversation Discourse for Dialogue Disentanglement 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03975v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03975v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bobo Li, Hao Fei, Fei Li, Shengqiong Wu, Lizi Liao, Yinwei Wei, Tat-Seng Chua, Donghong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dialogue disentanglement aims to detach the chronologically ordered
utterances into several independent sessions. Conversation utterances are
essentially organized and described by the underlying discourse, and thus
dialogue disentanglement requires the full understanding and harnessing of the
intrinsic discourse attribute. In this paper, we propose enhancing dialogue
disentanglement by taking full advantage of the dialogue discourse
characteristics. First of all, \textbf{in feature encoding stage}, we construct
the heterogeneous graph representations to model the various dialogue-specific
discourse structural features, including the static speaker-role structures
(i.e., speaker-utterance and speaker-mentioning structure) and the dynamic
contextual structures (i.e., the utterance-distance and partial-replying
structure). We then develop a structure-aware framework to integrate the rich
structural features for better modeling the conversational semantic context.
Second, \textbf{in model learning stage}, we perform optimization with a
hierarchical ranking loss mechanism, which groups dialogue utterances into
different discourse levels and carries training covering pair-wise and
session-wise levels hierarchically. Third, \textbf{in inference stage}, we
devise an easy-first decoding algorithm, which performs utterance pairing under
the easy-to-hard manner with a global context, breaking the constraint of
traditional sequential decoding order. On two benchmark datasets, our overall
system achieves new state-of-the-art performances on all evaluations. In-depth
analyses further demonstrate the efficacy of each proposed idea and also reveal
how our methods help advance the task. Our work has great potential to
facilitate broader multi-party multi-thread dialogue applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ TKDP: Threefold Knowledge-enriched Deep <span class="highlight-title">Prompt</span> Tuning for Few-shot Named
  Entity Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03974v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03974v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiang Liu, Hao Fei, Fei Li, Jingye Li, Bobo Li, Liang Zhao, Chong Teng, Donghong Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot named entity recognition (NER) exploits limited annotated instances
to identify named mentions. Effectively transferring the internal or external
resources thus becomes the key to few-shot NER. While the existing prompt
tuning methods have shown remarkable few-shot performances, they still fail to
make full use of knowledge. In this work, we investigate the integration of
rich knowledge to prompt tuning for stronger few-shot NER. We propose
incorporating the deep prompt tuning framework with threefold knowledge (namely
TKDP), including the internal 1) context knowledge and the external 2) label
knowledge & 3) sememe knowledge. TKDP encodes the three feature sources and
incorporates them into the soft prompt embeddings, which are further injected
into an existing pre-trained language model to facilitate predictions. On five
benchmark datasets, our knowledge-enriched model boosts by at most 11.53% F1
over the raw deep prompt method, and significantly outperforms 8
strong-performing baseline systems in 5-/10-/20-shot settings, showing great
potential in few-shot NER. Our TKDP can be broadly adapted to other few-shot
tasks without effort.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ECQED: Emotion-Cause Quadruple Extraction in Dialogs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03969v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03969v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Zheng, Donghong Ji, Fei Li, Hao Fei, Shengqiong Wu, Jingye Li, Bobo Li, Chong Teng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The existing emotion-cause pair extraction (ECPE) task, unfortunately,
ignores extracting the emotion type and cause type, while these fine-grained
meta-information can be practically useful in real-world applications, i.e.,
chat robots and empathic dialog generation. Also the current ECPE is limited to
the scenario of single text piece, while neglecting the studies at dialog level
that should have more realistic values. In this paper, we extend the ECPE task
with a broader definition and scenario, presenting a new task, Emotion-Cause
Quadruple Extraction in Dialogs (ECQED), which requires detecting emotion-cause
utterance pairs and emotion and cause types. We present an ECQED model based on
a structural and semantic heterogeneous graph as well as a parallel grid
tagging scheme, which advances in effectively incorporating the dialog context
structure, meanwhile solving the challenging overlapped quadruple issue. Via
experiments we show that introducing the fine-grained emotion and cause
features evidently helps better dialog generation. Also our proposed ECQED
system shows exceptional superiority over baselines on both the emotion-cause
quadruple or pair extraction tasks, meanwhile being highly efficient.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Explicit Procedural Instructions for Data-Efficient Action
  Prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julia White, Arushi Raghuvanshi, Yada Pruksachatkun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented dialogues often require agents to enact complex, multi-step
procedures in order to meet user requests. While large language models have
found success automating these dialogues in constrained environments, their
widespread deployment is limited by the substantial quantities of task-specific
data required for training. The following paper presents a data-efficient
solution to constructing dialogue systems, leveraging explicit instructions
derived from agent guidelines, such as company policies or customer service
manuals. Our proposed Knowledge-Augmented Dialogue System (KADS) combines a
large language model with a knowledge retrieval module that pulls documents
outlining relevant procedures from a predefined set of policies, given a
user-agent interaction. To train this system, we introduce a semi-supervised
pre-training scheme that employs dialogue-document matching and action-oriented
masked language modeling with partial parameter freezing. We evaluate the
effectiveness of our approach on prominent task-oriented dialogue datasets,
Action-Based Conversations Dataset and Schema-Guided Dialogue, for two dialogue
tasks: action state tracking and workflow discovery. Our results demonstrate
that procedural knowledge augmentation improves accuracy predicting in- and
out-of-distribution actions while preserving high performance in settings with
low or sparse data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Recognition of Handwritten Japanese Characters Using Ensemble of
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03954v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03954v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Angel I. Solis, Justin Zarkovacki, John Ly, Adham Atyabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Japanese writing system is complex, with three character types of
Hiragana, Katakana, and Kanji. Kanji consists of thousands of unique
characters, further adding to the complexity of character identification and
literature understanding. Being able to translate handwritten Japanese
characters into digital text is useful for data analysis, translation, learning
and cultural preservation. In this study, a machine learning approach to
analyzing and recognizing handwritten Japanese characters (Kanji) is proposed.
The study used an ensemble of three convolutional neural networks (CNNs) for
recognizing handwritten Kanji characters and utilized four datasets of MNIST,
K-MNIST, Kuzushiji-49 (K49) and the top 150 represented classes in the
Kuzushiji-Kanji (K-Kanji) dataset for its performance evaluation. The results
indicate feasibility of using proposed CNN-ensemble architecture for
recognizing handwritten characters, achieving 99.4%, 96.4%, 95.0% and 96.4%
classification accuracy on MNIST, K-MNIS, K49, and K-Kanji datasets
respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MISGENDERED: Limits of Large Language Models in Understanding Pronouns <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03950v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03950v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tamanna Hossain, Sunipa Dev, Sameer Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Content Warning: This paper contains examples of misgendering and erasure
that could be offensive and potentially triggering.
  Gender bias in language technologies has been widely studied, but research
has mostly been restricted to a binary paradigm of gender. It is essential also
to consider non-binary gender identities, as excluding them can cause further
harm to an already marginalized group. In this paper, we comprehensively
evaluate popular language models for their ability to correctly use English
gender-neutral pronouns (e.g., singular they, them) and neo-pronouns (e.g., ze,
xe, thon) that are used by individuals whose gender identity is not represented
by binary pronouns. We introduce MISGENDERED, a framework for evaluating large
language models' ability to correctly use preferred pronouns, consisting of (i)
instances declaring an individual's pronoun, followed by a sentence with a
missing pronoun, and (ii) an experimental setup for evaluating masked and
auto-regressive language models using a unified method. When prompted
out-of-the-box, language models perform poorly at correctly predicting
neo-pronouns (averaging 7.6% accuracy) and gender-neutral pronouns (averaging
31.0% accuracy). This inability to generalize results from a lack of
representation of non-binary pronouns in training data and memorized
associations. Few-shot adaptation with explicit examples in the prompt improves
the performance but plateaus at only 45.4% for neo-pronouns. We release the
full dataset, code, and demo at
https://tamannahossainkay.github.io/misgendered/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2023 as a long paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Turning large language models into cognitive models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03917v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03917v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marcel Binz, Eric Schulz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models are powerful systems that excel at many tasks, ranging
from translation to mathematical reasoning. Yet, at the same time, these models
often show unhuman-like characteristics. In the present paper, we address this
gap and ask whether large language models can be turned into cognitive models.
We find that -- after finetuning them on data from psychological experiments --
these models offer accurate representations of human behavior, even
outperforming traditional cognitive models in two decision-making domains. In
addition, we show that their representations contain the information necessary
to model behavior on the level of individual subjects. Finally, we demonstrate
that finetuning on multiple tasks enables large language models to predict
human behavior in a previously unseen task. Taken together, these results
suggest that large, pre-trained models can be adapted to become generalist
cognitive models, thereby opening up new research directions that could
transform cognitive psychology and the behavioral sciences as a whole.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Watermark for Large Language Models <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10226v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10226v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Potential harms of large language models can be mitigated by watermarking
model output, i.e., embedding signals into generated text that are invisible to
humans but algorithmically detectable from a short span of tokens. We propose a
watermarking framework for proprietary language models. The watermark can be
embedded with negligible impact on text quality, and can be detected using an
efficient open-source algorithm without access to the language model API or
parameters. The watermark works by selecting a randomized set of "green" tokens
before a word is generated, and then softly promoting use of green tokens
during sampling. We propose a statistical test for detecting the watermark with
interpretable p-values, and derive an information-theoretic framework for
analyzing the sensitivity of the watermark. We test the watermark using a
multi-billion parameter model from the Open Pretrained Transformer (OPT)
family, and discuss robustness and security.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages in the main body. Published at ICML 2023. Code is available
  at github.com/jwkirchenbauer/lm-watermarking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoPEFT: Automatic Configuration Search for Parameter-Efficient
  Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhou, Xingchen Wan, Ivan Vulić, Anna Korhonen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pretrained language models are widely used in downstream NLP tasks via
task-specific fine-tuning, but such procedures can be costly. Recently,
Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task
performance while updating a much smaller number of parameters compared to full
model fine-tuning (FFT). However, it is non-trivial to make informed design
choices on the PEFT configurations, such as their architecture, the number of
tunable parameters, and even the layers in which the PEFT modules are inserted.
Consequently, it is highly likely that the current, manually designed
configurations are suboptimal in terms of their performance-efficiency
trade-off. Inspired by advances in neural architecture search, we propose
AutoPEFT for automatic PEFT configuration selection: we first design an
expressive configuration search space with multiple representative PEFT modules
as building blocks. Using multi-objective Bayesian optimisation in a low-cost
setup, we then discover a Pareto-optimal set of configurations with strong
performance-cost trade-offs across different numbers of parameters that are
also highly transferable across different tasks. Empirically, on GLUE and
SuperGLUE tasks, we show that AutoPEFT-discovered configurations significantly
outperform existing PEFT methods and are on par or better than FFT, without
incurring substantial training efficiency costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ o<span class="highlight-title">BERT</span>a: Improving Sparse Transfer Learning via improved initialization,
  distillation, and pruning regimes <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17612v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17612v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Campos, Alexandre Marques, Mark Kurtz, ChengXiang Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce the range of oBERTa language models, an
easy-to-use set of language models which allows Natural Language Processing
(NLP) practitioners to obtain between 3.8 and 24.3 times faster models without
expertise in model compression. Specifically, oBERTa extends existing work on
pruning, knowledge distillation, and quantization and leverages frozen
embeddings improves distillation and model initialization to deliver higher
accuracy on a broad range of transfer tasks. In generating oBERTa, we explore
how the highly optimized RoBERTa differs from the BERT for pruning during
pre-training and finetuning. We find it less amenable to compression during
fine-tuning. We explore the use of oBERTa on seven representative NLP tasks and
find that the improved compression techniques allow a pruned oBERTa model to
match the performance of BERTbase and exceed the performance of Prune OFA Large
on the SQUAD V1.1 Question Answering dataset, despite being 8x and 2x,
respectively faster in inference. We release our code, training regimes, and
associated model for broad usage to encourage usage and experimentation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SustaiNLP2023 @ ACL 2023,9 pages, 2 figures, 45 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Enhancing Programming eTextbooks with Chat<span class="highlight-title">GPT</span> Generated
  Counterfactual-Thinking-Inspired Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00551v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00551v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arun Balajiee Lekshmi Narayanan, Rully Agus Hendrawan, Venktesh V
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Digital textbooks have become an integral part of everyday learning tasks. In
this work, we consider the use of digital textbooks for programming classes.
Generally, students struggle with utilizing textbooks on programming to the
maximum, with a possible reason being that the example programs provided as
illustration of concepts in these textbooks don't offer sufficient
interactivity for students, and thereby not sufficiently motivating to explore
or understand these programming examples better. In our work, we explore the
idea of enhancing the navigability of intelligent textbooks with the use of
``counterfactual'' questions, to make students think critically about these
programs and enhance possible program comprehension. Inspired from previous
works on nudging students on counter factual thinking, we present the
possibility to enhance digital textbooks with questions generated using GPT.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Detecting Harmful Content On Online Platforms: What Platforms Need Vs.
  Where Research Efforts Go 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2103.00153v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2103.00153v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arnav Arora, Preslav Nakov, Momchil Hardalov, Sheikh Muhammad Sarwar, Vibha Nayak, Yoan Dinkov, Dimitrina Zlatkova, Kyle Dent, Ameya Bhatawdekar, Guillaume Bouchard, Isabelle Augenstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of harmful content on online platforms is a major societal
problem, which comes in many different forms including hate speech, offensive
language, bullying and harassment, misinformation, spam, violence, graphic
content, sexual abuse, self harm, and many other. Online platforms seek to
moderate such content to limit societal harm, to comply with legislation, and
to create a more inclusive environment for their users. Researchers have
developed different methods for automatically detecting harmful content, often
focusing on specific sub-problems or on narrow communities, as what is
considered harmful often depends on the platform and on the context. We argue
that there is currently a dichotomy between what types of harmful content
online platforms seek to curb, and what research efforts there are to
automatically detect such content. We thus survey existing methods as well as
content moderation policies by online platforms in this light and we suggest
directions for future work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper has been accepted for publication to ACM Computing Surveys
  (CSUR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Make Your <span class="highlight-title">Pre-train</span>ed Model Reversible: From Parameter to Memory
  Efficient Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baohao Liao, Shaomu Tan, Christof Monz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs)
has emerged as a highly successful approach, with training only a small number
of parameters without sacrificing performance and becoming the de-facto
learning paradigm with the increasing size of PLMs. However, existing PEFT
methods are not memory-efficient, because they still require caching most of
the intermediate activations for the gradient calculation, akin to fine-tuning.
One effective way to reduce the activation memory is to apply a reversible
model, so the intermediate activations are not necessary to be cached and can
be recomputed. Nevertheless, modifying a PLM to its reversible variant with
PEFT is not straightforward, since the reversible model has a distinct
architecture from the currently released PLMs. In this paper, we first
investigate what is a key factor for the success of existing PEFT methods, and
realize that it's essential to preserve the PLM's starting point when
initializing a PEFT method. With this finding, we propose memory-efficient
fine-tuning (MEFT) that inserts adapters into a PLM, preserving the PLM's
starting point and making it reversible without additional pre-training. We
evaluate MEFT on the GLUE benchmark and five question-answering tasks with
various backbones, BERT, RoBERTa, BART and OPT. MEFT significantly reduces the
activation memory up to 84% of full fine-tuning with a negligible amount of
trainable parameters. Moreover, MEFT achieves the same score on GLUE and a
comparable score on the question-answering tasks as full fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at https://github.com/BaohaoLiao/mefts</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explanation-based Finetuning Makes Models More Robust to Spurious Cues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04990v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04990v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josh Magnus Ludan, Yixuan Meng, Tai Nguyen, Saurabh Shah, Qing Lyu, Marianna Apidianaki, Chris Callison-Burch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are so powerful that they sometimes learn
correlations between labels and features that are irrelevant to the task,
leading to poor generalization on out-of-distribution data. We propose
explanation-based finetuning as a general approach to mitigate LLMs' reliance
on spurious correlations. Unlike standard finetuning where the model only
predicts the answer given the input, we finetune the model to additionally
generate a free-text explanation supporting its answer. To evaluate our method,
we finetune the model on artificially constructed training sets containing
different types of spurious cues, and test it on a test set without these cues.
Compared to standard finetuning, our method makes GPT-3 (davinci) remarkably
more robust against spurious cues in terms of accuracy drop across four
classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC
(+6.5). The efficacy generalizes across multiple model families and scales,
with greater gains for larger models. Finally, our method also works well with
explanations generated by the model, implying its applicability to more
datasets without human-written explanations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Chain-of-Symbol <span class="highlight-title">Prompt</span>ing Elicits Planning in Large Langauge Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10276v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10276v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hanxu Hu, Hongyuan Lu, Huajian Zhang, Wai Lam, Yue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we take the initiative to investigate the performance of LLMs
on complex planning tasks that require LLMs to understand a virtual spatial
environment simulated via natural language and act correspondingly in text. We
propose a benchmark named Natural Language Planning and Action (Natala)
composed of a set of novel tasks: Brick World, NLVR-based Manipulations, and
Natural Language Navigation. We found that current popular LLMs such as ChatGPT
still lack abilities in complex planning. This arises a question -- do the LLMs
have a good understanding of the environments described in natural language, or
maybe other alternatives such as symbolic representations are neater and hence
better to be understood by LLMs? To this end, we propose a novel method called
CoS (Chain-of-Symbol Prompting) that represents the complex environments with
condensed symbolic spatial representations during the chained intermediate
thinking steps. CoS is easy to use and does not need additional training on
LLMs. Extensive experiments indicate that CoS clearly surpasses the performance
of the Chain-of-Thought (CoT) Prompting in all three planning tasks with even
fewer tokens used in the inputs compared with CoT on ChatGPT and InstructGPT.
The performance gain is strong, by up to 60.8% accuracy (from 31.8% to 92.6%)
on Brick World for ChatGPT. CoS also reduces the number of tokens in the prompt
obviously, by up to 65.8% of the tokens (from 407 to 139) for the intermediate
steps from demonstrations on Brick World.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Adaptive Named Entity Recognition by Retrieving Unstructured
  Knowledge <span class="chip">EACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.07523v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.07523v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kosuke Nishida, Naoki Yoshinaga, Kyosuke Nishida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although named entity recognition (NER) helps us to extract domain-specific
entities from text (e.g., artists in the music domain), it is costly to create
a large amount of training data or a structured knowledge base to perform
accurate NER in the target domain. Here, we propose self-adaptive NER, which
retrieves external knowledge from unstructured text to learn the usages of
entities that have not been learned well. To retrieve useful knowledge for NER,
we design an effective two-stage model that retrieves unstructured knowledge
using uncertain entities as queries. Our model predicts the entities in the
input and then finds those of which the prediction is not confident. Then, it
retrieves knowledge by using these uncertain entities as queries and
concatenates the retrieved text to the original input to revise the prediction.
Experiments on CrossNER datasets demonstrated that our model outperforms strong
baselines by 2.35 points in F1 metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EACL2023 (long)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DP-BART for Privatized Text Rewriting under Local Differential Privacy <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.07636v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.07636v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Timour Igamberdiev, Ivan Habernal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Privatized text rewriting with local differential privacy (LDP) is a recent
approach that enables sharing of sensitive textual documents while formally
guaranteeing privacy protection to individuals. However, existing systems face
several issues, such as formal mathematical flaws, unrealistic privacy
guarantees, privatization of only individual words, as well as a lack of
transparency and reproducibility. In this paper, we propose a new system
'DP-BART' that largely outperforms existing LDP systems. Our approach uses a
novel clipping method, iterative pruning, and further training of internal
representations which drastically reduces the amount of noise required for DP
guarantees. We run experiments on five textual datasets of varying sizes,
rewriting them at different privacy guarantees and evaluating the rewritten
texts on downstream text classification tasks. Finally, we thoroughly discuss
the privatized text rewriting approach and its limitations, including the
problem of the strict text adjacency constraint in the LDP paradigm that leads
to the high noise requirement.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL Findings 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MERT: Acoustic Music Understanding Model with Large-Scale
  <span class="highlight-title">Self-supervised</span> Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhi Li, Ruibin Yuan, Ge Zhang, Yinghao Ma, Xingran Chen, Hanzhi Yin, Chenghua Lin, Anton Ragni, Emmanouil Benetos, Norbert Gyenge, Roger Dannenberg, Ruibo Liu, Wenhu Chen, Gus Xia, Yemin Shi, Wenhao Huang, Yike Guo, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has recently emerged as a promising paradigm
for training generalisable models on large-scale data in the fields of vision,
text, and speech. Although SSL has been proven effective in speech and audio,
its application to music audio has yet to be thoroughly explored. This is
primarily due to the distinctive challenges associated with modelling musical
knowledge, particularly its tonal and pitched characteristics of music. To
address this research gap, we propose an acoustic Music undERstanding model
with large-scale self-supervised Training (MERT), which incorporates teacher
models to provide pseudo labels in the masked language modelling (MLM) style
acoustic pre-training. In our exploration, we identified a superior combination
of teacher models, which outperforms conventional speech and audio approaches
in terms of performance. This combination includes an acoustic teacher based on
Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a musical
teacher based on the Constant-Q Transform (CQT). These teachers effectively
guide our student model, a BERT-style transformer encoder, to better model
music audio. In addition, we introduce an in-batch noise mixture augmentation
to enhance the representation robustness. Furthermore, we explore a wide range
of settings to overcome the instability in acoustic language model
pre-training, which allows our designed paradigm to scale from 95M to 330M
parameters. Experimental results indicate that our model can generalise and
perform well on 14 music understanding tasks and attains state-of-the-art
(SOTA) overall scores. The code and models are online:
https://github.com/yizhilll/MERT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ How poor is the stimulus? Evaluating hierarchical generalization in
  neural networks trained on child-directed speech <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11462v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11462v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Yedetore, Tal Linzen, Robert Frank, R. Thomas McCoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  When acquiring syntax, children consistently choose hierarchical rules over
competing non-hierarchical possibilities. Is this preference due to a learning
bias for hierarchical structure, or due to more general biases that interact
with hierarchical cues in children's linguistic input? We explore these
possibilities by training LSTMs and Transformers - two types of neural networks
without a hierarchical bias - on data similar in quantity and content to
children's linguistic input: text from the CHILDES corpus. We then evaluate
what these models have learned about English yes/no questions, a phenomenon for
which hierarchical structure is crucial. We find that, though they perform well
at capturing the surface statistics of child-directed speech (as measured by
perplexity), both model types generalize in a way more consistent with an
incorrect linear rule than the correct hierarchical rule. These results suggest
that human-like generalization from text alone requires stronger biases than
the general sequence-processing biases of standard neural network
architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages plus references and appendices; accepted to ACL</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning Language Models with Preferences through f-divergence
  Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, Nahyeon Ryu, Marc Dymetman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning language models with preferences can be posed as approximating a
target distribution representing some desired behavior. Existing approaches
differ both in the functional form of the target distribution and the algorithm
used to approximate it. For instance, Reinforcement Learning from Human
Feedback (RLHF) corresponds to minimizing a reverse KL from an implicit target
distribution arising from a KL penalty in the objective. On the other hand,
Generative Distributional Control (GDC) has an explicit target distribution and
minimizes a forward KL from it using the Distributional Policy Gradient (DPG)
algorithm. In this paper, we propose a new approach, f-DPG, which allows the
use of any f-divergence to approximate any target distribution that can be
evaluated. f-DPG unifies both frameworks (RLHF, GDC) and the approximation
methods (DPG, RL with KL penalties). We show the practical benefits of various
choices of divergence objectives and demonstrate that there is no universally
optimal objective but that different divergences present different alignment
and diversity trade-offs. We show that Jensen-Shannon divergence strikes a good
balance between these objectives, and frequently outperforms forward KL
divergence by a wide margin, leading to significant improvements over prior
work. These distinguishing characteristics between divergences persist as the
model size increases, highlighting the importance of selecting appropriate
divergence objectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ GigaST: A 10,000-hour Pseudo Speech Translation Corpus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.03939v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.03939v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rong Ye, Chengqi Zhao, Tom Ko, Chutong Meng, Tao Wang, Mingxuan Wang, Jun Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces GigaST, a large-scale pseudo speech translation (ST)
corpus. We create the corpus by translating the text in GigaSpeech, an English
ASR corpus, into German and Chinese. The training set is translated by a strong
machine translation system and the test set is translated by human. ST models
trained with an addition of our corpus obtain new state-of-the-art results on
the MuST-C English-German benchmark test set. We provide a detailed description
of the translation process and verify its quality. We make the translated text
data public and hope to facilitate research in speech translation.
Additionally, we also release the training scripts on NeurST to make it easy to
replicate our systems. GigaST dataset is available at
https://st-benchmark.github.io/resources/GigaST.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech 2023. GigaST dataset is available at
  https://st-benchmark.github.io/resources/GigaST</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Comprehensive <span class="highlight-title">Survey</span> on Deep Learning for Relation Extraction: Recent
  Advances and New Frontiers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02051v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02051v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyan Zhao, Yang Deng, Min Yang, Lingzhi Wang, Rui Zhang, Hong Cheng, Wai Lam, Ying Shen, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation extraction (RE) involves identifying the relations between entities
from unstructured texts. RE serves as the foundation for many natural language
processing (NLP) applications, such as knowledge graph completion, question
answering, and information retrieval. In recent years, deep neural networks
have dominated the field of RE and made noticeable progress. Subsequently, the
large pre-trained language models (PLMs) have taken the state-of-the-art of RE
to a new level. This survey provides a comprehensive review of existing deep
learning techniques for RE. First, we introduce RE resources, including RE
datasets and evaluation metrics. Second, we propose a new taxonomy to
categorize existing works from three perspectives (text representation, context
encoding, and triplet prediction). Third, we discuss several important
challenges faced by RE and summarize potential techniques to tackle these
challenges. Finally, we outline some promising future directions and prospects
in this field. This survey is expected to facilitate researchers' collaborative
efforts to tackle the challenges of real-life RE systems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02858v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02858v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Zhang, Xin Li, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Video-LLaMA, a multi-modal framework that empowers Large Language
Models (LLMs) with the capability of understanding both visual and auditory
content in the video. Video-LLaMA bootstraps cross-modal training from the
frozen pre-trained visual & audio encoders and the frozen LLMs. Unlike previous
vision-LLMs that focus on static image comprehensions such as MiniGPT-4 and
LLaVA, Video-LLaMA mainly tackles two challenges in video understanding: (1)
capturing the temporal changes in visual scenes, (2) integrating audio-visual
signals. To counter the first challenge, we propose a Video Q-former to
assemble the pre-trained image encoder into our video encoder and introduce a
video-to-text generation task to learn video-language correspondence. For the
second challenge, we leverage ImageBind, a universal embedding model aligning
multiple modalities as the pre-trained audio encoder, and introduce an Audio
Q-former on top of ImageBind to learn reasonable auditory query embeddings for
the LLM module. To align the output of both visual & audio encoders with LLM's
embedding space, we train Video-LLaMA on massive video/image-caption pairs as
well as visual-instruction-tuning datasets of moderate amount but higher
quality. We found Video-LLaMA showcases the ability to perceive and comprehend
video content, generating meaningful responses that are grounded in the visual
and auditory information presented in the videos. This highlights the potential
of Video-LLaMA as a promising prototype for audio-visual AI assistants.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report; Code, Pretrained Model, and Dataset:
  https://github.com/DAMO-NLP-SG/Video-LLaMA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SI-LSTM: Speaker Hybrid Long-short Term Memory and Cross Modal Attention
  for Emotion Recognition in Conversation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03506v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03506v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingwei Liang, You Zou, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion Recognition in Conversation~(ERC) across modalities is of vital
importance for a variety of applications, including intelligent healthcare,
artificial intelligence for conversation, and opinion mining over chat history.
The crux of ERC is to model both cross-modality and cross-time interactions
throughout the conversation. Previous methods have made progress in learning
the time series information of conversation while lacking the ability to trace
down the different emotional states of each speaker in a conversation. In this
paper, we propose a recurrent structure called Speaker Information Enhanced
Long-Short Term Memory (SI-LSTM) for the ERC task, where the emotional states
of the distinct speaker can be tracked in a sequential way to enhance the
learning of the emotion in conversation. Further, to improve the learning of
multimodal features in ERC, we utilize a cross-modal attention component to
fuse the features between different modalities and model the interaction of the
important information from different modalities. Experimental results on two
benchmark datasets demonstrate the superiority of the proposed SI-LSTM against
the state-of-the-art baseline methods in the ERC task on multimodal data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>modification needed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can In-context Learners Learn a Reasoning Concept from Demonstrations? <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01692v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01692v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Štefánik, Marek Kadlčík
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models show an emergent ability to learn a new task from a
small number of input-output demonstrations. However, recent work shows that
in-context learners largely rely on their pre-trained knowledge, such as the
sentiment of the labels, instead of finding new associations in the input.
However, the commonly-used few-shot evaluation settings using a random
selection of in-context demonstrations can not disentangle models' ability to
learn a new skill from demonstrations, as most of the randomly-selected
demonstrations do not present relations informative for prediction beyond
exposing the new task distribution.
  To disentangle models' in-context learning ability independent of models'
memory, we introduce a Conceptual few-shot learning method selecting the
demonstrations sharing a possibly-informative concept with the predicted
sample. We extract a set of such concepts from annotated explanations and
measure how much can models benefit from presenting these concepts in few-shot
demonstrations.
  We find that smaller models are more sensitive to the presented concepts.
While some of the models are able to benefit from concept-presenting
demonstrations for each assessed concept, we find that none of the assessed
in-context learners can benefit from all presented reasoning concepts
consistently, leaving the in-context concept learning an open challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2023 Natural Language Reasoning workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph2topic: an opensource topic modeling framework based on sentence
  embedding and community detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06653v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06653v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leihang Zhang, Jiapeng Liu, Qiang Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has been reported that clustering-based topic models, which cluster
high-quality sentence embeddings with an appropriate word selection method, can
generate better topics than generative probabilistic topic models. However,
these approaches suffer from the inability to select appropriate parameters and
incomplete models that overlook the quantitative relation between words with
topics and topics with text. To solve these issues, we propose graph to topic
(G2T), a simple but effective framework for topic modelling. The framework is
composed of four modules. First, document representation is acquired using
pretrained language models. Second, a semantic graph is constructed according
to the similarity between document representations. Third, communities in
document semantic graphs are identified, and the relationship between topics
and documents is quantified accordingly. Fourth, the word--topic distribution
is computed based on a variant of TFIDF. Automatic evaluation suggests that G2T
achieved state-of-the-art performance on both English and Chinese documents
with different lengths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Politics of Language Choice: How the Russian-Ukrainian War
  Influences Ukrainians' Language Use on Twitter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.02770v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.02770v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Racek, Brittany I. Davidson, Paul W. Thurner, Xiao Xiang Zhu, Göran Kauermann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The use of language is innately political and often a vehicle of cultural
identity as well as the basis for nation building. Here, we examine language
choice and tweeting activity of Ukrainian citizens based on more than 4 million
geo-tagged tweets from over 62,000 users before and during the
Russian-Ukrainian War, from January 2020 to October 2022. Using statistical
models, we disentangle sample effects, arising from the in- and outflux of
users on Twitter, from behavioural effects, arising from behavioural changes of
the users. We observe a steady shift from the Russian language towards the
Ukrainian language already before the war, which drastically speeds up with its
outbreak. We attribute these shifts in large part to users' behavioural
changes. Notably, we find that more than half of the Russian-tweeting users
shift towards Ukrainian as a result of the war.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topological Data Analysis for Speech Processing <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.17223v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.17223v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva, Daniil Cherniavskii, Serguei Barannikov, Irina Piontkovskaya, Sergey Nikolenko, Evgeny Burnaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We apply topological data analysis (TDA) to speech classification problems
and to the introspection of a pretrained speech model, HuBERT. To this end, we
introduce a number of topological and algebraic features derived from
Transformer attention maps and embeddings. We show that a simple linear
classifier built on top of such features outperforms a fine-tuned
classification head. In particular, we achieve an improvement of about $9\%$
accuracy and $5\%$ ERR on four common datasets; on CREMA-D, the proposed
feature set reaches a new state of the art performance with accuracy $80.155$.
We also show that topological features are able to reveal functional roles of
speech Transformer heads; e.g., we find the heads capable to distinguish
between pairs of sample sources (natural/synthetic) or voices without any
downstream fine-tuning. Our results demonstrate that TDA is a promising new
approach for speech analysis, especially for tasks that require structural
prediction. Appendices, an introduction to TDA, and other additional materials
are available here - https://topohubert.github.io/speech-topology-webpages/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to INTERSPEECH 2023 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Guess the Instruction! Flipped Learning Makes Language Models Stronger
  Zero-Shot Learners <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.02969v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.02969v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seonghyeon Ye, Doyoung Kim, Joel Jang, Joongbo Shin, Minjoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Meta-training, which fine-tunes the language model (LM) on various downstream
tasks by maximizing the likelihood of the target label given the task
instruction and input instance, has improved the zero-shot task generalization
performance. However, meta-trained LMs still struggle to generalize to
challenging tasks containing novel labels unseen during meta-training. In this
paper, we propose Flipped Learning, an alternative method of meta-training
which trains the LM to generate the task instruction given the input instance
and label. During inference, the LM trained with Flipped Learning, referred to
as Flipped, selects the label option that is most likely to generate the task
instruction. On 14 tasks of the BIG-bench benchmark, the 11B-sized Flipped
outperforms zero-shot T0-11B and even a 16 times larger 3-shot GPT-3 (175B) on
average by 8.4% and 9.7% points, respectively. Flipped gives particularly large
improvements on tasks with unseen labels, outperforming T0-11B by up to +20%
average F1 score. This indicates that the strong task generalization of Flipped
comes from improved generalization to novel labels. We release our code at
https://github.com/seonghyeonye/Flipped-Learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Self-supervised</span> Predictive Coding Models Encode Speaker and Phonetic
  Information in Orthogonal Subspaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12464v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12464v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Oli Liu, Hao Tang, Sharon Goldwater
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised speech representations are known to encode both speaker and
phonetic information, but how they are distributed in the high-dimensional
space remains largely unexplored. We hypothesize that they are encoded in
orthogonal subspaces, a property that lends itself to simple disentanglement.
Applying principal component analysis to representations of two predictive
coding models, we identify two subspaces that capture speaker and phonetic
variances, and confirm that they are nearly orthogonal. Based on this property,
we propose a new speaker normalization method which collapses the subspace that
encodes speaker information, without requiring transcriptions. Probing
experiments show that our method effectively eliminates speaker information and
outperforms a previous baseline in phone discrimination tasks. Moreover, the
approach generalizes and can be used to remove information of unseen speakers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Kernel-Based View of Language Model Fine-Tuning <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.05643v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.05643v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, Sanjeev Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has become standard to solve NLP tasks by fine-tuning pre-trained language
models (LMs), especially in low-data settings. There is minimal theoretical
understanding of empirical success, e.g., why fine-tuning a model with $10^8$
or more parameters on a couple dozen training points does not result in
overfitting. We investigate whether the Neural Tangent Kernel (NTK) - which
originated as a model to study the gradient descent dynamics of infinitely wide
networks with suitable random initialization - describes fine-tuning of
pre-trained LMs. This study was inspired by the decent performance of NTK for
computer vision tasks (Wei et al., 2022). We extend the NTK formalism to Adam
and use Tensor Programs (Yang, 2020) to characterize conditions under which the
NTK lens may describe fine-tuning updates to pre-trained language models.
Extensive experiments on 14 NLP tasks validate our theory and show that
formulating the downstream task as a masked word prediction problem through
prompting often induces kernel-based dynamics during fine-tuning. Finally, we
use this kernel view to propose an explanation for the success of
parameter-efficient subspace-based fine-tuning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023. Code and pre-computed kernels are publicly
  available at https://github.com/princeton-nlp/LM-Kernel-FT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models Can Be Easily Distracted by Irrelevant Context <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00093v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00093v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Freda Shi, Xinyun Chen, Kanishka Misra, Nathan Scales, David Dohan, Ed Chi, Nathanael Schärli, Denny Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models have achieved impressive performance on various natural
language processing tasks. However, so far they have been evaluated primarily
on benchmarks where all information in the input context is relevant for
solving the task. In this work, we investigate the distractibility of large
language models, i.e., how the model problem-solving accuracy can be influenced
by irrelevant context. In particular, we introduce Grade-School Math with
Irrelevant Context (GSM-IC), an arithmetic reasoning dataset with irrelevant
information in the problem description. We use this benchmark to measure the
distractibility of cutting-edge prompting techniques for large language models,
and find that the model performance is dramatically decreased when irrelevant
information is included. We also identify several approaches for mitigating
this deficiency, such as decoding with self-consistency and adding to the
prompt an instruction that tells the language model to ignore the irrelevant
information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Benchmarking Spatial Relationships in Text-to-Image Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10015v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10015v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tejas Gokhale, Hamid Palangi, Besmira Nushi, Vibhav Vineet, Eric Horvitz, Ece Kamar, Chitta Baral, Yezhou Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spatial understanding is a fundamental aspect of computer vision and integral
for human-level reasoning about images, making it an important component for
grounded language understanding. While recent text-to-image synthesis (T2I)
models have shown unprecedented improvements in photorealism, it is unclear
whether they have reliable spatial understanding capabilities. We investigate
the ability of T2I models to generate correct spatial relationships among
objects and present VISOR, an evaluation metric that captures how accurately
the spatial relationship described in text is generated in the image. To
benchmark existing models, we introduce a dataset, SR2D, that contains
sentences describing two objects and the spatial relationship between them. We
construct an automated evaluation pipeline to recognize objects and their
spatial relationships, and employ it in a large-scale evaluation of T2I models.
Our experiments reveal a surprising finding that, although state-of-the-art T2I
models exhibit high image quality, they are severely limited in their ability
to generate multiple objects or the specified spatial relations between them.
Our analyses demonstrate several biases and artifacts of T2I models such as the
difficulty with generating multiple objects, a bias towards generating the
first object mentioned, spatially inconsistent outputs for equivalent
relationships, and a correlation between object co-occurrence and spatial
understanding capabilities. We conduct a human study that shows the alignment
between VISOR and human judgement about spatial understanding. We offer the
SR2D dataset and the VISOR metric to the community in support of T2I reasoning
research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>preprint; Code and Data at https://github.com/microsoft/VISOR and
  https://huggingface.co/datasets/tgokhale/sr2d_visor</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting the Gold Standard: Grounding Summarization Evaluation with
  Robust Human Evaluation <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.07981v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.07981v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yixin Liu, Alexander R. Fabbri, Pengfei Liu, Yilun Zhao, Linyong Nan, Ruilin Han, Simeng Han, Shafiq Joty, Chien-Sheng Wu, Caiming Xiong, Dragomir Radev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Human evaluation is the foundation upon which the evaluation of both
summarization systems and automatic metrics rests. However, existing human
evaluation studies for summarization either exhibit a low inter-annotator
agreement or have insufficient scale, and an in-depth analysis of human
evaluation is lacking. Therefore, we address the shortcomings of existing
summarization evaluation along the following axes: (1) We propose a modified
summarization salience protocol, Atomic Content Units (ACUs), which is based on
fine-grained semantic units and allows for a high inter-annotator agreement.
(2) We curate the Robust Summarization Evaluation (RoSE) benchmark, a large
human evaluation dataset consisting of 22,000 summary-level annotations over 28
top-performing systems on three datasets. (3) We conduct a comparative study of
four human evaluation protocols, underscoring potential confounding factors in
evaluation setups. (4) We evaluate 50 automatic metrics and their variants
using the collected human annotations across evaluation protocols and
demonstrate how our benchmark leads to more statistically stable and
significant results. The metrics we benchmarked include recent methods based on
large language models (LLMs), GPTScore and G-Eval. Furthermore, our findings
have important implications for evaluating LLMs, as we show that LLMs adjusted
by human feedback (e.g., GPT-3.5) may overfit unconstrained human evaluation,
which is affected by the annotators' prior, input-agnostic preferences, calling
for more robust, targeted evaluation methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 Camera Ready</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CCpdf: Building a High Quality Corpus for Visually Rich Documents from
  Web Crawl Data <span class="chip">ICDAR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michał Turski, Tomasz Stanisławek, Karol Kaczmarek, Paweł Dyda, Filip Graliński
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In recent years, the field of document understanding has progressed a lot. A
significant part of this progress has been possible thanks to the use of
language models pretrained on large amounts of documents. However, pretraining
corpora used in the domain of document understanding are single domain,
monolingual, or nonpublic. Our goal in this paper is to propose an efficient
pipeline for creating a big-scale, diverse, multilingual corpus of PDF files
from all over the Internet using Common Crawl, as PDF files are the most
canonical types of documents as considered in document understanding. We
analysed extensively all of the steps of the pipeline and proposed a solution
which is a trade-off between data quality and processing time. We also share a
CCpdf corpus in a form or an index of PDF files along with a script for
downloading them, which produces a collection useful for language model
pretraining. The dataset and tools published with this paper offer researchers
the opportunity to develop even better multilingual language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICDAR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MultiLegalPile: A 689GB Multilingual Legal Corpus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02069v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02069v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joel Niklaus, Veton Matoshi, Matthias Stürmer, Ilias Chalkidis, Daniel E. Ho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large, high-quality datasets are crucial for training Large Language Models
(LLMs). However, so far, there are few datasets available for specialized
critical domains such as law and the available ones are often only for the
English language. We curate and release MultiLegalPile, a 689GB corpus in 24
languages from 17 jurisdictions. The MultiLegalPile corpus, which includes
diverse legal data sources with varying licenses, allows for pretraining NLP
models under fair use, with more permissive licenses for the Eurlex Resources
and Legal mC4 subsets. We pretrain two RoBERTa models and one Longformer
multilingually, and 24 monolingual models on each of the language-specific
subsets and evaluate them on LEXTREME. Additionally, we evaluate the English
and multilingual models on LexGLUE. Our multilingual models set a new SotA on
LEXTREME and our English models on LexGLUE. We release the dataset, the trained
models, and all of the code under the most open possible licenses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NAIST-SIC-Aligned: Automatically-Aligned English-Japanese Simultaneous
  Interpretation Corpus 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.11766v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.11766v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinming Zhao, Yuka Ko, Kosuke Doi, Ryo Fukuda, Katsuhito Sudoh, Satoshi Nakamura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It remains a question that how simultaneous interpretation (SI) data affects
simultaneous machine translation (SiMT). Research has been limited due to the
lack of a large-scale training corpus. In this work, we aim to fill in the gap
by introducing NAIST-SIC-Aligned, which is an automatically-aligned parallel
English-Japanese SI dataset. Starting with a non-aligned corpus NAIST-SIC, we
propose a two-stage alignment approach to make the corpus parallel and thus
suitable for model training. The first stage is coarse alignment where we
perform a many-to-many mapping between source and target sentences, and the
second stage is fine-grained alignment where we perform intra- and
inter-sentence filtering to improve the quality of aligned pairs. To ensure the
quality of the corpus, each step has been validated either quantitatively or
qualitatively. This is the first open-sourced large-scale parallel SI dataset
in the literature. We also manually curated a small test set for evaluation
purposes. We hope our work advances research on SI corpora construction and
SiMT. Please find our data at \url{https://github.com/mingzi151/AHC-SI}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fixed typos</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DuNST: Dual Noisy Self Training for Semi-Supervised Controllable Text
  Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08724v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08724v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuxi Feng, Xiaoyuan Yi, Xiting Wang, Laks V. S. Lakshmanan, Xing Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-training (ST) has prospered again in language understanding by
augmenting the fine-tuning of pre-trained language models when labeled data is
insufficient. However, it remains challenging to incorporate ST into
attribute-controllable language generation. Augmented by only self-generated
pseudo text, generation models over-emphasize exploitation of the previously
learned space, suffering from a constrained generalization boundary. We revisit
ST and propose a novel method, DuNST to alleviate this problem. DuNST jointly
models text generation and classification with a shared Variational AutoEncoder
and corrupts the generated pseudo text by two kinds of flexible noise to
disturb the space. In this way, our model could construct and utilize both
pseudo text from given labels and pseudo labels from available unlabeled text,
which are gradually refined during the ST process. We theoretically demonstrate
that DuNST can be regarded as enhancing exploration towards the potential real
text space, providing a guarantee of improved performance. Experiments on three
controllable generation tasks show that DuNST could significantly boost control
accuracy while maintaining comparable generation fluency and diversity against
several strong baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Multi-Step Reasoning by Solving Arithmetic Tasks <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01707v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01707v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianduo Wang, Wei Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mathematical reasoning is regarded as a necessary ability for Language Models
(LMs). Recent works demonstrate large LMs' impressive performance in solving
math problems. The success is attributed to their Chain-of-Thought (CoT)
reasoning abilities, i.e., the ability to decompose complex questions into
step-by-step reasoning chains, but such ability seems only to emerge from
models with abundant parameters. This work investigates how to incorporate
relatively small LMs with the capabilities of multi-step reasoning. We propose
to inject such abilities by continually pre-training LMs on a synthetic dataset
MsAT which is composed of Multi-step Arithmetic Tasks. Our experiments on four
math word problem datasets show the effectiveness of the proposed method in
enhancing LMs' math reasoning abilities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023. Code and data are available at
  https://github.com/TianduoWang/MsAT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CapText: Large Language Model-based Caption Generation From Image
  Context and Description 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00301v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00301v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shinjini Ghosh, Sagnik Anupam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While deep-learning models have been shown to perform well on image-to-text
datasets, it is difficult to use them in practice for captioning images. This
is because captions traditionally tend to be context-dependent and offer
complementary information about an image, while models tend to produce
descriptions that describe the visual features of the image. Prior research in
caption generation has explored the use of models that generate captions when
provided with the images alongside their respective descriptions or contexts.
We propose and evaluate a new approach, which leverages existing large language
models to generate captions from textual descriptions and context alone,
without ever processing the image directly. We demonstrate that after
fine-tuning, our approach outperforms current state-of-the-art image-text
alignment models like OSCAR-VinVL on this task on the CIDEr metric.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update 6/6/23: Fixed typographic error in abstract</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02254v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02254v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunwoong Ko, Kichang Yang, Minho Ryu, Taekyoon Choi, Seungmu Yang, Jiwung Hyun, Sungho Park, Kyubyong Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polyglot is a pioneering project aimed at enhancing the non-English language
performance of multilingual language models. Despite the availability of
various multilingual models such as mBERT (Devlin et al., 2019), XGLM (Lin et
al., 2022), and BLOOM (Scao et al., 2022), researchers and developers often
resort to building monolingual models in their respective languages due to the
dissatisfaction with the current multilingual models non-English language
capabilities. Addressing this gap, we seek to develop advanced multilingual
language models that offer improved performance in non-English languages. In
this paper, we introduce the Polyglot Korean models, which represent a specific
focus rather than being multilingual in nature. In collaboration with TUNiB,
our team collected 1.2TB of Korean data meticulously curated for our research
journey. We made a deliberate decision to prioritize the development of Korean
models before venturing into multilingual models. This choice was motivated by
multiple factors: firstly, the Korean models facilitated performance
comparisons with existing multilingual models; and finally, they catered to the
specific needs of Korean companies and researchers. This paper presents our
work in developing the Polyglot Korean models, which propose some steps towards
addressing the non-English language performance gap in multilingual language
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Do <span class="highlight-title">GPT</span>s Produce Less Literal Translations? <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16806v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16806v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vikas Raunak, Arul Menezes, Matt Post, Hany Hassan Awadalla
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) such as GPT-3 have emerged as general-purpose
language models capable of addressing many natural language generation or
understanding tasks. On the task of Machine Translation (MT), multiple works
have investigated few-shot prompting mechanisms to elicit better translations
from LLMs. However, there has been relatively little investigation on how such
translations differ qualitatively from the translations generated by standard
Neural Machine Translation (NMT) models. In this work, we investigate these
differences in terms of the literalness of translations produced by the two
systems. Using literalness measures involving word alignment and monotonicity,
we find that translations out of English (E-X) from GPTs tend to be less
literal, while exhibiting similar or better scores on MT quality metrics. We
demonstrate that this finding is borne out in human evaluations as well. We
then show that these differences are especially pronounced when translating
sentences that contain idiomatic expressions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Universal Discriminator for Zero-Shot Generalization <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.08099v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.08099v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haike Xu, Zongyu Lin, Jing Zhou, Yanan Zheng, Zhilin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative modeling has been the dominant approach for large-scale
pretraining and zero-shot generalization. In this work, we challenge this
convention by showing that discriminative approaches perform substantially
better than generative ones on a large number of NLP tasks. Technically, we
train a single discriminator to predict whether a text sample comes from the
true data distribution, similar to GANs. Since many NLP tasks can be formulated
as selecting from a few options, we use this discriminator to predict the
concatenation of input and which option has the highest probability of coming
from the true data distribution. This simple formulation achieves
state-of-the-art zero-shot results on the T0 benchmark, outperforming T0 by
16.0\%, 7.8\%, and 11.5\% respectively on different scales. In the finetuning
setting, our approach also achieves new state-of-the-art results on a wide
range of NLP tasks, with only 1/4 parameters of previous methods. Meanwhile,
our approach requires minimal prompting efforts, which largely improves
robustness and is essential for real-world applications. Furthermore, we also
jointly train a generalized UD in combination with generative tasks, which
maintains its advantage on discriminative tasks and simultaneously works on
generative tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 main conference (Long paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PDFVQA: A New <span class="highlight-title">Dataset</span> for Real-World VQA on PDF Documents <span class="chip">ECML-PKDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06447v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06447v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihao Ding, Siwen Luo, Hyunsuk Chung, Soyeon Caren Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document-based Visual Question Answering examines the document understanding
of document images in conditions of natural language questions. We proposed a
new document-based VQA dataset, PDF-VQA, to comprehensively examine the
document understanding from various aspects, including document element
recognition, document layout structural understanding as well as contextual
understanding and key information extraction. Our PDF-VQA dataset extends the
current scale of document understanding that limits on the single document page
to the new scale that asks questions over the full document of multiple pages.
We also propose a new graph-based VQA model that explicitly integrates the
spatial and hierarchically structural relationships between different document
elements to boost the document structural understanding. The performances are
compared with several baselines over different question types and
tasks\footnote{The full dataset will be released after paper acceptance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ECML-PKDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Controllable Dialogue Simulation with In-Context Learning <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04185v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04185v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zekun Li, Wenhu Chen, Shiyang Li, Hong Wang, Jing Qian, Xifeng Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Building dialogue systems requires a large corpus of annotated dialogues.
Such datasets are usually created via crowdsourcing, which is expensive and
time-consuming. In this paper, we propose \textsc{Dialogic}, a novel dialogue
simulation method based on large language model in-context learning to automate
dataset creation. Seeded with a few annotated dialogues, \textsc{Dialogic}
automatically selects in-context examples for demonstration and prompts GPT-3
to generate new dialogues and annotations in a controllable way. Our method can
rapidly expand a small set of dialogue data with minimum or zero \textit{human
involvement} and \textit{parameter update} and is thus much more cost-efficient
and time-saving than crowdsourcing. Experimental results on the MultiWOZ
dataset demonstrate that training a model on the simulated dialogues leads to
even better performance than using the same amount of human-generated dialogues
under the challenging low-resource settings, with as few as 85 dialogues as a
seed. When enough data is available, our method can still serve as an effective
data augmentation method. Human evaluation results also show that our simulated
dialogues have near-human fluency and annotation accuracy. The code and data
are available at \textbf{\url{https://github.com/Leezekun/dialogic}}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>EMNLP 2022 Findings, code and data are available at
  https://github.com/Leezekun/dialogic</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Zero-Shot <span class="highlight-title">Prompt</span>ing for Implicit Intent Prediction and Recommendation
  with Commonsense Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.05901v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.05901v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hui-Chi Kuo, Yun-Nung Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent virtual assistants are currently designed to perform tasks or
services explicitly mentioned by users, so multiple related domains or tasks
need to be performed one by one through a long conversation with many explicit
intents. Instead, human assistants are capable of reasoning (multiple) implicit
intents based on user utterances via commonsense knowledge, reducing complex
interactions and improving practicality. Therefore, this paper proposes a
framework of multi-domain dialogue systems, which can automatically infer
implicit intents based on user utterances and then perform zero-shot prompting
using a large pre-trained language model to trigger suitable single
task-oriented bots. The proposed framework is demonstrated effective to realize
implicit intents and recommend associated bots in a zero-shot manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A logical word embedding for learning grammar 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.14590v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.14590v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sean Deyo, Veit Elser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the logical grammar emdebbing (LGE), a model inspired by
pregroup grammars and categorial grammars to enable unsupervised inference of
lexical categories and syntactic rules from a corpus of text. LGE produces
comprehensible output summarizing its inferences, has a completely transparent
process for producing novel sentences, and can learn from as few as a hundred
sentences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Retrieval-Augmented Multimodal Language Modeling <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, Wen-tau Yih
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent multimodal models such as DALL-E and CM3 have achieved remarkable
progress in text-to-image and image-to-text generation. However, these models
store all learned knowledge (e.g., the appearance of the Eiffel Tower) in the
model parameters, requiring increasingly larger models and training data to
capture more knowledge. To integrate knowledge in a more scalable and modular
way, we propose a retrieval-augmented multimodal model, which enables a base
multimodal model (generator) to refer to relevant text and images fetched by a
retriever from external memory (e.g., documents on the web). Specifically, for
the retriever, we use a pretrained CLIP, and for the generator, we train a CM3
Transformer on the LAION dataset. Our resulting model, named
Retrieval-Augmented CM3 (RA-CM3), is the first multimodal model that can
retrieve and generate both text and images. We show that RA-CM3 significantly
outperforms baseline multimodal models such as DALL-E and CM3 on both image and
caption generation tasks (12 FID and 17 CIDEr improvements on MS-COCO), while
requiring much less compute for training (<30% of DALL-E). Moreover, we show
that RA-CM3 exhibits novel capabilities, such as faithful image generation and
multimodal in-context learning (e.g., image generation from demonstrations).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICML 2023. Blog post available at
  https://cs.stanford.edu/~myasu/blog/racm3/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Protecting Language Generation Models via Invisible Watermarking <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03162v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03162v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xuandong Zhao, Yu-Xiang Wang, Lei Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language generation models have been an increasingly powerful enabler for
many applications. Many such models offer free or affordable API access, which
makes them potentially vulnerable to model extraction attacks through
distillation. To protect intellectual property (IP) and ensure fair use of
these models, various techniques such as lexical watermarking and synonym
replacement have been proposed. However, these methods can be nullified by
obvious countermeasures such as "synonym randomization". To address this issue,
we propose GINSEW, a novel method to protect text generation models from being
stolen through distillation. The key idea of our method is to inject secret
signals into the probability vector of the decoding steps for each target
token. We can then detect the secret message by probing a suspect model to tell
if it is distilled from the protected one. Experimental results show that
GINSEW can effectively identify instances of IP infringement with minimal
impact on the generation quality of protected APIs. Our method demonstrates an
absolute improvement of 19 to 29 points on mean average precision (mAP) in
detecting suspects compared to previous methods against watermark removal
attacks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fine-Tuning Language Models with Advantage-Induced Policy Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02231v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02231v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Banghua Zhu, Hiteshi Sharma, Felipe Vieira Frujeri, Shi Dong, Chenguang Zhu, Michael I. Jordan, Jiantao Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning from human feedback (RLHF) has emerged as a reliable
approach to aligning large language models (LLMs) to human preferences. Among
the plethora of RLHF techniques, proximal policy optimization (PPO) is of the
most widely used methods. Despite its popularity, however, PPO may suffer from
mode collapse, instability, and poor sample efficiency. We show that these
issues can be alleviated by a novel algorithm that we refer to as
Advantage-Induced Policy Alignment (APA), which leverages a squared error loss
function based on the estimated advantages. We demonstrate empirically that APA
consistently outperforms PPO in language tasks by a large margin, when a
separate reward model is employed as the evaluator. In addition, compared with
PPO, APA offers a more stable form of control over the deviation from the
model's initial policy, ensuring that the model improves its performance
without collapsing to deterministic output. In addition to empirical results,
we also provide a theoretical justification supporting the design of our loss
function.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ People and Places of Historical Europe: Bootstrapping Annotation
  Pipeline and a New Corpus of Named Entities in Late Medieval Texts <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16718v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16718v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Vít Novotný, Kristýna Luger, Michal Štefánik, Tereza Vrabcová, Aleš Horák
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although pre-trained named entity recognition (NER) models are highly
accurate on modern corpora, they underperform on historical texts due to
differences in language OCR errors. In this work, we develop a new NER corpus
of 3.6M sentences from late medieval charters written mainly in Czech, Latin,
and German.
  We show that we can start with a list of known historical figures and
locations and an unannotated corpus of historical texts, and use information
retrieval techniques to automatically bootstrap a NER-annotated corpus. Using
our corpus, we train a NER model that achieves entity-level Precision of
72.81-93.98% with 58.14-81.77% Recall on a manually-annotated test dataset.
Furthermore, we show that using a weighted loss function helps to combat class
imbalance in token classification tasks. To make it easy for others to
reproduce and build upon our work, we publicly release our corpus, models, and
experimental code.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Findings of the Association for Computational
  Linguistics: ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Privacy-Preserving Domain Adaptation of Semantic Parsers 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10520v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10520v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fatemehsadat Mireshghallah, Richard Shin, Yu Su, Tatsunori Hashimoto, Jason Eisner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Task-oriented dialogue systems often assist users with personal or
confidential matters. For this reason, the developers of such a system are
generally prohibited from observing actual usage. So how can they know where
the system is failing and needs more training data or new functionality? In
this work, we study ways in which realistic user utterances can be generated
synthetically, to help increase the linguistic and functional coverage of the
system, without compromising the privacy of actual users. To this end, we
propose a two-stage Differentially Private (DP) generation method which first
generates latent semantic parses, and then generates utterances based on the
parses. Our proposed approach improves MAUVE by 2.5X and parse tree function
type overlap by 1.3X relative to current approaches for private synthetic data
generation, improving both on fluency and semantic coverage. We further
validate our approach on a realistic domain adaptation task of adding new
functionality from private user data to a semantic parser, and show overall
gains of 8.5% points in accuracy with the new feature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FEED PETs: Further Experimentation and Expansion on the Disambiguation
  of Potentially Euphemistic Terms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00217v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00217v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Lee, Iyanuoluwa Shode, Alain Chirino Trujillo, Yuan Zhao, Olumide Ebenezer Ojo, Diana Cuevas Plancarte, Anna Feldman, Jing Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers have been shown to work well for the task of English euphemism
disambiguation, in which a potentially euphemistic term (PET) is classified as
euphemistic or non-euphemistic in a particular context. In this study, we
expand on the task in two ways. First, we annotate PETs for vagueness, a
linguistic property associated with euphemisms, and find that transformers are
generally better at classifying vague PETs, suggesting linguistic differences
in the data that impact performance. Second, we present novel euphemism corpora
in three different languages: Yoruba, Spanish, and Mandarin Chinese. We perform
euphemism disambiguation experiments in each language using multilingual
transformer models mBERT and XLM-RoBERTa, establishing preliminary results from
which to launch future work.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Optimization and Control <span class="chip" style="font-size: 60%">37</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Characterization of transport optimizers via graphs and applications to
  Stackelberg-Cournot-Nash equilibria 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03843v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03843v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beatrice Acciaio, Berenice Anne Neumann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce graphs associated to transport problems between discrete
marginals, that allow to characterize the set of all optimizers given one
primal optimizer. In particular, we establish that connectivity of those graphs
is a necessary and sufficient condition for uniqueness of the dual optimizers.
Moreover, we provide an algorithm that can efficiently compute the dual
optimizer that is the limit, as the regularization parameter goes to zero, of
the dual entropic optimizers. Our results find an application in a
Stackelberg-Cournot-Nash game, for which we obtain existence and
characterization of the equilibria.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Weak KAM Theory and Aubry-Mather Theory for sub-Riemannian control
  systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Piermarco Cannarsa, Cristian Mendico
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The aim of this work is to provide a systemic study and generalization of the
celebrated weak KAM theory and Aubry-Mather theory in sub-Riemannian setting,
or equivalently, on a Carnot-Caratheodory metric space. In this framework we
consider an optimal control problem with state equation of sub-Riemannian type,
namely, admissible trajectories are solutions of a linear in control and
nonlinear in space ODE. Such a nonlinearity is given by a family of smooth
vector fields satisfying the Hormander condition which implies the
controllability of the system. In this case, the Hamiltonian function
associated with the above control problem fails to be coercive and thus the
results in the Tonelli setting can not be applied. In order to overcome this
issue, our approach is based on metric properties of the geometry induced on
the state space by the sub-Riemannian structure.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2204.12544</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exact controllability of incompressible ideal magnetohydrodynamics in
  $2$D 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03712v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03712v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manuel Rissel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work examines the controllability of planar incompressible ideal
magnetohydrodynamics (MHD). Interior controls are obtained for problems posed
in doubly-connected regions; simply-connected configurations are driven by
boundary controls. Up to now, only straight channels regulated at opposing
walls have been studied. Hence, the present program adds to the literature an
exploration of interior controllability, extends the known boundary
controllability results, and contributes ideas for treating general domains. To
transship obstacles stemming from the MHD coupling and the magnetic field
topology, a divide-and-control strategy is proposed. This leads to a family of
nonlinear velocity-controlled sub-problems which are solved using J.-M. Coron's
return method. The latter is here developed based on a reference trajectory in
the domain's first cohomology space.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>41 pages, 11 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Learning under Adversarial Nonlinear Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavel Kolev, Georg Martius, Michael Muehlebach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many applications, learning systems are required to process continuous
non-stationary data streams. We study this problem in an online learning
framework and propose an algorithm that can deal with adversarial time-varying
and nonlinear constraints. As we show in our work, the algorithm called
Constraint Violation Velocity Projection (CVV-Pro) achieves $\sqrt{T}$ regret
and converges to the feasible set at a rate of $1/\sqrt{T}$, despite the fact
that the feasible set is slowly time-varying and a priori unknown to the
learner. CVV-Pro only relies on local sparse linear approximations of the
feasible set and therefore avoids optimizing over the entire set at each
iteration, which is in sharp contrast to projected gradients or Frank-Wolfe
methods. We also empirically evaluate our algorithm on two-player games, where
the players are subjected to a shared constraint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Progressive Training Through the Framework of Randomized
  Coordinate Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafał Szlendak, Elnur Gasanov, Peter Richtárik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a Randomized Progressive Training algorithm (RPT) -- a stochastic
proxy for the well-known Progressive Training method (PT) (Karras et al.,
2017). Originally designed to train GANs (Goodfellow et al., 2014), PT was
proposed as a heuristic, with no convergence analysis even for the simplest
objective functions. On the contrary, to the best of our knowledge, RPT is the
first PT-type algorithm with rigorous and sound theoretical guarantees for
general smooth objective functions. We cast our method into the established
framework of Randomized Coordinate Descent (RCD) (Nesterov, 2012; Richt\'arik &
Tak\'a\v{c}, 2014), for which (as a by-product of our investigations) we also
propose a novel, simple and general convergence analysis encapsulating
strongly-convex, convex and nonconvex objectives. We then use this framework to
establish a convergence theory for RPT. Finally, we validate the effectiveness
of our method through extensive computational experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ From coordinate subspaces over finite fields to ideal multipartite
  uniform clutters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03613v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03613v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmad Abdi, Dabeen Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Take a prime power $q$, an integer $n\geq 2$, and a coordinate subspace
$S\subseteq GF(q)^n$ over the Galois field $GF(q)$. One can associate with $S$
an $n$-partite $n$-uniform clutter $\mathcal{C}$, where every part has size $q$
and there is a bijection between the vectors in $S$ and the members of
$\mathcal{C}$.
  In this paper, we determine when the clutter $\mathcal{C}$ is ideal, a
property developed in connection to Packing and Covering problems in the areas
of Integer Programming and Combinatorial Optimization. Interestingly, the
characterization differs depending on whether $q$ is $2,4$, a higher power of
$2$, or otherwise. Each characterization uses crucially that idealness is a
minor-closed property: first the list of excluded minors is identified, and
only then is the global structure determined. A key insight is that idealness
of $\mathcal{C}$ depends solely on the underlying matroid of $S$.
  Our theorems also extend from idealness to the stronger max-flow min-cut
property. As a consequence, we prove the Replication and $\tau=2$ Conjectures
for this class of clutters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ New Relaxation Modulus Based Iterative Method for Large and Sparse
  Implicit Complementarity Problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03563v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03563v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bharat Kumar,  Deepmala, A. K. Das
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article presents a class of new relaxation modulus-based iterative
methods to process the large and sparse implicit complementarity problem (ICP).
Using two positive diagonal matrices, we formulate a fixed-point equation and
prove that it is equivalent to ICP. Also, we provide sufficient convergence
conditions for the proposed methods when the system matrix is a $P$-matrix or
an $H_+$-matrix.
  Keyword: Implicit complementarity problem, $H_{+}$-matrix, $P$-matrix, matrix
splitting, convergence
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2303.12519</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergent Bregman Plug-and-Play Image Restoration for Poisson Inverse
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Hurault, Ulugbek Kamilov, Arthur Leclaire, Nicolas Papadakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Plug-and-Play (PnP) methods are efficient iterative algorithms for solving
ill-posed image inverse problems. PnP methods are obtained by using deep
Gaussian denoisers instead of the proximal operator or the gradient-descent
step within proximal algorithms. Current PnP schemes rely on data-fidelity
terms that have either Lipschitz gradients or closed-form proximal operators,
which is not applicable to Poisson inverse problems. Based on the observation
that the Gaussian noise is not the adequate noise model in this setting, we
propose to generalize PnP using theBregman Proximal Gradient (BPG) method. BPG
replaces the Euclidean distance with a Bregman divergence that can better
capture the smoothness properties of the problem. We introduce the Bregman
Score Denoiser specifically parametrized and trained for the new Bregman
geometry and prove that it corresponds to the proximal operator of a nonconvex
potential. We propose two PnP algorithms based on the Bregman Score Denoiser
for solving Poisson inverse problems. Extending the convergence results of BPG
in the nonconvex settings, we show that the proposed methods converge,
targeting stationary points of an explicit global functional. Experimental
evaluations conducted on various Poisson inverse problems validate the
convergence results and showcase effective restoration performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Unscented Kalman Filter for Nonlinear Parameter Identification of
  Adaptive Cruise Control Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03458v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03458v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Konstantinos Ampountolas
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper develops and investigates a dual unscented Kalman filter (DUKF)
for the joint nonlinear state and parameter identification of commercial
adaptive cruise control (ACC) systems. Although the core functionality of stock
ACC systems, including their proprietary control logic and parameters, is not
publicly available, this work considers a car-following scenario with a
human-driven vehicle (leader) and an ACC engaged ego vehicle (follower) that
employs a constant time-headway policy (CTHP). The objective of the DUKF is to
determine the CTHP parameters of the ACC by using real-time observations of
space-gap and relative velocity from the vehicle's onboard sensors. Real-time
parameter identification of stock ACC systems is essential for assessing their
string stability, large-scale deployment on motorways, and impact on traffic
flow and throughput. In this regard, $L_2$ and $L_\infty$ string stability
conditions are considered. The observability rank condition for nonlinear
systems is adopted to evaluate the ability of the proposed estimation scheme to
estimate stock ACC system parameters using empirical data. The proposed filter
is evaluated using empirical data collected from the onboard sensors of two
2019 SUV vehicles, namely Hyundai Nexo and SsangYong Rexton, equipped with
stock ACC systems; and is compared with batch and recursive least-squares
optimization. The set of ACC model parameters obtained from the proposed filter
revealed that the commercially implemented ACC system of the considered vehicle
(Hyundai Nexo) is neither $L_2$ nor $L_\infty$ string stable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 papes, 3 Figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Lightweight Method for Tackling Unknown Participation Probabilities in
  Federated Averaging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03401v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03401v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shiqiang Wang, Mingyue Ji
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In federated learning (FL), clients usually have diverse participation
probabilities that are unknown a priori, which can significantly harm the
performance of FL if not handled properly. Existing works aiming at addressing
this problem are usually based on global variance reduction, which requires a
substantial amount of additional memory in a multiplicative factor equal to the
total number of clients. An important open problem is to find a lightweight
method for FL in the presence of clients with unknown participation rates. In
this paper, we address this problem by adapting the aggregation weights in
federated averaging (FedAvg) based on the participation history of each client.
We first show that, with heterogeneous participation probabilities, FedAvg with
non-optimal aggregation weights can diverge from the optimal solution of the
original FL objective, indicating the need of finding optimal aggregation
weights. However, it is difficult to compute the optimal weights when the
participation probabilities are unknown. To address this problem, we present a
new algorithm called FedAU, which improves FedAvg by adaptively weighting the
client updates based on online estimates of the optimal weights without knowing
the probabilities of client participation. We provide a theoretical convergence
analysis of FedAU using a novel methodology to connect the estimation error and
convergence. Our theoretical results reveal important and interesting insights,
while showing that FedAU converges to an optimal solution of the original
objective and has desirable properties such as linear speedup. Our experimental
results also verify the advantage of FedAU over baseline methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting oscillations in relay feedback systems, using fixed points of
  Poincaré maps, and Hopf bifurcations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03394v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03394v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maben Rabi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The relay autotuning method identifies plant parameters, from oscillations of
the plant under relay feedback. To predict the presence and nature of such
oscillations, we apply the following two approaches: (a) analysis of the
switching dynamics, while using an ideal relay, and (b) bifurcation analysis,
while using a smooth approximation of the relay. For stable plants with
positive DC gains, our analyses predict that: (i) a periodic orbit is
guaranteed, for a class of non-minimum phase plants of relative degree one,
whose step response starts with an inverse response, and (ii) for a wider class
of plants, whose root locus diagrams cross the imaginary axis at complex
conjugate values, limit cycles are merely suggested.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to the IEEE transactions on Automatic Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On seeded subgraph-to-subgraph matching: The ssSGM Algorithm and
  matchability information theory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.04016v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.04016v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingyao Meng, Mengqi Lou, Jianyu Lin, Vince Lyzinski, Donniell E. Fishkind
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The subgraph-subgraph matching problem is, given a pair of graphs and a
positive integer $K$, to find $K$ vertices in the first graph, $K$ vertices in
the second graph, and a bijection between them, so as to minimize the number of
adjacency disagreements across the bijection; it is ``seeded" if some of this
bijection is fixed. The problem is intractable, and we present the ssSGM
algorithm, which uses Frank-Wolfe methodology to efficiently find an
approximate solution. Then, in the context of a generalized correlated random
Bernoulli graph model, in which the pair of graphs naturally have a core of $K$
matched pairs of vertices, we provide and prove mild conditions for the
subgraph-subgraph matching problem solution to almost always be the correct $K$
matched pairs of vertices.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 16 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explainable AI using expressive Boolean formulas 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03976v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03976v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gili Rosenberg, J. Kyle Brubaker, Martin J. A. Schuetz, Grant Salton, Zhihuai Zhu, Elton Yechao Zhu, Serdar Kadıoğlu, Sima E. Borujeni, Helmut G. Katzgraber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose and implement an interpretable machine learning classification
model for Explainable AI (XAI) based on expressive Boolean formulas. Potential
applications include credit scoring and diagnosis of medical conditions. The
Boolean formula defines a rule with tunable complexity (or interpretability),
according to which input data are classified. Such a formula can include any
operator that can be applied to one or more Boolean variables, thus providing
higher expressivity compared to more rigid rule-based and tree-based
approaches. The classifier is trained using native local optimization
techniques, efficiently searching the space of feasible formulas. Shallow rules
can be determined by fast Integer Linear Programming (ILP) or Quadratic
Unconstrained Binary Optimization (QUBO) solvers, potentially powered by
special purpose hardware or quantum devices. We combine the expressivity and
efficiency of the native local optimizer with the fast operation of these
devices by executing non-local moves that optimize over subtrees of the full
Boolean formula. We provide extensive numerical benchmarking results featuring
several baselines on well-known public datasets. Based on the results, we find
that the native local rule classifier is generally competitive with the other
classifiers. The addition of non-local moves achieves similar results with
fewer iterations, and therefore using specialized or quantum hardware could
lead to a speedup by fast proposal of non-local moves.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>28 pages, 16 figures, 4 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Optimality conditions in control problems with random state constraints
  in probabilistic or almost-sure form 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03965v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03965v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Caroline Geiersbach, René Henrion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we discuss optimality conditions for optimization problems
subject to random state constraints, which are modeled in probabilistic or
almost sure form. While the latter can be understood as the limiting case of
the former, the derivation of optimality conditions requires substantially
different approaches. We apply them to a linear elliptic partial differential
equation (PDE) with random inputs. In the probabilistic case, we rely on the
spherical-radial decomposition of Gaussian random vectors in order to formulate
fully explicit optimality conditions involving a spherical integral. In the
almost sure case, we derive optimality conditions and compare them to a model
based on robust constraints with respect to the (compact) support of the given
distribution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equilibrium in Functional Stochastic Games with Mean-Field Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.05433v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.05433v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduardo Abi Jaber, Eyal Neuman, Moritz Voß
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider a general class of finite-player stochastic games with mean-field
interaction, in which the linear-quadratic cost functional includes linear
operators acting on controls in $L^2$. We propose a novel approach for deriving
the Nash equilibrium of the game explicitly in terms of operator resolvents, by
reducing the associated first order conditions to a system of stochastic
Fredholm equations of the second kind and deriving their closed form solution.
Furthermore, by proving stability results for the system of stochastic Fredholm
equations we derive the convergence of the equilibrium of the $N$-player game
to the corresponding mean-field equilibrium. As a by-product we also derive an
$\varepsilon$-Nash equilibrium for the mean-field game, which is valuable in
this setting as we show that the conditions for existence of an equilibrium in
the mean-field limit are less restrictive than in the finite-player game.
Finally we apply our general framework to solve various examples, such as
stochastic Volterra linear-quadratic games, models of systemic risk and
advertising with delay, and optimal liquidation games with transient price
impact.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>48 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Power of Preconditioning in Overparameterized Low-Rank Matrix
  Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Xu, Yandi Shen, Yuejie Chi, Cong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose $\textsf{ScaledGD($\lambda$)}$, a preconditioned gradient descent
method to tackle the low-rank matrix sensing problem when the true rank is
unknown, and when the matrix is possibly ill-conditioned. Using
overparametrized factor representations, $\textsf{ScaledGD($\lambda$)}$ starts
from a small random initialization, and proceeds by gradient descent with a
specific form of damped preconditioning to combat bad curvatures induced by
overparameterization and ill-conditioning. At the expense of light
computational overhead incurred by preconditioners,
$\textsf{ScaledGD($\lambda$)}$ is remarkably robust to ill-conditioning
compared to vanilla gradient descent ($\textsf{GD}$) even with
overprameterization. Specifically, we show that, under the Gaussian design,
$\textsf{ScaledGD($\lambda$)}$ converges to the true low-rank matrix at a
constant linear rate after a small number of iterations that scales only
logarithmically with respect to the condition number and the problem dimension.
This significantly improves over the convergence rate of vanilla $\textsf{GD}$
which suffers from a polynomial dependency on the condition number. Our work
provides evidence on the power of preconditioning in accelerating the
convergence without hurting generalization in overparameterized learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic
  Optimization <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lesi Chen, Jing Xu, Luo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the optimization problem of the form $\min_{x \in \mathbb{R}^d}
f(x) \triangleq \mathbb{E}_{\xi} [F(x; \xi)]$, where the component $F(x;\xi)$
is $L$-mean-squared Lipschitz but possibly nonconvex and nonsmooth. The
recently proposed gradient-free method requires at most $\mathcal{O}( L^4
d^{3/2} \epsilon^{-4} + \Delta L^3 d^{3/2} \delta^{-1} \epsilon^{-4})$
stochastic zeroth-order oracle complexity to find a
$(\delta,\epsilon)$-Goldstein stationary point of objective function, where
$\Delta = f(x_0) - \inf_{x \in \mathbb{R}^d} f(x)$ and $x_0$ is the initial
point of the algorithm. This paper proposes a more efficient algorithm using
stochastic recursive gradient estimators, which improves the complexity to
$\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1}
\epsilon^{-3})$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Queue replacement principle for corridor problems with heterogeneous
  commuters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.03357v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.03357v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Takara Sakai, Takashi Akamatsu, Koki Satsukawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the theoretical properties of a departure time choice
problem considering commuters' heterogeneity with respect to the value of
schedule delay in corridor networks. Specifically, we develop an analytical
method to solve the dynamic system optimal (DSO) and dynamic user equilibrium
(DUE) problems. To derive the DSO solution, we first demonstrate the
bottleneck-based decomposition property, i.e., the DSO problem can be
decomposed into multiple single bottleneck problems. Subsequently, we obtain
the analytical solution by applying the theory of optimal transport to each
decomposed problem and derive optimal congestion prices to achieve the DSO
state. To derive the DUE solution, we prove the queue replacement principle
(QRP) that the time-varying optimal congestion prices are equal to the queueing
delay in the DUE state at every bottleneck. This principle enables us to derive
a closed-form DUE solution based on the DSO solution. Moreover, as an
application of the QRP, we prove that the equilibrium solution under various
policies (e.g., on-ramp metering, on-ramp pricing, and its partial
implementation) can be obtained analytically. Finally, we compare these
equilibria with the DSO state.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>38 pages, 15 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Control of dynamic systems with restrictions on input and output signals 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.14123v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.14123v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Igor Furtat, Pavel Gushchin, Nguyen Ba Huy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper considers the generalization of the method proposed by I.B. Furtat,
P.A. Gushchin in "Automation and Remote Control", 2021, No. 4 for systems with
an arbitrary ratio of the number of input and output signals and with a
guarantee of their being in a given set. To solve the problem, two coordinate
changes are proposed. The first coordinate change reduces the output variable
of the system to a new variable which dimension does not exceed the control
dimension. The second coordinate change allows one to pass from a constrained
control problem to an unconstrained one. In order to illustrate the efficiency
of the method, the solution of two problems is considered. The first task is
state feedback control of linear systems, taking into account the constraints
on the control signal and phase variables. The second task is output feedback
control of linear systems with a restriction on output and control. In both
problems, checking the stability of the closed-loop system is formulated in
terms of the solvability of linear matrix inequalities. The obtained results
are accompanied by examples of modeling that illustrate the efficiency of the
proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>in Russian</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Generalized Alternating Method for Bilevel Learning under the
  Polyak-Łojasiewicz Condition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02422v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02422v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Xiao, Songtao Lu, Tianyi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilevel optimization has recently regained interest owing to its applications
in emerging machine learning fields such as hyperparameter optimization,
meta-learning, and reinforcement learning. Recent results have shown that
simple alternating (implicit) gradient-based algorithms can achieve the same
convergence rate of single-level gradient descent (GD) for bilevel problems
with a strongly convex lower-level objective. However, it remains unclear
whether this result can be generalized to bilevel problems beyond this basic
setting. In this paper, we propose a Generalized ALternating mEthod for bilevel
opTimization (GALET) with a nonconvex lower-level objective that satisfies the
Polyak-{\L}ojasiewicz (PL) condition. We first introduce a stationary metric
for the considered bilevel problems, which generalizes the existing metric. We
then establish that GALET achieves an $\epsilon$-stationary metric for the
considered problem within $\tilde{\cal O}(\epsilon^{-1})$ iterations, which
matches the iteration complexity of GD for smooth nonconvex problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Forward-Reflected-Backward and Shadow-Douglas--Rachford with partial
  inverse for Solving Monotone Inclusions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17500v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17500v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fernando Roldán
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we study two methods for solving monotone inclusions in real
Hilbert spaces involving the sum of a maximally monotone operator, a
monotone-Lipschitzian operator, a cocoercive operator, and a normal cone to a
vector subspace. Our algorithms split and exploits the intrinsic properties of
each operator involved in the inclusion. We derive our methods by combining
partial inverse techniques with the forward-reflected-backward algorithm and
with the shadow-Douglas--Rachford algorithm, respectively. Our methods inherit
the advantages of those methods, requiring only one activation of the
Lipschitzian operator, one activation of the cocoercive operator, two
projections onto the closed vector subspace, and one calculation of the
resolvent of the maximally monotone operator. Furthermore, we develop methods
for solving primal-dual inclusions involving a mixtureof sums, linear
compositions, parallel sums, Lipschitzian operators, cocoercive operators, and
normal cones. We apply our methods to constrained composite convex optimization
problems as a specific example. Finally, in order to compare our methods with
existing methods in the literature, we provide numerical experiments on
constrained total variation least-squares optimization problems. We obtain
promising numerical results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Fast Algorithm for Onboard Atmospheric Powered Descent Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.04157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.04157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yushu Chen, Guangwen Yang, Lu Wang, Qingzhong Gan, Haipeng Chen, Quanyong Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Atmospheric powered descent guidance can be solved by successive
convexification; however, its onboard application is impeded by the sharp
increase in computation caused by nonlinear aerodynamic forces. The problem has
to be converted into a sequence of convex subproblems instead of a single
convex problem when aerodynamic forces are ignored. Besides, each subproblem is
significantly more complicated, which increases computation. A fast real-time
interior point method was presented to solve the correlated convex subproblems
onboard in the work. The main contributions are as follows: Firstly, an
algorithm was proposed to accelerate the solution of linear systems that cost
most of the computation in each iterative step by exploiting the specific
problem structure. Secondly, a warm-starting scheme was introduced to refine
the initial value of a subproblem with a rough approximate solution of the
former subproblem, which lessened the iterative steps required for each
subproblem. The method proposed reduced the run time by a factor of 9 compared
with the fastest publicly available solver tested in Monte Carlo simulations to
evaluate the efficiency of solvers. Runtimes on the order of 0.6 s are achieved
on a radiation-hardened flight processor, which demonstrated the potential of
the real-time onboard application.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>The paper is accepted by IEEE Transactions on Aerospace and
  Electronic Systems, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dissipative Control of Linear Time-Delay Systems: Full State Feedback 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.10397v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.10397v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qian Feng, Feng Xiao, Xiaoyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of dissipative control of autonomous linear delay systems remains
open when an unlimited number of delays and general distributed delays (DDs)
are considered. Existing results suffer from theoretical constraints or
numerical barriers, or are unable to cope with the complexity of DD kernels. We
propose an effective method as a first step to solve this open problem for the
case of dissipative full-state feedback using the Krasovski\u{\i} functional
(KF) approach, where all DDs can contain any number of square-integrable
functions. To circumvent the difficulties posed by the infinite dimensionality
of DDs, we introduce an equivalent decomposition-approximation approach that
allows for the factorization or approximation of any DD kernel without
introducing conservatism. The method can be effectively applied to construct a
complete-type KF whose integral kernels can include any number of
differentiable and linearly independent functions, supported by novel integral
inequalities derived from the least-squares principle. The proposed
optimization-based solution is presented in several theorems, along with an
iterative algorithm that can be used together to compute controller gains
without requiring nonlinear solvers. A challenging numerical example, which
cannot be addressed by existing methods, show the effectiveness of the proposed
method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This work has been submitted to the IEEE for possible publication.
  Copyright may be transferred without notice, after which this version may no
  longer be accessible</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Generalization of the Riccati Recursion for Equality-Constrained
  Linear Quadratic Optimal Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lander Vanroye, Joris De Schutter, Wilm Decré
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces a generalization of the well-known Riccati recursion
for solving the discrete-time equality-constrained linear quadratic optimal
control problem. The recursion can be used to compute the solutions as well as
optimal feedback control policies. Unlike other tailored approaches for this
problem class, the proposed method does not require restrictive regularity
conditions on the problem. This allows its use in nonlinear optimal control
problem solvers that use exact Lagrangian Hessian information. We demonstrate
that our approach can be implemented in a highly efficient algorithm that
scales linearly with the horizon length. Numerical tests show a significant
speed-up of up to two orders of magnitude with respect to state-of-the-art
general-purpose sparse linear solvers. Based on the proposed approach, faster
nonlinear optimal control problem solvers can be developed that are suitable
for more complex applications or for implementations on low-cost or low-power
computational platforms. The implementation of the proposed algorithm is made
available as open-source software.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational formulations of ODE-Net as a mean-field optimal control
  problem and existence results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05924v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05924v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noboru Isobe, Mizuho Okumura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a mathematical analysis of ODE-Net, a continuum model of
deep neural networks (DNNs). In recent years, Machine Learning researchers have
introduced ideas of replacing the deep structure of DNNs with ODEs as a
continuum limit. These studies regard the "learning" of ODE-Net as the
minimization of a "loss" constrained by a parametric ODE. Although the
existence of a minimizer for this minimization problem needs to be assumed,
only a few studies have investigated its existence analytically in detail. In
the present paper, the existence of a minimizer is discussed based on a
formulation of ODE-Net as a measure-theoretic mean-field optimal control
problem. The existence result is proved when a neural network, which describes
a vector field of ODE-Net, is linear with respect to learnable parameters. The
proof employs the measure-theoretic formulation combined with the direct method
of Calculus of Variations. Secondly, an idealized minimization problem is
proposed to remove the above linearity assumption. Such a problem is inspired
by a kinetic regularization associated with the Benamou--Brenier formula and
universal approximation theorems for neural networks. The proofs of these
existence results use variational methods, differential equations, and
mean-field optimal control theory. They will stand for a new analytic way to
investigate the learning process of deep neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Denise: Deep Robust Principal Component Analysis for Positive
  Semidefinite Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2004.13612v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2004.13612v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Calypso Herrera, Florian Krach, Anastasis Kratsios, Pierre Ruyssen, Josef Teichmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The robust PCA of covariance matrices plays an essential role when isolating
key explanatory features. The currently available methods for performing such a
low-rank plus sparse decomposition are matrix specific, meaning, those
algorithms must re-run for every new matrix. Since these algorithms are
computationally expensive, it is preferable to learn and store a function that
nearly instantaneously performs this decomposition when evaluated. Therefore,
we introduce Denise, a deep learning-based algorithm for robust PCA of
covariance matrices, or more generally, of symmetric positive semidefinite
matrices, which learns precisely such a function. Theoretical guarantees for
Denise are provided. These include a novel universal approximation theorem
adapted to our geometric deep learning problem and convergence to an optimal
solution to the learning problem. Our experiments show that Denise matches
state-of-the-art performance in terms of decomposition quality, while being
approximately $2000\times$ faster than the state-of-the-art, principal
component pursuit (PCP), and $200 \times$ faster than the current
speed-optimized method, fast PCP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Peeling for L0-Regularized Least-Squares with supplementary
  material 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14471v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14471v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Théo Guyard, Gilles Monnoyer, Clément Elvira, Cédric Herzet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new methodology dubbed ``safe peeling'' to accelerate the
resolution of L0-regularized least-squares problems via a Branch-and-Bound
(BnB) algorithm. Our procedure enables to tighten the convex relaxation
considered at each node of the BnB decision tree and therefore potentially
allows for more aggressive pruning. Numerical simulations show that our
proposed methodology leads to significant gains in terms of number of nodes
explored and overall solving time.s show that our proposed methodology leads to
significant gains in terms of number of nodes explored and overall solving
time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sizing Grid-Connected Wind Power Generation and Energy Storage
  Considering Wake Effect and Endogenous Uncertainty: A Distributionally Robust
  Method 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.14665v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.14665v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rui Xie, Wei Wei, Yue Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Wind power, as a green energy resource, is growing rapidly worldwide, along
with energy storage systems (ESSs) to mitigate its volatility. Sizing of wind
power generation and ESSs has become an important problem to be addressed. Wake
effect in a wind farm can cause wind speed deficits and a drop in downstream
wind turbine power generation, which however was rarely considered in the
sizing problem in power systems. In this paper, a bi-objective distributionally
robust optimization (DRO) model is proposed to determine the capacities of wind
power generation and ESSs considering the wake effect. An ambiguity set based
on Wasserstein metric is established to characterize the wind power and demand
uncertainties. In particular, wind power uncertainty is affected by the wind
power generation capacity which is determined in the first stage. Thus, the
proposed model is a DRO with endogenous uncertainty (or decision-dependent
uncertainty). To solve the proposed model, a stochastic programming
approximation method based on minimum Lipschitz constants is developed to turn
the DRO model into a linear program. Then, an iterative algorithm is built,
embedded with methods for evaluating the minimum Lipschitz constants. Case
studies demonstrate the necessity of considering wake effect and the
effectiveness of the proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>27 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Towards a Multimodal Charging Network: Joint Planning of Charging
  Stations and Battery Swapping Stations for Electrified Ride-Hailing Fleets 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.12677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.12677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhijie Lai, Sen Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper considers a multimodal charging network in which charging stations
and battery swapping stations are jointly built to support the electrified
ride-hailing fleet in a synergistic manner. Our central thesis is predicated on
the observation that charging stations are cost-effective, making them ideal
for scaling up electric vehicles in ride-hailing fleets in the beginning, while
battery swapping stations offer quick turnaround and can be deployed in tandem
with charging stations to improve fleet utilization and reduce operational
costs for the ride-hailing platform. To fulfill this vision, we consider a
ride-hailing platform that expands the multimodal charging network with a
multi-stage investment budget and operates a ride-hailing fleet to maximize its
profit. A multi-stage network expansion model is proposed to characterize the
coupled planning and operational decisions, which captures demand elasticity,
passenger waiting time, charging and swapping waiting times, as well as their
dependence on fleet status and charging infrastructure. The overall problem is
formulated as a nonconvex program. Instead of pursuing the globally optimal
solution, we establish a theoretical upper bound through relaxation,
reformulation, and decomposition so that the global optimality of solutions to
the nonconvex problem is verifiable. In the case study for Manhattan, we find
that ...
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Time-Consistent Asset Allocation for Risk Measures in a Lévy Market 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.09471v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.09471v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix Fießinger, Mitja Stadje
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Focusing on gains instead of terminal wealth, we consider an asset allocation
problem to maximize time-consistently a mean-risk reward function with a
general risk measure which is i) law-invariant, ii) cash- or shift-invariant,
and iii) positively homogeneous, and possibly plugged into a general function.
We model the market via a generalized version of the multi-dimensional
Black-Scholes model using $\alpha$-stable L\'evy processes and give
supplementary results for the classical Black-Scholes model. The optimal
solution to this problem is a Nash subgame equilibrium given by the solution of
an extended Hamilton-Jacobi-Bellman equation. Moreover, we show that the
optimal solution is deterministic and unique under appropriate assumptions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Negative curvature obstructs acceleration for geodesically convex
  optimization, even with exact first-order oracles <span class="chip">COLT 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2111.13263v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2111.13263v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Criscitiello, Nicolas Boumal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hamilton and Moitra (2021) showed that, in certain regimes, it is not
possible to accelerate Riemannian gradient descent in the hyperbolic plane if
we restrict ourselves to algorithms which make queries in a (large) bounded
domain and which receive gradients and function values corrupted by a (small)
amount of noise. We show that acceleration remains unachievable for any
deterministic algorithm which receives exact gradient and function-value
information (unbounded queries, no noise). Our results hold for the classes of
strongly and nonstrongly geodesically convex functions, and for a large class
of Hadamard manifolds including hyperbolic spaces and the symmetric space
$\mathrm{SL}(n) / \mathrm{SO}(n)$ of positive definite $n \times n$ matrices of
determinant one. This cements a surprising gap between the complexity of convex
optimization and geodesically convex optimization: for hyperbolic spaces,
Riemannian gradient descent is optimal on the class of smooth and and strongly
geodesically convex functions, in the regime where the condition number scales
with the radius of the optimization domain. The key idea for proving the lower
bound consists of perturbing the hard functions of Hamilton and Moitra (2021)
with sums of bump functions chosen by a resisting oracle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated and shortened to reflect the version published at COLT 2022.
  The results on (a) the nonstrongly g-convex case and (b) reduction to
  Euclidean convexity can now be found in the follow-up work "Curvature and
  Complexity: Better lower bounds for geodesically convex optimization"
  published at COLT 2023 (arXiv:2306.02959)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faster Lagrangian-Based Methods in Convex Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2010.14314v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2010.14314v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shoham Sabach, Marc Teboulle
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we aim at unifying, simplifying and improving the convergence
rate analysis of Lagrangian-based methods for convex optimization problems. We
first introduce the notion of nice primal algorithmic map, which plays a
central role in the unification and in the simplification of the analysis of
most Lagrangian-based methods. Equipped with a nice primal algorithmic map, we
then introduce a versatile generic scheme, which allows for the design and
analysis of Faster LAGrangian (FLAG) methods with new provably sublinear rate
of convergence expressed in terms of function values and feasibility violation
of the original (non-ergodic) generated sequence. To demonstrate the power and
versatility of our approach and results, we show that most well-known iconic
Lagrangian-based schemes admit a nice primal algorithmic map, and hence share
the new faster rate of convergence results within their corresponding FLAG.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Minor corrections</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ L-SVRG and L-Katyusha with Adaptive Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.13387v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.13387v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boxin Zhao, Boxiang Lyu, Mladen Kolar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic gradient-based optimization methods, such as L-SVRG and its
accelerated variant L-Katyusha (Kovalev et al., 2020), are widely used to train
machine learning models.The theoretical and empirical performance of L-SVRG and
L-Katyusha can be improved by sampling observations from a non-uniform
distribution (Qian et al., 2021). However,designing a desired sampling
distribution requires prior knowledge of smoothness constants, which can be
computationally intractable to obtain in practice when the dimension of the
model parameter is high. To address this issue, we propose an adaptive sampling
strategy for L-SVRG and L-Katyusha that can learn the sampling distribution
with little computational overhead, while allowing it to change with iterates,
and at the same time does not require any prior knowledge of the problem
parameters. We prove convergence guarantees for L-SVRG and L-Katyusha for
convex objectives when the sampling distribution changes with iterates. Our
results show that even without prior information, the proposed adaptive
sampling strategy matches, and in some cases even surpasses, the performance of
the sampling scheme in Qian et al. (2021). Extensive simulations support our
theory and the practical utility of the proposed sampling scheme on real data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (03/2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provable Benefit of Mixup for Finding Optimal Decision Boundaries <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00267v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00267v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junsoo Oh, Chulhee Yun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate how pair-wise data augmentation techniques like Mixup affect
the sample complexity of finding optimal decision boundaries in a binary linear
classification problem. For a family of data distributions with a separability
constant $\kappa$, we analyze how well the optimal classifier in terms of
training loss aligns with the optimal one in test accuracy (i.e., Bayes optimal
classifier). For vanilla training without augmentation, we uncover an
interesting phenomenon named the curse of separability. As we increase $\kappa$
to make the data distribution more separable, the sample complexity of vanilla
training increases exponentially in $\kappa$; perhaps surprisingly, the task of
finding optimal decision boundaries becomes harder for more separable
distributions. For Mixup training, we show that Mixup mitigates this problem by
significantly reducing the sample complexity. To this end, we develop new
concentration results applicable to $n^2$ pair-wise augmented data points
constructed from $n$ independent data, by carefully dealing with dependencies
between overlapping pairs. Lastly, we study other masking-based Mixup-style
techniques and show that they can distort the training loss and make its
minimizer converge to a suboptimal classifier in terms of test accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023 camera-ready version; 48 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Verification Theorems for Stochastic Control Problems of
  Reflected FBSDEs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.03470v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.03470v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lu Liu, Xinlei Hu, Qingmeng Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, the stochastic verification theorems for stochastic control
problems of reflected forward-backward stochastic differential equations are
studied. We carry out the work within the frameworks of classical and viscosity
solutions. The sufficient conditions of verifying the controls to be optimal
are given. We also construct the feedback optimal control laws from the
classical and viscosity solutions of the associated Hamilton-Jacobi-Bellman
equations with obstacles. Finally, we apply the theoretical results in two
concrete examples. One is for the case of the classical solution, and the other
is for the case of the viscosity solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Complexity of a Class of First-Order Objective-Function-Free
  Optimization Algorithms 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.01647v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.01647v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        S. Gratton, S. Jerad, Ph. L. Toint
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A parametric class of trust-region algorithms for unconstrained nonconvex
optimization is considered where the value of the objective function is never
computed. The class contains a deterministic version of the first-order Adagrad
method typically used for minimization of noisy function, but also allows the
use of (possibly approximate) second-order information when available. The rate
of convergence of methods in the class is analyzed and is shown to be identical
to that known for first-order optimization methods using both function and
gradients values, recovering existing results for purely-first order variants
and improving the explicit dependence on problem dimension. This rate is shown
to be essentially sharp. A new class of methods is also presented, for which a
slightly worse and essentially sharp complexity result holds. Limited numerical
experiments show that the new methods' performance may be comparable to that of
standard steepest descent, despite using significantly less information, and
that this performance is relatively insensitive to noise.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Compressing Branch-and-Bound Trees 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.02727v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.02727v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gonzalo Muñoz, Joseph Paat, Álinson S. Xavier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A branch-and-bound (BB) tree certifies a dual bound on the value of an
integer program. In this work, we introduce the tree compression problem (TCP):
Given a BB tree T that certifies a dual bound, can we obtain a smaller tree
with the same (or stronger) bound by either (1) applying a different
disjunction at some node in T or (2) removing leaves from T? We believe such
post-hoc analysis of BB trees may assist in identifying helpful general
disjunctions in BB algorithms. We initiate our study by considering
computational complexity and limitations of TCP. We then conduct experiments to
evaluate the compressibility of realistic branch-and-bound trees generated by
commonly-used branching strategies, using both an exact and a heuristic
compression algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>A short version of this article was accepted for publication at IPCO
  2023. This extended version contains more detailed discussions and proofs,
  and new computational contributions and experiments</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ ChatDB: Augmenting LLMs with Databases as Their Symbolic Memory 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03901v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03901v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxu Hu, Jie Fu, Chenzhuang Du, Simian Luo, Junbo Zhao, Hang Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) with memory are computationally universal.
However, mainstream LLMs are not taking full advantage of memory, and the
designs are heavily influenced by biological brains. Due to their approximate
nature and proneness to the accumulation of errors, conventional neural memory
mechanisms cannot support LLMs to simulate complex reasoning. In this paper, we
seek inspiration from modern computer architectures to augment LLMs with
symbolic memory for complex multi-hop reasoning. Such a symbolic memory
framework is instantiated as an LLM and a set of SQL databases, where the LLM
generates SQL instructions to manipulate the SQL databases. We validate the
effectiveness of the proposed memory framework on a synthetic dataset requiring
complex reasoning. The project website is available at
https://chatdatabase.github.io/ .
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Model Spider: Learning to Rank <span class="highlight-title">Pre-Train</span>ed Models Efficiently 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03900v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03900v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi-Kai Zhang, Ting-Ji Huang, Yao-Xiang Ding, De-Chuan Zhan, Han-Jia Ye
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Figuring out which Pre-Trained Model (PTM) from a model zoo fits the target
task is essential to take advantage of plentiful model resources. With the
availability of numerous heterogeneous PTMs from diverse fields, efficiently
selecting the most suitable PTM is challenging due to the time-consuming costs
of carrying out forward or backward passes over all PTMs. In this paper, we
propose Model Spider, which tokenizes both PTMs and tasks by summarizing their
characteristics into vectors to enable efficient PTM selection. By leveraging
the approximated performance of PTMs on a separate set of training tasks, Model
Spider learns to construct tokens and measure the fitness score between a
model-task pair via their tokens. The ability to rank relevant PTMs higher than
others generalizes to new tasks. With the top-ranked PTM candidates, we further
learn to enrich task tokens with their PTM-specific semantics to re-rank the
PTMs for better selection. Model Spider balances efficiency and selection
ability, making PTM selection like a spider preying on a web. Model Spider
demonstrates promising performance in various configurations of model zoos.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fast Context Adaptation in Cost-Aware Continual Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seyyidahmed Lahmer, Federico Mason, Federico Chiariotti, Andrea Zanella
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the past few years, DRL has become a valuable solution to automatically
learn efficient resource management strategies in complex networks with
time-varying statistics. However, the increased complexity of 5G and Beyond
networks requires correspondingly more complex learning agents and the learning
process itself might end up competing with users for communication and
computational resources. This creates friction: on the one hand, the learning
process needs resources to quickly convergence to an effective strategy; on the
other hand, the learning process needs to be efficient, i.e., take as few
resources as possible from the user's data plane, so as not to throttle users'
QoS. In this paper, we investigate this trade-off and propose a dynamic
strategy to balance the resources assigned to the data plane and those reserved
for learning. With the proposed approach, a learning agent can quickly converge
to an efficient resource allocation strategy and adapt to changes in the
environment as for the CL paradigm, while minimizing the impact on the users'
QoS. Simulation results show that the proposed method outperforms static
allocation methods with minimal learning overhead, almost reaching the
performance of an ideal out-of-band CL solution.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2211.16915</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deductive Verification of Chain-of-Thought Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03872v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03872v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhan Ling, Yunhao Fang, Xuanlin Li, Zhiao Huang, Mingu Lee, Roland Memisevic, Hao Su
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) significantly benefit from Chain-of-Thought
(CoT) prompting in performing various reasoning tasks. While CoT allows models
to produce more comprehensive reasoning processes, its emphasis on intermediate
reasoning steps can inadvertently introduce hallucinations and accumulated
errors, thereby limiting models' ability to solve complex reasoning tasks.
Inspired by how humans engage in careful and meticulous deductive logical
reasoning processes to solve tasks, we seek to enable language models to
perform explicit and rigorous deductive reasoning, and also ensure the
trustworthiness of their reasoning process through self-verification. However,
directly verifying the validity of an entire deductive reasoning process is
challenging, even with advanced models like ChatGPT. In light of this, we
propose to decompose a reasoning verification process into a series of
step-by-step subprocesses, each only receiving their necessary context and
premises. To facilitate this procedure, we propose Natural Program, a natural
language-based deductive reasoning format. Our approach enables models to
generate precise reasoning steps where subsequent steps are more rigorously
grounded on prior steps. It also empowers language models to carry out
reasoning self-verification in a step-by-step manner. By integrating this
verification process into each deductive reasoning stage, we significantly
enhance the rigor and trustfulness of generated reasoning steps. Along this
process, we also improve the answer correctness on complex reasoning tasks.
Code will be released at https://github.com/lz1oceani/verify_cot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spherical Fourier Neural Operators: Learning Stable Dynamics on the
  Sphere 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03838v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03838v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boris Bonev, Thorsten Kurth, Christian Hundt, Jaideep Pathak, Maximilian Baust, Karthik Kashinath, Anima Anandkumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fourier Neural Operators (FNOs) have proven to be an efficient and effective
method for resolution-independent operator learning in a broad variety of
application areas across scientific machine learning. A key reason for their
success is their ability to accurately model long-range dependencies in
spatio-temporal data by learning global convolutions in a computationally
efficient manner. To this end, FNOs rely on the discrete Fourier transform
(DFT), however, DFTs cause visual and spectral artifacts as well as pronounced
dissipation when learning operators in spherical coordinates since they
incorrectly assume a flat geometry. To overcome this limitation, we generalize
FNOs on the sphere, introducing Spherical FNOs (SFNOs) for learning operators
on spherical geometries. We apply SFNOs to forecasting atmospheric dynamics,
and demonstrate stable auto\-regressive rollouts for a year of simulated time
(1,460 steps), while retaining physically plausible dynamics. The SFNO has
important implications for machine learning-based simulation of climate
dynamics that could eventually help accelerate our response to climate change.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Atrial Septal Defect Detection in Children Based on Ultrasound Video
  Using Multiple Instances Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03835v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03835v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiman Liu, Qiming Huang, Xiaoxiang Han, Tongtong Liang, Zhifang Zhang, Lijun Chen, Jinfeng Wang, Angelos Stefanidis, Jionglong Su, Jiangang Chen, Qingli Li, Yuqi Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Purpose: Congenital heart defect (CHD) is the most common birth defect.
Thoracic echocardiography (TTE) can provide sufficient cardiac structure
information, evaluate hemodynamics and cardiac function, and is an effective
method for atrial septal defect (ASD) examination. This paper aims to study a
deep learning method based on cardiac ultrasound video to assist in ASD
diagnosis. Materials and methods: We select two standard views of the atrial
septum (subAS) and low parasternal four-compartment view (LPS4C) as the two
views to identify ASD. We enlist data from 300 children patients as part of a
double-blind experiment for five-fold cross-validation to verify the
performance of our model. In addition, data from 30 children patients (15
positives and 15 negatives) are collected for clinician testing and compared to
our model test results (these 30 samples do not participate in model training).
We propose an echocardiography video-based atrial septal defect diagnosis
system. In our model, we present a block random selection, maximal agreement
decision and frame sampling strategy for training and testing respectively,
resNet18 and r3D networks are used to extract the frame features and aggregate
them to build a rich video-level representation. Results: We validate our model
using our private dataset by five-cross validation. For ASD detection, we
achieve 89.33 AUC, 84.95 accuracy, 85.70 sensitivity, 81.51 specificity and
81.99 F1 score. Conclusion: The proposed model is multiple instances
learning-based deep learning model for video atrial septal defect detection
which effectively improves ASD detection accuracy when compared to the
performances of previous networks and clinical doctors.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MTS2Graph: Interpretable Multivariate Time Series Classification with
  Temporal Evolving Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raneen Younis, Abdul Hakmeh, Zahra Ahmadi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conventional time series classification approaches based on bags of patterns
or shapelets face significant challenges in dealing with a vast amount of
feature candidates from high-dimensional multivariate data. In contrast, deep
neural networks can learn low-dimensional features efficiently, and in
particular, Convolutional Neural Networks (CNN) have shown promising results in
classifying Multivariate Time Series (MTS) data. A key factor in the success of
deep neural networks is this astonishing expressive power. However, this power
comes at the cost of complex, black-boxed models, conflicting with the goals of
building reliable and human-understandable models. An essential criterion in
understanding such predictive deep models involves quantifying the contribution
of time-varying input variables to the classification. Hence, in this work, we
introduce a new framework for interpreting multivariate time series data by
extracting and clustering the input representative patterns that highly
activate CNN neurons. This way, we identify each signal's role and
dependencies, considering all possible combinations of signals in the MTS
input. Then, we construct a graph that captures the temporal relationship
between the extracted patterns for each layer. An effective graph merging
strategy finds the connection of each node to the previous layer's nodes.
Finally, a graph embedding algorithm generates new representations of the
created interpretable time-series features. To evaluate the performance of our
proposed framework, we run extensive experiments on eight datasets of the
UCR/UEA archive, along with HAR and PAM datasets. The experiments indicate the
benefit of our time-aware graph-based representation in MTS classification
while enriching them with more interpretability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Patient Dropout Prediction in Virtual Health: A Multimodal Dynamic
  Knowledge Graph and Text Mining Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuang Geng, Wenli Zhang, Jiaheng Xie, Gemin Liang, Ben Niu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Virtual health has been acclaimed as a transformative force in healthcare
delivery. Yet, its dropout issue is critical that leads to poor health
outcomes, increased health, societal, and economic costs. Timely prediction of
patient dropout enables stakeholders to take proactive steps to address
patients' concerns, potentially improving retention rates. In virtual health,
the information asymmetries inherent in its delivery format, between different
stakeholders, and across different healthcare delivery systems hinder the
performance of existing predictive methods. To resolve those information
asymmetries, we propose a Multimodal Dynamic Knowledge-driven Dropout
Prediction (MDKDP) framework that learns implicit and explicit knowledge from
doctor-patient dialogues and the dynamic and complex networks of various
stakeholders in both online and offline healthcare delivery systems. We
evaluate MDKDP by partnering with one of the largest virtual health platforms
in China. MDKDP improves the F1-score by 3.26 percentage points relative to the
best benchmark. Comprehensive robustness analyses show that integrating
stakeholder attributes, knowledge dynamics, and compact bilinear pooling
significantly improves the performance. Our work provides significant
implications for healthcare IT by revealing the value of mining relations and
knowledge across different service modalities. Practically, MDKDP offers a
novel design artifact for virtual health platforms in patient dropout
management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequential Principal-Agent Problems with Communication: Efficient
  Computation and Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Gan, Rupak Majumdar, Debmalya Mandal, Goran Radanovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a sequential decision making problem between a principal and an
agent with incomplete information on both sides. In this model, the principal
and the agent interact in a stochastic environment, and each is privy to
observations about the state not available to the other. The principal has the
power of commitment, both to elicit information from the agent and to provide
signals about her own information. The principal and the agent communicate
their signals to each other, and select their actions independently based on
this communication. Each player receives a payoff based on the state and their
joint actions, and the environment moves to a new state. The interaction
continues over a finite time horizon, and both players act to optimize their
own total payoffs over the horizon. Our model encompasses as special cases
stochastic games of incomplete information and POMDPs, as well as sequential
Bayesian persuasion and mechanism design problems. We study both computation of
optimal policies and learning in our setting. While the general problems are
computationally intractable, we study algorithmic solutions under a conditional
independence assumption on the underlying state-observation distributions. We
present an polynomial-time algorithm to compute the principal's optimal policy
up to an additive approximation. Additionally, we show an efficient learning
algorithm in the case where the transition probabilities are not known
beforehand. The algorithm guarantees sublinear regret for both players.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         <span class="highlight-title">★</span> GEO-Bench: Toward Foundation Models for Earth Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandre Lacoste, Nils Lehmann, Pau Rodriguez, Evan David Sherwin, Hannah Kerner, Björn Lütjens, Jeremy Andrew Irvin, David Dao, Hamed Alemohammad, Alexandre Drouin, Mehmet Gunturkun, Gabriel Huang, David Vazquez, Dava Newman, <span class="highlight-author">Yoshua Bengio</span>, Stefano Ermon, Xiao Xiang Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent progress in self-supervision has shown that pre-training large neural
networks on vast amounts of unsupervised data can lead to substantial increases
in generalization to downstream tasks. Such models, recently coined foundation
models, have been transformational to the field of natural language processing.
Variants have also been proposed for image data, but their applicability to
remote sensing tasks is limited. To stimulate the development of foundation
models for Earth monitoring, we propose a benchmark comprised of six
classification and six segmentation tasks, which were carefully curated and
adapted to be both relevant to the field and well-suited for model evaluation.
We accompany this benchmark with a robust methodology for evaluating models and
reporting aggregated results to enable a reliable assessment of progress.
Finally, we report results for 20 baselines to gain information about the
performance of existing models. We believe that this benchmark will be a driver
of progress across a variety of Earth monitoring tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Inductive Bias for Emergent Communication in a Continuous Setting <span class="chip">NIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03830v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03830v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Isak Fjellvang Villanger, Troels Arnfred Bojesen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study emergent communication in a multi-agent reinforcement learning
setting, where the agents solve cooperative tasks and have access to a
communication channel. The communication channel may consist of either discrete
symbols or continuous variables. We introduce an inductive bias to aid with the
emergence of good communication protocols for continuous messages, and we look
at the effect this type of inductive bias has for continuous and discrete
messages in itself or when used in combination with reinforcement learning. We
demonstrate that this type of inductive bias has a beneficial effect on the
communication protocols learnt in two toy environments, Negotiation and
Sequence Guess.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>NIPS 2023 Preprint. 12 pages, 5 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quick-Tune: Quickly Learning Which <span class="highlight-title">Pretrain</span>ed Model to Finetune and How 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03828v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03828v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sebastian Pineda Arango, Fabio Ferreira, Arlind Kadra, Frank Hutter Josif Grabocka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the ever-increasing number of pretrained models, machine learning
practitioners are continuously faced with which pretrained model to use, and
how to finetune it for a new dataset. In this paper, we propose a methodology
that jointly searches for the optimal pretrained model and the hyperparameters
for finetuning it. Our method transfers knowledge about the performance of many
pretrained models with multiple hyperparameter configurations on a series of
datasets. To this aim, we evaluated over 20k hyperparameter configurations for
finetuning 24 pretrained image classification models on 87 datasets to generate
a large-scale meta-dataset. We meta-learn a multi-fidelity performance
predictor on the learning curves of this meta-dataset and use it for fast
hyperparameter optimization on new datasets. We empirically demonstrate that
our resulting approach can quickly select an accurate pretrained model for a
new dataset together with its optimal hyperparameters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Generalization of Federated Learning via Stability:
  Heterogeneity Matters <span class="chip">NeurIPS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Sun, Xiaochun Niu, Ermin Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generalization performance is a key metric in evaluating machine learning
models when applied to real-world applications. Good generalization indicates
the model can predict unseen data correctly when trained under a limited number
of data. Federated learning (FL), which has emerged as a popular distributed
learning framework, allows multiple devices or clients to train a shared model
without violating privacy requirements. While the existing literature has
studied extensively the generalization performances of centralized machine
learning algorithms, similar analysis in the federated settings is either
absent or with very restrictive assumptions on the loss functions. In this
paper, we aim to analyze the generalization performances of federated learning
by means of algorithmic stability, which measures the change of the output
model of an algorithm when perturbing one data point. Three widely-used
algorithms are studied, including FedAvg, SCAFFOLD, and FedProx, under convex
and non-convex loss functions. Our analysis shows that the generalization
performances of models trained by these three algorithms are closely related to
the heterogeneity of clients' datasets as well as the convergence behaviors of
the algorithms. Particularly, in the i.i.d. setting, our results recover the
classical results of stochastic gradient descent (SGD).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to NeurIPS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LEACE: Perfect linear concept erasure in closed form 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nora Belrose, David Schneider-Joseph, Shauli Ravfogel, Ryan Cotterell, Edward Raff, Stella Biderman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept erasure aims to remove specified features from a representation. It
can be used to improve fairness (e.g. preventing a classifier from using gender
or race) and interpretability (e.g. removing a concept to observe changes in
model behavior). In this paper, we introduce LEAst-squares Concept Erasure
(LEACE), a closed-form method which provably prevents all linear classifiers
from detecting a concept while inflicting the least possible damage to the
representation. We apply LEACE to large language models with a novel procedure
called "concept scrubbing," which erases target concept information from every
layer in the network. We demonstrate the usefulness of our method on two tasks:
measuring the reliance of language models on part-of-speech information, and
reducing gender bias in BERT embeddings. Code is available at
https://github.com/EleutherAI/concept-erasure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computation with Sequences in the Brain 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03812v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03812v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Max Dabagia, Christos H. Papadimitriou, Santosh S. Vempala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Even as machine learning exceeds human-level performance on many
applications, the generality, robustness, and rapidity of the brain's learning
capabilities remain unmatched. How cognition arises from neural activity is a
central open question in neuroscience, inextricable from the study of
intelligence itself. A simple formal model of neural activity was proposed in
Papadimitriou [2020] and has been subsequently shown, through both mathematical
proofs and simulations, to be capable of implementing certain simple cognitive
operations via the creation and manipulation of assemblies of neurons. However,
many intelligent behaviors rely on the ability to recognize, store, and
manipulate temporal sequences of stimuli (planning, language, navigation, to
list a few). Here we show that, in the same model, time can be captured
naturally as precedence through synaptic weights and plasticity, and, as a
result, a range of computations on sequences of assemblies can be carried out.
In particular, repeated presentation of a sequence of stimuli leads to the
memorization of the sequence through corresponding neural assemblies: upon
future presentation of any stimulus in the sequence, the corresponding assembly
and its subsequent ones will be activated, one after the other, until the end
of the sequence. Finally, we show that any finite state machine can be learned
in a similar way, through the presentation of appropriate patterns of
sequences. Through an extension of this mechanism, the model can be shown to be
capable of universal computation. We support our analysis with a number of
experiments to probe the limits of learning in this model in key ways. Taken
together, these results provide a concrete hypothesis for the basis of the
brain's remarkable abilities to compute and learn, with sequences playing a
vital role.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 12 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The Emergence of Essential Sparsity in Large <span class="highlight-title">Pre-train</span>ed Models: The
  Weights that Matter 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03805v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03805v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ajay Jaiswal, Shiwei Liu, Tianlong Chen, Zhangyang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pre-trained transformers are show-stealer in modern-day deep learning,
and it becomes crucial to comprehend the parsimonious patterns that exist
within them as they grow in scale. With exploding parameter counts, Lottery
Ticket Hypothesis (LTH) and its variants, have lost their pragmatism in
sparsifying them due to high computation and memory bottleneck of the
repetitive train-prune-retrain routine of iterative magnitude pruning (IMP)
which worsens with increasing model size. In this paper, we comprehensively
study induced sparse patterns across multiple large pre-trained vision and
language transformers. We propose the existence of -- essential sparsity
defined with a sharp dropping point beyond which the performance declines much
faster w.r.t the rise of sparsity level, when we directly remove weights with
the smallest magnitudes in one-shot. In the sparsity-performance curve We also
present an intriguing emerging phenomenon of abrupt sparsification during the
pre-training of BERT, i.e., BERT suddenly becomes heavily sparse in
pre-training after certain iterations. Moreover, our observations also indicate
a counter-intuitive finding that BERT trained with a larger amount of
pre-training data tends to have a better ability to condense knowledge in
comparatively relatively fewer parameters. Lastly, we investigate the effect of
the pre-training loss on essential sparsity and discover that self-supervised
learning (SSL) objectives trigger stronger emergent sparsification properties
than supervised learning (SL). Our codes are available at
\url{https://github.com/VITA-Group/essential\_sparsity}.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stable Vectorization of Multiparameter Persistent Homology using Signed
  Barcodes as Measures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03801v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03801v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Loiseaux, Luis Scoccola, Mathieu Carrière, Magnus Bakke Botnan, Steve Oudot
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Persistent homology (PH) provides topological descriptors for geometric data,
such as weighted graphs, which are interpretable, stable to perturbations, and
invariant under, e.g., relabeling. Most applications of PH focus on the
one-parameter case -- where the descriptors summarize the changes in topology
of data as it is filtered by a single quantity of interest -- and there is now
a wide array of methods enabling the use of one-parameter PH descriptors in
data science, which rely on the stable vectorization of these descriptors as
elements of a Hilbert space. Although the multiparameter PH (MPH) of data that
is filtered by several quantities of interest encodes much richer information
than its one-parameter counterpart, the scarceness of stability results for MPH
descriptors has so far limited the available options for the stable
vectorization of MPH. In this paper, we aim to bring together the best of both
worlds by showing how the interpretation of signed barcodes -- a recent family
of MPH descriptors -- as signed measures leads to natural extensions of
vectorization strategies from one parameter to multiple parameters. The
resulting feature vectors are easy to define and to compute, and provably
stable. While, as a proof of concept, we focus on simple choices of signed
barcodes and vectorizations, we already see notable performance improvements
when comparing our feature vectors to state-of-the-art topology-based methods
on various types of data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 3 figures, 8 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI-Supported Assessment of Load Safety 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03795v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03795v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julius Schöning, Niklas Kruse
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Load safety assessment and compliance is an essential step in the corporate
process of every logistics service provider. In 2020, a total of 11,371 police
checks of trucks were carried out, during which 9.6% (1091) violations against
the load safety regulations were detected. For a logistic service provider,
every load safety violation results in height fines and damage to reputation.
An assessment of load safety supported by artificial intelligence (AI) will
reduce the risk of accidents by unsecured loads and fines during safety
assessments. This work shows how photos of the load, taken by the truck driver
or the loadmaster after the loading process, can be used to assess load safety.
By a trained two-stage artificial neural network (ANN), these photos are
classified into three different classes I) cargo loaded safely, II) cargo
loaded unsafely, and III) unusable image. By applying several architectures of
convolutional neural networks (CNN), it can be shown that it is possible to
distinguish between unusable and usable images for cargo safety assessment.
This distinction is quite crucial since the truck driver and the loadmaster
sometimes provide photos without the essential image features like the case
structure of the truck and the whole cargo. A human operator or another ANN
will then assess the load safety within the second stage.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ FAMO: Fast Adaptive Multitask Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03792v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03792v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bo Liu, Yihao Feng, Peter Stone, Qiang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  One of the grand enduring goals of AI is to create generalist agents that can
learn multiple different tasks from diverse data via multitask learning (MTL).
However, gradient descent (GD) on the average loss across all tasks may yield
poor multitask performance due to severe under-optimization of certain tasks.
Previous approaches that manipulate task gradients for a more balanced loss
decrease require storing and computing all task gradients (O(K) space and time
where K is the number of tasks), limiting their use in large-scale scenarios.
In this work, we introduce Fast Adaptive Multitask Optimization (FAMO), a
dynamic weighting method that decreases task losses in a balanced way using
O(1) space and time. We conduct an extensive set of experiments covering
multi-task supervised and reinforcement learning problems. Our results indicate
that FAMO achieves comparable or superior performance to state-of-the-art
gradient manipulation techniques while offering significant improvements in
space and computational efficiency. Code is available at
https://github.com/Cranial-XIX/FAMO.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Asymptotics of Bayesian Uncertainty Estimation in Random Features
  Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03783v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03783v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Youngsoo Baek, Samuel I. Berchuck, Sayan Mukherjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we compare and contrast the behavior of the posterior
predictive distribution to the risk of the maximum a posteriori estimator for
the random features regression model in the overparameterized regime. We will
focus on the variance of the posterior predictive distribution (Bayesian model
average) and compare its asymptotics to that of the risk of the MAP estimator.
In the regime where the model dimensions grow faster than any constant multiple
of the number of samples, asymptotic agreement between these two quantities is
governed by the phase transition in the signal-to-noise ratio. They also
asymptotically agree with each other when the number of samples grow faster
than any constant multiple of model dimensions. Numerical simulations
illustrate finer distributional properties of the two quantities for finite
dimensions. We conjecture they have Gaussian fluctuations and exhibit similar
properties as found by previous authors in a Gaussian sequence model, which is
of independent theoretical interest.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Matched Pair Calibration for Ranking Fairness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Korevaar, Chris McConnell, Edmund Tong, Erik Brinkman, Alana Shine, Misam Abbas, Blossom Metevier, Sam Corbett-Davies, Khalid El-Arini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a test of fairness in score-based ranking systems called matched
pair calibration. Our approach constructs a set of matched item pairs with
minimal confounding differences between subgroups before computing an
appropriate measure of ranking error over the set. The matching step ensures
that we compare subgroup outcomes between identically scored items so that
measured performance differences directly imply unfairness in subgroup-level
exposures. We show how our approach generalizes the fairness intuitions of
calibration from a binary classification setting to ranking and connect our
approach to other proposals for ranking fairness measures. Moreover, our
strategy shows how the logic of marginal outcome tests extends to cases where
the analyst has access to model scores. Lastly, we provide an example of
applying matched pair calibration to a real-word ranking data set to
demonstrate its efficacy in detecting ranking bias.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>19 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph Classification Gaussian Processes via Spectral Features 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03770v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03770v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Felix L. Opolka, Yin-Cong Zhi, Pietro Liò, Xiaowen Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph classification aims to categorise graphs based on their structure and
node attributes. In this work, we propose to tackle this task using tools from
graph signal processing by deriving spectral features, which we then use to
design two variants of Gaussian process models for graph classification. The
first variant uses spectral features based on the distribution of energy of a
node feature signal over the spectrum of the graph. We show that even such a
simple approach, having no learned parameters, can yield competitive
performance compared to strong neural network and graph kernel baselines. A
second, more sophisticated variant is designed to capture multi-scale and
localised patterns in the graph by learning spectral graph wavelet filters,
obtaining improved performance on synthetic and real-world data sets. Finally,
we show that both models produce well calibrated uncertainty estimates,
enabling reliable decision making based on the model predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the effects of robotic design on learning and neural control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03757v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03757v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua Paul Powers
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The ongoing deep learning revolution has allowed computers to outclass humans
in various games and perceive features imperceptible to humans during
classification tasks. Current machine learning techniques have clearly
distinguished themselves in specialized tasks. However, we have yet to see
robots capable of performing multiple tasks at an expert level. Most work in
this field is focused on the development of more sophisticated learning
algorithms for a robot's controller given a largely static and presupposed
robotic design. By focusing on the development of robotic bodies, rather than
neural controllers, I have discovered that robots can be designed such that
they overcome many of the current pitfalls encountered by neural controllers in
multitask settings. Through this discovery, I also present novel metrics to
explicitly measure the learning ability of a robotic design and its resistance
to common problems such as catastrophic interference.
  Traditionally, the physical robot design requires human engineers to plan
every aspect of the system, which is expensive and often relies on human
intuition. In contrast, within the field of evolutionary robotics, evolutionary
algorithms are used to automatically create optimized designs, however, such
designs are often still limited in their ability to perform in a multitask
setting. The metrics created and presented here give a novel path to automated
design that allow evolved robots to synergize with their controller to improve
the computational efficiency of their learning while overcoming catastrophic
interference.
  Overall, this dissertation intimates the ability to automatically design
robots that are more general purpose than current robots and that can perform
various tasks while requiring less computation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2008.06397</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Soft Merging of Experts with Adaptive Routing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03745v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03745v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Muqeeth, Haokun Liu, Colin Raffel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparsely activated neural networks with conditional computation learn to
route their inputs through different "expert" subnetworks, providing a form of
modularity that densely activated models lack. Despite their possible benefits,
models with learned routing often underperform their parameter-matched densely
activated counterparts as well as models that use non-learned heuristic routing
strategies. In this paper, we hypothesize that these shortcomings stem from the
gradient estimation techniques used to train sparsely activated models that use
non-differentiable discrete routing decisions. To address this issue, we
introduce Soft Merging of Experts with Adaptive Routing (SMEAR), which avoids
discrete routing by using a single "merged" expert constructed via a weighted
average of all of the experts' parameters. By routing activations through a
single merged expert, SMEAR does not incur a significant increase in
computational costs and enables standard gradient-based training. We
empirically validate that models using SMEAR outperform models that route based
on metadata or learn sparse routing through gradient estimation. Furthermore,
we provide qualitative analysis demonstrating that the experts learned via
SMEAR exhibit a significant amount of specialization. All of the code used in
our experiments is publicly available.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Do or Learning While Doing: Reinforcement Learning and
  Bayesian Optimisation for Online Continuous Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03739v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03739v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Kaiser, Chenran Xu, Annika Eichler, Andrea Santamaria Garcia, Oliver Stein, Erik Bründermann, Willi Kuropka, Hannes Dinter, Frank Mayet, Thomas Vinatier, Florian Burkart, Holger Schlarb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online tuning of real-world plants is a complex optimisation problem that
continues to require manual intervention by experienced human operators.
Autonomous tuning is a rapidly expanding field of research, where
learning-based methods, such as Reinforcement Learning-trained Optimisation
(RLO) and Bayesian optimisation (BO), hold great promise for achieving
outstanding plant performance and reducing tuning times. Which algorithm to
choose in different scenarios, however, remains an open question. Here we
present a comparative study using a routine task in a real particle accelerator
as an example, showing that RLO generally outperforms BO, but is not always the
best choice. Based on the study's results, we provide a clear set of criteria
to guide the choice of algorithm for a given tuning task. These can ease the
adoption of learning-based autonomous tuning solutions to the operation of
complex real-world plants, ultimately improving the availability and pushing
the limits of operability of these facilities, thereby enabling scientific and
engineering advancements.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 8 figures, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Visual Foundational Models of Physical Scenes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03727v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03727v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chethan Parameshwara, Alessandro Achille, Matthew Trager, Xiaolong Li, Jiawei Mo, Matthew Trager, Ashwin Swaminathan, CJ Taylor, Dheera Venkatraman, Xiaohan Fei, Stefano Soatto
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We describe a first step towards learning general-purpose visual
representations of physical scenes using only image prediction as a training
criterion. To do so, we first define "physical scene" and show that, even
though different agents may maintain different representations of the same
scene, the underlying physical scene that can be inferred is unique. Then, we
show that NeRFs cannot represent the physical scene, as they lack extrapolation
mechanisms. Those, however, could be provided by Diffusion Models, at least in
theory. To test this hypothesis empirically, NeRFs can be combined with
Diffusion Models, a process we refer to as NeRF Diffusion, used as unsupervised
representations of the physical scene. Our analysis is limited to visual data,
without external grounding mechanisms that can be provided by independent
sensory modalities.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TLDR: Physical scenes are equivalence classes of sufficient
  statistics, and can be inferred uniquely by any agent measuring the same
  finite data; We formalize and implement an approach to representation
  learning that overturns "naive realism" in favor of an analytical approach of
  Russell and Koenderink. NeRFs cannot capture the physical scenes, but
  combined with Diffusion Models they can</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring Model Dynamics for Accumulative Poisoning Discovery <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03726v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03726v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianing Zhu, Xiawei Guo, Jiangchao Yao, Chao Du, Li He, Shuo Yuan, Tongliang Liu, Liang Wang, Bo Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adversarial poisoning attacks pose huge threats to various machine learning
applications. Especially, the recent accumulative poisoning attacks show that
it is possible to achieve irreparable harm on models via a sequence of
imperceptible attacks followed by a trigger batch. Due to the limited
data-level discrepancy in real-time data streaming, current defensive methods
are indiscriminate in handling the poison and clean samples. In this paper, we
dive into the perspective of model dynamics and propose a novel information
measure, namely, Memorization Discrepancy, to explore the defense via the
model-level information. By implicitly transferring the changes in the data
manipulation to that in the model outputs, Memorization Discrepancy can
discover the imperceptible poison samples based on their distinct dynamics from
the clean samples. We thoroughly explore its properties and propose
Discrepancy-aware Sample Correction (DSC) to defend against accumulative
poisoning attacks. Extensive experiments comprehensively characterized
Memorization Discrepancy and verified its effectiveness. The code is publicly
available at: https://github.com/tmlr-group/Memorization-Discrepancy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Memory-Efficient Training for Extremely Large Output Spaces --
  Learning with 500k Labels on a Single Commodity GPU 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Erik Schultheis, Rohit Babbar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In classification problems with large output spaces (up to millions of
labels), the last layer can require an enormous amount of memory. Using sparse
connectivity would drastically reduce the memory requirements, but as we show
below, it can result in much diminished predictive performance of the model.
Fortunately, we found that this can be mitigated by introducing a penultimate
layer of intermediate size. We further demonstrate that one can constrain the
connectivity of the sparse layer to be uniform, in the sense that each output
neuron will have the exact same number of incoming connections. This allows for
efficient implementations of sparse matrix multiplication and connection
redistribution on GPU hardware. Via a custom CUDA implementation, we show that
the proposed approach can scale to datasets with 670,000 labels on a single
commodity GPU with only 4GB memory.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Emotion-Conditioned Melody Harmonization with Hierarchical Variational
  Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03718v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03718v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shulei Ji, Xinyu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing melody harmonization models have made great progress in improving
the quality of generated harmonies, but most of them ignored the emotions
beneath the music. Meanwhile, the variability of harmonies generated by
previous methods is insufficient. To solve these problems, we propose a novel
LSTM-based Hierarchical Variational Auto-Encoder (LHVAE) to investigate the
influence of emotional conditions on melody harmonization, while improving the
quality of generated harmonies and capturing the abundant variability of chord
progressions. Specifically, LHVAE incorporates latent variables and emotional
conditions at different levels (piece- and bar-level) to model the global and
local music properties. Additionally, we introduce an attention-based melody
context vector at each step to better learn the correspondence between melodies
and harmonies. Experimental results of the objective evaluation show that our
proposed model outperforms other LSTM-based models. Through subjective
evaluation, we conclude that only altering the chords hardly changes the
overall emotion of the music. The qualitative analysis demonstrates the ability
of our model to generate variable harmonies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IEEE SMC 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unleashing Mask: Explore the Intrinsic Out-of-Distribution Detection
  Capability <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jianing Zhu, Hengzhuang Li, Jiangchao Yao, Tongliang Liu, Jianliang Xu, Bo Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection is an indispensable aspect of secure AI
when deploying machine learning models in real-world applications. Previous
paradigms either explore better scoring functions or utilize the knowledge of
outliers to equip the models with the ability of OOD detection. However, few of
them pay attention to the intrinsic OOD detection capability of the given
model. In this work, we generally discover the existence of an intermediate
stage of a model trained on in-distribution (ID) data having higher OOD
detection performance than that of its final stage across different settings,
and further identify one critical data-level attribution to be learning with
the atypical samples. Based on such insights, we propose a novel method,
Unleashing Mask, which aims to restore the OOD discriminative capabilities of
the well-trained model with ID data. Our method utilizes a mask to figure out
the memorized atypical samples, and then finetune the model or prune it with
the introduced mask to forget them. Extensive experiments and analysis
demonstrate the effectiveness of our method. The code is available at:
https://github.com/tmlr-group/Unleashing-Mask.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Bayesian post-hoc regularization of random forests 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03702v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03702v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bastian Pfeifer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Random Forests are powerful ensemble learning algorithms widely used in
various machine learning tasks. However, they have a tendency to overfit noisy
or irrelevant features, which can result in decreased generalization
performance. Post-hoc regularization techniques aim to mitigate this issue by
modifying the structure of the learned ensemble after its training. Here, we
propose Bayesian post-hoc regularization to leverage the reliable patterns
captured by leaf nodes closer to the root, while potentially reducing the
impact of more specific and potentially noisy leaf nodes deeper in the tree.
This approach allows for a form of pruning that does not alter the general
structure of the trees but rather adjusts the influence of leaf nodes based on
their proximity to the root node. We have evaluated the performance of our
method on various machine learning data sets. Our approach demonstrates
competitive performance with the state-of-the-art methods and, in certain
cases, surpasses them in terms of predictive accuracy and generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-grained Expressivity of Graph Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03698v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03698v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jan Böker, Ron Levie, Ningyuan Huang, Soledad Villar, Christopher Morris
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous recent works have analyzed the expressive power of message-passing
graph neural networks (MPNNs), primarily utilizing combinatorial techniques
such as the $1$-dimensional Weisfeiler-Leman test ($1$-WL) for the graph
isomorphism problem. However, the graph isomorphism objective is inherently
binary, not giving insights into the degree of similarity between two given
graphs. This work resolves this issue by considering continuous extensions of
both $1$-WL and MPNNs to graphons. Concretely, we show that the continuous
variant of $1$-WL delivers an accurate topological characterization of the
expressive power of MPNNs on graphons, revealing which graphs these networks
can distinguish and the level of difficulty in separating them. We identify the
finest topology where MPNNs separate points and prove a universal approximation
theorem. Consequently, we provide a theoretical framework for graph and graphon
similarity combining various topological variants of classical
characterizations of the $1$-WL. In particular, we characterize the expressive
power of MPNNs in terms of the tree distance, which is a graph distance based
on the concepts of fractional isomorphisms, and substructure counts via tree
homomorphisms, showing that these concepts have the same expressive power as
the $1$-WL and MPNNs on graphons. Empirically, we validate our theoretical
findings by showing that randomly initialized MPNNs, without training, exhibit
competitive performance compared to their trained counterparts. Moreover, we
evaluate different MPNN architectures based on their ability to preserve graph
distances, highlighting the significance of our continuous $1$-WL test in
understanding MPNNs' expressivity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Mildly Constrained Evaluation Policy for Offline Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linjie Xu, Zhengyao Jiang, Jinyu Wang, Lei Song, Jiang Bian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline reinforcement learning (RL) methodologies enforce constraints on the
policy to adhere closely to the behavior policy, thereby stabilizing value
learning and mitigating the selection of out-of-distribution (OOD) actions
during test time. Conventional approaches apply identical constraints for both
value learning and test time inference. However, our findings indicate that the
constraints suitable for value estimation may in fact be excessively
restrictive for action selection during test time. To address this issue, we
propose a Mildly Constrained Evaluation Policy (MCEP) for test time inference
with a more constrained target policy for value estimation. Since the target
policy has been adopted in various prior approaches, MCEP can be seamlessly
integrated with them as a plug-in. We instantiate MCEP based on TD3-BC
[Fujimoto and Gu, 2021] and AWAC [Nair et al., 2020] algorithms. The empirical
results on MuJoCo locomotion tasks show that the MCEP significantly outperforms
the target policy and achieves competitive results to state-of-the-art offline
RL methods. The codes are open-sourced at https://github.com/egg-west/MCEP.git.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Human-imperceptible, Machine-recognizable Images 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fusheng Hao, Fengxiang He, Yikai Wang, Fuxiang Wu, Jing Zhang, Jun Cheng, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Massive human-related data is collected to train neural networks for computer
vision tasks. A major conflict is exposed relating to software engineers
between better developing AI systems and distancing from the sensitive training
data. To reconcile this conflict, this paper proposes an efficient
privacy-preserving learning paradigm, where images are first encrypted to
become ``human-imperceptible, machine-recognizable'' via one of the two
encryption strategies: (1) random shuffling to a set of equally-sized patches
and (2) mixing-up sub-patches of the images. Then, minimal adaptations are made
to vision transformer to enable it to learn on the encrypted images for vision
tasks, including image classification and object detection. Extensive
experiments on ImageNet and COCO show that the proposed paradigm achieves
comparable accuracy with the competitive methods. Decrypting the encrypted
images requires solving an NP-hard jigsaw puzzle or an ill-posed inverse
problem, which is empirically shown intractable to be recovered by various
attackers, including the powerful vision transformer-based attacker. We thus
show that the proposed paradigm can ensure the encrypted images have become
human-imperceptible while preserving machine-recognizable information. The code
is available at \url{https://github.com/FushengHao/PrivacyPreservingML.}
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Schema First! Learn Versatile Knowledge Graph Embeddings by Capturing
  Semantics with MASCHInE 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03659v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03659v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nicolas Hubert, Heiko Paulheim, Pierre Monnin, Armelle Brun, Davy Monticolo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Knowledge graph embedding models (KGEMs) have gained considerable traction in
recent years. These models learn a vector representation of knowledge graph
entities and relations, a.k.a. knowledge graph embeddings (KGEs). Learning
versatile KGEs is desirable as it makes them useful for a broad range of tasks.
However, KGEMs are usually trained for a specific task, which makes their
embeddings task-dependent. In parallel, the widespread assumption that KGEMs
actually create a semantic representation of the underlying entities and
relations (e.g., project similar entities closer than dissimilar ones) has been
challenged. In this work, we design heuristics for generating protographs --
small, modified versions of a KG that leverage schema-based information. The
learnt protograph-based embeddings are meant to encapsulate the semantics of a
KG, and can be leveraged in learning KGEs that, in turn, also better capture
semantics. Extensive experiments on various evaluation benchmarks demonstrate
the soundness of this approach, which we call Modular and Agnostic SCHema-based
Integration of protograph Embeddings (MASCHInE). In particular, MASCHInE helps
produce more versatile KGEs that yield substantially better performance for
entity clustering and node classification tasks. For link prediction, using
MASCHInE has little impact on rank-based performance but increases the number
of semantically valid predictions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Learning under Adversarial Nonlinear Constraints 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03655v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03655v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pavel Kolev, Georg Martius, Michael Muehlebach
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many applications, learning systems are required to process continuous
non-stationary data streams. We study this problem in an online learning
framework and propose an algorithm that can deal with adversarial time-varying
and nonlinear constraints. As we show in our work, the algorithm called
Constraint Violation Velocity Projection (CVV-Pro) achieves $\sqrt{T}$ regret
and converges to the feasible set at a rate of $1/\sqrt{T}$, despite the fact
that the feasible set is slowly time-varying and a priori unknown to the
learner. CVV-Pro only relies on local sparse linear approximations of the
feasible set and therefore avoids optimizing over the entire set at each
iteration, which is in sharp contrast to projected gradients or Frank-Wolfe
methods. We also empirically evaluate our algorithm on two-player games, where
the players are subjected to a shared constraint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Supervised Knowledge May Hurt Novel Class Discovery Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03648v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03648v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziyun Li, Jona Otholt, Ben Dai, Di Hu, Christoph Meinel, Haojin Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Novel class discovery (NCD) aims to infer novel categories in an unlabeled
dataset by leveraging prior knowledge of a labeled set comprising disjoint but
related classes. Given that most existing literature focuses primarily on
utilizing supervised knowledge from a labeled set at the methodology level,
this paper considers the question: Is supervised knowledge always helpful at
different levels of semantic relevance? To proceed, we first establish a novel
metric, so-called transfer flow, to measure the semantic similarity between
labeled/unlabeled datasets. To show the validity of the proposed metric, we
build up a large-scale benchmark with various degrees of semantic similarities
between labeled/unlabeled datasets on ImageNet by leveraging its hierarchical
class structure. The results based on the proposed benchmark show that the
proposed transfer flow is in line with the hierarchical class structure; and
that NCD performance is consistent with the semantic similarities (measured by
the proposed metric). Next, by using the proposed transfer flow, we conduct
various empirical experiments with different levels of semantic similarity,
yielding that supervised knowledge may hurt NCD performance. Specifically,
using supervised information from a low-similarity labeled set may lead to a
suboptimal result as compared to using pure self-supervised knowledge. These
results reveal the inadequacy of the existing NCD literature which usually
assumes that supervised knowledge is beneficial. Finally, we develop a
pseudo-version of the transfer flow as a practical reference to decide if
supervised knowledge should be used in NCD. Its effectiveness is supported by
our empirical studies, which show that the pseudo transfer flow (with or
without supervised knowledge) is consistent with the corresponding accuracy
based on various datasets. Code is released at
https://github.com/J-L-O/SK-Hurt-NCD
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>TMLR 2023 accepted paper. arXiv admin note: substantial text overlap
  with arXiv:2209.09120</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Proximal Symmetric Non-negative Latent Factor Analysis: A Novel Approach
  to Highly-Accurate Representation of Undirected Weighted Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03647v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03647v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yurong Zhong, Zhe Xie, Weiling Li, Xin Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An Undirected Weighted Network (UWN) is commonly found in big data-related
applications. Note that such a network's information connected with its nodes,
and edges can be expressed as a Symmetric, High-Dimensional and Incomplete
(SHDI) matrix. However, existing models fail in either modeling its intrinsic
symmetry or low-data density, resulting in low model scalability or
representation learning ability. For addressing this issue, a Proximal
Symmetric Nonnegative Latent-factor-analysis (PSNL) model is proposed. It
incorporates a proximal term into symmetry-aware and data density-oriented
objective function for high representation accuracy. Then an adaptive
Alternating Direction Method of Multipliers (ADMM)-based learning scheme is
implemented through a Tree-structured of Parzen Estimators (TPE) method for
high computational efficiency. Empirical studies on four UWNs demonstrate that
PSNL achieves higher accuracy gain than state-of-the-art models, as well as
highly competitive computational efficiency.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Dance Generation by Sound Symbolic Words 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miki Okamura, Naruya Kondo, Tatsuki Fushimi Maki Sakamoto, Yoichi Ochiai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study introduces a novel approach to generate dance motions using
onomatopoeia as input, with the aim of enhancing creativity and diversity in
dance generation. Unlike text and music, onomatopoeia conveys rhythm and
meaning through abstract word expressions without constraints on expression and
without need for specialized knowledge. We adapt the AI Choreographer framework
and employ the Sakamoto system, a feature extraction method for onomatopoeia
focusing on phonemes and syllables. Additionally, we present a new dataset of
40 onomatopoeia-dance motion pairs collected through a user survey. Our results
demonstrate that the proposed method enables more intuitive dance generation
and can create dance motions using sound-symbolic words from a variety of
languages, including those without onomatopoeia. This highlights the potential
for diverse dance creation across different languages and cultures, accessible
to a wider audience. Qualitative samples from our model can be found at:
https://sites.google.com/view/onomatopoeia-dance/home/.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding Progressive Training Through the Framework of Randomized
  Coordinate Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03626v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03626v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rafał Szlendak, Elnur Gasanov, Peter Richtárik
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a Randomized Progressive Training algorithm (RPT) -- a stochastic
proxy for the well-known Progressive Training method (PT) (Karras et al.,
2017). Originally designed to train GANs (Goodfellow et al., 2014), PT was
proposed as a heuristic, with no convergence analysis even for the simplest
objective functions. On the contrary, to the best of our knowledge, RPT is the
first PT-type algorithm with rigorous and sound theoretical guarantees for
general smooth objective functions. We cast our method into the established
framework of Randomized Coordinate Descent (RCD) (Nesterov, 2012; Richt\'arik &
Tak\'a\v{c}, 2014), for which (as a by-product of our investigations) we also
propose a novel, simple and general convergence analysis encapsulating
strongly-convex, convex and nonconvex objectives. We then use this framework to
establish a convergence theory for RPT. Finally, we validate the effectiveness
of our method through extensive computational experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fair and Robust Estimation of Heterogeneous Treatment Effects for Policy
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03625v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03625v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kwangho Kim, José R. Zubizarreta
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a simple and general framework for nonparametric estimation of
heterogeneous treatment effects under fairness constraints. Under standard
regularity conditions, we show that the resulting estimators possess the double
robustness property. We use this framework to characterize the trade-off
between fairness and the maximum welfare achievable by the optimal policy. We
evaluate the methods in a simulation study and illustrate them in a real-world
case study.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Spike-based computation using classical recurrent neural networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florent De Geeter, Damien Ernst, Guillaume Drion
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spiking neural networks are a type of artificial neural networks in which
communication between neurons is only made of events, also called spikes. This
property allows neural networks to make asynchronous and sparse computations
and therefore to drastically decrease energy consumption when run on
specialized hardware. However, training such networks is known to be difficult,
mainly due to the non-differentiability of the spike activation, which prevents
the use of classical backpropagation. This is because state-of-the-art spiking
neural networks are usually derived from biologically-inspired neuron models,
to which are applied machine learning methods for training. Nowadays, research
about spiking neural networks focuses on the design of training algorithms
whose goal is to obtain networks that compete with their non-spiking version on
specific tasks. In this paper, we attempt the symmetrical approach: we modify
the dynamics of a well-known, easily trainable type of recurrent neural network
to make it event-based. This new RNN cell, called the Spiking Recurrent Cell,
therefore communicates using events, i.e. spikes, while being completely
differentiable. Vanilla backpropagation can thus be used to train any network
made of such RNN cell. We show that this new network can achieve performance
comparable to other types of spiking networks in the MNIST benchmark and its
variants, the Fashion-MNIST and the Neuromorphic-MNIST. Moreover, we show that
this new cell makes the training of deep spiking networks achievable.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Zero-shot Preference Learning for Offline RL via Optimal Transport 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03615v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03615v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Runze Liu, Yali Du, Fengshuo Bai, Jiafei Lyu, Xiu Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Preference-based Reinforcement Learning (PbRL) has demonstrated remarkable
efficacy in aligning rewards with human intentions. However, a significant
challenge lies in the need of substantial human labels, which is costly and
time-consuming. Additionally, the expensive preference data obtained from prior
tasks is not typically reusable for subsequent task learning, leading to
extensive labeling for each new task. In this paper, we propose a novel
zero-shot preference-based RL algorithm that leverages labeled preference data
from source tasks to infer labels for target tasks, eliminating the requirement
for human queries. Our approach utilizes Gromov-Wasserstein distance to align
trajectory distributions between source and target tasks. The solved optimal
transport matrix serves as a correspondence between trajectories of two tasks,
making it possible to identify corresponding trajectory pairs between tasks and
transfer the preference labels. However, learning directly from inferred labels
that contains a fraction of noisy labels will result in an inaccurate reward
function, subsequently affecting policy performance. To this end, we introduce
Robust Preference Transformer, which models the rewards as Gaussian
distributions and incorporates reward uncertainty in addition to reward mean.
The empirical results on robotic manipulation tasks of Meta-World and Robomimic
show that our method has strong capabilities of transferring preferences
between tasks and learns reward functions from noisy labels robustly.
Furthermore, we reveal that our method attains near-oracle performance with a
small proportion of scripted labels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Buying Information for Stochastic Optimization <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03607v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03607v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mingchen Ma, Christos Tzamos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Stochastic optimization is one of the central problems in Machine Learning
and Theoretical Computer Science. In the standard model, the algorithm is given
a fixed distribution known in advance. In practice though, one may acquire at a
cost extra information to make better decisions. In this paper, we study how to
buy information for stochastic optimization and formulate this question as an
online learning problem. Assuming the learner has an oracle for the original
optimization problem, we design a $2$-competitive deterministic algorithm and a
$e/(e-1)$-competitive randomized algorithm for buying information. We show that
this ratio is tight as the problem is equivalent to a robust generalization of
the ski-rental problem, which we call super-martingale stopping.
  We also consider an adaptive setting where the learner can choose to buy
information after taking some actions for the underlying optimization problem.
We focus on the classic optimization problem, Min-Sum Set Cover, where the goal
is to quickly find an action that covers a given request drawn from a known
distribution. We provide an $8$-competitive algorithm running in polynomial
time that chooses actions and decides when to buy information about the
underlying request.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Avoid Adversarial Adaption in Federated Learning by Multi-Metric
  Investigations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03600v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03600v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Torsten Krauß, Alexandra Dmitrienko
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated Learning (FL) trains machine learning models on data distributed
across multiple devices, avoiding data transfer to a central location. This
improves privacy, reduces communication costs, and enhances model performance.
However, FL is prone to poisoning attacks, which can be untargeted aiming to
reduce the model performance, or targeted, so-called backdoors, which add
adversarial behavior that can be triggered with appropriately crafted inputs.
Striving for stealthiness, backdoor attacks are harder to deal with.
  Mitigation techniques against poisoning attacks rely on monitoring certain
metrics and filtering malicious model updates. However, previous works didn't
consider real-world adversaries and data distributions. To support our
statement, we define a new notion of strong adaptive adversaries that can
simultaneously adapt to multiple objectives and demonstrate through extensive
tests, that existing defense methods can be circumvented in this adversary
model. We also demonstrate, that existing defenses have limited effectiveness
when no assumptions are made about underlying data distributions.
  To address realistic scenarios and adversary models, we propose
Metric-Cascades (MESAS) a new defense that leverages multiple detection metrics
simultaneously for the filtering of poisoned model updates. This approach
forces adaptive attackers into a heavy multi-objective optimization problem,
and our evaluation with nine backdoors and three datasets shows that even our
strong adaptive attacker cannot evade MESAS's detection. We show that MESAS
outperforms existing defenses in distinguishing backdoors from distortions
originating from different data distributions within and across the clients.
Overall, MESAS is the first defense that is robust against strong adaptive
adversaries and is effective in real-world data scenarios while introducing a
low overhead of 24.37s on average.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 12 figures, 27 tables, 11 equations</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How does over-squashing affect the power of GNNs? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03589v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03589v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Francesco Di Giovanni, T. Konstantin Rusch, Michael M. Bronstein, Andreea Deac, Marc Lackenby, Siddhartha Mishra, Petar Veličković
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) are the state-of-the-art model for machine
learning on graph-structured data. The most popular class of GNNs operate by
exchanging information between adjacent nodes, and are known as Message Passing
Neural Networks (MPNNs). Given their widespread use, understanding the
expressive power of MPNNs is a key question. However, existing results
typically consider settings with uninformative node features. In this paper, we
provide a rigorous analysis to determine which function classes of node
features can be learned by an MPNN of a given capacity. We do so by measuring
the level of pairwise interactions between nodes that MPNNs allow for. This
measure provides a novel quantitative characterization of the so-called
over-squashing effect, which is observed to occur when a large volume of
messages is aggregated into fixed-size vectors. Using our measure, we prove
that, to guarantee sufficient communication between pairs of nodes, the
capacity of the MPNN must be large enough, depending on properties of the input
graph structure, such as commute times. For many relevant scenarios, our
analysis results in impossibility statements in practice, showing that
over-squashing hinders the expressive power of MPNNs. We validate our
theoretical findings through extensive controlled experiments and ablation
studies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ L-C2ST: Local Diagnostics for Posterior Approximations in
  Simulation-Based Inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03580v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03580v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julia Linhart, Alexandre Gramfort, Pedro L. C. Rodrigues
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many recent works in simulation-based inference (SBI) rely on deep generative
models to approximate complex, high-dimensional posterior distributions.
However, evaluating whether or not these approximations can be trusted remains
a challenge. Most approaches evaluate the posterior estimator only in
expectation over the observation space. This limits their interpretability and
is not sufficient to identify for which observations the approximation can be
trusted or should be improved. Building upon the well-known classifier
two-sample test (C2ST), we introduce L-C2ST, a new method that allows for a
local evaluation of the posterior estimator at any given observation. It offers
theoretically grounded and easy to interpret - e.g. graphical - diagnostics,
and unlike C2ST, does not require access to samples from the true posterior. In
the case of normalizing flow-based posterior estimators, L-C2ST can be
specialized to offer better statistical power, while being computationally more
efficient. On standard SBI benchmarks, L-C2ST provides comparable results to
C2ST and outperforms alternative local approaches such as coverage tests based
on highest predictive density (HPD). We further highlight the importance of
local evaluation and the benefit of interpretability of L-C2ST on a challenging
application from computational neuroscience.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 4 figures, 7 appendices, in proceedings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Personalization Disentanglement for Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Yan, Guodong Long
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized federated learning (PFL) jointly trains a variety of local
models through balancing between knowledge sharing across clients and model
personalization per client. This paper addresses PFL via explicit disentangling
latent representations into two parts to capture the shared knowledge and
client-specific personalization, which leads to more reliable and effective
PFL. The disentanglement is achieved by a novel Federated Dual Variational
Autoencoder (FedDVA), which employs two encoders to infer the two types of
representations. FedDVA can produce a better understanding of the trade-off
between global knowledge sharing and local personalization in PFL. Moreover, it
can be integrated with existing FL methods and turn them into personalized
models for heterogeneous downstream tasks. Extensive experiments validate the
advantages caused by disentanglement and show that models trained with
disentangled representations substantially outperform those vanilla methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Memory-Based Dual Gaussian Processes for Sequential Learning <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03566v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03566v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Paul E. Chang, Prakhar Verma, S. T. John, Arno Solin, Mohammad Emtiyaz Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential learning with Gaussian processes (GPs) is challenging when access
to past data is limited, for example, in continual and active learning. In such
cases, errors can accumulate over time due to inaccuracies in the posterior,
hyperparameters, and inducing points, making accurate learning challenging.
Here, we present a method to keep all such errors in check using the recently
proposed dual sparse variational GP. Our method enables accurate inference for
generic likelihoods and improves learning by actively building and updating a
memory of past data. We demonstrate its effectiveness in several applications
involving Bayesian optimization, active learning, and continual learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Machine Learning (ICML) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CIN++: Enhancing Topological Message Passing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lorenzo Giusti, Teodora Reu, Francesco Ceccarelli, Cristian Bodnar, Pietro Liò
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have demonstrated remarkable success in learning
from graph-structured data. However, they face significant limitations in
expressive power, struggling with long-range interactions and lacking a
principled approach to modeling higher-order structures and group interactions.
Cellular Isomorphism Networks (CINs) recently addressed most of these
challenges with a message passing scheme based on cell complexes. Despite their
advantages, CINs make use only of boundary and upper messages which do not
consider a direct interaction between the rings present in the underlying
complex. Accounting for these interactions might be crucial for learning
representations of many real-world complex phenomena such as the dynamics of
supramolecular assemblies, neural activity within the brain, and gene
regulation processes. In this work, we propose CIN++, an enhancement of the
topological message passing scheme introduced in CINs. Our message passing
scheme accounts for the aforementioned limitations by letting the cells to
receive also lower messages within each layer. By providing a more
comprehensive representation of higher-order and long-range interactions, our
enhanced topological message passing scheme achieves state-of-the-art results
on large-scale and long-range chemistry benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Unlearning: A <span class="highlight-title">Survey</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03558v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03558v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, Philip S. Yu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning has attracted widespread attention and evolved into an
enabling technology for a wide range of highly successful applications, such as
intelligent computer vision, speech recognition, medical diagnosis, and more.
Yet a special need has arisen where, due to privacy, usability, and/or the
right to be forgotten, information about some specific samples needs to be
removed from a model, called machine unlearning. This emerging technology has
drawn significant interest from both academics and industry due to its
innovation and practicality. At the same time, this ambitious problem has led
to numerous research efforts aimed at confronting its challenges. To the best
of our knowledge, no study has analyzed this complex topic or compared the
feasibility of existing unlearning solutions in different kinds of scenarios.
Accordingly, with this survey, we aim to capture the key concepts of unlearning
techniques. The existing solutions are classified and summarized based on their
characteristics within an up-to-date and comprehensive review of each
category's advantages and limitations. The survey concludes by highlighting
some of the outstanding issues with unlearning techniques, along with some
feasible directions for new research opportunities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ State Regularized Policy Optimization on Data with Dynamics Shift 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03552v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03552v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenghai Xue, Qingpeng Cai, Shuchang Liu, Dong Zheng, Peng Jiang, Kun Gai, Bo An
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real-world scenarios, Reinforcement Learning (RL) algorithms are
trained on data with dynamics shift, i.e., with different underlying
environment dynamics. A majority of current methods address such issue by
training context encoders to identify environment parameters. Data with
dynamics shift are separated according to their environment parameters to train
the corresponding policy. However, these methods can be sample inefficient as
data are used \textit{ad hoc}, and policies trained for one dynamics cannot
benefit from data collected in all other environments with different dynamics.
In this paper, we find that in many environments with similar structures and
different dynamics, optimal policies have similar stationary state
distributions. We exploit such property and learn the stationary state
distribution from data with dynamics shift for efficient data reuse. Such
distribution is used to regularize the policy trained in a new environment,
leading to the SRPO (\textbf{S}tate \textbf{R}egularized \textbf{P}olicy
\textbf{O}ptimization) algorithm. To conduct theoretical analyses, the
intuition of similar environment structures is characterized by the notion of
homomorphous MDPs. We then demonstrate a lower-bound performance guarantee on
policies regularized by the stationary state distribution. In practice, SRPO
can be an add-on module to context-based algorithms in both online and offline
RL settings. Experimental results show that SRPO can make several context-based
algorithms far more data efficient and significantly improve their overall
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Preprint. Under Review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Scalable Concept Extraction in Industry 4.0 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03551v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03551v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrés Felipe Posada-Moreno, Kai Müller, Florian Brillowski, Friedrich Solowjow, Thomas Gries, Sebastian Trimpe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The industry 4.0 is leveraging digital technologies and machine learning
techniques to connect and optimize manufacturing processes. Central to this
idea is the ability to transform raw data into human understandable knowledge
for reliable data-driven decision-making. Convolutional Neural Networks (CNNs)
have been instrumental in processing image data, yet, their ``black box''
nature complicates the understanding of their prediction process. In this
context, recent advances in the field of eXplainable Artificial Intelligence
(XAI) have proposed the extraction and localization of concepts, or which
visual cues intervene on the prediction process of CNNs. This paper tackles the
application of concept extraction (CE) methods to industry 4.0 scenarios. To
this end, we modify a recently developed technique, ``Extracting Concepts with
Local Aggregated Descriptors'' (ECLAD), improving its scalability.
Specifically, we propose a novel procedure for calculating concept importance,
utilizing a wrapper function designed for CNNs. This process is aimed at
decreasing the number of times each image needs to be evaluated. Subsequently,
we demonstrate the potential of CE methods, by applying them in three
industrial use cases. We selected three representative use cases in the context
of quality control for material design (tailored textiles), manufacturing
(carbon fiber reinforcement), and maintenance (photovoltaic module inspection).
In these examples, CE was able to successfully extract and locate concepts
directly related to each task. This is, the visual cues related to each
concept, coincided with what human experts would use to perform the task
themselves, even when the visual cues were entangled between multiple classes.
Through empirical results, we show that CE can be applied for understanding
CNNs in an industrial context, giving useful insights that can relate to domain
knowledge.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Dynamical Systems from Noisy Data with Inverse-Explicit
  Integrators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03548v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03548v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Håkon Noren, Sølve Eidnes, Elena Celledoni
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce the mean inverse integrator (MII), a novel approach to increase
the accuracy when training neural networks to approximate vector fields of
dynamical systems from noisy data. This method can be used to average multiple
trajectories obtained by numerical integrators such as Runge-Kutta methods. We
show that the class of mono-implicit Runge-Kutta methods (MIRK) has particular
advantages when used in connection with MII. When training vector field
approximations, explicit expressions for the loss functions are obtained when
inserting the training data in the MIRK formulae, unlocking symmetric and
high-order integrators that would otherwise be implicit for initial value
problems. The combined approach of applying MIRK within MII yields a
significantly lower error compared to the plain use of the numerical integrator
without averaging the trajectories. This is demonstrated with experiments using
data from several (chaotic) Hamiltonian systems. Additionally, we perform a
sensitivity analysis of the loss functions under normally distributed
perturbations, supporting the favorable performance of MII.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ How to Select Which Active Learning Strategy is Best Suited for Your
  Specific Problem and Budget 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03543v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03543v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guy Hacohen, Daphna Weinshall
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Active Learning (AL), a learner actively chooses which unlabeled examples
to query for labels from an oracle, under some budget constraints. Different AL
query strategies are more suited to different problems and budgets. Therefore,
in practice, knowing in advance which AL strategy is most suited for the
problem at hand remains an open problem. To tackle this challenge, we propose a
practical derivative-based method that dynamically identifies the best strategy
for each budget. We provide theoretical analysis of a simplified case to
motivate our approach and build intuition. We then introduce a method to
dynamically select an AL strategy based on the specific problem and budget.
Empirical results showcase the effectiveness of our approach across diverse
budgets and computer vision tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Masked Autoencoders are Efficient Continual Federated Learners 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03542v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03542v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subarnaduti Paul, Lars-Joel Frey, Roshni Kamath, Kristian Kersting, Martin Mundt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning is typically framed from a perspective of i.i.d., and more
importantly, isolated data. In parts, federated learning lifts this assumption,
as it sets out to solve the real-world challenge of collaboratively learning a
shared model from data distributed across clients. However, motivated primarily
by privacy and computational constraints, the fact that data may change,
distributions drift, or even tasks advance individually on clients, is seldom
taken into account. The field of continual learning addresses this separate
challenge and first steps have recently been taken to leverage synergies in
distributed supervised settings, in which several clients learn to solve
changing classification tasks over time without forgetting previously seen
ones. Motivated by these prior works, we posit that such federated continual
learning should be grounded in unsupervised learning of representations that
are shared across clients; in the loose spirit of how humans can indirectly
leverage others' experience without exposure to a specific task. For this
purpose, we demonstrate that masked autoencoders for distribution estimation
are particularly amenable to this setup. Specifically, their masking strategy
can be seamlessly integrated with task attention mechanisms to enable selective
knowledge transfer between clients. We empirically corroborate the latter
statement through several continual federated scenarios on both image and
binary datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Pitfalls of Test-Time Adaptation <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03536v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03536v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Zhao, Yuejiang Liu, Alexandre Alahi, Tao Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Test-Time Adaptation (TTA) has recently emerged as a promising approach for
tackling the robustness challenge under distribution shifts. However, the lack
of consistent settings and systematic studies in prior literature hinders
thorough assessments of existing methods. To address this issue, we present
TTAB, a test-time adaptation benchmark that encompasses ten state-of-the-art
algorithms, a diverse array of distribution shifts, and two evaluation
protocols. Through extensive experiments, our benchmark reveals three common
pitfalls in prior efforts. First, selecting appropriate hyper-parameters,
especially for model selection, is exceedingly difficult due to online batch
dependency. Second, the effectiveness of TTA varies greatly depending on the
quality and properties of the model being adapted. Third, even under optimal
algorithmic conditions, none of the existing methods are capable of addressing
all common types of distribution shifts. Our findings underscore the need for
future research in the field to conduct rigorous evaluations on a broader set
of models and shifts, and to re-examine the assumptions behind the empirical
success of TTA. Our code is available at
\url{https://github.com/lins-lab/ttab}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Learning in Linear Classification on Separable Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Itay Evron, Edward Moroshko, Gon Buzaglo, Maroun Khriesh, Badea Marjieh, Nathan Srebro, Daniel Soudry
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We analyze continual learning on a sequence of separable linear
classification tasks with binary labels. We show theoretically that learning
with weak regularization reduces to solving a sequential max-margin problem,
corresponding to a special case of the Projection Onto Convex Sets (POCS)
framework. We then develop upper bounds on the forgetting and other quantities
of interest under various settings with recurring tasks, including cyclic and
random orderings of tasks. We discuss several practical implications to popular
training practices like regularization scheduling and weighting. We point out
several theoretical differences between our continual classification setting
and a recently studied continual regression setting.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BackpropTools: A Fast, Portable Deep Reinforcement Learning Library for
  Continuous Control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03530v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03530v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Eschmann, Dario Albani, Giuseppe Loianno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Reinforcement Learning (RL) has been demonstrated to yield capable
agents and control policies in several domains but is commonly plagued by
prohibitively long training times. Additionally, in the case of continuous
control problems, the applicability of learned policies on real-world embedded
devices is limited due to the lack of real-time guarantees and portability of
existing deep learning libraries. To address these challenges, we present
BackpropTools, a dependency-free, header-only, pure C++ library for deep
supervised and reinforcement learning. Leveraging the template meta-programming
capabilities of recent C++ standards, we provide composable components that can
be tightly integrated by the compiler. Its novel architecture allows
BackpropTools to be used seamlessly on a heterogeneous set of platforms, from
HPC clusters over workstations and laptops to smartphones, smartwatches, and
microcontrollers. Specifically, due to the tight integration of the RL
algorithms with simulation environments, BackpropTools can solve popular RL
problems like the Pendulum-v1 swing-up about 7 to 15 times faster in terms of
wall-clock training time compared to other popular RL frameworks when using
TD3. We also provide a low-overhead and parallelized interface to the MuJoCo
simulator, showing that our PPO implementation achieves state of the art
returns in the Ant-v4 environment while achieving a 25 to 30 percent faster
wall-clock training time. Finally, we also benchmark the policy inference on a
diverse set of microcontrollers and show that in most cases our optimized
inference implementation is much faster than even the manufacturer's DSP
libraries. To the best of our knowledge, BackpropTools enables the first-ever
demonstration of training a deep RL algorithm directly on a microcontroller,
giving rise to the field of Tiny Reinforcement Learning (TinyRL). Project page:
https://backprop.tools
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Project page: https://backprop.tools</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Rec4Ad: A Free Lunch to Mitigate Sample Selection Bias for Ads CTR
  Prediction in Taobao 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jingyue Gao, Shuguang Han, Han Zhu, Siran Yang, Yuning Jiang, Jian Xu, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Click-Through Rate (CTR) prediction serves as a fundamental component in
online advertising. A common practice is to train a CTR model on advertisement
(ad) impressions with user feedback. Since ad impressions are purposely
selected by the model itself, their distribution differs from the inference
distribution and thus exhibits sample selection bias (SSB) that affects model
performance. Existing studies on SSB mainly employ sample re-weighting
techniques which suffer from high variance and poor model calibration. Another
line of work relies on costly uniform data that is inadequate to train
industrial models. Thus mitigating SSB in industrial models with a
uniform-data-free framework is worth exploring. Fortunately, many platforms
display mixed results of organic items (i.e., recommendations) and sponsored
items (i.e., ads) to users, where impressions of ads and recommendations are
selected by different systems but share the same user decision rationales.
Based on the above characteristics, we propose to leverage recommendations
samples as a free lunch to mitigate SSB for ads CTR model (Rec4Ad). After
elaborating data augmentation, Rec4Ad learns disentangled representations with
alignment and decorrelation modules for enhancement. When deployed in Taobao
display advertising system, Rec4Ad achieves substantial gains in key business
metrics, with a lift of up to +6.6\% CTR and +2.9\% RPM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Functional Data Perspective and Baseline On Multi-Layer
  Out-of-Distribution Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03522v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03522v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduardo Dadalto, Pierre Colombo, Guillaume Staerman, Nathan Noiry, Pablo Piantanida
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A key feature of out-of-distribution (OOD) detection is to exploit a trained
neural network by extracting statistical patterns and relationships through the
multi-layer classifier to detect shifts in the expected input data
distribution. Despite achieving solid results, several state-of-the-art methods
rely on the penultimate or last layer outputs only, leaving behind valuable
information for OOD detection. Methods that explore the multiple layers either
require a special architecture or a supervised objective to do so. This work
adopts an original approach based on a functional view of the network that
exploits the sample's trajectories through the various layers and their
statistical dependencies. It goes beyond multivariate features aggregation and
introduces a baseline rooted in functional anomaly detection. In this new
framework, OOD detection translates into detecting samples whose trajectories
differ from the typical behavior characterized by the training set. We validate
our method and empirically demonstrate its effectiveness in OOD detection
compared to strong state-of-the-art baselines on computer vision benchmarks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine learning in and out of equilibrium 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03521v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03521v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shishir Adhikari, Alkan Kabakçıoğlu, Alexander Strang, Deniz Yuret, Michael Hinczewski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The algorithms used to train neural networks, like stochastic gradient
descent (SGD), have close parallels to natural processes that navigate a
high-dimensional parameter space -- for example protein folding or evolution.
Our study uses a Fokker-Planck approach, adapted from statistical physics, to
explore these parallels in a single, unified framework. We focus in particular
on the stationary state of the system in the long-time limit, which in
conventional SGD is out of equilibrium, exhibiting persistent currents in the
space of network parameters. As in its physical analogues, the current is
associated with an entropy production rate for any given training trajectory.
The stationary distribution of these rates obeys the integral and detailed
fluctuation theorems -- nonequilibrium generalizations of the second law of
thermodynamics. We validate these relations in two numerical examples, a
nonlinear regression network and MNIST digit classification. While the
fluctuation theorems are universal, there are other aspects of the stationary
state that are highly sensitive to the training details. Surprisingly, the
effective loss landscape and diffusion matrix that determine the shape of the
stationary distribution vary depending on the simple choice of minibatching
done with or without replacement. We can take advantage of this nonequilibrium
sensitivity to engineer an equilibrium stationary state for a particular
application: sampling from a posterior distribution of network weights in
Bayesian machine learning. We propose a new variation of stochastic gradient
Langevin dynamics (SGLD) that harnesses without replacement minibatching. In an
example system where the posterior is exactly known, this SGWORLD algorithm
outperforms SGLD, converging to the posterior orders of magnitude faster as a
function of the learning rate.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>24 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COPR: Consistency-Oriented Pre-Ranking for Online Advertising 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03516v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03516v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhishan Zhao, Jingyue Gao, Yu Zhang, Shuguang Han, Siyuan Lou, Xiang-Rong Sheng, Zhe Wang, Han Zhu, Yuning Jiang, Jian Xu, Bo Zheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cascading architecture has been widely adopted in large-scale advertising
systems to balance efficiency and effectiveness. In this architecture, the
pre-ranking model is expected to be a lightweight approximation of the ranking
model, which handles more candidates with strict latency requirements. Due to
the gap in model capacity, the pre-ranking and ranking models usually generate
inconsistent ranked results, thus hurting the overall system effectiveness. The
paradigm of score alignment is proposed to regularize their raw scores to be
consistent. However, it suffers from inevitable alignment errors and error
amplification by bids when applied in online advertising. To this end, we
introduce a consistency-oriented pre-ranking framework for online advertising,
which employs a chunk-based sampling module and a plug-and-play rank alignment
module to explicitly optimize consistency of ECPM-ranked results. A $\Delta
NDCG$-based weighting mechanism is adopted to better distinguish the importance
of inter-chunk samples in optimization. Both online and offline experiments
have validated the superiority of our framework. When deployed in Taobao
display advertising system, it achieves an improvement of up to +12.3\% CTR and
+5.6\% RPM.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Logic Diffusion for Knowledge Graph Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03515v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03515v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoying Xie, Biao Gong, Yiliang Lv, Zhen Han, Guoshuai Zhao, Xueming Qian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most recent works focus on answering first order logical queries to explore
the knowledge graph reasoning via multi-hop logic predictions. However,
existing reasoning models are limited by the circumscribed logical paradigms of
training samples, which leads to a weak generalization of unseen logic. To
address these issues, we propose a plug-in module called Logic Diffusion (LoD)
to discover unseen queries from surroundings and achieves dynamical equilibrium
between different kinds of patterns. The basic idea of LoD is relation
diffusion and sampling sub-logic by random walking as well as a special
training mechanism called gradient adaption. Besides, LoD is accompanied by a
novel loss function to further achieve the robust logical diffusion when facing
noisy data in training or testing sets. Extensive experiments on four public
datasets demonstrate the superiority of mainstream knowledge graph reasoning
models with LoD over state-of-the-art. Moreover, our ablation study proves the
general effectiveness of LoD on the noise-rich knowledge graph.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Subgraph Networks Based Contrastive Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03506v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03506v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinhuan Wang, Jiafei Shao, Zeyu Wang, Shanqing Yu, Qi Xuan, Xiaoniu Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph contrastive learning (GCL), as a self-supervised learning method, can
solve the problem of annotated data scarcity. It mines explicit features in
unannotated graphs to generate favorable graph representations for downstream
tasks. Most existing GCL methods focus on the design of graph augmentation
strategies and mutual information estimation operations. Graph augmentation
produces augmented views by graph perturbations. These views preserve a locally
similar structure and exploit explicit features. However, these methods have
not considered the interaction existing in subgraphs. To explore the impact of
substructure interactions on graph representations, we propose a novel
framework called subgraph network-based contrastive learning (SGNCL). SGNCL
applies a subgraph network generation strategy to produce augmented views. This
strategy converts the original graph into an Edge-to-Node mapping network with
both topological and attribute features. The single-shot augmented view is a
first-order subgraph network that mines the interaction between nodes,
node-edge, and edges. In addition, we also investigate the impact of the
second-order subgraph augmentation on mining graph structure interactions, and
further, propose a contrastive objective that fuses the first-order and
second-order subgraph information. We compare SGNCL with classical and
state-of-the-art graph contrastive learning methods on multiple benchmark
datasets of different domains. Extensive experiments show that SGNCL achieves
competitive or better performance (top three) on all datasets in unsupervised
learning settings. Furthermore, SGNCL achieves the best average gain of 6.9\%
in transfer learning compared to the best method. Finally, experiments also
demonstrate that mining substructure interactions have positive implications
for graph contrastive learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Russo-Ukrainian War: Prediction and explanation of Twitter suspension 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03502v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03502v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander Shevtsov, Despoina Antonakaki, Ioannis Lamprou, Ioannis Kontogiorgakis, Polyvios Pratikakis, Sotiris Ioannidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  On 24 February 2022, Russia invaded Ukraine, starting what is now known as
the Russo-Ukrainian War, initiating an online discourse on social media.
Twitter as one of the most popular SNs, with an open and democratic character,
enables a transparent discussion among its large user base. Unfortunately, this
often leads to Twitter's policy violations, propaganda, abusive actions, civil
integrity violation, and consequently to user accounts' suspension and
deletion. This study focuses on the Twitter suspension mechanism and the
analysis of shared content and features of the user accounts that may lead to
this. Toward this goal, we have obtained a dataset containing 107.7M tweets,
originating from 9.8 million users, using Twitter API. We extract the
categories of shared content of the suspended accounts and explain their
characteristics, through the extraction of text embeddings in junction with
cosine similarity clustering. Our results reveal scam campaigns taking
advantage of trending topics regarding the Russia-Ukrainian conflict for
Bitcoin and Ethereum fraud, spam, and advertisement campaigns. Additionally, we
apply a machine learning methodology including a SHapley Additive
explainability model to understand and explain how user accounts get suspended.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Transition role of entangled data in quantum machine learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03481v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03481v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinbiao Wang, Yuxuan Du, Zhuozhuo Tu, Yong Luo, Xiao Yuan, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entanglement serves as the resource to empower quantum computing. Recent
progress has highlighted its positive impact on learning quantum dynamics,
wherein the integration of entanglement into quantum operations or measurements
of quantum machine learning (QML) models leads to substantial reductions in
training data size, surpassing a specified prediction error threshold. However,
an analytical understanding of how the entanglement degree in data affects
model performance remains elusive. In this study, we address this knowledge gap
by establishing a quantum no-free-lunch (NFL) theorem for learning quantum
dynamics using entangled data. Contrary to previous findings, we prove that the
impact of entangled data on prediction error exhibits a dual effect, depending
on the number of permitted measurements. With a sufficient number of
measurements, increasing the entanglement of training data consistently reduces
the prediction error or decreases the required size of the training data to
achieve the same prediction error. Conversely, when few measurements are
allowed, employing highly entangled data could lead to an increased prediction
error. The achieved results provide critical guidance for designing advanced
QML protocols, especially for those tailored for execution on early-stage
quantum computers with limited access to quantum resources.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ GSHOT: Few-shot Generative Modeling of Labeled Graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03480v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03480v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sahil Manchanda, Shubham Gupta, Sayan Ranu, Srikanta Bedathur
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep graph generative modeling has gained enormous attraction in recent years
due to its impressive ability to directly learn the underlying hidden graph
distribution. Despite their initial success, these techniques, like much of the
existing deep generative methods, require a large number of training samples to
learn a good model. Unfortunately, large number of training samples may not
always be available in scenarios such as drug discovery for rare diseases. At
the same time, recent advances in few-shot learning have opened door to
applications where available training data is limited. In this work, we
introduce the hitherto unexplored paradigm of few-shot graph generative
modeling. Towards this, we develop GSHOT, a meta-learning based framework for
few-shot labeled graph generative modeling. GSHOT learns to transfer
meta-knowledge from similar auxiliary graph datasets. Utilizing these prior
experiences, GSHOT quickly adapts to an unseen graph dataset through self-paced
fine-tuning. Through extensive experiments on datasets from diverse domains
having limited training samples, we establish that GSHOT generates graphs of
superior fidelity compared to existing baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Convergent Bregman Plug-and-Play Image Restoration for Poisson Inverse
  Problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03466v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03466v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Hurault, Ulugbek Kamilov, Arthur Leclaire, Nicolas Papadakis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Plug-and-Play (PnP) methods are efficient iterative algorithms for solving
ill-posed image inverse problems. PnP methods are obtained by using deep
Gaussian denoisers instead of the proximal operator or the gradient-descent
step within proximal algorithms. Current PnP schemes rely on data-fidelity
terms that have either Lipschitz gradients or closed-form proximal operators,
which is not applicable to Poisson inverse problems. Based on the observation
that the Gaussian noise is not the adequate noise model in this setting, we
propose to generalize PnP using theBregman Proximal Gradient (BPG) method. BPG
replaces the Euclidean distance with a Bregman divergence that can better
capture the smoothness properties of the problem. We introduce the Bregman
Score Denoiser specifically parametrized and trained for the new Bregman
geometry and prove that it corresponds to the proximal operator of a nonconvex
potential. We propose two PnP algorithms based on the Bregman Score Denoiser
for solving Poisson inverse problems. Extending the convergence results of BPG
in the nonconvex settings, we show that the proposed methods converge,
targeting stationary points of an explicit global functional. Experimental
evaluations conducted on various Poisson inverse problems validate the
convergence results and showcase effective restoration performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Natural Language Commanding via Program Synthesis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03460v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03460v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Apurva Gandhi, Thong Q. Nguyen, Huitian Jiao, Robert Steen, Ameya Bhatawdekar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Semantic Interpreter, a natural language-friendly AI system for
productivity software such as Microsoft Office that leverages large language
models (LLMs) to execute user intent across application features. While LLMs
are excellent at understanding user intent expressed as natural language, they
are not sufficient for fulfilling application-specific user intent that
requires more than text-to-text transformations. We therefore introduce the
Office Domain Specific Language (ODSL), a concise, high-level language
specialized for performing actions in and interacting with entities in Office
applications. Semantic Interpreter leverages an Analysis-Retrieval prompt
construction method with LLMs for program synthesis, translating natural
language user utterances to ODSL programs that can be transpiled to application
APIs and then executed. We focus our discussion primarily on a research
exploration for Microsoft PowerPoint.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ blob loss: instance imbalance aware loss functions for semantic
  segmentation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.08209v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.08209v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Florian Kofler, Suprosanna Shit, Ivan Ezhov, Lucas Fidon, Izabela Horvath, Rami Al-Maskari, Hongwei Li, Harsharan Bhatia, Timo Loehr, Marie Piraud, Ali Erturk, Jan Kirschke, Jan C. Peeken, Tom Vercauteren, Claus Zimmer, Benedikt Wiestler, Bjoern Menze
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep convolutional neural networks (CNN) have proven to be remarkably
effective in semantic segmentation tasks. Most popular loss functions were
introduced targeting improved volumetric scores, such as the Dice coefficient
(DSC). By design, DSC can tackle class imbalance, however, it does not
recognize instance imbalance within a class. As a result, a large foreground
instance can dominate minor instances and still produce a satisfactory DSC.
Nevertheless, detecting tiny instances is crucial for many applications, such
as disease monitoring. For example, it is imperative to locate and surveil
small-scale lesions in the follow-up of multiple sclerosis patients. We propose
a novel family of loss functions, \emph{blob loss}, primarily aimed at
maximizing instance-level detection metrics, such as F1 score and sensitivity.
\emph{Blob loss} is designed for semantic segmentation problems where detecting
multiple instances matters. We extensively evaluate a DSC-based \emph{blob
loss} in five complex 3D semantic segmentation tasks featuring pronounced
instance heterogeneity in terms of texture and morphology. Compared to soft
Dice loss, we achieve 5% improvement for MS lesions, 3% improvement for liver
tumor, and an average 2% improvement for microscopy segmentation tasks
considering F1 score.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>23 pages, 7 figures // corrected one mistake where it said beta
  instead of alpha in the text</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Watermark for Large Language Models <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.10226v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.10226v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Kirchenbauer, Jonas Geiping, Yuxin Wen, Jonathan Katz, Ian Miers, Tom Goldstein
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Potential harms of large language models can be mitigated by watermarking
model output, i.e., embedding signals into generated text that are invisible to
humans but algorithmically detectable from a short span of tokens. We propose a
watermarking framework for proprietary language models. The watermark can be
embedded with negligible impact on text quality, and can be detected using an
efficient open-source algorithm without access to the language model API or
parameters. The watermark works by selecting a randomized set of "green" tokens
before a word is generated, and then softly promoting use of green tokens
during sampling. We propose a statistical test for detecting the watermark with
interpretable p-values, and derive an information-theoretic framework for
analyzing the sensitivity of the watermark. We test the watermark using a
multi-billion parameter model from the Open Pretrained Transformer (OPT)
family, and discuss robustness and security.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages in the main body. Published at ICML 2023. Code is available
  at github.com/jwkirchenbauer/lm-watermarking</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Concept-based Explanations for Out-Of-Distribution Detectors <span class="chip">ICML'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.02586v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.02586v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jihye Choi, Jayaram Raghuram, Ryan Feng, Jiefeng Chen, Somesh Jha, Atul Prakash
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Out-of-distribution (OOD) detection plays a crucial role in ensuring the safe
deployment of deep neural network (DNN) classifiers. While a myriad of methods
have focused on improving the performance of OOD detectors, a critical gap
remains in interpreting their decisions. We help bridge this gap by providing
explanations for OOD detectors based on learned high-level concepts. We first
propose two new metrics for assessing the effectiveness of a particular set of
concepts for explaining OOD detectors: 1) detection completeness, which
quantifies the sufficiency of concepts for explaining an OOD-detector's
decisions, and 2) concept separability, which captures the distributional
separation between in-distribution and OOD data in the concept space. Based on
these metrics, we propose an unsupervised framework for learning a set of
concepts that satisfy the desired properties of high detection completeness and
concept separability, and demonstrate its effectiveness in providing
concept-based explanations for diverse off-the-shelf OOD detectors. We also
show how to identify prominent concepts contributing to the detection results,
and provide further reasoning about their decisions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Paper published at International Conference on Machine Learning
  (ICML'23)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Trustworthiness Score to Evaluate CNNs Predictions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08839v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08839v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Abanoub Ghobrial, Darryl Hond, Hamid Asgari, Kerstin Eder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Due to the black box nature of Convolutional Neural Networks (CNNs), the
continuous validation of CNNs during operation is challenging with the absence
of a human monitor. As a result this makes it difficult for developers and
regulators to gain confidence in the deployment of autonomous systems employing
CNNs. It is critical for safety during operation to know when CNN's predictions
are trustworthy or suspicious. With the absence of a human monitor, the basic
approach is to use the model's output confidence score to assess if predictions
are trustworthy or suspicious. However, the model's confidence score is a
result of computations coming from a black box, therefore lacks transparency
and makes it challenging to automatedly credit trustworthiness to predictions.
We introduce the trustworthiness score (TS), a simple metric that provides a
more transparent and effective way of providing confidence in CNNs predictions
compared to model's confidence score. The metric quantifies the trustworthiness
in a prediction by checking for the existence of certain features in the
predictions made by the CNN. We also use the underlying idea of the TS metric,
to provide a suspiciousness score (SS) in the overall input frame to help in
the detection of suspicious frames where false negatives exist. We conduct a
case study using YOLOv5 on persons detection to demonstrate our method and
usage of TS and SS. The case study shows that using our method consistently
improves the precision of predictions compared to relying on model confidence
score alone, for both 1) approving of trustworthy predictions (~20%
improvement) and 2) detecting suspicious frames (~5% improvement).
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Theory of Link Prediction via Relational Weisfeiler-Leman 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02209v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02209v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyue Huang, Miguel Romero Orth, İsmail İlkan Ceylan, Pablo Barceló
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks are prominent models for representation learning over
graph-structured data. While the capabilities and limitations of these models
are well-understood for simple graphs, our understanding remains incomplete in
the context of knowledge graphs. Our goal is to provide a systematic
understanding of the landscape of graph neural networks for knowledge graphs
pertaining to the prominent task of link prediction. Our analysis entails a
unifying perspective on seemingly unrelated models and unlocks a series of
other models. The expressive power of various models is characterized via a
corresponding relational Weisfeiler-Leman algorithm. This analysis is extended
to provide a precise logical characterization of the class of functions
captured by a class of graph neural networks. The theoretical findings
presented in this paper explain the benefits of some widely employed practical
design choices, which are validated empirically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ AutoPEFT: Automatic Configuration Search for Parameter-Efficient
  Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Zhou, Xingchen Wan, Ivan Vulić, Anna Korhonen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pretrained language models are widely used in downstream NLP tasks via
task-specific fine-tuning, but such procedures can be costly. Recently,
Parameter-Efficient Fine-Tuning (PEFT) methods have achieved strong task
performance while updating a much smaller number of parameters compared to full
model fine-tuning (FFT). However, it is non-trivial to make informed design
choices on the PEFT configurations, such as their architecture, the number of
tunable parameters, and even the layers in which the PEFT modules are inserted.
Consequently, it is highly likely that the current, manually designed
configurations are suboptimal in terms of their performance-efficiency
trade-off. Inspired by advances in neural architecture search, we propose
AutoPEFT for automatic PEFT configuration selection: we first design an
expressive configuration search space with multiple representative PEFT modules
as building blocks. Using multi-objective Bayesian optimisation in a low-cost
setup, we then discover a Pareto-optimal set of configurations with strong
performance-cost trade-offs across different numbers of parameters that are
also highly transferable across different tasks. Empirically, on GLUE and
SuperGLUE tasks, we show that AutoPEFT-discovered configurations significantly
outperform existing PEFT methods and are on par or better than FFT, without
incurring substantial training efficiency costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 7 figures, 9 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Physics Inspired Approaches To Understanding Gaussian Processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10748v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10748v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maximilian P. Niroomand, Luke Dicks, Edward O. Pyzer-Knapp, David J. Wales
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Prior beliefs about the latent function to shape inductive biases can be
incorporated into a Gaussian Process (GP) via the kernel. However, beyond
kernel choices, the decision-making process of GP models remains poorly
understood. In this work, we contribute an analysis of the loss landscape for
GP models using methods from physics. We demonstrate $\nu$-continuity for
Matern kernels and outline aspects of catastrophe theory at critical points in
the loss landscape. By directly including $\nu$ in the hyperparameter
optimisation for Matern kernels, we find that typical values of $\nu$ are far
from optimal in terms of performance, yet prevail in the literature due to the
increased computational speed. We also provide an a priori method for
evaluating the effect of GP ensembles and discuss various voting approaches
based on physical properties of the loss landscape. The utility of these
approaches is demonstrated for various synthetic and real datasets. Our
findings provide an enhanced understanding of the decision-making process
behind GPs and offer practical guidance for improving their performance and
interpretability in a range of applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Stochastic Gradient Descent-Induced Drift of Representation in a
  Two-Layer Neural Network <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02563v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02563v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farhad Pashakhanloo, Alexei Koulakov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representational drift refers to over-time changes in neural activation
accompanied by a stable task performance. Despite being observed in the brain
and in artificial networks, the mechanisms of drift and its implications are
not fully understood. Motivated by recent experimental findings of
stimulus-dependent drift in the piriform cortex, we use theory and simulations
to study this phenomenon in a two-layer linear feedforward network.
Specifically, in a continual online learning scenario, we study the drift
induced by the noise inherent in the Stochastic Gradient Descent (SGD). By
decomposing the learning dynamics into the normal and tangent spaces of the
minimum-loss manifold, we show the former corresponds to a finite variance
fluctuation, while the latter could be considered as an effective diffusion
process on the manifold. We analytically compute the fluctuation and the
diffusion coefficients for the stimuli representations in the hidden layer as
functions of network parameters and input distribution. Further, consistent
with experiments, we show that the drift rate is slower for a more frequently
presented stimulus. Overall, our analysis yields a theoretical framework for
better understanding of the drift phenomenon in biological and artificial
neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>International Conference on Machine Learning (ICML) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Overcoming Simplicity Bias in Deep Networks using a Feature Sieve <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13293v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13293v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rishabh Tiwari, Pradeep Shenoy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Simplicity bias is the concerning tendency of deep networks to over-depend on
simple, weakly predictive features, to the exclusion of stronger, more complex
features. This is exacerbated in real-world applications by limited training
data and spurious feature-label correlations, leading to biased, incorrect
predictions. We propose a direct, interventional method for addressing
simplicity bias in DNNs, which we call the feature sieve. We aim to
automatically identify and suppress easily-computable spurious features in
lower layers of the network, thereby allowing the higher network levels to
extract and utilize richer, more meaningful representations. We provide
concrete evidence of this differential suppression & enhancement of relevant
features on both controlled datasets and real-world images, and report
substantial gains on many real-world debiasing benchmarks (11.4% relative gain
on Imagenet-A; 3.2% on BAR, etc). Crucially, we do not depend on prior
knowledge of spurious attributes or features, and in fact outperform many
baselines that explicitly incorporate such information. We believe that our
feature sieve work opens up exciting new research directions in automated
adversarial feature extraction and representation learning for deep networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Functional sufficient dimension reduction through information
  maximization with application to classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10880v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10880v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinyu Li, Jianjun Xu, Wenquan Cui, Haoyang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Considering the case where the response variable is a categorical variable
and the predictor is a random function, two novel functional sufficient
dimensional reduction (FSDR) methods are proposed based on mutual information
and square loss mutual information. Compared to the classical FSDR methods,
such as functional sliced inverse regression and functional sliced average
variance estimation, the proposed methods are appealing because they are
capable of estimating multiple effective dimension reduction directions in the
case of a relatively small number of categories, especially for the binary
response. Moreover, the proposed methods do not require the restrictive linear
conditional mean assumption and the constant covariance assumption. They avoid
the inverse problem of the covariance operator which is often encountered in
the functional sufficient dimension reduction. The functional principal
component analysis with truncation be used as a regularization mechanism. Under
some mild conditions, the statistical consistency of the proposed methods is
established. It is demonstrated that the two methods are competitive compared
with some existing FSDR methods by simulations and real data analyses.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Power of Preconditioning in Overparameterized Low-Rank Matrix
  Sensing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01186v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01186v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingyu Xu, Yandi Shen, Yuejie Chi, Cong Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose $\textsf{ScaledGD($\lambda$)}$, a preconditioned gradient descent
method to tackle the low-rank matrix sensing problem when the true rank is
unknown, and when the matrix is possibly ill-conditioned. Using
overparametrized factor representations, $\textsf{ScaledGD($\lambda$)}$ starts
from a small random initialization, and proceeds by gradient descent with a
specific form of damped preconditioning to combat bad curvatures induced by
overparameterization and ill-conditioning. At the expense of light
computational overhead incurred by preconditioners,
$\textsf{ScaledGD($\lambda$)}$ is remarkably robust to ill-conditioning
compared to vanilla gradient descent ($\textsf{GD}$) even with
overprameterization. Specifically, we show that, under the Gaussian design,
$\textsf{ScaledGD($\lambda$)}$ converges to the true low-rank matrix at a
constant linear rate after a small number of iterations that scales only
logarithmically with respect to the condition number and the problem dimension.
This significantly improves over the convergence rate of vanilla $\textsf{GD}$
which suffers from a polynomial dependency on the condition number. Our work
provides evidence on the power of preconditioning in accelerating the
convergence without hurting generalization in overparameterized learning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ o<span class="highlight-title">BERT</span>a: Improving Sparse Transfer Learning via improved initialization,
  distillation, and pruning regimes <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.17612v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.17612v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Campos, Alexandre Marques, Mark Kurtz, ChengXiang Zhai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we introduce the range of oBERTa language models, an
easy-to-use set of language models which allows Natural Language Processing
(NLP) practitioners to obtain between 3.8 and 24.3 times faster models without
expertise in model compression. Specifically, oBERTa extends existing work on
pruning, knowledge distillation, and quantization and leverages frozen
embeddings improves distillation and model initialization to deliver higher
accuracy on a broad range of transfer tasks. In generating oBERTa, we explore
how the highly optimized RoBERTa differs from the BERT for pruning during
pre-training and finetuning. We find it less amenable to compression during
fine-tuning. We explore the use of oBERTa on seven representative NLP tasks and
find that the improved compression techniques allow a pruned oBERTa model to
match the performance of BERTbase and exceed the performance of Prune OFA Large
on the SQUAD V1.1 Question Answering dataset, despite being 8x and 2x,
respectively faster in inference. We release our code, training regimes, and
associated model for broad usage to encourage usage and experimentation
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SustaiNLP2023 @ ACL 2023,9 pages, 2 figures, 45 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discovering New Interpretable Conservation Laws as Sparse Invariants 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19525v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19525v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Liu, Patrick Obin Sturm, Saketh Bharadwaj, Sam Silva, Max Tegmark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering conservation laws for a given dynamical system is important but
challenging. In a theorist setup (differential equations and basis functions
are both known), we propose the Sparse Invariant Detector (SID), an algorithm
that auto-discovers conservation laws from differential equations. Its
algorithmic simplicity allows robustness and interpretability of the discovered
conserved quantities. We show that SID is able to rediscover known and even
discover new conservation laws in a variety of systems. For two examples in
fluid mechanics and atmospheric chemistry, SID discovers 14 and 3 conserved
quantities, respectively, where only 12 and 2 were previously known to domain
experts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Optimally tackling covariate shift in RKHS-based nonparametric
  regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.02986v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.02986v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Cong Ma, Reese Pathak, Martin J. Wainwright
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the covariate shift problem in the context of nonparametric
regression over a reproducing kernel Hilbert space (RKHS). We focus on two
natural families of covariate shift problems defined using the likelihood
ratios between the source and target distributions. When the likelihood ratios
are uniformly bounded, we prove that the kernel ridge regression (KRR)
estimator with a carefully chosen regularization parameter is minimax
rate-optimal (up to a log factor) for a large family of RKHSs with regular
kernel eigenvalues. Interestingly, KRR does not require full knowledge of
likelihood ratios apart from an upper bound on them. In striking contrast to
the standard statistical setting without covariate shift, we also demonstrate
that a naive estimator, which minimizes the empirical risk over the function
class, is strictly sub-optimal under covariate shift as compared to KRR. We
then address the larger class of covariate shift problems where the likelihood
ratio is possibly unbounded yet has a finite second moment. Here, we propose a
reweighted KRR estimator that weights samples based on a careful truncation of
the likelihood ratios. Again, we are able to show that this estimator is
minimax rate-optimal, up to logarithmic factors.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to appear in the Annals of Statistics</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Communication-efficient Algorithm with Linear Convergence for
  Federated Minimax Learning <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.01132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.01132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhenyu Sun, Ermin Wei
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study a large-scale multi-agent minimax optimization
problem, which models many interesting applications in statistical learning and
game theory, including Generative Adversarial Networks (GANs). The overall
objective is a sum of agents' private local objective functions. We first
analyze an important special case, empirical minimax problem, where the overall
objective approximates a true population minimax risk by statistical samples.
We provide generalization bounds for learning with this objective through
Rademacher complexity analysis. Then, we focus on the federated setting, where
agents can perform local computation and communicate with a central server.
Most existing federated minimax algorithms either require communication per
iteration or lack performance guarantees with the exception of Local Stochastic
Gradient Descent Ascent (SGDA), a multiple-local-update descent ascent
algorithm which guarantees convergence under a diminishing stepsize. By
analyzing Local SGDA under the ideal condition of no gradient noise, we show
that generally it cannot guarantee exact convergence with constant stepsizes
and thus suffers from slow rates of convergence. To tackle this issue, we
propose FedGDA-GT, an improved Federated (Fed) Gradient Descent Ascent (GDA)
method based on Gradient Tracking (GT). When local objectives are Lipschitz
smooth and strongly-convex-strongly-concave, we prove that FedGDA-GT converges
linearly with a constant stepsize to global $\epsilon$-approximation solution
with $\mathcal{O}(\log (1/\epsilon))$ rounds of communication, which matches
the time complexity of centralized GDA method. Finally, we numerically show
that FedGDA-GT outperforms Local SGDA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by NeurIPS 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ewald-based Long-Range Message Passing for Molecular Graphs <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.04791v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.04791v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arthur Kosmala, Johannes Gasteiger, Nicholas Gao, Stephan Günnemann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural architectures that learn potential energy surfaces from molecular data
have undergone fast improvement in recent years. A key driver of this success
is the Message Passing Neural Network (MPNN) paradigm. Its favorable scaling
with system size partly relies upon a spatial distance limit on messages. While
this focus on locality is a useful inductive bias, it also impedes the learning
of long-range interactions such as electrostatics and van der Waals forces. To
address this drawback, we propose Ewald message passing: a nonlocal Fourier
space scheme which limits interactions via a cutoff on frequency instead of
distance, and is theoretically well-founded in the Ewald summation method. It
can serve as an augmentation on top of existing MPNN architectures as it is
computationally inexpensive and agnostic to architectural details. We test the
approach with four baseline models and two datasets containing diverse periodic
(OC20) and aperiodic structures (OE62). We observe robust improvements in
energy mean absolute errors across all models and datasets, averaging 10% on
OC20 and 16% on OE62. Our analysis shows an outsize impact of these
improvements on structures with high long-range contributions to the ground
truth energy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at the 40th International Conference on Machine Learning
  (ICML 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Bridging the Gap: Enhancing the Utility of Synthetic Data via
  Post-Processing Techniques 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10118v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10118v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Andrea Lampis, Eugenio Lomurno, Matteo Matteucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Acquiring and annotating suitable datasets for training deep learning models
is challenging. This often results in tedious and time-consuming efforts that
can hinder research progress. However, generative models have emerged as a
promising solution for generating synthetic datasets that can replace or
augment real-world data. Despite this, the effectiveness of synthetic data is
limited by their inability to fully capture the complexity and diversity of
real-world data. To address this issue, we explore the use of Generative
Adversarial Networks to generate synthetic datasets for training classifiers
that are subsequently evaluated on real-world images. To improve the quality
and diversity of the synthetic dataset, we propose three novel post-processing
techniques: Dynamic Sample Filtering, Dynamic Dataset Recycle, and Expansion
Trick. In addition, we introduce a pipeline called Gap Filler (GaFi), which
applies these techniques in an optimal and coordinated manner to maximise
classification accuracy on real-world data. Our experiments show that GaFi
effectively reduces the gap with real-accuracy scores to an error of 2.03%,
1.78%, and 3.99% on the Fashion-MNIST, CIFAR-10, and CIFAR-100 datasets,
respectively. These results represent a new state of the art in Classification
Accuracy Score and highlight the effectiveness of post-processing techniques in
improving the quality of synthetic datasets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prediction of Post-Operative Renal and Pulmonary Complications Using
  <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00698v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00698v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Reza Shirkavand, Fei Zhang, Heng Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Postoperative complications pose a significant challenge in the healthcare
industry, resulting in elevated healthcare expenses and prolonged hospital
stays, and in rare instances, patient mortality. To improve patient outcomes
and reduce healthcare costs, healthcare providers rely on various perioperative
risk scores to guide clinical decisions and prioritize care. In recent years,
machine learning techniques have shown promise in predicting postoperative
complications and fatality, with deep learning models achieving remarkable
success in healthcare applications. However, research on the application of
deep learning models to intra-operative anesthesia management data is limited.
In this paper, we evaluate the performance of transformer-based models in
predicting postoperative acute renal failure, postoperative pulmonary
complications, and postoperative in-hospital mortality. We compare our method's
performance with state-of-the-art tabular data prediction models, including
gradient boosting trees and sequential attention models, on a clinical dataset.
Our results demonstrate that transformer-based models can achieve superior
performance in predicting postoperative complications and outperform
traditional machine learning models. This work highlights the potential of deep
learning techniques, specifically transformer-based models, in revolutionizing
the healthcare industry's approach to postoperative care.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seeing is Believing: Brain-Inspired Modular Training for Mechanistic
  Interpretability 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08746v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08746v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziming Liu, Eric Gan, Max Tegmark
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Brain-Inspired Modular Training (BIMT), a method for making
neural networks more modular and interpretable. Inspired by brains, BIMT embeds
neurons in a geometric space and augments the loss function with a cost
proportional to the length of each neuron connection. We demonstrate that BIMT
discovers useful modular neural networks for many simple tasks, revealing
compositional structures in symbolic formulas, interpretable decision
boundaries and features for classification, and mathematical structure in
algorithmic datasets. The ability to directly see modules with the naked eye
can complement current mechanistic interpretability strategies such as probes,
interventions or staring at all weights.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Codes are available here: https://github.com/KindXiaoming/BIMT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Make Your <span class="highlight-title">Pre-train</span>ed Model Reversible: From Parameter to Memory
  Efficient Fine-Tuning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baohao Liao, Shaomu Tan, Christof Monz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient fine-tuning (PEFT) of pre-trained language models (PLMs)
has emerged as a highly successful approach, with training only a small number
of parameters without sacrificing performance and becoming the de-facto
learning paradigm with the increasing size of PLMs. However, existing PEFT
methods are not memory-efficient, because they still require caching most of
the intermediate activations for the gradient calculation, akin to fine-tuning.
One effective way to reduce the activation memory is to apply a reversible
model, so the intermediate activations are not necessary to be cached and can
be recomputed. Nevertheless, modifying a PLM to its reversible variant with
PEFT is not straightforward, since the reversible model has a distinct
architecture from the currently released PLMs. In this paper, we first
investigate what is a key factor for the success of existing PEFT methods, and
realize that it's essential to preserve the PLM's starting point when
initializing a PEFT method. With this finding, we propose memory-efficient
fine-tuning (MEFT) that inserts adapters into a PLM, preserving the PLM's
starting point and making it reversible without additional pre-training. We
evaluate MEFT on the GLUE benchmark and five question-answering tasks with
various backbones, BERT, RoBERTa, BART and OPT. MEFT significantly reduces the
activation memory up to 84% of full fine-tuning with a negligible amount of
trainable parameters. Moreover, MEFT achieves the same score on GLUE and a
comparable score on the question-answering tasks as full fine-tuning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at https://github.com/BaohaoLiao/mefts</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Bellman Errors for Offline Model Selection <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00141v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00141v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joshua P. Zitovsky, Daniel de Marchi, Rishabh Agarwal, Michael R. Kosorok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Offline model selection (OMS), that is, choosing the best policy from a set
of many policies given only logged data, is crucial for applying offline RL in
real-world settings. One idea that has been extensively explored is to select
policies based on the mean squared Bellman error (MSBE) of the associated
Q-functions. However, previous work has struggled to obtain adequate OMS
performance with Bellman errors, leading many researchers to abandon the idea.
To this end, we elucidate why previous work has seen pessimistic results with
Bellman errors and identify conditions under which OMS algorithms based on
Bellman errors will perform well. Moreover, we develop a new estimator of the
MSBE that is more accurate than prior methods. Our estimator obtains impressive
OMS performance on diverse discrete control tasks, including Atari games.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Certified Reinforcement Learning with Logic Guidance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1902.00778v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1902.00778v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hosein Hasanbeig, Daniel Kroening, Alessandro Abate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement Learning (RL) is a widely employed machine learning
architecture that has been applied to a variety of control problems. However,
applications in safety-critical domains require a systematic and formal
approach to specifying requirements as tasks or goals. We propose a model-free
RL algorithm that enables the use of Linear Temporal Logic (LTL) to formulate a
goal for unknown continuous-state/action Markov Decision Processes (MDPs). The
given LTL property is translated into a Limit-Deterministic Generalised Buchi
Automaton (LDGBA), which is then used to shape a synchronous reward function
on-the-fly. Under certain assumptions, the algorithm is guaranteed to
synthesise a control policy whose traces satisfy the LTL specification with
maximal probability.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Criteria Tell You More than Ratings: Criteria Preference-Aware Light
  Graph Convolution for Effective Multi-Criteria Recommendation <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18885v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18885v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jin-Duk Park, Siqing Li, Xin Cao, Won-Yong Shin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The multi-criteria (MC) recommender system, which leverages MC rating
information in a wide range of e-commerce areas, is ubiquitous nowadays.
Surprisingly, although graph neural networks (GNNs) have been widely applied to
develop various recommender systems due to GNN's high expressive capability in
learning graph representations, it has been still unexplored how to design MC
recommender systems with GNNs. In light of this, we make the first attempt
towards designing a GNN-aided MC recommender system. Specifically, rather than
straightforwardly adopting existing GNN-based recommendation methods, we devise
a novel criteria preference-aware light graph convolution CPA-LGC method, which
is capable of precisely capturing the criteria preference of users as well as
the collaborative signal in complex high-order connectivities. To this end, we
first construct an MC expansion graph that transforms user--item MC ratings
into an expanded bipartite graph to potentially learn from the collaborative
signal in MC ratings. Next, to strengthen the capability of criteria preference
awareness, CPA-LGC incorporates newly characterized embeddings, including
user-specific criteria-preference embeddings and item-specific criterion
embeddings, into our graph convolution model. Through comprehensive evaluations
using four real-world datasets, we demonstrate (a) the superiority over
benchmark MC recommendation methods and benchmark recommendation methods using
GNNs with tremendous gains, (b) the effectiveness of core components in
CPA-LGC, and (c) the computational efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 10 figures, 5 tables; 29th ACM SIGKDD Conference on
  Knowledge Discovery & Data (KDD 2023) (to appear) (Please cite our conference
  version.)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A <span class="highlight-title">Survey</span> of Learning on Small Data: Generalization, Optimization, and
  Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.14443v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.14443v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaofeng Cao, Weixin Bu, Shengjun Huang, Minling Zhang, Ivor W. Tsang, Yew Soon Ong, James T. Kwok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning on big data brings success for artificial intelligence (AI), but the
annotation and training costs are expensive. In future, learning on small data
that approximates the generalization ability of big data is one of the ultimate
purposes of AI, which requires machines to recognize objectives and scenarios
relying on small data as humans. A series of learning topics is going on this
way such as active learning and few-shot learning. However, there are few
theoretical guarantees for their generalization performance. Moreover, most of
their settings are passive, that is, the label distribution is explicitly
controlled by finite training resources from known distributions. This survey
follows the agnostic active sampling theory under a PAC (Probably Approximately
Correct) framework to analyze the generalization error and label complexity of
learning on small data in model-agnostic supervised and unsupervised fashion.
Considering multiple learning communities could produce small data
representation and related topics have been well surveyed, we thus subjoin
novel geometric representation perspectives for small data: the Euclidean and
non-Euclidean (hyperbolic) mean, where the optimization solutions including the
Euclidean gradients, non-Euclidean gradients, and Stein gradient are presented
and discussed. Later, multiple learning communities that may be improved by
learning on small data are summarized, which yield data-efficient
representations, such as transfer learning, contrastive learning, graph
representation learning. Meanwhile, we find that the meta-learning may provide
effective parameter update policies for learning on small data. Then, we
explore multiple challenging scenarios for small data, such as the weak
supervision and multi-label. Finally, multiple data applications that may
benefit from efficient small data representation are surveyed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Regions of Reliability in the Evaluation of Multivariate Probabilistic
  Forecasts <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.09836v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.09836v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Étienne Marcotte, Valentina Zantedeschi, Alexandre Drouin, Nicolas Chapados
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multivariate probabilistic time series forecasts are commonly evaluated via
proper scoring rules, i.e., functions that are minimal in expectation for the
ground-truth distribution. However, this property is not sufficient to
guarantee good discrimination in the non-asymptotic regime. In this paper, we
provide the first systematic finite-sample study of proper scoring rules for
time-series forecasting evaluation. Through a power analysis, we identify the
"region of reliability" of a scoring rule, i.e., the set of practical
conditions where it can be relied on to identify forecasting errors. We carry
out our analysis on a comprehensive synthetic benchmark, specifically designed
to test several key discrepancies between ground-truth and forecast
distributions, and we gauge the generalizability of our findings to real-world
tasks with an application to an electricity production problem. Our results
reveal critical shortcomings in the evaluation of multivariate probabilistic
forecasts as commonly performed in the literature.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>47 pages, 37 figures, camera-ready version, Fortieth International
  Conference on Machine Learning (ICML 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Faster Gradient-Free Algorithms for Nonsmooth Nonconvex Stochastic
  Optimization <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.06428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.06428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lesi Chen, Jing Xu, Luo Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the optimization problem of the form $\min_{x \in \mathbb{R}^d}
f(x) \triangleq \mathbb{E}_{\xi} [F(x; \xi)]$, where the component $F(x;\xi)$
is $L$-mean-squared Lipschitz but possibly nonconvex and nonsmooth. The
recently proposed gradient-free method requires at most $\mathcal{O}( L^4
d^{3/2} \epsilon^{-4} + \Delta L^3 d^{3/2} \delta^{-1} \epsilon^{-4})$
stochastic zeroth-order oracle complexity to find a
$(\delta,\epsilon)$-Goldstein stationary point of objective function, where
$\Delta = f(x_0) - \inf_{x \in \mathbb{R}^d} f(x)$ and $x_0$ is the initial
point of the algorithm. This paper proposes a more efficient algorithm using
stochastic recursive gradient estimators, which improves the complexity to
$\mathcal{O}(L^3 d^{3/2} \epsilon^{-3}+ \Delta L^2 d^{3/2} \delta^{-1}
\epsilon^{-3})$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Explanation-based Finetuning Makes Models More Robust to Spurious Cues 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04990v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04990v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Josh Magnus Ludan, Yixuan Meng, Tai Nguyen, Saurabh Shah, Qing Lyu, Marianna Apidianaki, Chris Callison-Burch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) are so powerful that they sometimes learn
correlations between labels and features that are irrelevant to the task,
leading to poor generalization on out-of-distribution data. We propose
explanation-based finetuning as a general approach to mitigate LLMs' reliance
on spurious correlations. Unlike standard finetuning where the model only
predicts the answer given the input, we finetune the model to additionally
generate a free-text explanation supporting its answer. To evaluate our method,
we finetune the model on artificially constructed training sets containing
different types of spurious cues, and test it on a test set without these cues.
Compared to standard finetuning, our method makes GPT-3 (davinci) remarkably
more robust against spurious cues in terms of accuracy drop across four
classification tasks: ComVE (+1.2), CREAK (+9.1), e-SNLI (+15.4), and SBIC
(+6.5). The efficacy generalizes across multiple model families and scales,
with greater gains for larger models. Finally, our method also works well with
explanations generated by the model, implying its applicability to more
datasets without human-written explanations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Domain Generalization for Mammographic Image Analysis via Contrastive
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10226v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10226v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheren Li, Zhiming Cui, Lichi Zhang, Sheng Wang, Chenjin Lei, Xi Ouyang, Dongdong Chen, Xiangyu Zhao, Yajia Gu, Zaiyi Liu, Chunling Liu, Dinggang Shen, Jie-Zhi Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The deep learning technique has been shown to be effective in addressing
several image analysis tasks within the computer-aided diagnosis scheme for
mammography. The training of an efficacious deep learning model requires large
amounts of data with sufficient diversity in terms of image style and quality.
In particular, the diversity of image styles may be primarily attributed to the
vendor factor. However, the collection of mammograms from large and diverse
vendors is very expensive and sometimes impractical. Motivatedly, a novel
contrastive learning method is developed to equip the deep learning models with
better generalization capability. Specifically, the multi-style and multi-view
unsupervised self-learning scheme is carried out to seek robust feature
embedding against various vendor styles as a pre-trained model. Afterward, the
pre-trained network is further fine-tuned to the downstream tasks, e.g., mass
detection, matching, BI-RADS rating, and breast density classification. The
proposed method has been extensively and rigorously evaluated with mammograms
from various vendor-style domains and several public datasets. The experimental
results suggest that the proposed domain generalization method can effectively
improve the performance of four mammographic image tasks on data from either
seen or unseen domains and outperform many state-of-the-art (SOTA)
generalization methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: text overlap with arXiv:2111.10827</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Machine Learned Calabi-Yau Metrics and Curvature 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09801v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09801v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Per Berglund, Giorgi Butbaia, Tristan Hübsch, Vishnu Jejjala, Damián Mayorga Peña, Challenger Mishra, Justin Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finding Ricci-flat (Calabi-Yau) metrics is a long standing problem in
geometry with deep implications for string theory and phenomenology. A new
attack on this problem uses neural networks to engineer approximations to the
Calabi-Yau metric within a given K\"ahler class. In this paper we investigate
numerical Ricci-flat metrics over smooth and singular K3 surfaces and
Calabi-Yau threefolds. Using these Ricci-flat metric approximations for the
Cefal\'u family of quartic twofolds and the Dwork family of quintic threefolds,
we study characteristic forms on these geometries. We observe that the
numerical stability of the numerically computed topological characteristic is
heavily influenced by the choice of the neural network model, in particular, we
briefly discuss a different neural network model, namely Spectral networks,
which correctly approximate the topological characteristic of a Calabi-Yau.
Using persistent homology, we show that high curvature regions of the manifolds
form clusters near the singular points. For our neural network approximations,
we observe a Bogomolov--Yau type inequality $3c_2 \geq c_1^2$ and observe an
identity when our geometries have isolated $A_1$ type singularities. We sketch
a proof that $\chi(X~\smallsetminus~\mathrm{Sing}\,{X}) +
2~|\mathrm{Sing}\,{X}| = 24$ also holds for our numerical approximations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Version accepted for publication: 48 pages, 32 figures, 8 tables, 3
  appendices</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ The Role of Relevance in Fair Ranking <span class="chip">SIGIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.05608v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.05608v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aparna Balagopalan, Abigail Z. Jacobs, Asia Biega
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Online platforms mediate access to opportunity: relevance-based rankings
create and constrain options by allocating exposure to job openings and job
candidates in hiring platforms, or sellers in a marketplace. In order to do so
responsibly, these socially consequential systems employ various fairness
measures and interventions, many of which seek to allocate exposure based on
worthiness. Because these constructs are typically not directly observable,
platforms must instead resort to using proxy scores such as relevance and infer
them from behavioral signals such as searcher clicks. Yet, it remains an open
question whether relevance fulfills its role as such a worthiness score in
high-stakes fair rankings. In this paper, we combine perspectives and tools
from the social sciences, information retrieval, and fairness in machine
learning to derive a set of desired criteria that relevance scores should
satisfy in order to meaningfully guide fairness interventions. We then
empirically show that not all of these criteria are met in a case study of
relevance inferred from biased user click data. We assess the impact of these
violations on the estimated system fairness and analyze whether existing
fairness interventions may mitigate the identified issues. Our analyses and
results surface the pressing need for new approaches to relevance collection
and generation that are suitable for use in fair ranking.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in SIGIR 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Responsible Design Patterns for Machine Learning Pipelines 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01788v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01788v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saud Hakem Al Harbi, Lionel Nganyewou Tidjon, Foutse Khomh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Integrating ethical practices into the AI development process for artificial
intelligence (AI) is essential to ensure safe, fair, and responsible operation.
AI ethics involves applying ethical principles to the entire life cycle of AI
systems. This is essential to mitigate potential risks and harms associated
with AI, such as algorithm biases. To achieve this goal, responsible design
patterns (RDPs) are critical for Machine Learning (ML) pipelines to guarantee
ethical and fair outcomes. In this paper, we propose a comprehensive framework
incorporating RDPs into ML pipelines to mitigate risks and ensure the ethical
development of AI systems. Our framework comprises new responsible AI design
patterns for ML pipelines identified through a survey of AI ethics and data
management experts and validated through real-world scenarios with expert
feedback. The framework guides AI developers, data scientists, and
policy-makers to implement ethical practices in AI development and deploy
responsible AI systems in production.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 4 figures, 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Transforming to Yoked Neural Networks to Improve ANN Structure 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02157v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02157v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xinshun Liu, Yizhi Fang, Yichao Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most existing classical artificial neural networks (ANN) are designed as a
tree structure to imitate neural networks. In this paper, we argue that the
connectivity of a tree is not sufficient to characterize a neural network. The
nodes of the same level of a tree cannot be connected with each other, i.e.,
these neural unit cannot share information with each other, which is a major
drawback of ANN. Although ANN has been significantly improved in recent years
to more complex structures, such as the directed acyclic graph (DAG), these
methods also have unidirectional and acyclic bias for ANN. In this paper, we
propose a method to build a bidirectional complete graph for the nodes in the
same level of an ANN, which yokes the nodes of the same level to formulate a
neural module. We call our model as YNN in short. YNN promotes the information
transfer significantly which obviously helps in improving the performance of
the method. Our YNN can imitate neural networks much better compared with the
traditional ANN. In this paper, we analyze the existing structural bias of ANN
and propose a model YNN to efficiently eliminate such structural bias. In our
model, nodes also carry out aggregation and transformation of features, and
edges determine the flow of information. We further impose auxiliary sparsity
constraint to the distribution of connectedness, which promotes the learned
structure to focus on critical connections. Finally, based on the optimized
structure, we also design small neural module structure based on the minimum
cut technique to reduce the computational burden of the YNN model. This
learning process is compatible with the existing networks and different tasks.
The obtained quantitative experimental results reflect that the learned
connectivity is superior to the traditional NN structure.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prototype-Sample Relation Distillation: Towards Replay-Free Continual
  Learning <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.14771v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.14771v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nader Asadi, MohammadReza Davari, Sudhir Mudur, Rahaf Aljundi, Eugene Belilovsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In Continual learning (CL) balancing effective adaptation while combating
catastrophic forgetting is a central challenge. Many of the recent
best-performing methods utilize various forms of prior task data, e.g. a replay
buffer, to tackle the catastrophic forgetting problem. Having access to
previous task data can be restrictive in many real-world scenarios, for example
when task data is sensitive or proprietary. To overcome the necessity of using
previous tasks' data, in this work, we start with strong representation
learning methods that have been shown to be less prone to forgetting. We
propose a holistic approach to jointly learn the representation and class
prototypes while maintaining the relevance of old class prototypes and their
embedded similarities. Specifically, samples are mapped to an embedding space
where the representations are learned using a supervised contrastive loss.
Class prototypes are evolved continually in the same latent space, enabling
learning and prediction at any point. To continually adapt the prototypes
without keeping any prior task data, we propose a novel distillation loss that
constrains class prototypes to maintain relative similarities as compared to
new task data. This method yields state-of-the-art performance in the
task-incremental setting, outperforming methods relying on large amounts of
data, and provides strong performance in the class-incremental setting without
using any stored data points.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Toward Efficient Gradient-Based Value Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.13757v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.13757v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Arsalan Sharifnassab, Richard Sutton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Gradient-based methods for value estimation in reinforcement learning have
favorable stability properties, but they are typically much slower than
Temporal Difference (TD) learning methods. We study the root causes of this
slowness and show that Mean Square Bellman Error (MSBE) is an ill-conditioned
loss function in the sense that its Hessian has large condition-number. To
resolve the adverse effect of poor conditioning of MSBE on gradient based
methods, we propose a low complexity batch-free proximal method that
approximately follows the Gauss-Newton direction and is asymptotically robust
to parameterization. Our main algorithm, called RANS, is efficient in the sense
that it is significantly faster than the residual gradient methods while having
almost the same computational complexity, and is competitive with TD on the
classic problems that we tested.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UnRectDepthNet: <span class="highlight-title">Self-Supervised</span> Monocular Depth Estimation using a
  Generic Framework for Handling Common Camera Distortion Models <span class="chip">IROS 2020</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2007.06676v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2007.06676v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Varun Ravi Kumar, Senthil Yogamani, Markus Bach, Christian Witt, Stefan Milz, Patrick Mader
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In classical computer vision, rectification is an integral part of multi-view
depth estimation. It typically includes epipolar rectification and lens
distortion correction. This process simplifies the depth estimation
significantly, and thus it has been adopted in CNN approaches. However,
rectification has several side effects, including a reduced field of view
(FOV), resampling distortion, and sensitivity to calibration errors. The
effects are particularly pronounced in case of significant distortion (e.g.,
wide-angle fisheye cameras). In this paper, we propose a generic scale-aware
self-supervised pipeline for estimating depth, euclidean distance, and visual
odometry from unrectified monocular videos. We demonstrate a similar level of
precision on the unrectified KITTI dataset with barrel distortion comparable to
the rectified KITTI dataset. The intuition being that the rectification step
can be implicitly absorbed within the CNN model, which learns the distortion
model without increasing complexity. Our approach does not suffer from a
reduced field of view and avoids computational costs for rectification at
inference time. To further illustrate the general applicability of the proposed
framework, we apply it to wide-angle fisheye cameras with 190$^\circ$
horizontal field of view. The training framework UnRectDepthNet takes in the
camera distortion model as an argument and adapts projection and unprojection
functions accordingly. The proposed algorithm is evaluated further on the KITTI
rectified dataset, and we achieve state-of-the-art results that improve upon
our previous work FisheyeDistanceNet. Qualitative results on a distorted test
scene video sequence indicate excellent performance
https://youtu.be/K6pbx3bU4Ss.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Minor fixes added after IROS 2020 Camera ready submission. IROS 2020
  presentation video - https://www.youtube.com/watch?v=3Br2KSWZRrY</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Generalized Alternating Method for Bilevel Learning under the
  Polyak-Łojasiewicz Condition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02422v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02422v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Xiao, Songtao Lu, Tianyi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilevel optimization has recently regained interest owing to its applications
in emerging machine learning fields such as hyperparameter optimization,
meta-learning, and reinforcement learning. Recent results have shown that
simple alternating (implicit) gradient-based algorithms can achieve the same
convergence rate of single-level gradient descent (GD) for bilevel problems
with a strongly convex lower-level objective. However, it remains unclear
whether this result can be generalized to bilevel problems beyond this basic
setting. In this paper, we propose a Generalized ALternating mEthod for bilevel
opTimization (GALET) with a nonconvex lower-level objective that satisfies the
Polyak-{\L}ojasiewicz (PL) condition. We first introduce a stationary metric
for the considered bilevel problems, which generalizes the existing metric. We
then establish that GALET achieves an $\epsilon$-stationary metric for the
considered problem within $\tilde{\cal O}(\epsilon^{-1})$ iterations, which
matches the iteration complexity of GD for smooth nonconvex problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03013v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03013v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kostadin Garov, Dimitar I. Dimitrov, Nikola Jovanović, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Malicious server (MS) attacks have enabled the scaling of data stealing in
federated learning to large batch sizes and secure aggregation, settings
previously considered private. However, many concerns regarding client-side
detectability of MS attacks were raised, questioning their practicality once
they are publicly known. In this work, for the first time, we thoroughly study
the problem of client-side detectability.We demonstrate that most prior MS
attacks, which fundamentally rely on one of two key principles, are detectable
by principled client-side checks. Further, we formulate desiderata for
practical MS attacks and propose SEER, a novel attack framework that satisfies
all desiderata, while stealing user data from gradients of realistic networks,
even for large batch sizes (up to 512 in our experiments) and under secure
aggregation. The key insight of SEER is the use of a secret decoder, which is
jointly trained with the shared model. Our work represents a promising first
step towards more principled treatment of MS attacks, paving the way for
realistic data stealing that can compromise user privacy in real-world
deployments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ No-Regret Caching via Online Mirror Descent 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2101.12588v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2101.12588v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        T. Si Salem, G. Neglia, S. Ioannidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study an online caching problem in which requests can be served by a local
cache to avoid retrieval costs from a remote server. The cache can update its
state after a batch of requests and store an arbitrarily small fraction of each
file. We study no-regret algorithms based on Online Mirror Descent (OMD)
strategies. We show that bounds for the regret crucially depend on the
diversity of the request process, provided by the diversity ratio R/h, where R
is the size of the batch, and h is the maximum multiplicity of a request in a
given batch. We characterize the optimality of OMD caching policies w.r.t.
regret under different diversity regimes. We also prove that, when the cache
must store the entire file, rather than a fraction, OMD strategies can be
coupled with a randomized rounding scheme that preserves regret guarantees,
even when update costs cannot be neglected. We provide a formal
characterization of the rounding problem through optimal transport theory, and
moreover we propose a computationally efficient randomized rounding scheme.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MERT: Acoustic Music Understanding Model with Large-Scale
  <span class="highlight-title">Self-supervised</span> Training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00107v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00107v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhi Li, Ruibin Yuan, Ge Zhang, Yinghao Ma, Xingran Chen, Hanzhi Yin, Chenghua Lin, Anton Ragni, Emmanouil Benetos, Norbert Gyenge, Roger Dannenberg, Ruibo Liu, Wenhu Chen, Gus Xia, Yemin Shi, Wenhao Huang, Yike Guo, Jie Fu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning (SSL) has recently emerged as a promising paradigm
for training generalisable models on large-scale data in the fields of vision,
text, and speech. Although SSL has been proven effective in speech and audio,
its application to music audio has yet to be thoroughly explored. This is
primarily due to the distinctive challenges associated with modelling musical
knowledge, particularly its tonal and pitched characteristics of music. To
address this research gap, we propose an acoustic Music undERstanding model
with large-scale self-supervised Training (MERT), which incorporates teacher
models to provide pseudo labels in the masked language modelling (MLM) style
acoustic pre-training. In our exploration, we identified a superior combination
of teacher models, which outperforms conventional speech and audio approaches
in terms of performance. This combination includes an acoustic teacher based on
Residual Vector Quantization - Variational AutoEncoder (RVQ-VAE) and a musical
teacher based on the Constant-Q Transform (CQT). These teachers effectively
guide our student model, a BERT-style transformer encoder, to better model
music audio. In addition, we introduce an in-batch noise mixture augmentation
to enhance the representation robustness. Furthermore, we explore a wide range
of settings to overcome the instability in acoustic language model
pre-training, which allows our designed paradigm to scale from 95M to 330M
parameters. Experimental results indicate that our model can generalise and
perform well on 14 music understanding tasks and attains state-of-the-art
(SOTA) overall scores. The code and models are online:
https://github.com/yizhilll/MERT.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ConCerNet: A Contrastive Learning Based Framework for Automated
  Conservation Law Discovery and Trustworthy Dynamical System Prediction <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05783v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05783v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wang Zhang, Tsui-Wei Weng, Subhro Das, Alexandre Megretski, Luca Daniel, Lam M. Nguyen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks (DNN) have shown great capacity of modeling a dynamical
system; nevertheless, they usually do not obey physics constraints such as
conservation laws. This paper proposes a new learning framework named ConCerNet
to improve the trustworthiness of the DNN based dynamics modeling to endow the
invariant properties. ConCerNet consists of two steps: (i) a contrastive
learning method to automatically capture the system invariants (i.e.
conservation properties) along the trajectory observations; (ii) a neural
projection layer to guarantee that the learned dynamics models preserve the
learned invariants. We theoretically prove the functional relationship between
the learned latent representation and the unknown system invariant function.
Experiments show that our method consistently outperforms the baseline neural
networks in both coordinate error and conservation metrics by a large margin.
With neural network based parameterization and no dependence on prior
knowledge, our method can be extended to complex and large-scale dynamics by
leveraging an autoencoder.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tier Balancing: Towards Dynamic Fairness over Underlying Causal Factors 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08987v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08987v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Tang, Yatong Chen, Yang Liu, Kun Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The pursuit of long-term fairness involves the interplay between
decision-making and the underlying data generating process. In this paper,
through causal modeling with a directed acyclic graph (DAG) on the
decision-distribution interplay, we investigate the possibility of achieving
long-term fairness from a dynamic perspective. We propose Tier Balancing, a
technically more challenging but more natural notion to achieve in the context
of long-term, dynamic fairness analysis. Different from previous fairness
notions that are defined purely on observed variables, our notion goes one step
further, capturing behind-the-scenes situation changes on the unobserved latent
causal factors that directly carry out the influence from the current decision
to the future data distribution. Under the specified dynamics, we prove that in
general one cannot achieve the long-term fairness goal only through one-step
interventions. Furthermore, in the effort of approaching long-term fairness, we
consider the mission of "getting closer to" the long-term fairness goal and
present possibility and impossibility results accordingly.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Block-wise Training of Residual Networks via the Minimizing Movement
  Scheme <span class="chip">AAAI 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.00949v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.00949v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Skander Karkar, Ibrahim Ayed, Emmanuel de Bézenac, Patrick Gallinari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  End-to-end backpropagation has a few shortcomings: it requires loading the
entire model during training, which can be impossible in constrained settings,
and suffers from three locking problems (forward locking, update locking and
backward locking), which prohibit training the layers in parallel. Solving
layer-wise optimization problems can address these problems and has been used
in on-device training of neural networks. We develop a layer-wise training
method, particularly welladapted to ResNets, inspired by the minimizing
movement scheme for gradient flows in distribution space. The method amounts to
a kinetic energy regularization of each block that makes the blocks optimal
transport maps and endows them with regularity. It works by alleviating the
stagnation problem observed in layer-wise training, whereby greedily-trained
early layers overfit and deeper layers stop increasing test accuracy after a
certain depth. We show on classification tasks that the test accuracy of
block-wise trained ResNets is improved when using our method, whether the
blocks are trained sequentially or in parallel.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1st International Workshop on Practical Deep Learning in the Wild at
  AAAI 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Provable Dynamic Fusion for Low-Quality Multimodal Data <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02050v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02050v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qingyang Zhang, Haitao Wu, Changqing Zhang, Qinghua Hu, Huazhu Fu, Joey Tianyi Zhou, Xi Peng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The inherent challenge of multimodal fusion is to precisely capture the
cross-modal correlation and flexibly conduct cross-modal interaction. To fully
release the value of each modality and mitigate the influence of low-quality
multimodal data, dynamic multimodal fusion emerges as a promising learning
paradigm. Despite its widespread use, theoretical justifications in this field
are still notably lacking. Can we design a provably robust multimodal fusion
method? This paper provides theoretical understandings to answer this question
under a most popular multimodal fusion framework from the generalization
perspective. We proceed to reveal that several uncertainty estimation solutions
are naturally available to achieve robust multimodal fusion. Then a novel
multimodal fusion framework termed Quality-aware Multimodal Fusion (QMF) is
proposed, which can improve the performance in terms of classification accuracy
and model robustness. Extensive experimental results on multiple benchmarks can
support our findings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to predict 3D rotational dynamics from images of a rigid body
  with unknown mass distribution 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.11355v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.11355v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justice Mason, Christine Allen-Blanchette, Nicholas Zolman, Elizabeth Davison, Naomi Leonard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many real-world settings, image observations of freely rotating 3D rigid
bodies, may be available when low-dimensional measurements are not. However,
the high-dimensionality of image data precludes the use of classical estimation
techniques to learn the dynamics. The usefulness of standard deep learning
methods is also limited because an image of a rigid body reveals nothing about
the distribution of mass inside the body, which, together with initial angular
velocity, is what determines how the body will rotate. We present a
physics-informed neural network model to estimate and predict 3D rotational
dynamics from image sequences. We achieve this using a multi-stage prediction
pipeline that maps individual images to a latent representation homeomorphic to
$\mathbf{SO}(3)$, computes angular velocities from latent pairs, and predicts
future latent states using the Hamiltonian equations of motion. We demonstrate
the efficacy of our approach on new rotating rigid-body datasets of sequences
of synthetic images of rotating objects, including cubes, prisms and
satellites, with unknown uniform and non-uniform mass distributions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fast Rates for Maximum Entropy Exploration <span class="chip">ICML-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.08059v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.08059v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniil Tiapkin, Denis Belomestny, Daniele Calandriello, Eric Moulines, Remi Munos, Alexey Naumov, Pierre Perrault, Yunhao Tang, Michal Valko, Pierre Menard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We address the challenge of exploration in reinforcement learning (RL) when
the agent operates in an unknown environment with sparse or no rewards. In this
work, we study the maximum entropy exploration problem of two different types.
The first type is visitation entropy maximization previously considered by
Hazan et al.(2019) in the discounted setting. For this type of exploration, we
propose a game-theoretic algorithm that has
$\widetilde{\mathcal{O}}(H^3S^2A/\varepsilon^2)$ sample complexity thus
improving the $\varepsilon$-dependence upon existing results, where $S$ is a
number of states, $A$ is a number of actions, $H$ is an episode length, and
$\varepsilon$ is a desired accuracy. The second type of entropy we study is the
trajectory entropy. This objective function is closely related to the
entropy-regularized MDPs, and we propose a simple algorithm that has a sample
complexity of order
$\widetilde{\mathcal{O}}(\mathrm{poly}(S,A,H)/\varepsilon)$. Interestingly, it
is the first theoretical result in RL literature that establishes the potential
statistical advantage of regularized MDPs for exploration. Finally, we apply
developed regularization techniques to reduce sample complexity of visitation
entropy maximization to $\widetilde{\mathcal{O}}(H^2SA/\varepsilon^2)$,
yielding a statistical separation between maximum entropy exploration and
reward-free exploration.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML-2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ From Random Search to Bandit Learning in Metric Measure Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11509v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11509v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chuying Han, Yasong Feng, Tianyu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Random Search is one of the most widely-used method for Hyperparameter
Optimization, and is critical to the success of deep learning models. Despite
its astonishing performance, little non-heuristic theory has been developed to
describe the underlying working mechanism. This paper gives a theoretical
accounting of Random Search. We introduce the concept of \emph{scattering
dimension} that describes the landscape of the underlying function, and
quantifies the performance of random search. We show that, when the environment
is noise-free, the output of random search converges to the optimal value in
probability at rate $ \widetilde{\mathcal{O}} \left( \left( \frac{1}{T}
\right)^{ \frac{1}{d_s} } \right) $, where $ d_s \ge 0 $ is the scattering
dimension of the underlying function. When the observed function values are
corrupted by bounded $iid$ noise, the output of random search converges to the
optimal value in probability at rate $ \widetilde{\mathcal{O}} \left( \left(
\frac{1}{T} \right)^{ \frac{1}{d_s + 1} } \right) $. In addition, based on the
principles of random search, we introduce an algorithm, called BLiN-MOS, for
Lipschitz bandits in doubling metric spaces that are also endowed with a Borel
measure, and show that BLiN-MOS achieves a regret rate of order $
\widetilde{\mathcal{O}} \left( T^{ \frac{d_z}{d_z + 1} } \right) $, where $d_z$
is the zooming dimension of the problem instance. Our results show that under
certain conditions, the known information-theoretical lower bounds for
Lipschitz bandits $\Omega \left( T^{\frac{d_z+1}{d_z+2}} \right)$ can be
improved.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Fisher Information Embedding for Node and Graph Learning <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07580v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07580v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dexiong Chen, Paolo Pellizzoni, Karsten Borgwardt
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention-based graph neural networks (GNNs), such as graph attention
networks (GATs), have become popular neural architectures for processing
graph-structured data and learning node embeddings. Despite their empirical
success, these models rely on labeled data and the theoretical properties of
these models have yet to be fully understood. In this work, we propose a novel
attention-based node embedding framework for graphs. Our framework builds upon
a hierarchical kernel for multisets of subgraphs around nodes (e.g.
neighborhoods) and each kernel leverages the geometry of a smooth statistical
manifold to compare pairs of multisets, by "projecting" the multisets onto the
manifold. By explicitly computing node embeddings with a manifold of Gaussian
mixtures, our method leads to a new attention mechanism for neighborhood
aggregation. We provide theoretical insights into generalizability and
expressivity of our embeddings, contributing to a deeper understanding of
attention-based GNNs. We propose both efficient unsupervised and supervised
methods for learning the embeddings. Through experiments on several node
classification benchmarks, we demonstrate that our proposed method outperforms
existing attention-based graph models like GATs. Our code is available at
https://github.com/BorgwardtLab/fisher_information_embedding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Gaussian Mixture Representations for Tensor Time Series
  Forecasting <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00390v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00390v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiewen Deng, Jinliang Deng, Renhe Jiang, Xuan Song
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Tensor time series (TTS) data, a generalization of one-dimensional time
series on a high-dimensional space, is ubiquitous in real-world scenarios,
especially in monitoring systems involving multi-source spatio-temporal data
(e.g., transportation demands and air pollutants). Compared to modeling time
series or multivariate time series, which has received much attention and
achieved tremendous progress in recent years, tensor time series has been paid
less effort. Properly coping with the tensor time series is a much more
challenging task, due to its high-dimensional and complex inner structure. In
this paper, we develop a novel TTS forecasting framework, which seeks to
individually model each heterogeneity component implied in the time, the
location, and the source variables. We name this framework as GMRL, short for
Gaussian Mixture Representation Learning. Experiment results on two real-world
TTS datasets verify the superiority of our approach compared with the
state-of-the-art baselines. Code and data are published on
https://github.com/beginner-sketch/GMRL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJCAI 2023 Main Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Aligning Language Models with Preferences through f-divergence
  Minimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.08215v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.08215v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongyoung Go, Tomasz Korbak, Germán Kruszewski, Jos Rozen, Nahyeon Ryu, Marc Dymetman
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Aligning language models with preferences can be posed as approximating a
target distribution representing some desired behavior. Existing approaches
differ both in the functional form of the target distribution and the algorithm
used to approximate it. For instance, Reinforcement Learning from Human
Feedback (RLHF) corresponds to minimizing a reverse KL from an implicit target
distribution arising from a KL penalty in the objective. On the other hand,
Generative Distributional Control (GDC) has an explicit target distribution and
minimizes a forward KL from it using the Distributional Policy Gradient (DPG)
algorithm. In this paper, we propose a new approach, f-DPG, which allows the
use of any f-divergence to approximate any target distribution that can be
evaluated. f-DPG unifies both frameworks (RLHF, GDC) and the approximation
methods (DPG, RL with KL penalties). We show the practical benefits of various
choices of divergence objectives and demonstrate that there is no universally
optimal objective but that different divergences present different alignment
and diversity trade-offs. We show that Jensen-Shannon divergence strikes a good
balance between these objectives, and frequently outperforms forward KL
divergence by a wide margin, leading to significant improvements over prior
work. These distinguishing characteristics between divergences persist as the
model size increases, highlighting the importance of selecting appropriate
divergence objectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Homomorphism Autoencoder -- Learning Group Structured Representations
  from Observed Transitions <span class="chip">ICML2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2207.12067v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2207.12067v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hamza Keurti, Hsiao-Ru Pan, Michel Besserve, Benjamin F. Grewe, Bernhard Schölkopf
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  How can agents learn internal models that veridically represent interactions
with the real world is a largely open question. As machine learning is moving
towards representations containing not just observational but also
interventional knowledge, we study this problem using tools from representation
learning and group theory. We propose methods enabling an agent acting upon the
world to learn internal representations of sensory information that are
consistent with actions that modify it. We use an autoencoder equipped with a
group representation acting on its latent space, trained using an
equivariance-derived loss in order to enforce a suitable homomorphism property
on the group representation. In contrast to existing work, our approach does
not require prior knowledge of the group and does not restrict the set of
actions the agent can perform. We motivate our method theoretically, and show
empirically that it can learn a group representation of the actions, thereby
capturing the structure of the set of transformations applied to the
environment. We further show that this allows agents to predict the effect of
sequences of future actions with improved accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML2023, 26 pages, 17 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Distribution-Free Matrix Prediction Under Arbitrary Missing Pattern 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.11640v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.11640v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Meijia Shao, Yuan Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper studies the open problem of conformalized entry prediction in a
row/column-exchangeable matrix. The matrix setting presents novel and unique
challenges, but there exists little work on this interesting topic. We
meticulously define the problem, differentiate it from closely related
problems, and rigorously delineate the boundary between achievable and
impossible goals. We then propose two practical algorithms. The first method
provides a fast emulation of the full conformal prediction, while the second
method leverages the technique of algorithmic stability for acceleration. Both
methods are computationally efficient and can effectively safeguard coverage
validity in presence of arbitrary missing pattern. Further, we quantify the
impact of missingness on prediction accuracy and establish fundamental limit
results. Empirical evidence from synthetic and real-world data sets
corroborates the superior performance of our proposed methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unification Framework for Euclidean and Hyperbolic Graph Neural
  Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04285v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04285v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehrdad Khatir, Nurendra Choudhary, Sutanay Choudhury, Khushbu Agarwal, Chandan K. Reddy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Hyperbolic neural networks can effectively capture the inherent hierarchy of
graph datasets, and consequently a powerful choice of GNNs. However, they
entangle multiple incongruent (gyro-)vector spaces within a layer, which makes
them limited in terms of generalization and scalability. In this work, we
propose the Poincare disk model as our search space, and apply all
approximations on the disk (as if the disk is a tangent space derived from the
origin), thus getting rid of all inter-space transformations. Such an approach
enables us to propose a hyperbolic normalization layer and to further simplify
the entire hyperbolic model to a Euclidean model cascaded with our hyperbolic
normalization layer. We applied our proposed nonlinear hyperbolic normalization
to the current state-of-the-art homogeneous and multi-relational graph
networks. We demonstrate that our model not only leverages the power of
Euclidean networks such as interpretability and efficient execution of various
model components, but also outperforms both Euclidean and hyperbolic
counterparts on various benchmarks. Our code is made publicly available at
https://github.com/oom-debugger/ijcai23.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An Evidential Real-Time Multi-Mode Fault Diagnosis Approach Based on
  Broad Learning System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00169v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00169v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Li, Zeyi Liu, Limin Wang, Minyue Li, Xiao He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fault diagnosis is a crucial area of research in industry. Industrial
processes exhibit diverse operating conditions, where data often have
non-Gaussian, multi-mode, and center-drift characteristics. Data-driven
approaches are currently the main focus in the field, but continuous fault
classification and parameter updates of fault classifiers pose challenges for
multiple operating modes and real-time settings. Thus, a pressing issue is to
achieve real-time multi-mode fault diagnosis in industrial systems. In this
paper, a novel approach to achieve real-time multi-mode fault diagnosis is
proposed for industrial applications, which addresses this critical research
problem. Our approach uses an extended evidence reasoning (ER) algorithm to
fuse information and merge outputs from different base classifiers. These base
classifiers based on broad learning system (BLS) are trained to ensure maximum
fault diagnosis accuracy. Furthermore, pseudo-label learning is used to update
model parameters in real-time. The effectiveness of the proposed approach is
demonstrated on the multi-mode Tennessee Eastman process dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 11 figures, Accepted by the 34th Chinese Process Control
  Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rigid body flows for sampling molecular crystal structures 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11355v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11355v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jonas Köhler, Michele Invernizzi, Pim de Haan, Frank Noé
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Normalizing flows (NF) are a class of powerful generative models that have
gained popularity in recent years due to their ability to model complex
distributions with high flexibility and expressiveness. In this work, we
introduce a new type of normalizing flow that is tailored for modeling
positions and orientations of multiple objects in three-dimensional space, such
as molecules in a crystal. Our approach is based on two key ideas: first, we
define smooth and expressive flows on the group of unit quaternions, which
allows us to capture the continuous rotational motion of rigid bodies; second,
we use the double cover property of unit quaternions to define a proper density
on the rotation group. This ensures that our model can be trained using
standard likelihood-based methods or variational inference with respect to a
thermodynamic target density. We evaluate the method by training Boltzmann
generators for two molecular examples, namely the multi-modal density of a
tetrahedral system in an external field and the ice XI phase in the TIP4P water
model. Our flows can be combined with flows operating on the internal degrees
of freedom of molecules, and constitute an important step towards the modeling
of distributions of many interacting molecules.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Can In-context Learners Learn a Reasoning Concept from Demonstrations? <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01692v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01692v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michal Štefánik, Marek Kadlčík
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models show an emergent ability to learn a new task from a
small number of input-output demonstrations. However, recent work shows that
in-context learners largely rely on their pre-trained knowledge, such as the
sentiment of the labels, instead of finding new associations in the input.
However, the commonly-used few-shot evaluation settings using a random
selection of in-context demonstrations can not disentangle models' ability to
learn a new skill from demonstrations, as most of the randomly-selected
demonstrations do not present relations informative for prediction beyond
exposing the new task distribution.
  To disentangle models' in-context learning ability independent of models'
memory, we introduce a Conceptual few-shot learning method selecting the
demonstrations sharing a possibly-informative concept with the predicted
sample. We extract a set of such concepts from annotated explanations and
measure how much can models benefit from presenting these concepts in few-shot
demonstrations.
  We find that smaller models are more sensitive to the presented concepts.
While some of the models are able to benefit from concept-presenting
demonstrations for each assessed concept, we find that none of the assessed
in-context learners can benefit from all presented reasoning concepts
consistently, leaving the in-context concept learning an open challenge.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2023 Natural Language Reasoning workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Predicting malaria dynamics in Burundi using deep Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02685v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02685v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daxelle Sakubu, Kelly Joelle Gatore Sinigirira, David Niyukuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Malaria continues to be a major public health problem on the African
continent, particularly in Sub-Saharan Africa. Nonetheless, efforts are
ongoing, and significant progress has been made. In Burundi, malaria is among
the main public health concerns. In the literature, there are limited
prediction models for Burundi. We know that such tools are much needed for
interventions design. In our study, we built machine-learning based models to
estimates malaria cases in Burundi. The forecast of malaria cases was carried
out at province level and national scale as well. Long short term memory (LSTM)
model, a type of deep learning model has been used to achieve best results
using climate-change related factors such as temperature, rainfal, and relative
humidity, together with malaria historical data and human population. With this
model, the results showed that at country level different tuning of parameters
can be used in order to determine the minimum and maximum expected malaria
cases. The univariate version of that model (LSTM) which learns from previous
dynamics of malaria cases give more precise estimates at province-level, but
both models have same trends overall at provnce-level and country-level
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe AI for health and beyond -- Monitoring to transform a health
  service 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.01513v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.01513v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mahed Abroshan, Michael Burkhart, Oscar Giles, Sam Greenbury, Zoe Kourtzi, Jack Roberts, Mihaela van der Schaar, Jannetta S Steyn, Alan Wilson, May Yong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine learning techniques are effective for building predictive models
because they identify patterns in large datasets. Development of a model for
complex real-life problems often stop at the point of publication, proof of
concept or when made accessible through some mode of deployment. However, a
model in the medical domain risks becoming obsolete as patient demographics,
systems and clinical practices change. The maintenance and monitoring of
predictive model performance post-publication is crucial to enable their safe
and effective long-term use. We will assess the infrastructure required to
monitor the outputs of a machine learning algorithm, and present two scenarios
with examples of monitoring and updates of models, firstly on a breast cancer
prognosis model trained on public longitudinal data, and secondly on a
neurodegenerative stratification algorithm that is currently being developed
and tested in clinic.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph2topic: an opensource topic modeling framework based on sentence
  embedding and community detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06653v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06653v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Leihang Zhang, Jiapeng Liu, Qiang Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has been reported that clustering-based topic models, which cluster
high-quality sentence embeddings with an appropriate word selection method, can
generate better topics than generative probabilistic topic models. However,
these approaches suffer from the inability to select appropriate parameters and
incomplete models that overlook the quantitative relation between words with
topics and topics with text. To solve these issues, we propose graph to topic
(G2T), a simple but effective framework for topic modelling. The framework is
composed of four modules. First, document representation is acquired using
pretrained language models. Second, a semantic graph is constructed according
to the similarity between document representations. Third, communities in
document semantic graphs are identified, and the relationship between topics
and documents is quantified accordingly. Fourth, the word--topic distribution
is computed based on a variant of TFIDF. Automatic evaluation suggests that G2T
achieved state-of-the-art performance on both English and Chinese documents
with different lengths.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Diffusion Models and Semi-Supervised Learners Benefit Mutually with Few
  Labels 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.10586v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.10586v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zebin You, Yong Zhong, Fan Bao, Jiacheng Sun, Chongxuan Li, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In an effort to further advance semi-supervised generative and classification
tasks, we propose a simple yet effective training strategy called dual pseudo
training (DPT), built upon strong semi-supervised learners and diffusion
models. DPT operates in three stages: training a classifier on partially
labeled data to predict pseudo-labels; training a conditional generative model
using these pseudo-labels to generate pseudo images; and retraining the
classifier with a mix of real and pseudo images. Empirically, DPT consistently
achieves SOTA performance of semi-supervised generation and classification
across various settings. In particular, with one or two labels per class, DPT
achieves a Fr\'echet Inception Distance (FID) score of 3.08 or 2.52 on ImageNet
256x256, surpassing strong diffusion models with full labels, such as IDDPM,
CDM, ADM, and LDM. Besides, DPT outperforms competitive semi-supervised
baselines substantially on ImageNet classification tasks, achieving top-1
accuracies of 59.0 (+2.8), 69.5 (+3.0), and 74.4 (+2.0) with one, two, or five
labels per class, respectively. Notably, our results demonstrate that diffusion
can generate realistic images with only a few labels (e.g., <0.1%) and
generative augmentation remains viable for semi-supervised classification.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topological Data Analysis for Speech Processing <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.17223v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.17223v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduard Tulchinskii, Kristian Kuznetsov, Laida Kushnareva, Daniil Cherniavskii, Serguei Barannikov, Irina Piontkovskaya, Sergey Nikolenko, Evgeny Burnaev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We apply topological data analysis (TDA) to speech classification problems
and to the introspection of a pretrained speech model, HuBERT. To this end, we
introduce a number of topological and algebraic features derived from
Transformer attention maps and embeddings. We show that a simple linear
classifier built on top of such features outperforms a fine-tuned
classification head. In particular, we achieve an improvement of about $9\%$
accuracy and $5\%$ ERR on four common datasets; on CREMA-D, the proposed
feature set reaches a new state of the art performance with accuracy $80.155$.
We also show that topological features are able to reveal functional roles of
speech Transformer heads; e.g., we find the heads capable to distinguish
between pairs of sample sources (natural/synthetic) or voices without any
downstream fine-tuning. Our results demonstrate that TDA is a promising new
approach for speech analysis, especially for tasks that require structural
prediction. Appendices, an introduction to TDA, and other additional materials
are available here - https://topohubert.github.io/speech-topology-webpages/
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to INTERSPEECH 2023 conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SELTO: Sample-Efficient Learned Topology Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.05098v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.05098v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sören Dittmer, David Erzmann, Henrik Harms, Peter Maass
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent developments in Deep Learning (DL) suggest a vast potential for
Topology Optimization (TO). However, while there are some promising attempts,
the subfield still lacks a firm footing regarding basic methods and datasets.
We aim to address both points. First, we explore physics-based preprocessing
and equivariant networks to create sample-efficient components for TO DL
pipelines. We evaluate them in a large-scale ablation study using end-to-end
supervised training. The results demonstrate a drastic improvement in sample
efficiency and the predictions' physical correctness. Second, to improve
comparability and future progress, we publish the two first TO datasets
containing problems and corresponding ground truth solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 10 figures, submitted to the International Journal for
  Numerical Methods in Engineering</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Communication-Constrained Bandits under Additive Gaussian Noise 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.12680v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.12680v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Prathamesh Mayekar, Jonathan Scarlett, Vincent Y. F. Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study a distributed stochastic multi-armed bandit where a client supplies
the learner with communication-constrained feedback based on the rewards for
the corresponding arm pulls. In our setup, the client must encode the rewards
such that the second moment of the encoded rewards is no more than $P$, and
this encoded reward is further corrupted by additive Gaussian noise of variance
$\sigma^2$; the learner only has access to this corrupted reward. For this
setting, we derive an information-theoretic lower bound of
$\Omega\left(\sqrt{\frac{KT}{\mathtt{SNR} \wedge1}} \right)$ on the minimax
regret of any scheme, where $ \mathtt{SNR} := \frac{P}{\sigma^2}$, and $K$ and
$T$ are the number of arms and time horizon, respectively. Furthermore, we
propose a multi-phase bandit algorithm, $\mathtt{UE\text{-}UCB++}$, which
matches this lower bound to a minor additive factor. $\mathtt{UE\text{-}UCB++}$
performs uniform exploration in its initial phases and then utilizes the {\em
upper confidence bound }(UCB) bandit algorithm in its final phase. An
interesting feature of $\mathtt{UE\text{-}UCB++}$ is that the coarser estimates
of the mean rewards formed during a uniform exploration phase help to refine
the encoding protocol in the next phase, leading to more accurate mean
estimates of the rewards in the subsequent phase. This positive reinforcement
cycle is critical to reducing the number of uniform exploration rounds and
closely matching our lower bound.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Variational formulations of ODE-Net as a mean-field optimal control
  problem and existence results 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.05924v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.05924v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Noboru Isobe, Mizuho Okumura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a mathematical analysis of ODE-Net, a continuum model of
deep neural networks (DNNs). In recent years, Machine Learning researchers have
introduced ideas of replacing the deep structure of DNNs with ODEs as a
continuum limit. These studies regard the "learning" of ODE-Net as the
minimization of a "loss" constrained by a parametric ODE. Although the
existence of a minimizer for this minimization problem needs to be assumed,
only a few studies have investigated its existence analytically in detail. In
the present paper, the existence of a minimizer is discussed based on a
formulation of ODE-Net as a measure-theoretic mean-field optimal control
problem. The existence result is proved when a neural network, which describes
a vector field of ODE-Net, is linear with respect to learnable parameters. The
proof employs the measure-theoretic formulation combined with the direct method
of Calculus of Variations. Secondly, an idealized minimization problem is
proposed to remove the above linearity assumption. Such a problem is inspired
by a kinetic regularization associated with the Benamou--Brenier formula and
universal approximation theorems for neural networks. The proofs of these
existence results use variational methods, differential equations, and
mean-field optimal control theory. They will stand for a new analytic way to
investigate the learning process of deep neural networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PlaNeRF: SVD Unsupervised 3D Plane Regularization for NeRF Large-Scale
  Scene Reconstruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16914v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16914v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fusang Wang, Arnaud Louys, Nathan Piasco, Moussab Bennehar, Luis Roldão, Dzmitry Tsishkou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural Radiance Fields (NeRF) enable 3D scene reconstruction from 2D images
and camera poses for Novel View Synthesis (NVS). Although NeRF can produce
photorealistic results, it often suffers from overfitting to training views,
leading to poor geometry reconstruction, especially in low-texture areas. This
limitation restricts many important applications which require accurate
geometry, such as extrapolated NVS, HD mapping and scene editing. To address
this limitation, we propose a new method to improve NeRF's 3D structure using
only RGB images and semantic maps. Our approach introduces a novel plane
regularization based on Singular Value Decomposition (SVD), that does not rely
on any geometric prior. In addition, we leverage the Structural Similarity
Index Measure (SSIM) in our loss design to properly initialize the volumetric
representation of NeRF. Quantitative and qualitative results show that our
method outperforms popular regularization approaches in accurate geometry
reconstruction for large-scale outdoor scenes and achieves SoTA rendering
quality on the KITTI-360 NVS benchmark.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I Know What You Trained Last Summer: A <span class="highlight-title">Survey</span> on Stealing Machine
  Learning Models and Defences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.08451v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.08451v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daryna Oliynyk, Rudolf Mayer, Andreas Rauber
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Machine Learning-as-a-Service (MLaaS) has become a widespread paradigm,
making even the most complex machine learning models available for clients via
e.g. a pay-per-query principle. This allows users to avoid time-consuming
processes of data collection, hyperparameter tuning, and model training.
However, by giving their customers access to the (predictions of their) models,
MLaaS providers endanger their intellectual property, such as sensitive
training data, optimised hyperparameters, or learned model parameters.
Adversaries can create a copy of the model with (almost) identical behavior
using the the prediction labels only. While many variants of this attack have
been described, only scattered defence strategies have been proposed,
addressing isolated threats. This raises the necessity for a thorough
systematisation of the field of model stealing, to arrive at a comprehensive
understanding why these attacks are successful, and how they could be
holistically defended against. We address this by categorising and comparing
model stealing attacks, assessing their performance, and exploring
corresponding defence techniques in different settings. We propose a taxonomy
for attack and defence approaches, and provide guidelines on how to select the
right attack or defence strategy based on the goal and available resources.
Finally, we analyse which defences are rendered less effective by current
attack strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACM Computing Surveys, 2023:
  https://doi.org/10.1145/3595292</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy
  Actor-Critic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02865v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02865v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianying Ji, Yu Luo, Fuchun Sun, Xianyuan Zhan, Jianwei Zhang, Huazhe Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning high-quality Q-value functions plays a key role in the success of
many modern off-policy deep reinforcement learning (RL) algorithms. Previous
works focus on addressing the value overestimation issue, an outcome of
adopting function approximators and off-policy learning. Deviating from the
common viewpoint, we observe that Q-values are indeed underestimated in the
latter stage of the RL training process, primarily related to the use of
inferior actions from the current policy in Bellman updates as compared to the
more optimal action samples in the replay buffer. We hypothesize that this
long-neglected phenomenon potentially hinders policy learning and reduces
sample efficiency. Our insight to address this issue is to incorporate
sufficient exploitation of past successes while maintaining exploration
optimism. We propose the Blended Exploitation and Exploration (BEE) operator, a
simple yet effective approach that updates Q-value using both historical
best-performing actions and the current policy. The instantiations of our
method in both model-free and model-based settings outperform state-of-the-art
methods in various continuous control tasks and achieve strong performance in
failure-prone scenarios and real-world robot tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Denise: Deep Robust Principal Component Analysis for Positive
  Semidefinite Matrices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2004.13612v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2004.13612v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Calypso Herrera, Florian Krach, Anastasis Kratsios, Pierre Ruyssen, Josef Teichmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The robust PCA of covariance matrices plays an essential role when isolating
key explanatory features. The currently available methods for performing such a
low-rank plus sparse decomposition are matrix specific, meaning, those
algorithms must re-run for every new matrix. Since these algorithms are
computationally expensive, it is preferable to learn and store a function that
nearly instantaneously performs this decomposition when evaluated. Therefore,
we introduce Denise, a deep learning-based algorithm for robust PCA of
covariance matrices, or more generally, of symmetric positive semidefinite
matrices, which learns precisely such a function. Theoretical guarantees for
Denise are provided. These include a novel universal approximation theorem
adapted to our geometric deep learning problem and convergence to an optimal
solution to the learning problem. Our experiments show that Denise matches
state-of-the-art performance in terms of decomposition quality, while being
approximately $2000\times$ faster than the state-of-the-art, principal
component pursuit (PCP), and $200 \times$ faster than the current
speed-optimized method, fast PCP.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ArrayFlex: A Systolic Array Architecture with Configurable Transparent
  Pipelining <span class="chip">DATE 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12600v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12600v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        C. Peltekis, D. Filippas, G. Dimitrakopoulos, C. Nicopoulos, D. Pnevmatikatos
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Convolutional Neural Networks (CNNs) are the state-of-the-art solution for
many deep learning applications. For maximum scalability, their computation
should combine high performance and energy efficiency. In practice, the
convolutions of each CNN layer are mapped to a matrix multiplication that
includes all input features and kernels of each layer and is computed using a
systolic array. In this work, we focus on the design of a systolic array with
configurable pipeline with the goal to select an optimal pipeline configuration
for each CNN layer. The proposed systolic array, called ArrayFlex, can operate
in normal, or in shallow pipeline mode, thus balancing the execution time in
cycles and the operating clock frequency. By selecting the appropriate pipeline
configuration per CNN layer, ArrayFlex reduces the inference latency of
state-of-the-art CNNs by 11%, on average, as compared to a traditional
fixed-pipeline systolic array. Most importantly, this result is achieved while
using 13%-23% less power, for the same applications, thus offering a combined
energy-delay-product efficiency between 1.4x and 1.8x.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>DATE 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In Search of Insights, Not Magic Bullets: Towards Demystification of the
  Model Selection Dilemma in Heterogeneous Treatment Effect Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02923v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02923v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alicia Curth, Mihaela van der Schaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized treatment effect estimates are often of interest in high-stakes
applications -- thus, before deploying a model estimating such effects in
practice, one needs to be sure that the best candidate from the ever-growing
machine learning toolbox for this task was chosen. Unfortunately, due to the
absence of counterfactual information in practice, it is usually not possible
to rely on standard validation metrics for doing so, leading to a well-known
model selection dilemma in the treatment effect estimation literature. While
some solutions have recently been investigated, systematic understanding of the
strengths and weaknesses of different model selection criteria is still
lacking. In this paper, instead of attempting to declare a global `winner', we
therefore empirically investigate success- and failure modes of different
selection criteria. We highlight that there is a complex interplay between
selection strategies, candidate estimators and the data used for comparing
them, and provide interesting insights into the relative (dis)advantages of
different criteria alongside desiderata for the design of further illuminating
empirical studies in this context.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Proceedings of the 40th International Conference on
  Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Kernel-Based View of Language Model Fine-Tuning <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.05643v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.05643v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sadhika Malladi, Alexander Wettig, Dingli Yu, Danqi Chen, Sanjeev Arora
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  It has become standard to solve NLP tasks by fine-tuning pre-trained language
models (LMs), especially in low-data settings. There is minimal theoretical
understanding of empirical success, e.g., why fine-tuning a model with $10^8$
or more parameters on a couple dozen training points does not result in
overfitting. We investigate whether the Neural Tangent Kernel (NTK) - which
originated as a model to study the gradient descent dynamics of infinitely wide
networks with suitable random initialization - describes fine-tuning of
pre-trained LMs. This study was inspired by the decent performance of NTK for
computer vision tasks (Wei et al., 2022). We extend the NTK formalism to Adam
and use Tensor Programs (Yang, 2020) to characterize conditions under which the
NTK lens may describe fine-tuning updates to pre-trained language models.
Extensive experiments on 14 NLP tasks validate our theory and show that
formulating the downstream task as a masked word prediction problem through
prompting often induces kernel-based dynamics during fine-tuning. Finally, we
use this kernel view to propose an explanation for the success of
parameter-efficient subspace-based fine-tuning methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023. Code and pre-computed kernels are publicly
  available at https://github.com/princeton-nlp/LM-Kernel-FT</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RHINO: Rotated DETR with Dynamic Denoising via Hungarian Matching for
  Oriented Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.07598v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.07598v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hakjin Lee, Minki Song, Jamyoung Koo, Junghoon Seo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the publication of DINO, a variant of the Detection Transformer (DETR),
Detection Transformers are breaking the record in the object detection
benchmark with the merits of their end-to-end design and scalability. However,
the extension of DETR to oriented object detection has not been thoroughly
studied although more benefits from its end-to-end architecture are expected
such as removing NMS and anchor-related costs. In this paper, we propose a
first strong DINO-based baseline for oriented object detection. We found that
straightforward employment of DETRs for oriented object detection does not
guarantee non-duplicate prediction, and propose a simple cost to mitigate this.
Furthermore, we introduce a $\textit{dynamic denoising}$ strategy that uses
Hungarian matching to filter redundant noised queries and $\textit{query
alignment}$ to preserve matching consistency between Transformer decoder
layers. Our proposed model outperforms previous rotated DETRs and other
counterparts, achieving state-of-the-art performance in DOTA-v1.0/v1.5/v2.0,
and DIOR-R benchmarks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>State-of-the-art rotated object detector in DOTA v1.0/v1.5/v2.0 and
  DIOR-R at the time of publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Peeling for L0-Regularized Least-Squares with supplementary
  material 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14471v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14471v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Théo Guyard, Gilles Monnoyer, Clément Elvira, Cédric Herzet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new methodology dubbed ``safe peeling'' to accelerate the
resolution of L0-regularized least-squares problems via a Branch-and-Bound
(BnB) algorithm. Our procedure enables to tighten the convex relaxation
considered at each node of the BnB decision tree and therefore potentially
allows for more aggressive pruning. Numerical simulations show that our
proposed methodology leads to significant gains in terms of number of nodes
explored and overall solving time.s show that our proposed methodology leads to
significant gains in terms of number of nodes explored and overall solving
time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ I Prefer not to Say: Protecting User Consent in Models with Optional
  Personal Data <span class="chip">NeurIPS 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.13954v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.13954v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tobias Leemann, Martin Pawelczyk, Christian Thomas Eberle, Gjergji Kasneci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We examine machine learning models in a setup where individuals have the
choice to share optional personal information with a decision-making system, as
seen in modern insurance pricing models. Some users consent to their data being
used whereas others object and keep their data undisclosed. In this work, we
show that the decision not to share data can be considered as information in
itself that should be protected to respect users' privacy. This observation
raises the overlooked problem of how to ensure that users who protect their
personal data do not suffer any disadvantages as a result. To address this
problem, we formalize protection requirements for models which only use the
information for which active user consent was obtained. This excludes implicit
information contained in the decision to share data or not. We offer the first
solution to this problem by proposing the notion of Protected User Consent
(PUC), which we prove to be loss-optimal under our protection requirement. To
learn PUC-compliant models, we devise a model-agnostic data augmentation
strategy with finite sample convergence guarantees. Finally, we analyze the
implications of PUC on a variety of challenging real-world datasets, tasks, and
models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated Version. v1 accepted at NeurIPS 2022 Workshop on Algorithmic
  Fairness through the Lens of Causality and Privacy (AFCP)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Context-aware multi-head self-attentional neural network model for next
  location prediction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.01953v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.01953v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ye Hong, Yatao Zhang, Konrad Schindler, Martin Raubal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate activity location prediction is a crucial component of many mobility
applications and is particularly required to develop personalized, sustainable
transportation systems. Despite the widespread adoption of deep learning
models, next location prediction models lack a comprehensive discussion and
integration of mobility-related spatio-temporal contexts. Here, we utilize a
multi-head self-attentional (MHSA) neural network that learns location
transition patterns from historical location visits, their visit time and
activity duration, as well as their surrounding land use functions, to infer an
individual's next location. Specifically, we adopt point-of-interest data and
latent Dirichlet allocation for representing locations' land use contexts at
multiple spatial scales, generate embedding vectors of the spatio-temporal
features, and learn to predict the next location with an MHSA network. Through
experiments on two large-scale GNSS tracking datasets, we demonstrate that the
proposed model outperforms other state-of-the-art prediction models, and reveal
the contribution of various spatio-temporal contexts to the model's
performance. Moreover, we find that the model trained on population data
achieves higher prediction performance with fewer parameters than
individual-level models due to learning from collective movement patterns. We
also reveal mobility conducted in the recent past and one week before has the
largest influence on the current prediction, showing that learning from a
subset of the historical mobility is sufficient to obtain an accurate location
prediction result. We believe that the proposed model is vital for
context-aware mobility prediction. The gained insights will help to understand
location prediction models and promote their implementation for mobility
applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated figures and added more descriptions in Appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Global Context Vision <span class="highlight-title">Transformer</span>s <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.09959v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.09959v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Hatamizadeh, Hongxu Yin, Greg Heinrich, Jan Kautz, Pavlo Molchanov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose global context vision transformer (GC ViT), a novel architecture
that enhances parameter and compute utilization for computer vision. Our method
leverages global context self-attention modules, joint with standard local
self-attention, to effectively and efficiently model both long and short-range
spatial interactions, without the need for expensive operations such as
computing attention masks or shifting local windows. In addition, we address
the lack of the inductive bias in ViTs, and propose to leverage a modified
fused inverted residual blocks in our architecture. Our proposed GC ViT
achieves state-of-the-art results across image classification, object detection
and semantic segmentation tasks. On ImageNet-1K dataset for classification, the
variants of GC ViT with 51M, 90M and 201M parameters achieve 84.3%, 85.0% and
85.7% Top-1 accuracy, respectively, at 224 image resolution and without any
pre-training, hence surpassing comparably-sized prior art such as CNN-based
ConvNeXt and ViT-based MaxViT and Swin Transformer by a large margin.
Pre-trained GC ViT backbones in downstream tasks of object detection, instance
segmentation, and semantic segmentation using MS COCO and ADE20K datasets
outperform prior work consistently. Specifically, GC ViT with a 4-scale DINO
detection head achieves a box AP of 58.3 on MS COCO dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Rough Differential Equations for Traffic Forecasting 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10909v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10909v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeongwhan Choi, Noseong Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Traffic forecasting is one of the most popular spatio-temporal tasks in the
field of machine learning. A prevalent approach in the field is to combine
graph convolutional networks and recurrent neural networks for the
spatio-temporal processing. There has been fierce competition and many novel
methods have been proposed. In this paper, we present the method of
spatio-temporal graph neural rough differential equation (STG-NRDE). Neural
rough differential equations (NRDEs) are a breakthrough concept for processing
time-series data. Their main concept is to use the log-signature transform to
convert a time-series sample into a relatively shorter series of feature
vectors. We extend the concept and design two NRDEs: one for the temporal
processing and the other for the spatial processing. After that, we combine
them into a single framework. We conduct experiments with 6 benchmark datasets
and 27 baselines. STG-NRDE shows the best accuracy in all cases, outperforming
all those 27 baselines by non-trivial margins.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACM Transactions on Intelligent Systems and Technology
  (ACM TIST). arXiv admin note: substantial text overlap with arXiv:2112.03558</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DL-DRL: A double-level deep reinforcement learning approach for
  large-scale task scheduling of multi-UAV 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.02447v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.02447v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiao Mao, Zhiguang Cao, Mingfeng Fan, Guohua Wu, Witold Pedrycz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Exploiting unmanned aerial vehicles (UAVs) to execute tasks is gaining
growing popularity recently. To solve the underlying task scheduling problem,
the deep reinforcement learning (DRL) based methods demonstrate notable
advantage over the conventional heuristics as they rely less on hand-engineered
rules. However, their decision space will become prohibitively huge as the
problem scales up, thus deteriorating the computation efficiency. To alleviate
this issue, we propose a double-level deep reinforcement learning (DL-DRL)
approach based on a divide and conquer framework (DCF), where we decompose the
task scheduling of multi-UAV into task allocation and route planning.
Particularly, we design an encoder-decoder structured policy network in our
upper-level DRL model to allocate the tasks to different UAVs, and we exploit
another attention based policy network in our lower-level DRL model to
construct the route for each UAV, with the objective to maximize the number of
executed tasks given the maximum flight distance of the UAV. To effectively
train the two models, we design an interactive training strategy (ITS), which
includes pre-training, intensive training and alternate training. Experimental
results show that our DL-DRL performs favorably against the learning-based and
conventional baselines including the OR-Tools, in terms of solution quality and
computation efficiency. We also verify the generalization performance of our
approach by applying it to larger sizes of up to 1000 tasks. Moreover, we also
show via an ablation study that our ITS can help achieve a balance between the
performance and training efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cold PAWS: Unsupervised class discovery and addressing the cold-start
  problem for semi-supervised learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10071v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10071v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Evelyn J. Mannix, Howard D. Bondell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In many machine learning applications, labeling datasets can be an arduous
and time-consuming task. Although research has shown that semi-supervised
learning techniques can achieve high accuracy with very few labels within the
field of computer vision, little attention has been given to how images within
a dataset should be selected for labeling. In this paper, we propose a novel
approach based on well-established self-supervised learning, clustering, and
manifold learning techniques that address this challenge of selecting an
informative image subset to label in the first instance, which is known as the
cold-start or unsupervised selective labelling problem. We test our approach
using several publicly available datasets, namely CIFAR10, Imagenette,
DeepWeeds, and EuroSAT, and observe improved performance with both supervised
and semi-supervised learning strategies when our label selection strategy is
used, in comparison to random sampling. We also obtain superior performance for
the datasets considered with a much simpler approach compared to other methods
in the literature.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep learning for diffusion in porous media 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.02104v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.02104v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Krzysztof M. Graczyk, Dawid Strzelczyk, Maciej Matyka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We adopt convolutional neural networks (CNN) to predict the basic properties
of the porous media. Two different media types are considered: one mimics the
sand packings, and the other mimics the systems derived from the extracellular
space of biological tissues. The Lattice Boltzmann Method is used to obtain the
labeled data necessary for performing supervised learning. We distinguish two
tasks. In the first, networks based on the analysis of the system's geometry
predict porosity and effective diffusion coefficient. In the second, networks
reconstruct the concentration map. In the first task, we propose two types of
CNN models: the C-Net and the encoder part of the U-Net. Both networks are
modified by adding a self-normalization module [Graczyk \textit{et al.}, Sci
Rep 12, 10583 (2022)]. The models predict with reasonable accuracy but only
within the data type, they are trained on. For instance, the model trained on
sand packings-like samples overshoots or undershoots for biological-like
samples. In the second task, we propose the usage of the U-Net architecture. It
accurately reconstructs the concentration fields. In contrast to the first
task, the network trained on one data type works well for the other. For
instance, the model trained on sand packings-like samples works perfectly on
biological-like samples. Eventually, for both types of the data, we fit
exponents in the Archie's law to find tortuosity that is used to describe the
dependence of the effective diffusion on porosity.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages, 17 figures, to appear in Sci. Rep</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Entropy-driven Unsupervised Keypoint Representation Learning in Videos <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.15404v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.15404v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Younes, Simone Schaub-Meyer, Georgia Chalvatzaki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Extracting informative representations from videos is fundamental for
effectively learning various downstream tasks. We present a novel approach for
unsupervised learning of meaningful representations from videos, leveraging the
concept of image spatial entropy (ISE) that quantifies the per-pixel
information in an image. We argue that \textit{local entropy} of pixel
neighborhoods and their temporal evolution create valuable intrinsic
supervisory signals for learning prominent features. Building on this idea, we
abstract visual features into a concise representation of keypoints that act as
dynamic information transmitters, and design a deep learning model that learns,
purely unsupervised, spatially and temporally consistent representations
\textit{directly} from video frames. Two original information-theoretic losses,
computed from local entropy, guide our model to discover consistent keypoint
representations; a loss that maximizes the spatial information covered by the
keypoints and a loss that optimizes the keypoints' information transportation
over time. We compare our keypoint representation to strong baselines for
various downstream tasks, \eg, learning object dynamics. Our empirical results
show superior performance for our information-driven keypoints that resolve
challenges like attendance to static and dynamic objects or objects abruptly
entering and leaving the scene.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>29 pages, 14 figures, Accepted at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-06-05T00:00:00Z">2023-06-05</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">100</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RepoBench: Benchmarking Repository-Level Code Auto-Completion Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03091v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03091v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyang Liu, Canwen Xu, Julian McAuley
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have greatly advanced code auto-completion
systems, with a potential for substantial productivity enhancements for
developers. However, current benchmarks mainly focus on single-file tasks,
leaving an assessment gap for more complex, real-world, multi-file programming
scenarios. To fill this gap, we introduce RepoBench, a new benchmark
specifically designed for evaluating repository-level code auto-completion
systems. RepoBench consists of three interconnected evaluation tasks:
RepoBench-R (Retrieval), RepoBench-C (Code Completion), and RepoBench-P
(Pipeline). Each task respectively measures the system's ability to retrieve
the most relevant code snippets from other files as cross-file context, predict
the next line of code with cross-file and in-file context, and handle complex
tasks that require a combination of both retrieval and next-line prediction.
RepoBench aims to facilitate a more complete comparison of performance and
encouraging continuous improvement in auto-completion systems. RepoBench is
publicly available at https://github.com/Leolty/repobench.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Is Chat<span class="highlight-title">GPT</span> a Good Teacher Coach? Measuring Zero-Shot Performance For
  Scoring and Providing Actionable Insights on Classroom Instruction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03090v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03090v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rose E. Wang, Dorottya Demszky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Coaching, which involves classroom observation and expert feedback, is a
widespread and fundamental part of teacher training. However, the majority of
teachers do not have access to consistent, high quality coaching due to limited
resources and access to expertise. We explore whether generative AI could
become a cost-effective complement to expert feedback by serving as an
automated teacher coach. In doing so, we propose three teacher coaching tasks
for generative AI: (A) scoring transcript segments based on classroom
observation instruments, (B) identifying highlights and missed opportunities
for good instructional strategies, and (C) providing actionable suggestions for
eliciting more student reasoning. We recruit expert math teachers to evaluate
the zero-shot performance of ChatGPT on each of these tasks for elementary math
classroom transcripts. Our results reveal that ChatGPT generates responses that
are relevant to improving instruction, but they are often not novel or
insightful. For example, 82% of the model's suggestions point to places in the
transcript where the teacher is already implementing that suggestion. Our work
highlights the challenges of producing insightful, novel and truthful feedback
for teachers while paving the way for future research to address these
obstacles and improve the capacity of generative AI to coach teachers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>In the Proceedings of Innovative Use of NLP for Building Educational
  Applications 2023; The code and model outputs are open-sourced here:
  https://github.com/rosewang2008/zero-shot-teacher-feedback</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sequential Monte Carlo Steering of Large Language Models using
  Probabilistic Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03081v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03081v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexander K. Lew, Tan Zhi-Xuan, Gabriel Grand, Vikash K. Mansinghka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Even after fine-tuning and reinforcement learning, large language models
(LLMs) can be difficult, if not impossible, to control reliably with prompts
alone. We propose a new inference-time approach to enforcing syntactic and
semantic constraints on the outputs of LLMs, called sequential Monte Carlo
(SMC) steering. The key idea is to specify language generation tasks as
posterior inference problems in a class of discrete probabilistic sequence
models, and replace standard decoding with sequential Monte Carlo inference.
For a computational cost similar to that of beam search, SMC can steer LLMs to
solve diverse tasks, including infilling, generation under syntactic
constraints, and prompt intersection. To facilitate experimentation with SMC
steering, we present a probabilistic programming library, LLaMPPL
(https://github.com/probcomp/LLaMPPL), for concisely specifying new generation
tasks as language model probabilistic programs, and automating steering of
LLaMA-family Transformers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Machine Learning and Statistical Approaches to Measuring Similarity of
  Political Parties 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03079v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03079v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daria Boratyn, Damian Brzyski, Beata Kosowska-Gąstoł, Jan Rybicki, Wojciech Słomczyński, Dariusz Stolicki
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Mapping political party systems to metric policy spaces is one of the major
methodological problems in political science. At present, in most political
science project this task is performed by domain experts relying on purely
qualitative assessments, with all the attendant problems of subjectivity and
labor intensiveness. We consider how advances in natural language processing,
including large transformer-based language models, can be applied to solve that
issue. We apply a number of texts similarity measures to party political
programs, analyze how they correlate with each other, and -- in the absence of
a satisfactory benchmark -- evaluate them against other measures, including
those based on expert surveys, voting records, electoral patterns, and
candidate networks. Finally, we consider the prospects of relying on those
methods to correct, supplement, and eventually replace expert judgments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language model (LLM) pretraining have led to
high-quality LLMs with impressive abilities. By compressing such LLMs via
quantization to 3-4 bits per parameter, they can fit into memory-limited
devices such as laptops and mobile phones, enabling personalized use. However,
quantization down to 3-4 bits per parameter usually leads to moderate-to-high
accuracy losses, especially for smaller models in the 1-10B parameter range,
which are well-suited for edge deployments. To address this accuracy issue, we
introduce the Sparse-Quantized Representation (SpQR), a new compressed format
and quantization technique which enables for the first time near-lossless
compression of LLMs across model scales, while reaching similar compression
levels to previous methods. SpQR works by identifying and isolating outlier
weights, which cause particularly-large quantization errors, and storing them
in higher precision, while compressing all other weights to 3-4 bits, and
achieves relative accuracy losses of less than 1% in perplexity for
highly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B
parameter LLM on a single 24 GB consumer GPU without any performance
degradation at 15% speedup thus making powerful LLMs available to consumer
without any downsides. SpQR comes with efficient algorithms for both encoding
weights into its format, as well as decoding them efficiently at runtime.
Specifically, we provide an efficient GPU inference algorithm for SpQR which
yields faster inference than 16-bit baselines at similar accuracy, while
enabling memory compression gains of more than 4x.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interactive Editing for Text Summarization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03067v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03067v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujia Xie, Xun Wang, Si-Qing Chen, Wayne Xiong, Pengcheng He
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Summarizing lengthy documents is a common and essential task in our daily
lives. Although recent advancements in neural summarization models can assist
in crafting general-purpose summaries, human writers often have specific
requirements that call for a more customized approach. To address this need, we
introduce REVISE (Refinement and Editing via Iterative Summarization
Enhancement), an innovative framework designed to facilitate iterative editing
and refinement of draft summaries by human writers. Within our framework,
writers can effortlessly modify unsatisfactory segments at any location or
length and provide optional starting phrases -- our system will generate
coherent alternatives that seamlessly integrate with the existing summary. At
its core, REVISE incorporates a modified fill-in-the-middle model with the
encoder-decoder architecture while developing novel evaluation metrics tailored
for the summarization task. In essence, our framework empowers users to create
high-quality, personalized summaries by effectively harnessing both human
expertise and AI capabilities, ultimately transforming the summarization
process into a truly collaborative and adaptive experience.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Structured Voronoi Sampling 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03061v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03061v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Afra Amini, Li Du, Ryan Cotterell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, there has been a growing interest in the development of
gradient-based sampling algorithms for text generation, especially in the
context of controlled generation. However, there exists a lack of theoretically
grounded and principled approaches for this task. In this paper, we take an
important step toward building a principled approach for sampling from language
models with gradient-based methods. We use discrete distributions given by
language models to define densities and develop an algorithm based on
Hamiltonian Monte Carlo to sample from them. We name our gradient-based
technique Structured Voronoi Sampling (SVS). In an experimental setup where the
reference distribution is known, we show that the empirical distribution of SVS
samples is closer to the reference distribution compared to alternative
sampling schemes. Furthermore, in a controlled generation task, SVS is able to
generate fluent and diverse samples while following the control targets
significantly better than other methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Analyzing Syntactic Generalization Capacity of <span class="highlight-title">Pre-train</span>ed Language
  Models on Japanese Honorific Conversion <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03055v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03055v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryo Sekizawa, Hitomi Yanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Using Japanese honorifics is challenging because it requires not only
knowledge of the grammatical rules but also contextual information, such as
social relationships. It remains unclear whether pre-trained large language
models (LLMs) can flexibly handle Japanese honorifics like humans. To analyze
this, we introduce an honorific conversion task that considers social
relationships among people mentioned in a conversation. We construct a Japanese
honorifics dataset from problem templates of various sentence structures to
investigate the syntactic generalization capacity of GPT-3, one of the leading
LLMs, on this task under two settings: fine-tuning and prompt learning. Our
results showed that the fine-tuned GPT-3 performed better in a context-aware
honorific conversion task than the prompt-based one. The fine-tuned model
demonstrated overall syntactic generalizability towards compound honorific
sentences, except when tested with the data involving direct speech.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Proceedings of the 12th Joint Conference on Lexical
  and Computational Semantics (*SEM2023) with ACL2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Benchmarking Large Language Models on CMExam -- A Comprehensive Chinese
  Medical Exam <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03030v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03030v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Junling Liu, Peilin Zhou, Yining Hua, Dading Chong, Zhongyu Tian, Andrew Liu, Helin Wang, Chenyu You, Zhenhua Guo, Lei Zhu, Michael Lingzhi Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in large language models (LLMs) have transformed the
field of question answering (QA). However, evaluating LLMs in the medical field
is challenging due to the lack of standardized and comprehensive datasets. To
address this gap, we introduce CMExam, sourced from the Chinese National
Medical Licensing Examination. CMExam consists of 60K+ multiple-choice
questions for standardized and objective evaluations, as well as solution
explanations for model reasoning evaluation in an open-ended manner. For
in-depth analyses of LLMs, we invited medical professionals to label five
additional question-wise annotations, including disease groups, clinical
departments, medical disciplines, areas of competency, and question difficulty
levels. Alongside the dataset, we further conducted thorough experiments with
representative LLMs and QA algorithms on CMExam. The results show that GPT-4
had the best accuracy of 61.5% and a weighted F1 score of 0.616. These results
highlight a great disparity when compared to human accuracy, which stood at
71.6%. For explanation tasks, while LLMs could generate relevant reasoning and
demonstrate improved performance after finetuning, they fall short of a desired
standard, indicating ample room for improvement. To the best of our knowledge,
CMExam is the first Chinese medical exam dataset to provide comprehensive
medical annotations. The experiments and findings of LLM evaluation also
provide valuable insights into the challenges and potential solutions in
developing Chinese medical QA systems and LLM evaluation pipelines. The dataset
and relevant code are available at https://github.com/williamliujl/CMExam.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PokemonChat: Auditing Chat<span class="highlight-title">GPT</span> for Pokémon Universe Knowledge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03024v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03024v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Laura Cabello, Jiaang Li, Ilias Chalkidis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The recently released ChatGPT model demonstrates unprecedented capabilities
in zero-shot question-answering. In this work, we probe ChatGPT for its
conversational understanding and introduce a conversational framework
(protocol) that can be adopted in future studies. The Pok\'emon universe serves
as an ideal testing ground for auditing ChatGPT's reasoning capabilities due to
its closed world assumption. After bringing ChatGPT's background knowledge (on
the Pok\'emon universe) to light, we test its reasoning process when using
these concepts in battle scenarios. We then evaluate its ability to acquire new
knowledge and include it in its reasoning process. Our ultimate goal is to
assess ChatGPT's ability to generalize, combine features, and to acquire and
reason over newly introduced knowledge from human feedback. We find that
ChatGPT has prior knowledge of the Pokemon universe, which can reason upon in
battle scenarios to a great extent, even when new information is introduced.
The model performs better with collaborative feedback and if there is an
initial phase of information retrieval, but also hallucinates occasionally and
is susceptible to adversarial attacks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PolyVoice: Language Models for Speech to Speech Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02982v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02982v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qianqian Dong, Zhiying Huang, Chen Xu, Yunlong Zhao, Kexin Wang, Xuxin Cheng, Tom Ko, Qiao Tian, Tang Li, Fengpeng Yue, Ye Bai, Xi Chen, Lu Lu, Zejun Ma, Yuping Wang, Mingxuan Wang, Yuxuan Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose PolyVoice, a language model-based framework for speech-to-speech
translation (S2ST) system. Our framework consists of two language models: a
translation language model and a speech synthesis language model. We use
discretized speech units, which are generated in a fully unsupervised way, and
thus our framework can be used for unwritten languages. For the speech
synthesis part, we adopt the existing VALL-E X approach and build a unit-based
audio language model. This grants our framework the ability to preserve the
voice characteristics and the speaking style of the original speech. We examine
our system on Chinese $\rightarrow$ English and English $\rightarrow$ Spanish
pairs. Experimental results show that our system can generate speech with high
translation quality and audio quality. Speech samples are available at
https://speechtranslation.github.io/polyvoice.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ KNOW How to Make Up Your Mind! Adversarially Detecting and Alleviating
  Inconsistencies in Natural Language Explanations <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02980v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02980v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Myeongjun Jang, Bodhisattwa Prasad Majumder, Julian McAuley, Thomas Lukasiewicz, Oana-Maria Camburu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While recent works have been considerably improving the quality of the
natural language explanations (NLEs) generated by a model to justify its
predictions, there is very limited research in detecting and alleviating
inconsistencies among generated NLEs. In this work, we leverage external
knowledge bases to significantly improve on an existing adversarial attack for
detecting inconsistent NLEs. We apply our attack to high-performing NLE models
and show that models with higher NLE quality do not necessarily generate fewer
inconsistencies. Moreover, we propose an off-the-shelf mitigation method to
alleviate inconsistencies by grounding the model into external background
knowledge. Our method decreases the inconsistencies of previous high-performing
NLE models as detected by our attack.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Short paper, ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Which Argumentative Aspects of Hate Speech in Social Media can be
  reliably identified? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02978v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02978v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Damián Furman, Pablo Torres, José A. Rodríguez, Diego Letzen, Vanina Martínez, Laura Alonso Alemany
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the increasing diversity of use cases of large language models, a more
informative treatment of texts seems necessary. An argumentative analysis could
foster a more reasoned usage of chatbots, text completion mechanisms or other
applications. However, it is unclear which aspects of argumentation can be
reliably identified and integrated in language models. In this paper, we
present an empirical assessment of the reliability with which different
argumentative aspects can be automatically identified in hate speech in social
media. We have enriched the Hateval corpus (Basile et al. 2019) with a manual
annotation of some argumentative components, adapted from Wagemans (2016)'s
Periodic Table of Arguments. We show that some components can be identified
with reasonable reliability. For those that present a high error ratio, we
analyze the patterns of disagreement between expert annotators and errors in
automatic procedures, and we propose adaptations of those categories that can
be more reliably reproduced.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 Pages plus reference and appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple and Flexible Modeling for Mental Disorder Detection by Learning
  from Clinical Questionnaires <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoyun Song, Jisu Shin, Huije Lee, Jong C. Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media is one of the most highly sought resources for analyzing
characteristics of the language by its users. In particular, many researchers
utilized various linguistic features of mental health problems from social
media. However, existing approaches to detecting mental disorders face critical
challenges, such as the scarcity of high-quality data or the trade-off between
addressing the complexity of models and presenting interpretable results
grounded in expert domain knowledge. To address these challenges, we design a
simple but flexible model that preserves domain-based interpretability. We
propose a novel approach that captures the semantic meanings directly from the
text and compares them to symptom-related descriptions. Experimental results
demonstrate that our model outperforms relevant baselines on various mental
disorder detection tasks. Our detailed analysis shows that the proposed model
is effective at leveraging domain knowledge, transferable to other mental
disorders, and providing interpretable detection results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023, 15 pages, 11 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MidMed: Towards Mixed-Type Dialogues for Medical Consultation <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02923v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02923v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoming Shi, Zeming Liu, Chuan Wang, Haitao Leng, Kui Xue, Xiaofan Zhang, Shaoting Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most medical dialogue systems assume that patients have clear goals (medicine
querying, surgical operation querying, etc.) before medical consultation.
However, in many real scenarios, due to the lack of medical knowledge, it is
usually difficult for patients to determine clear goals with all necessary
slots. In this paper, we identify this challenge as how to construct medical
consultation dialogue systems to help patients clarify their goals. To mitigate
this challenge, we propose a novel task and create a human-to-human mixed-type
medical consultation dialogue corpus, termed MidMed, covering five dialogue
types: task-oriented dialogue for diagnosis, recommendation, knowledge-grounded
dialogue, QA, and chitchat. MidMed covers four departments
(otorhinolaryngology, ophthalmology, skin, and digestive system), with 8,175
dialogues. Furthermore, we build baselines on MidMed and propose an
instruction-guiding medical dialogue generation framework, termed InsMed, to
address this task. Experimental results show the effectiveness of InsMed.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2023 Main conference. First two authors contributed
  equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Second Language Acquisition of Neural Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02920v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02920v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Miyu Oba, Tatsuki Kuribayashi, Hiroki Ouchi, Taro Watanabe
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With the success of neural language models (LMs), their language acquisition
has gained much attention. This work sheds light on the second language (L2)
acquisition of LMs, while previous work has typically explored their first
language (L1) acquisition. Specifically, we trained bilingual LMs with a
scenario similar to human L2 acquisition and analyzed their cross-lingual
transfer from linguistic perspectives. Our exploratory experiments demonstrated
that the L1 pretraining accelerated their linguistic generalization in L2, and
language transfer configurations (e.g., the L1 choice, and presence of parallel
texts) substantially affected their generalizations. These clarify their
(non-)human-like L2 acquisition in particular aspects.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SelfEvolve: A Code Evolution Framework via Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02907v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02907v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shuyang Jiang, Yuhao Wang, Yu Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have already revolutionized code generation,
after being pretrained on publicly available code data. However, while various
methods have been proposed to augment LLMs with retrieved knowledge and enhance
the quality of code generation, the performance of these retrieval-based
methods is limited by the strength of the retrievers used. In addition, while
LLMs show great emergent ability, they still struggle to produce the correct
code in one turn. To address these challenges, we propose a novel two-step
pipeline, called \autoknow, that leverages LLMs as both knowledge providers and
self-reflective programmers. Unlike retrieval-based methods, \autoknow~obtains
the knowledge from input prompts and generates intermediate code based on the
generated knowledge. After that, \autoknow~asks LLM to act as an expert
programmer to perform debugging for the generated code. This is achieved by
receiving the error message from the interpreter, without requiring special
test cases for correctness verification. We evaluate \autoknow~on three code
generation datasets, including DS-1000 for data science code, HumanEval for
software engineering code, and TransCoder for C++-to-Python translation. Our
empirical experiments show that \autoknow~outperforms strong baselines by a
significant margin on all datasets. We also conduct exhaustive analytical
experiments to validate the effectiveness of the two stages of \autoknow, and
find that both are superior to other prompting-based methods. Further
scalability analysis demonstrates that \autoknow~can be adapted to other more
advanced models, such as GPT-4, and bring consistent efficacy improvement.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ N-Shot Benchmarking of Whisper on Diverse Arabic Speech Recognition <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02902v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02902v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bashar Talafha, Abdul Waheed, Muhammad Abdul-Mageed
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Whisper, the recently developed multilingual weakly supervised model, is
reported to perform well on multiple speech recognition benchmarks in both
monolingual and multilingual settings. However, it is not clear how Whisper
would fare under diverse conditions even on languages it was evaluated on such
as Arabic. In this work, we address this gap by comprehensively evaluating
Whisper on several varieties of Arabic speech for the ASR task. Our evaluation
covers most publicly available Arabic speech data and is performed under n-shot
(zero-, few-, and full) finetuning. We also investigate the robustness of
Whisper under completely novel conditions, such as in dialect-accented standard
Arabic and in unseen dialects for which we develop evaluation data. Our
experiments show that although Whisper zero-shot outperforms fully finetuned
XLS-R models on all datasets, its performance deteriorates significantly in the
zero-shot setting for five unseen dialects (i.e., Algeria, Jordan, Palestine,
UAE, and Yemen).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4 pages, INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gen-IR @ SIGIR 2023: The First Workshop on Generative Information
  Retrieval <span class="chip">SIGIR 23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02887v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02887v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gabriel Bénédict, Ruqing Zhang, Donald Metzler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative information retrieval (IR) has experienced substantial growth
across multiple research communities (e.g., information retrieval, computer
vision, natural language processing, and machine learning), and has been highly
visible in the popular press. Theoretical, empirical, and actual user-facing
products have been released that retrieve documents (via generation) or
directly generate answers given an input request. We would like to investigate
whether end-to-end generative models are just another trend or, as some claim,
a paradigm change for IR. This necessitates new metrics, theoretical grounding,
evaluation methods, task definitions, models, user interfaces, etc. The goal of
this workshop (https://coda.io/@sigir/gen-ir) is to focus on previously
explored Generative IR techniques like document retrieval and direct Grounded
Answer Generation, while also offering a venue for the discussion and
exploration of how Generative IR can be applied to new domains like
recommendation systems, summarization, etc. The format of the workshop is
interactive, including roundtable and keynote sessions and tends to avoid the
one-sided dialogue of a mini-conference.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted SIGIR 23 workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DecompX: Explaining <span class="highlight-title">Transformer</span>s Decisions by Propagating Token
  Decomposition <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02873v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02873v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Modarressi, Mohsen Fayyaz, Ehsan Aghazadeh, Yadollah Yaghoobzadeh, Mohammad Taher Pilehvar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  An emerging solution for explaining Transformer-based models is to use
vector-based analysis on how the representations are formed. However, providing
a faithful vector-based explanation for a multi-layer model could be
challenging in three aspects: (1) Incorporating all components into the
analysis, (2) Aggregating the layer dynamics to determine the information flow
and mixture throughout the entire model, and (3) Identifying the connection
between the vector-based analysis and the model's predictions. In this paper,
we present DecompX to tackle these challenges. DecompX is based on the
construction of decomposed token representations and their successive
propagation throughout the model without mixing them in between layers.
Additionally, our proposal provides multiple advantages over existing solutions
for its inclusion of all encoder components (especially nonlinear feed-forward
networks) and the classification head. The former allows acquiring precise
vectors while the latter transforms the decomposition into meaningful
prediction-based values, eliminating the need for norm- or summation-based
vector aggregation. According to the standard faithfulness evaluations, DecompX
consistently outperforms existing gradient-based and vector-based approaches on
various datasets. Our code is available at
https://github.com/mohsenfayyaz/DecompX.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2023 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Text-To-KG Alignment: Comparing Current Methods on Classification Tasks <span class="chip">ATC</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02871v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02871v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sondre Wold, Lilja Øvrelid, Erik Velldal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In contrast to large text corpora, knowledge graphs (KG) provide dense and
structured representations of factual information. This makes them attractive
for systems that supplement or ground the knowledge found in pre-trained
language models with an external knowledge source. This has especially been the
case for classification tasks, where recent work has focused on creating
pipeline models that retrieve information from KGs like ConceptNet as
additional context. Many of these models consist of multiple components, and
although they differ in the number and nature of these parts, they all have in
common that for some given text query, they attempt to identify and retrieve a
relevant subgraph from the KG. Due to the noise and idiosyncrasies often found
in KGs, it is not known how current methods compare to a scenario where the
aligned subgraph is completely relevant to the query. In this work, we try to
bridge this knowledge gap by reviewing current approaches to text-to-KG
alignment and evaluating them on two datasets where manually created graphs are
available, providing insights into the effectiveness of current methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Camera ready version for MATCHING workshop at ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On "Scientific Debt" in NLP: A Case for More Rigour in Language Model
  <span class="highlight-title">Pre-Train</span>ing Research <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02870v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02870v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Made Nindyatama Nityasya, Haryo Akbarianto Wibowo, Alham Fikri Aji, Genta Indra Winata, Radityo Eko Prasojo, Phil Blunsom, Adhiguna Kuncoro
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This evidence-based position paper critiques current research practices
within the language model pre-training literature. Despite rapid recent
progress afforded by increasingly better pre-trained language models (PLMs),
current PLM research practices often conflate different possible sources of
model improvement, without conducting proper ablation studies and principled
comparisons between different models under comparable conditions. These
practices (i) leave us ill-equipped to understand which pre-training approaches
should be used under what circumstances; (ii) impede reproducibility and credit
assignment; and (iii) render it difficult to understand: "How exactly does each
factor contribute to the progress that we have today?" We provide a case in
point by revisiting the success of BERT over its baselines, ELMo and GPT-1, and
demonstrate how -- under comparable conditions where the baselines are tuned to
a similar extent -- these baselines (and even-simpler variants thereof) can, in
fact, achieve competitive or better performance than BERT. These findings
demonstrate how disentangling different factors of model improvements can lead
to valuable new insights. We conclude with recommendations for how to encourage
and incentivize this line of work, and accelerate progress towards a better and
more systematic understanding of what factors drive the progress of our
foundation models today.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leveraging Large Language Models for Topic Classification in the Domain
  of Public Affairs <span class="chip">ICDAR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02864v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02864v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alejandro Peña, Aythami Morales, Julian Fierrez, Ignacio Serna, Javier Ortega-Garcia, Iñigo Puente, Jorge Cordova, Gonzalo Cordova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The analysis of public affairs documents is crucial for citizens as it
promotes transparency, accountability, and informed decision-making. It allows
citizens to understand government policies, participate in public discourse,
and hold representatives accountable. This is crucial, and sometimes a matter
of life or death, for companies whose operation depend on certain regulations.
Large Language Models (LLMs) have the potential to greatly enhance the analysis
of public affairs documents by effectively processing and understanding the
complex language used in such documents. In this work, we analyze the
performance of LLMs in classifying public affairs documents. As a natural
multi-label task, the classification of these documents presents important
challenges. In this work, we use a regex-powered tool to collect a database of
public affairs documents with more than 33K samples and 22.5M tokens. Our
experiments assess the performance of 4 different Spanish LLMs to classify up
to 30 different topics in the data in different configurations. The results
shows that LLMs can be of great use to process domain-specific documents, such
as those in the domain of public affairs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in ICDAR 2023 Workshop on Automatic Domain-Adapted and
  Personalized Document Analysis</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Video-LLaMA: An Instruction-tuned Audio-Visual Language Model for Video
  Understanding 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02858v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02858v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hang Zhang, Xin Li, Lidong Bing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present Video-LLaMA, a multi-modal framework that empowers Large Language
Models (LLMs) with the capability of understanding both visual and auditory
content in the video. Video-LLaMA bootstraps cross-modal training from the
frozen pre-trained visual \& audio encoders and the frozen LLMs. Unlike
previous vision- LLMs that focus on static image comprehensions such as
MiniGPT-4~\citep{zhu2023minigpt} and LLaVA~\citep{liu2023visualit}, Video-LLaMA
tackles two challenges in video understanding: (1) capturing the temporal
changes in visual scenes, (2) integrating audio-visual signals. For the first
challenge, we propose Video Q-former to extend the pre-trained image encoder to
a video encoder and introduce a video-to-text generation task to learn
video-language correspondence. For the second challenge, we leverage
ImageBind~\citep{girdhar2023imagebind} as the pre-trained audio encoder which
performs exceptionally well in aligning different modalities to a common
embedding space. And then introduce an Audio Q-former to learn auditory query
tokens. To align the output of both visual \& audio encoder with LLM's
embedding space, we train Video-LLaMA on a large-scale vision caption dataset
and a hign-quantity vision-instruction-tuning dataset. We found Video-LLaMA
showcases the ability to perceive and comprehend video content, generating
meaningful responses that are grounded in the visual and auditory information
present in the videos. This highlights the potential of Video-LLaMA as a
promising prototype for audio-visual AI assistants. Our code, pre-trained
model, and demo are available at
\url{https://github.com/DAMO-NLP-SG/Video-LLaMA}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Technical Report</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Conversational Recommendation Systems via Counterfactual Data
  Simulation <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02842v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02842v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaolei Wang, Kun Zhou, Xinyu Tang, Wayne Xin Zhao, Fan Pan, Zhao Cao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational recommender systems (CRSs) aim to provide recommendation
services via natural language conversations. Although a number of approaches
have been proposed for developing capable CRSs, they typically rely on
sufficient training data for training. Since it is difficult to annotate
recommendation-oriented dialogue datasets, existing CRS approaches often suffer
from the issue of insufficient training due to the scarcity of training data.
To address this issue, in this paper, we propose a CounterFactual data
simulation approach for CRS, named CFCRS, to alleviate the issue of data
scarcity in CRSs. Our approach is developed based on the framework of
counterfactual data augmentation, which gradually incorporates the rewriting to
the user preference from a real dialogue without interfering with the entire
conversation flow. To develop our approach, we characterize user preference and
organize the conversation flow by the entities involved in the dialogue, and
design a multi-stage recommendation dialogue simulator based on a conversation
flow language model. Under the guidance of the learned user preference and
dialogue schema, the flow language model can produce reasonable, coherent
conversation flows, which can be further realized into complete dialogues.
Based on the simulator, we perform the intervention at the representations of
the interacted entities of target users, and design an adversarial training
method with a curriculum schedule that can gradually optimize the data
augmentation strategy. Extensive experiments show that our approach can
consistently boost the performance of several competitive CRSs, and outperform
other data augmentation methods, especially when the training data is limited.
Our code is publicly available at https://github.com/RUCAIBox/CFCRS.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2023. Code: https://github.com/RUCAIBox/CFCRS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Substitute Spans towards Improving Compositional
  Generalization <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyi Li, Ying Wei, Defu Lian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rising prevalence of neural sequence models, recent empirical
evidences suggest their deficiency in compositional generalization. One of the
current de-facto solutions to this problem is compositional data augmentation,
aiming to incur additional compositional inductive bias. Nonetheless, the
improvement offered by existing handcrafted augmentation strategies is limited
when successful systematic generalization of neural sequence models requires
multi-grained compositional bias (i.e., not limited to either lexical or
structural biases only) or differentiation of training sequences in an
imbalanced difficulty distribution. To address the two challenges, we first
propose a novel compositional augmentation strategy dubbed \textbf{Span}
\textbf{Sub}stitution (SpanSub) that enables multi-grained composition of
substantial substructures in the whole training set. Over and above that, we
introduce the \textbf{L}earning \textbf{to} \textbf{S}ubstitute \textbf{S}pan
(L2S2) framework which empowers the learning of span substitution probabilities
in SpanSub in an end-to-end manner by maximizing the loss of neural sequence
models, so as to outweigh those challenging compositions with elusive concepts
and novel surroundings. Our empirical results on three standard compositional
generalization benchmarks, including SCAN, COGS and GeoQuery (with an
improvement of at most 66.5\%, 10.3\%, 1.2\%, respectively), demonstrate the
superiority of SpanSub, %the learning framework L2S2 and their combination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ UNIDECOR: A Unified Deception Corpus for Cross-Corpus Deception
  Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02827v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02827v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aswathy Velutharambath, Roman Klinger
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Verbal deception has been studied in psychology, forensics, and computational
linguistics for a variety of reasons, like understanding behaviour patterns,
identifying false testimonies, and detecting deception in online communication.
Varying motivations across research fields lead to differences in the domain
choices to study and in the conceptualization of deception, making it hard to
compare models and build robust deception detection systems for a given
language. With this paper, we improve this situation by surveying available
English deception datasets which include domains like social media reviews,
court testimonials, opinion statements on specific topics, and deceptive
dialogues from online strategy games. We consolidate these datasets into a
single unified corpus. Based on this resource, we conduct a correlation
analysis of linguistic cues of deception across datasets to understand the
differences and perform cross-corpus modeling experiments which show that a
cross-domain generalization is challenging to achieve. The unified deception
corpus (UNIDECOR) can be obtained from
https://www.ims.uni-stuttgart.de/data/unidecor.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing Language Representation with Constructional Information for
  Natural Language Understanding <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02819v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02819v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lvxiaowei Xu, Jianwang Wu, Jiawei Peng, Zhilin Gong, Ming Cai, Tianxiang Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language understanding (NLU) is an essential branch of natural
language processing, which relies on representations generated by pre-trained
language models (PLMs). However, PLMs primarily focus on acquiring
lexico-semantic information, while they may be unable to adequately handle the
meaning of constructions. To address this issue, we introduce construction
grammar (CxG), which highlights the pairings of form and meaning, to enrich
language representation. We adopt usage-based construction grammar as the basis
of our work, which is highly compatible with statistical models such as PLMs.
Then a HyCxG framework is proposed to enhance language representation through a
three-stage solution. First, all constructions are extracted from sentences via
a slot-constraints approach. As constructions can overlap with each other,
bringing redundancy and imbalance, we formulate the conditional max coverage
problem for selecting the discriminative constructions. Finally, we propose a
relational hypergraph attention network to acquire representation from
constructional information by capturing high-order word interactions among
constructions. Extensive experiments demonstrate the superiority of the
proposed model on a variety of NLU tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Long paper, accepted at the ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Human-like Concept Learning with Bayesian Inference over
  Natural Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Ellis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We model learning of abstract symbolic concepts by performing Bayesian
inference over utterances in natural language. For efficient inference, we use
a large language model as a proposal distribution. We fit a prior to human data
to better model human learners, and evaluate on both generative and logical
concepts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MCTS: A Multi-Reference Chinese Text Simplification <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02796v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02796v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruining Chong, Luming Lu, Liner Yang, Jinran Nie, Shuhan Zhou, Yaoxin Li, Erhong Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Text simplification aims to make the text easier to understand by applying
rewriting transformations. There has been very little research on Chinese text
simplification for a long time. The lack of generic evaluation data is an
essential reason for this phenomenon. In this paper, we introduce MCTS, a
multi-reference Chinese text simplification dataset. We describe the annotation
process of the dataset and provide a detailed analysis of it. Furthermore, we
evaluate the performance of some unsupervised methods and advanced large
language models. We hope to build a basic understanding of Chinese text
simplification through the foundational work and provide references for future
research. We release our data at https://github.com/blcuicall/mcts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring the Relationship between Alignment and Cross-lingual Transfer
  in Multilingual <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02790v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02790v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Félix Gaschi, Patricio Cerda, Parisa Rastin, Yannick Toussaint
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Without any explicit cross-lingual training data, multilingual language
models can achieve cross-lingual transfer. One common way to improve this
transfer is to perform realignment steps before fine-tuning, i.e., to train the
model to build similar representations for pairs of words from translated
sentences. But such realignment methods were found to not always improve
results across languages and tasks, which raises the question of whether
aligned representations are truly beneficial for cross-lingual transfer. We
provide evidence that alignment is actually significantly correlated with
cross-lingual transfer across languages, models and random seeds. We show that
fine-tuning can have a significant impact on alignment, depending mainly on the
downstream task and the model. Finally, we show that realignment can, in some
instances, improve cross-lingual transfer, and we identify conditions in which
realignment methods provide significant improvements. Namely, we find that
realignment works better on tasks for which alignment is correlated with
cross-lingual transfer when generalizing to a distant language and with smaller
models, as well as when using a bilingual dictionary rather than FastAlign to
extract realignment pairs. For example, for POS-tagging, between English and
Arabic, realignment can bring a +15.8 accuracy improvement on distilmBERT, even
outperforming XLM-R Large by 1.7. We thus advocate for further research on
realignment methods for smaller multilingual models as an alternative to
scaling.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ German CheXpert Chest X-ray Radiology Report Labeler 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02777v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02777v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alessandro Wollek, Sardi Hyska, Thomas Sedlmeyr, Philip Haitzer, Johannes Rueckel, Bastian O. Sabel, Michael Ingrisch, Tobias Lasser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study aimed to develop an algorithm to automatically extract annotations
for chest X-ray classification models from German thoracic radiology reports.
An automatic label extraction model was designed based on the CheXpert
architecture, and a web-based annotation interface was created for iterative
improvements. Results showed that automated label extraction can reduce time
spent on manual labeling and improve overall modeling performance. The model
trained on automatically extracted labels performed competitively to manually
labeled data and strongly outperformed the model trained on publicly available
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying the style by a qualified reader on a short fragment of
  generated poetry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boris Orekhov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Style is an important concept in today's challenges in natural language
generating. After the success in the field of image style transfer, the task of
text style transfer became actual and attractive. Researchers are also
interested in the tasks of style reproducing in generation of the poetic text.
Evaluation of style reproducing in natural poetry generation remains a problem.
I used 3 character-based LSTM-models to work with style reproducing assessment.
All three models were trained on the corpus of texts by famous Russian-speaking
poets. Samples were shown to the assessors and 4 answer options were offered,
the style of which poet this sample reproduces. In addition, the assessors were
asked how well they were familiar with the work of the poet they had named.
Students studying history of literature were the assessors, 94 answers were
received. It has appeared that accuracy of definition of style increases if the
assessor can quote the poet by heart. Each model showed at least 0.7
macro-average accuracy. The experiment showed that it is better to involve a
professional rather than a naive reader in the evaluation of style in the tasks
of poetry generation, while lstm models are good at reproducing the style of
Russian poets even on a limited training corpus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Lingual Transfer with Target Language-Ready Task Adapters <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02767v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02767v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marinela Parović, Alan Ansell, Ivan Vulić, Anna Korhonen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adapters have emerged as a modular and parameter-efficient approach to
(zero-shot) cross-lingual transfer. The established MAD-X framework employs
separate language and task adapters which can be arbitrarily combined to
perform the transfer of any task to any target language. Subsequently, BAD-X,
an extension of the MAD-X framework, achieves improved transfer at the cost of
MAD-X's modularity by creating "bilingual" adapters specific to the
source-target language pair. In this work, we aim to take the best of both
worlds by (i) fine-tuning task adapters adapted to the target language(s)
(so-called "target language-ready" (TLR) adapters) to maintain high transfer
performance, but (ii) without sacrificing the highly modular design of MAD-X.
The main idea of "target language-ready" adapters is to resolve the
training-vs-inference discrepancy of MAD-X: the task adapter "sees" the target
language adapter for the very first time during inference, and thus might not
be fully compatible with it. We address this mismatch by exposing the task
adapter to the target language adapter during training, and empirically
validate several variants of the idea: in the simplest form, we alternate
between using the source and target language adapters during task adapter
training, which can be generalized to cycling over any set of language
adapters. We evaluate different TLR-based transfer configurations with varying
degrees of generality across a suite of standard cross-lingual benchmarks, and
find that the most general (and thus most modular) configuration consistently
outperforms MAD-X and BAD-X on most tasks and languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to Findings of ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PULSAR: <span class="highlight-title">Pre-train</span>ing with Extracted Healthcare Terms for Summarising
  Patients' Problems and Data Augmentation with Black-box Large Language Models <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02754v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02754v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Li, Yuping Wu, Viktor Schlegel, Riza Batista-Navarro, Thanh-Tung Nguyen, Abhinav Ramesh Kashyap, Xiaojun Zeng, Daniel Beck, Stefan Winkler, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Medical progress notes play a crucial role in documenting a patient's
hospital journey, including his or her condition, treatment plan, and any
updates for healthcare providers. Automatic summarisation of a patient's
problems in the form of a problem list can aid stakeholders in understanding a
patient's condition, reducing workload and cognitive bias. BioNLP 2023 Shared
Task 1A focuses on generating a list of diagnoses and problems from the
provider's progress notes during hospitalisation. In this paper, we introduce
our proposed approach to this task, which integrates two complementary
components. One component employs large language models (LLMs) for data
augmentation; the other is an abstractive summarisation LLM with a novel
pre-training objective for generating the patients' problems summarised as a
list. Our approach was ranked second among all submissions to the shared task.
The performance of our model on the development and test datasets shows that
our approach is more robust on unknown data, with an improvement of up to 3.1
points over the same size of the larger model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2023's workshop BioNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiple output samples for each input in a single-output Gaussian
  process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy H. M. Wong, Huayun Zhang, Nancy F. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The standard Gaussian Process (GP) only considers a single output sample per
input in the training set. Datasets for subjective tasks, such as spoken
language assessment, may be annotated with output labels from multiple human
raters per input. This paper proposes to generalise the GP to allow for these
multiple output samples in the training set, and thus make use of available
output uncertainty information. This differs from a multi-output GP, as all
output samples are from the same task here. The output density function is
formulated to be the joint likelihood of observing all output samples, and
latent variables are not repeated to reduce computation cost. The test set
predictions are inferred similarly to a standard GP, with a difference being in
the optimised hyper-parameters. This is evaluated on speechocean762, showing
that it allows the GP to compute a test set output distribution that is more
similar to the collection of reference outputs from the multiple human raters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Orca: Progressive Learning from Complex Explanation Traces of <span class="highlight-title">GPT</span>-4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has focused on enhancing the capability of smaller models
through imitation learning, drawing on the outputs generated by large
foundation models (LFMs). A number of issues impact the quality of these
models, ranging from limited imitation signals from shallow LFM outputs; small
scale homogeneous training data; and most notably a lack of rigorous evaluation
resulting in overestimating the small model's capability as they tend to learn
to imitate the style, but not the reasoning process of LFMs. To address these
challenges, we develop Orca (We are working with our legal team to publicly
release a diff of the model weights in accordance with LLaMA's release policy
to be published at https://aka.ms/orca-lm), a 13-billion parameter model that
learns to imitate the reasoning process of LFMs. Orca learns from rich signals
from GPT-4 including explanation traces; step-by-step thought processes; and
other complex instructions, guided by teacher assistance from ChatGPT. To
promote this progressive learning, we tap into large-scale and diverse
imitation data with judicious sampling and selection. Orca surpasses
conventional state-of-the-art instruction-tuned models such as Vicuna-13B by
more than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard
(BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH
benchmark and shows competitive performance (4 pts gap with optimized system
message) in professional and academic examinations like the SAT, LSAT, GRE, and
GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our
research indicates that learning from step-by-step explanations, whether these
are generated by humans or more advanced AI models, is a promising direction to
improve model capabilities and skills.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CELDA: Leveraging Black-box Language Model as Enhanced Classifier
  without Labels <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02693v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02693v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunsoo Cho, Youna Kim, Sang-goo Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Utilizing language models (LMs) without internal access is becoming an
attractive paradigm in the field of NLP as many cutting-edge LMs are released
through APIs and boast a massive scale. The de-facto method in this type of
black-box scenario is known as prompting, which has shown progressive
performance enhancements in situations where data labels are scarce or
unavailable. Despite their efficacy, they still fall short in comparison to
fully supervised counterparts and are generally brittle to slight
modifications. In this paper, we propose Clustering-enhanced Linear
Discriminative Analysis, a novel approach that improves the text classification
accuracy with a very weak-supervision signal (i.e., name of the labels). Our
framework draws a precise decision boundary without accessing weights or
gradients of the LM model or data labels. The core ideas of CELDA are twofold:
(1) extracting a refined pseudo-labeled dataset from an unlabeled dataset, and
(2) training a lightweight and robust model on the top of LM, which learns an
accurate decision boundary from an extracted noisy dataset. Throughout in-depth
investigations on various datasets, we demonstrated that CELDA reaches new
state-of-the-art in weakly-supervised text classification and narrows the gap
with a fully-supervised model. Additionally, our proposed methodology can be
applied universally to any LM and has the potential to scale to larger models,
making it a more viable option for utilizing large LMs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Word-Level Pronunciation Assessment with MASK <span class="highlight-title">Pre-train</span>ing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02682v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02682v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yukang Liang, Kaitao Song, Shaoguang Mao, Huiqiang Jiang, Luna Qiu, Yuqing Yang, Dongsheng Li, Linli Xu, Lili Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pronunciation assessment is a major challenge in the computer-aided
pronunciation training system, especially at the word (phoneme)-level. To
obtain word (phoneme)-level scores, current methods usually rely on aligning
components to obtain acoustic features of each word (phoneme), which limits the
performance of assessment to the accuracy of alignments. Therefore, to address
this problem, we propose a simple yet effective method, namely
\underline{M}asked pre-training for \underline{P}ronunciation
\underline{A}ssessment (MPA). Specifically, by incorporating a mask-predict
strategy, our MPA supports end-to-end training without leveraging any aligning
components and can solve misalignment issues to a large extent during
prediction. Furthermore, we design two evaluation strategies to enable our
model to conduct assessments in both unsupervised and supervised settings.
Experimental results on SpeechOcean762 dataset demonstrate that MPA could
achieve better performance than previous methods, without any explicit
alignment. In spite of this, MPA still has some limitations, such as requiring
more inference time and reference text. They expect to be addressed in future
work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by InterSpeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ BeAts: Bengali Speech Acts Recognition using Multimodal Attention Fusion <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02680v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02680v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahana Deb, Sayan Nag, Ayan Mahapatra, Soumitri Chattopadhyay, Aritra Marik, Pijush Kanti Gayen, Shankha Sanyal, Archi Banerjee, Samir Karmakar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Spoken languages often utilise intonation, rhythm, intensity, and structure,
to communicate intention, which can be interpreted differently depending on the
rhythm of speech of their utterance. These speech acts provide the foundation
of communication and are unique in expression to the language. Recent
advancements in attention-based models, demonstrating their ability to learn
powerful representations from multilingual datasets, have performed well in
speech tasks and are ideal to model specific tasks in low resource languages.
Here, we develop a novel multimodal approach combining two models, wav2vec2.0
for audio and MarianMT for text translation, by using multimodal attention
fusion to predict speech acts in our prepared Bengali speech corpus. We also
show that our model BeAts ($\underline{\textbf{Be}}$ngali speech acts
recognition using Multimodal $\underline{\textbf{At}}$tention
Fu$\underline{\textbf{s}}$ion) significantly outperforms both the unimodal
baseline using only speech data and a simpler bimodal fusion using both speech
and text data. Project page: https://soumitri2001.github.io/BeAts
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Joint <span class="highlight-title">Pre-train</span>ing and Local Re-training: Transferable Representation
  Learning on Multi-source Knowledge Graphs <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02679v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02679v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zequn Sun, Jiacheng Huang, Jinghao Lin, Xiaozhou Xu, Qijin Chen, Wei Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present the ``joint pre-training and local re-training''
framework for learning and applying multi-source knowledge graph (KG)
embeddings. We are motivated by the fact that different KGs contain
complementary information to improve KG embeddings and downstream tasks. We
pre-train a large teacher KG embedding model over linked multi-source KGs and
distill knowledge to train a student model for a task-specific KG. To enable
knowledge transfer across different KGs, we use entity alignment to build a
linked subgraph for connecting the pre-trained KGs and the target KG. The
linked subgraph is re-trained for three-level knowledge distillation from the
teacher to the student, i.e., feature knowledge distillation, network knowledge
distillation, and prediction knowledge distillation, to generate more
expressive embeddings. The teacher model can be reused for different target KGs
and tasks without having to train from scratch. We conduct extensive
experiments to demonstrate the effectiveness and efficiency of our framework.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the 29th ACM SIGKDD International Conference on Knowledge
  Discovery and Data Mining (KDD 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improving Grammar-based Sequence-to-Sequence Modeling with Decomposition
  and Constraints <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02671v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02671v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chao Lou, Kewei Tu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Neural QCFG is a grammar-based sequence-tosequence (seq2seq) model with
strong inductive biases on hierarchical structures. It excels in
interpretability and generalization but suffers from expensive inference. In
this paper, we study two low-rank variants of Neural QCFG for faster inference
with different trade-offs between efficiency and expressiveness. Furthermore,
utilizing the symbolic interface provided by the grammar, we introduce two soft
constraints over tree hierarchy and source coverage. We experiment with various
datasets and find that our models outperform vanilla Neural QCFG in most
settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Colexifications for Bootstrapping Cross-lingual <span class="highlight-title">Dataset</span>s: The Case of
  Phonology, Concreteness, and Affectiveness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02646v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02646v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yiyi Chen, Johannes Bjerva
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Colexification refers to the linguistic phenomenon where a single lexical
form is used to convey multiple meanings. By studying cross-lingual
colexifications, researchers have gained valuable insights into fields such as
psycholinguistics and cognitive sciences [Jackson et al.,2019]. While several
multilingual colexification datasets exist, there is untapped potential in
using this information to bootstrap datasets across such semantic features. In
this paper, we aim to demonstrate how colexifications can be leveraged to
create such cross-lingual datasets. We showcase curation procedures which
result in a dataset covering 142 languages across 21 language families across
the world. The dataset includes ratings of concreteness and affectiveness,
mapped with phonemes and phonological features. We further analyze the dataset
along different dimensions to demonstrate potential of the proposed procedures
in facilitating further interdisciplinary research in psychology, cognitive
science, and multilingual natural language processing (NLP). Based on initial
investigations, we observe that i) colexifications that are closer in
concreteness/affectiveness are more likely to colexify; ii) certain
initial/last phonemes are significantly correlated with
concreteness/affectiveness intra language families, such as /k/ as the initial
phoneme in both Turkic and Tai-Kadai correlated with concreteness, and /p/ in
Dravidian and Sino-Tibetan correlated with Valence; iii) the type-to-token
ratio (TTR) of phonemes are positively correlated with concreteness across
several language families, while the length of phoneme segments are negatively
correlated with concreteness; iv) certain phonological features are negatively
correlated with concreteness across languages. The dataset is made public
online for further research.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>13 pages, 4 figures, accepted to SIGMORPHON 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Do-GOOD: Towards Distribution Shift Evaluation for <span class="highlight-title">Pre-Train</span>ed Visual
  Document Understanding Models <span class="chip">SIGIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02623v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02623v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiabang He, Yi Hu, Lei Wang, Xing Xu, Ning Liu, Hui Liu, Heng Tao Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Numerous pre-training techniques for visual document understanding (VDU) have
recently shown substantial improvements in performance across a wide range of
document tasks. However, these pre-trained VDU models cannot guarantee
continued success when the distribution of test data differs from the
distribution of training data. In this paper, to investigate how robust
existing pre-trained VDU models are to various distribution shifts, we first
develop an out-of-distribution (OOD) benchmark termed Do-GOOD for the
fine-Grained analysis on Document image-related tasks specifically. The Do-GOOD
benchmark defines the underlying mechanisms that result in different
distribution shifts and contains 9 OOD datasets covering 3 VDU related tasks,
e.g., document information extraction, classification and question answering.
We then evaluate the robustness and perform a fine-grained analysis of 5 latest
VDU pre-trained models and 2 typical OOD generalization algorithms on these OOD
datasets. Results from the experiments demonstrate that there is a significant
performance gap between the in-distribution (ID) and OOD settings for document
images, and that fine-grained analysis of distribution shifts can reveal the
brittle nature of existing pre-trained VDU models and OOD generalization
algorithms. The code and datasets for our Do-GOOD benchmark can be found at
https://github.com/MAEHCM/Do-GOOD.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR 2023. The code and datasets for our Do-GOOD benchmark can be
  found at https://github.com/MAEHCM/Do-GOOD</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ What Makes Entities Similar? A Similarity Flooding Perspective for
  Multi-sourced Knowledge Graph Embeddings <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02622v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02622v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zequn Sun, Jiacheng Huang, Xiaozhou Xu, Qijin Chen, Weijun Ren, Wei Hu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Joint representation learning over multi-sourced knowledge graphs (KGs)
yields transferable and expressive embeddings that improve downstream tasks.
Entity alignment (EA) is a critical step in this process. Despite recent
considerable research progress in embedding-based EA, how it works remains to
be explored. In this paper, we provide a similarity flooding perspective to
explain existing translation-based and aggregation-based EA models. We prove
that the embedding learning process of these models actually seeks a fixpoint
of pairwise similarities between entities. We also provide experimental
evidence to support our theoretical analysis. We propose two simple but
effective methods inspired by the fixpoint computation in similarity flooding,
and demonstrate their effectiveness on benchmark datasets. Our work bridges the
gap between recent embedding-based models and the conventional similarity
flooding algorithm. It would improve our understanding of and increase our
faith in embedding-based EA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in the 40th International Conference on Machine Learning
  (ICML 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Building Resilient SMEs: Harnessing Large Language Models for Cyber
  Security in Australia 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02612v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02612v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benjamin Kereopa-Yorke
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The escalating digitalisation of our lives and enterprises has led to a
parallel growth in the complexity and frequency of cyber-attacks. Small and
medium-sized enterprises (SMEs), particularly in Australia, are experiencing
increased vulnerability to cyber threats, posing a significant challenge to the
nation's cyber security landscape. Embracing transformative technologies such
as Artificial Intelligence (AI), Machine Learning (ML) and Large Language
Models (LLMs) can potentially strengthen cyber security policies for Australian
SMEs. However, their practical application, advantages, and limitations remain
underexplored, with prior research mainly focusing on large corporations. This
study aims to address this gap by providing a comprehensive understanding of
the potential role of LLMs in enhancing cyber security policies for Australian
SMEs. Employing a mixed-methods study design, this research includes a
literature review, qualitative analysis of SME case studies, and a quantitative
assessment of LLM performance metrics in cyber security applications. The
findings highlight the promising potential of LLMs across various performance
criteria, including relevance, accuracy, and applicability, though gaps remain
in areas such as completeness and clarity. The study underlines the importance
of integrating human expertise with LLM technology and refining model
development to address these limitations. By proposing a robust conceptual
framework guiding the effective adoption of LLMs, this research aims to
contribute to a safer and more resilient cyber environment for Australian SMEs,
enabling sustainable growth and competitiveness in the digital era.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>8 pages, 1 figure</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Early Rumor Detection Using Neural Hawkes Process with a New Benchmark
  <span class="highlight-title">Dataset</span> <span class="chip">NAACL 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02597v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02597v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengzhu Zeng, Wei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Little attention has been paid on \underline{EA}rly \underline{R}umor
\underline{D}etection (EARD), and EARD performance was evaluated
inappropriately on a few datasets where the actual early-stage information is
largely missing. To reverse such situation, we construct BEARD, a new
\underline{B}enchmark dataset for \underline{EARD}, based on claims from
fact-checking websites by trying to gather as many early relevant posts as
possible. We also propose HEARD, a novel model based on neural
\underline{H}awkes process for \underline{EARD}, which can guide a generic
rumor detection model to make timely, accurate and stable predictions.
Experiments show that HEARD achieves effective EARD performance on two commonly
used general rumor detection datasets and our BEARD dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at NAACL 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Novel Interpretable and Generalizable Re-synchronization Model for
  Cued Speech based on a Multi-Cuer Corpus <span class="chip">INTERSPEECH2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02596v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02596v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lufei Gao, Shan Huang, Li Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cued Speech (CS) is a multi-modal visual coding system combining lip reading
with several hand cues at the phonetic level to make the spoken language
visible to the hearing impaired. Previous studies solved asynchronous problems
between lip and hand movements by a cuer\footnote{The people who perform Cued
Speech are called the cuer.}-dependent piecewise linear model for English and
French CS. In this work, we innovatively propose three statistical measure on
the lip stream to build an interpretable and generalizable model for predicting
hand preceding time (HPT), which achieves cuer-independent by a proper
normalization. Particularly, we build the first Mandarin CS corpus comprising
annotated videos from five speakers including three normal and two hearing
impaired individuals. Consequently, we show that the hand preceding phenomenon
exists in Mandarin CS production with significant differences between normal
and hearing impaired people. Extensive experiments demonstrate that our model
outperforms the baseline and the previous state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, 4 figures, Accepted to INTERSPEECH2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Graph-Aware Language Model <span class="highlight-title">Pre-Train</span>ing on a Large Graph Corpus Can Help
  Multiple Graph Applications <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02592v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02592v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Han Xie, Da Zheng, Jun Ma, Houyu Zhang, Vassilis N. Ioannidis, Xiang Song, Qing Ping, Sheng Wang, Carl Yang, Yi Xu, Belinda Zeng, Trishul Chilimbi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Model pre-training on large text corpora has been demonstrated effective for
various downstream applications in the NLP domain. In the graph mining domain,
a similar analogy can be drawn for pre-training graph models on large graphs in
the hope of benefiting downstream graph applications, which has also been
explored by several recent studies. However, no existing study has ever
investigated the pre-training of text plus graph models on large heterogeneous
graphs with abundant textual information (a.k.a. large graph corpora) and then
fine-tuning the model on different related downstream applications with
different graph schemas. To address this problem, we propose a framework of
graph-aware language model pre-training (GALM) on a large graph corpus, which
incorporates large language models and graph neural networks, and a variety of
fine-tuning methods on downstream applications. We conduct extensive
experiments on Amazon's real internal datasets and large public datasets.
Comprehensive empirical results and in-depth analysis demonstrate the
effectiveness of our proposed methods along with lessons learned.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To be published in the KDD 2023 proceedings as a full paper</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Cross-Lingual Transfer Learning for Phrase Break Prediction with
  Multilingual Language Model <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02579v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02579v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoyeon Lee, Hyun-Wook Yoon, Jong-Hwan Kim, Jae-Min Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Phrase break prediction is a crucial task for improving the prosody
naturalness of a text-to-speech (TTS) system. However, most proposed phrase
break prediction models are monolingual, trained exclusively on a large amount
of labeled data. In this paper, we address this issue for low-resource
languages with limited labeled data using cross-lingual transfer. We
investigate the effectiveness of zero-shot and few-shot cross-lingual transfer
for phrase break prediction using a pre-trained multilingual language model. We
use manually collected datasets in four Indo-European languages: one
high-resource language and three with limited resources. Our findings
demonstrate that cross-lingual transfer learning can be a particularly
effective approach, especially in the few-shot setting, for improving
performance in low-resource languages. This suggests that cross-lingual
transfer can be inexpensive and effective for developing TTS front-end in
resource-poor languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Prompt</span> to be Consistent is Better than Self-Consistent? Few-Shot and
  Zero-Shot Fact Verification with <span class="highlight-title">Pre-train</span>ed Language Models <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02569v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02569v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengzhu Zeng, Wei Gao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Few-shot or zero-shot fact verification only relies on a few or no labeled
training examples. In this paper, we propose a novel method called ProToCo, to
\underline{Pro}mpt pre-trained language models (PLMs) \underline{To} be
\underline{Co}nsistent, for improving the factuality assessment capability of
PLMs in the few-shot and zero-shot settings. Given a claim-evidence pair,
ProToCo generates multiple variants of the claim with different relations and
frames a simple consistency mechanism as constraints for making compatible
predictions across these variants. We update PLMs by using parameter-efficient
fine-tuning (PEFT), leading to more accurate predictions in few-shot and
zero-shot fact verification tasks. Our experiments on three public verification
datasets show that ProToCo significantly outperforms state-of-the-art few-shot
fact verification baselines. With a small number of unlabeled instances,
ProToCo also outperforms the strong zero-shot learner T0 on zero-shot
verification. Compared to large PLMs using in-context learning (ICL) method,
ProToCo outperforms OPT-30B and the Self-Consistency-enabled OPT-6.7B model in
both few- and zero-shot settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as ACL 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LLM-Blender: Ensembling Large Language Models with Pairwise Ranking and
  Generative Fusion <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02561v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02561v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dongfu Jiang, Xiang Ren, Bill Yuchen Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present LLM-Blender, an ensembling framework designed to attain
consistently superior performance by leveraging the diverse strengths of
multiple open-source large language models (LLMs). Our framework consists of
two modules: PairRanker and GenFuser, addressing the observation that optimal
LLMs for different examples can significantly vary. PairRanker employs a
specialized pairwise comparison method to distinguish subtle differences
between candidate outputs. It jointly encodes the input text and a pair of
candidates, using cross-attention encoders to determine the superior one. Our
results demonstrate that PairRanker exhibits the highest correlation with
ChatGPT-based ranking. Then, GenFuser aims to merge the top-ranked candidates,
generating an improved output by capitalizing on their strengths and mitigating
their weaknesses. To facilitate large-scale evaluation, we introduce a
benchmark dataset, MixInstruct, which is a mixture of multiple instruction
datasets featuring oracle pairwise comparisons. Our LLM-Blender significantly
outperform individual LLMs and baseline methods across various metrics,
establishing a substantial performance gap.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 (Main conference). Project website:
  https://yuchenlin.xyz/LLM-Blender/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Relate to Previous Turns in Conversational Search <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02553v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02553v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fengran Mo, Jian-Yun Nie, Kaiyu Huang, Kelong Mao, Yutao Zhu, Peng Li, Yang Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conversational search allows a user to interact with a search system in
multiple turns. A query is strongly dependent on the conversation context. An
effective way to improve retrieval effectiveness is to expand the current query
with historical queries. However, not all the previous queries are related to,
and useful for expanding the current query. In this paper, we propose a new
method to select relevant historical queries that are useful for the current
query. To cope with the lack of labeled training data, we use a pseudo-labeling
approach to annotate useful historical queries based on their impact on the
retrieval results. The pseudo-labeled data are used to train a selection model.
We further propose a multi-task learning framework to jointly train the
selector and the retriever during fine-tuning, allowing us to mitigate the
possible inconsistency between the pseudo labels and the changed retriever.
Extensive experiments on four conversational search datasets demonstrate the
effectiveness and broad applicability of our method compared with several
strong baselines.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by SIGKDD 2023 Research Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluation of AI Chatbots for Patient-Specific EHR Questions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02549v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02549v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alaleh Hamidi, Kirk Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates the use of artificial intelligence chatbots for
patient-specific question answering (QA) from clinical notes using several
large language model (LLM) based systems: ChatGPT (versions 3.5 and 4), Google
Bard, and Claude. We evaluate the accuracy, relevance, comprehensiveness, and
coherence of the answers generated by each model using a 5-point Likert scale
on a set of patient-specific questions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Incorporating L2 Phonemes Using Articulatory Features for Robust Speech
  Recognition <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02534v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02534v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jisung Wang, Haram Lee, Myungwoo Oh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The limited availability of non-native speech datasets presents a major
challenge in automatic speech recognition (ASR) to narrow the performance gap
between native and non-native speakers. To address this, the focus of this
study is on the efficient incorporation of the L2 phonemes, which in this work
refer to Korean phonemes, through articulatory feature analysis. This not only
enables accurate modeling of pronunciation variants but also allows for the
utilization of both native Korean and English speech datasets. We employ the
lattice-free maximum mutual information (LF-MMI) objective in an end-to-end
manner, to train the acoustic model to align and predict one of multiple
pronunciation candidates. Experimental results show that the proposed method
improves ASR accuracy for Korean L2 speech by training solely on L1 speech
data. Furthermore, fine-tuning on L2 speech improves recognition accuracy for
both L1 and L2 speech without performance trade-offs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ PLANNER: Generating Diversified Paragraph via Latent Language Diffusion
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02531v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02531v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yizhe Zhang, Jiatao Gu, Zhuofeng Wu, Shuangfei Zhai, Josh Susskind, Navdeep Jaitly
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Autoregressive models for text sometimes generate repetitive and low-quality
output because errors accumulate during the steps of generation. This issue is
often attributed to exposure bias - the difference between how a model is
trained, and how it is used during inference. Denoising diffusion models
provide an alternative approach in which a model can revisit and revise its
output. However, they can be computationally expensive and prior efforts on
text have led to models that produce less fluent output compared to
autoregressive models, especially for longer text and paragraphs. In this
paper, we propose PLANNER, a model that combines latent semantic diffusion with
autoregressive generation, to generate fluent text while exercising global
control over paragraphs. The model achieves this by combining an autoregressive
"decoding" module with a "planning" module that uses latent diffusion to
generate semantic paragraph embeddings in a coarse-to-fine manner. The proposed
method is evaluated on various conditional generation tasks, and results on
semantic generation, text completion and summarization show its effectiveness
in generating high-quality long-form text in an efficient manner.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Study of Situational Reasoning for Traffic Understanding <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02520v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02520v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiarui Zhang, Filip Ilievski, Kaixin Ma, Aravinda Kollaa, Jonathan Francis, Alessandro Oltramari
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Intelligent Traffic Monitoring (ITMo) technologies hold the potential for
improving road safety/security and for enabling smart city infrastructure.
Understanding traffic situations requires a complex fusion of perceptual
information with domain-specific and causal commonsense knowledge. Whereas
prior work has provided benchmarks and methods for traffic monitoring, it
remains unclear whether models can effectively align these information sources
and reason in novel scenarios. To address this assessment gap, we devise three
novel text-based tasks for situational reasoning in the traffic domain: i)
BDD-QA, which evaluates the ability of Language Models (LMs) to perform
situational decision-making, ii) TV-QA, which assesses LMs' abilities to reason
about complex event causality, and iii) HDT-QA, which evaluates the ability of
models to solve human driving exams. We adopt four knowledge-enhanced methods
that have shown generalization capability across language reasoning tasks in
prior work, based on natural language inference, commonsense knowledge-graph
self-supervision, multi-QA joint training, and dense retrieval of domain
information. We associate each method with a relevant knowledge source,
including knowledge graphs, relevant benchmarks, and driving manuals. In
extensive experiments, we benchmark various knowledge-aware methods against the
three datasets, under zero-shot evaluation; we provide in-depth analyses of
model performance on data partitions and examine model predictions
categorically, to yield useful insights on traffic understanding, given
different background knowledge and reasoning strategies.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 6 figures, 5 tables, camera ready version of SIGKDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Jambu: A historical linguistic database for South Asian languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02514v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02514v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aryaman Arora, Adam Farris, Samopriya Basu, Suresh Kolichala
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce Jambu, a cognate database of South Asian languages which unifies
dozens of previous sources in a structured and accessible format. The database
includes 287k lemmata from 602 lects, grouped together in 23k sets of cognates.
We outline the data wrangling necessary to compile the dataset and train neural
models for reflex prediction on the Indo-Aryan subset of the data. We hope that
Jambu is an invaluable resource for all historical linguists and Indologists,
and look towards further improvement and expansion of the database.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages main text, 10 pages total. To appear at SIGMORPHON</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ CoSiNES: Contrastive Siamese Network for Entity Standardization <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03316v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03316v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiaqing Yuan, Michele Merler, Mihir Choudhury, Raju Pavuluri, Munindar P. Singh, Maja Vukovic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Entity standardization maps noisy mentions from free-form text to standard
entities in a knowledge base. The unique challenge of this task relative to
other entity-related tasks is the lack of surrounding context and numerous
variations in the surface form of the mentions, especially when it comes to
generalization across domains where labeled data is scarce. Previous research
mostly focuses on developing models either heavily relying on context, or
dedicated solely to a specific domain. In contrast, we propose CoSiNES, a
generic and adaptable framework with Contrastive Siamese Network for Entity
Standardization that effectively adapts a pretrained language model to capture
the syntax and semantics of the entities in a new domain.
  We construct a new dataset in the technology domain, which contains 640
technical stack entities and 6,412 mentions collected from industrial content
management systems. We demonstrate that CoSiNES yields higher accuracy and
faster runtime than baselines derived from leading methods in this domain.
CoSiNES also achieves competitive performance in four standard datasets from
the chemistry, medicine, and biomedical domains, demonstrating its cross-domain
applicability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by Matching Workshop at ACL2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Few Shot Rationale Generation using Self-Training with Dual Teachers <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03315v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03315v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aditya Srikanth Veerubhotla, Lahari Poddar, Jun Yin, György Szarvas, Sharanya Eswaran
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-rationalizing models that also generate a free-text explanation for
their predicted labels are an important tool to build trustworthy AI
applications. Since generating explanations for annotated labels is a laborious
and costly pro cess, recent models rely on large pretrained language models
(PLMs) as their backbone and few-shot learning. In this work we explore a
self-training approach leveraging both labeled and unlabeled data to further
improve few-shot models, under the assumption that neither human written
rationales nor annotated task labels are available at scale. We introduce a
novel dual-teacher learning framework, which learns two specialized teacher
models for task prediction and rationalization using self-training and distills
their knowledge into a multi-tasking student model that can jointly generate
the task label and rationale. Furthermore, we formulate a new loss function,
Masked Label Regularization (MLR) which promotes explanations to be strongly
conditioned on predicted labels. Evaluation on three public datasets
demonstrate that the proposed methods are effective in modeling task labels and
generating faithful rationales.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL Findings 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Scalable and Adaptive System to Infer the Industry Sectors of
  Companies: <span class="highlight-title">Prompt</span> + Model Tuning of Generative Language Models <span class="chip">IJCAI2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03313v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03313v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lele Cao, Vilhelm von Ehrenheim, Astrid Berghult, Cecilia Henje, Richard Anselmo Stahl, Joar Wandborg, Sebastian Stan, Armin Catovic, Erik Ferm, Hannes Ingelhag
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Private Equity (PE) firms operate investment funds by acquiring and
managing companies to achieve a high return upon selling. Many PE funds are
thematic, meaning investment professionals aim to identify trends by covering
as many industry sectors as possible, and picking promising companies within
these sectors. So, inferring sectors for companies is critical to the success
of thematic PE funds. In this work, we standardize the sector framework and
discuss the typical challenges; we then introduce our sector inference system
addressing these challenges. Specifically, our system is built on a
medium-sized generative language model, finetuned with a prompt + model tuning
procedure. The deployed model demonstrates a superior performance than the
common baselines. The system has been serving many PE professionals for over a
year, showing great scalability to data volume and adaptability to any change
in sector framework and/or annotation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by FinNLP (Financial Technology and Natural Language
  Processing) @ IJCAI2023 as long paper (8 pages and 8 figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Stack Over-Flowing with Results: The Case for Domain-Specific
  <span class="highlight-title">Pre-Train</span>ing Over One-Size-Fits-All Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03268v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03268v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Manisha Mukherjee, Vincent J. Hellendoorn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large pre-trained neural language models have brought immense progress to
both NLP and software engineering. Models in OpenAI's GPT series now dwarf
Google's BERT and Meta's RoBERTa, which previously set new benchmarks on a wide
range of NLP applications. These models are trained on massive corpora of
heterogeneous data from web crawls, which enables them to learn general
language patterns and semantic relationships. However, the largest models are
both expensive to train and deploy and are often closed-source, so we lack
access to their data and design decisions. We argue that this trend towards
large, general-purpose models should be complemented with single-purpose, more
modestly sized pre-trained models. In this work, we take StackOverflow (SO) as
a domain example in which large volumes of rich aligned code and text data is
available. We adopt standard practices for pre-training large language models,
including using a very large context size (2,048 tokens), batch size (0.5M
tokens) and training set (27B tokens), coupled with a powerful toolkit
(Megatron-LM), to train two models: SOBertBase, with 109M parameters, and
SOBertLarge with 762M parameters, at a budget of just $187 and $800 each. We
compare the performance of our models with both the previous SOTA model trained
on SO data exclusively as well general-purpose BERT models and OpenAI's ChatGPT
on four SO-specific downstream tasks - question quality prediction, closed
question prediction, named entity recognition and obsoletion prediction (a new
task we introduce). Not only do our models consistently outperform all
baselines, the smaller model is often sufficient for strong results. Both
models are released to the public. These results demonstrate that pre-training
both extensively and properly on in-domain data can yield a powerful and
affordable alternative to leveraging closed-source general-purpose models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ shs-nlp at RadSum23: Domain-Adaptive <span class="highlight-title">Pre-train</span>ing of Instruction-tuned
  LLMs for Radiology Report Impression Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03264v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03264v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanjeev Kumar Karn, Rikhiya Ghosh, Kusuma P, Oladimeji Farri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction-tuned generative Large language models (LLMs) like ChatGPT and
Bloomz possess excellent generalization abilities, but they face limitations in
understanding radiology reports, particularly in the task of generating the
IMPRESSIONS section from the FINDINGS section. They tend to generate either
verbose or incomplete IMPRESSIONS, mainly due to insufficient exposure to
medical text data during training. We present a system which leverages
large-scale medical text data for domain-adaptive pre-training of
instruction-tuned LLMs to enhance its medical knowledge and performance on
specific medical tasks. We show that this system performs better in a zero-shot
setting than a number of pretrain-and-finetune adaptation methods on the
IMPRESSIONS generation task, and ranks 1st among participating systems in Task
1B: Radiology Report Summarization at the BioNLP 2023 workshop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>1st Place in Task 1B: Radiology Report Summarization at BioNLP 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Understanding the Effectiveness of Early Weight Averaging for Training
  Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03241v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03241v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sunny Sanyal, Jean Kaddour, Abhishek Kumar, Sujay Sanghavi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Training LLMs is expensive, and recent evidence indicates training all the
way to convergence is inefficient. In this paper, we investigate the ability of
a simple idea, checkpoint averaging along the trajectory of a training run to
improve the quality of models before they have converged. This approach incurs
no extra cost during training or inference. Specifically, we analyze the
training trajectories of Pythia LLMs with 1 to 12 billion parameters and
demonstrate that, particularly during the early to mid stages of training, this
idea accelerates convergence and improves both test and zero-shot
generalization. Loss spikes are a well recognized problem in LLM training; in
our analysis we encountered two instances of this in the underlying
trajectories, and both instances were mitigated by our averaging.
  For a 6.9B parameter LLM, for example, our early weight averaging recipe can
save upto 4200 hours of GPU time, which corresponds to significant savings in
cloud compute costs.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, 12 figures, under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ NLU on Data Diets: Dynamic Data Subset Selection for NLP Classification
  Tasks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03208v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03208v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jean-Michel Attendu, Jean-Philippe Corbeil
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Finetuning large language models inflates the costs of NLU applications and
remains the bottleneck of development cycles. Recent works in computer vision
use data pruning to reduce training time. Pruned data selection with static
methods is based on a score calculated for each training example prior to
finetuning, which involves important computational overhead. Moreover, the
score may not necessarily be representative of sample importance throughout the
entire training duration. We propose to address these issues with a refined
version of dynamic data pruning, a curriculum which periodically scores and
discards unimportant examples during finetuning. Our method leverages an EL2N
metric that we extend to the joint intent and slot classification task, and an
initial finetuning phase on the full train set. Our results on the GLUE
benchmark and four joint NLU datasets show a better time-accuracy trade-off
compared to static methods. Our method preserves full accuracy while training
on 50% of the data points and reduces computational times by up to 41%. If we
tolerate instead a minor drop of accuracy of 1%, we can prune 80% of the
training examples for a reduction in finetuning time reaching 66%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Static Evaluation of Code Completion by Large Language Models <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03203v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03203v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hantian Ding, Varun Kumar, Yuchen Tian, Zijian Wang, Rob Kwiatkowski, Xiaopeng Li, Murali Krishna Ramanathan, Baishakhi Ray, Parminder Bhatia, Sudipta Sengupta, Dan Roth, Bing Xiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models trained on code have shown great potential to increase
productivity of software developers. Several execution-based benchmarks have
been proposed to evaluate functional correctness of model-generated code on
simple programming problems. Nevertheless, it is expensive to perform the same
evaluation on complex real-world projects considering the execution cost. On
the contrary, static analysis tools such as linters, which can detect errors
without running the program, haven't been well explored for evaluating code
generation models. In this work, we propose a static evaluation framework to
quantify static errors in Python code completions, by leveraging Abstract
Syntax Trees. Compared with execution-based evaluation, our method is not only
more efficient, but also applicable to code in the wild. For experiments, we
collect code context from open source repos to generate one million function
bodies using public models. Our static analysis reveals that Undefined Name and
Unused Variable are the most common errors among others made by language
models. Through extensive studies, we also show the impact of sampling
temperature, model size, and context on static errors in code completions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2023 industry track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AutoScrum: Automating Project Planning Using Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03197v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03197v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Martin Schroder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advancements in the field of large language models have made it
possible to use language models for advanced reasoning. In this paper we
leverage this ability for designing complex project plans based only on knowing
the current state and the desired state. Two approaches are demonstrated - a
scrum based approach and a shortcut plan approach. The scrum based approach
executes an automated process of requirements gathering, user story mapping,
feature identification, task decomposition and finally generates questions and
search terms for seeking out domain specific information to assist with task
completion. The shortcut approach looks at most recent snapshot of the current
and desired state and generates the next most reasonable task to do in order to
get to the desired state as quickly as possible. In this paper we automate
everything using a novel concept of "Language Programs". These are programs
written in natural language designed to process input data through the language
model. Guidance language is used for all LLM programs. All demo source code for
this paper is available at https://github.com/autoscrum/autoscrum
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 3 figures, demo: https://github.com/autoscrum/autoscrum</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Easy-to-Read in Germany: A <span class="highlight-title">Survey</span> on its Current State and Available
  Resources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03189v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03189v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Margot Madina, Itziar Gonzalez-Dios, Melanie Siegel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Easy-to-Read Language (E2R) is a controlled language variant that makes any
written text more accessible through the use of clear, direct and simple
language. It is mainly aimed at people with cognitive or intellectual
disabilities, among other target users. Plain Language (PL), on the other hand,
is a variant of a given language, which aims to promote the use of simple
language to communicate information. German counts with Leichte Sprache (LS),
its version of E2R, and Einfache Sprache (ES), its version of PL. In recent
years, important developments have been conducted in the field of LS. This
paper offers an updated overview of the existing Natural Language Processing
(NLP) tools and resources for LS. Besides, it also aims to set out the
situation with regard to LS and ES in Germany.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10th Language & Technology Conference: Human Language Technologies as
  a Challenge for Computer Science and Linguistics, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Composition and Deformance: Measuring Imageability with a Text-to-Image
  Model 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03168v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03168v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Si Wu, David A. Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although psycholinguists and psychologists have long studied the tendency of
linguistic strings to evoke mental images in hearers or readers, most
computational studies have applied this concept of imageability only to
isolated words. Using recent developments in text-to-image generation models,
such as DALLE mini, we propose computational methods that use generated images
to measure the imageability of both single English words and connected text. We
sample text prompts for image generation from three corpora: human-generated
image captions, news article sentences, and poem lines. We subject these
prompts to different deformances to examine the model's ability to detect
changes in imageability caused by compositional change. We find high
correlation between the proposed computational measures of imageability and
human judgments of individual words. We also find the proposed measures more
consistently respond to changes in compositionality than baseline approaches.
We discuss possible effects of model training and implications for the study of
compositionality in text-to-image models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unsupervised Dense Retrieval with Relevance-Aware Contrastive
  <span class="highlight-title">Pre-Train</span>ing <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03166v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03166v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibin Lei, Liang Ding, Yu Cao, Changtong Zan, Andrew Yates, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Dense retrievers have achieved impressive performance, but their demand for
abundant training data limits their application scenarios. Contrastive
pre-training, which constructs pseudo-positive examples from unlabeled data,
has shown great potential to solve this problem. However, the pseudo-positive
examples crafted by data augmentations can be irrelevant. To this end, we
propose relevance-aware contrastive learning. It takes the intermediate-trained
model itself as an imperfect oracle to estimate the relevance of positive pairs
and adaptively weighs the contrastive loss of different pairs according to the
estimated relevance. Our method consistently improves the SOTA unsupervised
Contriever model on the BEIR and open-domain QA retrieval benchmarks. Further
exploration shows that our method can not only beat BM25 after further
pre-training on the target corpus but also serves as a good few-shot learner.
Our code is publicly available at https://github.com/Yibin-Lei/ReContriever.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 Findings (Short), 5 pages main + 1 page references + 1 page
  appendix</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PaLI: A Jointly-Scaled Multilingual Language-Image Model <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.06794v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.06794v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme, Andreas Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, Radu Soricut
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Effective scaling and a flexible task interface enable large language models
to excel at many tasks. We present PaLI (Pathways Language and Image model), a
model that extends this approach to the joint modeling of language and vision.
PaLI generates text based on visual and textual inputs, and with this interface
performs many vision, language, and multimodal tasks, in many languages. To
train PaLI, we make use of large pre-trained encoder-decoder language models
and Vision Transformers (ViTs). This allows us to capitalize on their existing
capabilities and leverage the substantial cost of training them. We find that
joint scaling of the vision and language components is important. Since
existing Transformers for language are much larger than their vision
counterparts, we train a large, 4-billion parameter ViT (ViT-e) to quantify the
benefits from even larger-capacity vision models. To train PaLI, we create a
large multilingual mix of pretraining tasks, based on a new image-text training
set containing 10B images and texts in over 100 languages. PaLI achieves
state-of-the-art in multiple vision and language tasks (such as captioning,
visual question-answering, scene-text understanding), while retaining a simple,
modular, and scalable design.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023 (Notable-top-5%)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-context Example Selection with Influences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11042v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11042v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tai Nguyen, Eric Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) is a powerful paradigm emerged from large language
models (LLMs). Despite its promises, ICL performance is known to be highly
sensitive to input examples. In this work, we use $\textit{in-context
influences}$ to analyze few-shot ICL performance directly from the in-context
examples. Our proposed influence-based example selection method can identify
both positive and negative examples, outperforming several baselines when
evaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a $16.3\%$
performance gap between using the most negative in-context examples compared to
the most positive. In a case study, we apply our influence-based framework to
quantify the phenomena of recency bias in example ordering for few-shot ICL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ NusaCrowd: Open Source Initiative for Indonesian NLP Resources 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09648v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09648v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Cahyawijaya, Holy Lovenia, Alham Fikri Aji, Genta Indra Winata, Bryan Wilie, Rahmad Mahendra, Christian Wibisono, Ade Romadhony, Karissa Vincentio, Fajri Koto, Jennifer Santoso, David Moeljadi, Cahya Wirawan, Frederikus Hudi, Ivan Halim Parmonangan, Ika Alfina, Muhammad Satrio Wicaksono, Ilham Firdausi Putra, Samsul Rahmadani, Yulianti Oenang, Ali Akbar Septiandri, James Jaya, Kaustubh D. Dhole, Arie Ardiyanti Suryani, Rifki Afina Putri, Dan Su, Keith Stevens, Made Nindyatama Nityasya, Muhammad Farid Adilazuarda, Ryan Ignatius, Ryandito Diandaru, Tiezheng Yu, Vito Ghifari, Wenliang Dai, Yan Xu, Dyah Damapuspita, Cuk Tho, Ichwanul Muslim Karo Karo, Tirana Noor Fatyanosa, Ziwei Ji, Pascale Fung, Graham Neubig, Timothy Baldwin, Sebastian Ruder, Herry Sujaini, Sakriani Sakti, Ayu Purwarianti
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present NusaCrowd, a collaborative initiative to collect and unify
existing resources for Indonesian languages, including opening access to
previously non-public resources. Through this initiative, we have brought
together 137 datasets and 118 standardized data loaders. The quality of the
datasets has been assessed manually and automatically, and their value is
demonstrated through multiple experiments. NusaCrowd's data collection enables
the creation of the first zero-shot benchmarks for natural language
understanding and generation in Indonesian and the local languages of
Indonesia. Furthermore, NusaCrowd brings the creation of the first multilingual
automatic speech recognition benchmark in Indonesian and the local languages of
Indonesia. Our work strives to advance natural language processing (NLP)
research for languages that are under-represented despite being widely spoken.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Systematic Study and Comprehensive Evaluation of Chat<span class="highlight-title">GPT</span> on Benchmark
  <span class="highlight-title">Dataset</span>s <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18486v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18486v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, Jimmy Xiangji Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of large language models (LLMs) such as ChatGPT has brought a
lot of attention recently. However, their evaluation in the benchmark academic
datasets remains under-explored due to the difficulty of evaluating the
generative outputs produced by this model against the ground truth. In this
paper, we aim to present a thorough evaluation of ChatGPT's performance on
diverse academic datasets, covering tasks like question-answering, text
summarization, code generation, commonsense reasoning, mathematical
problem-solving, machine translation, bias detection, and ethical
considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze
255K responses it generates in these datasets. This makes our work the largest
evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate
the strengths and weaknesses of ChatGPT in various tasks and provide insights
for future research using LLMs. We also report a new emergent ability to follow
multi-query instructions that we mostly found in ChatGPT and other
instruction-tuned models. Our extensive evaluation shows that even though
ChatGPT is capable of performing a wide variety of tasks, and may obtain
impressive performance in several benchmark datasets, it is still far from
achieving the ability to reliably solve many challenging tasks. By providing a
thorough assessment of ChatGPT's performance across diverse NLP tasks, this
paper sets the stage for a targeted deployment of ChatGPT-like LLMs in
real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2023 Findings. The first three authors contributed
  equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Inter-Bilingual Semantic Parsing for Indian Languages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.13005v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.13005v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Divyanshu Aggarwal, Vivek Gupta, Anoop Kunchukuttan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite significant progress in Natural Language Generation for Indian
languages (IndicNLP), there is a lack of datasets around complex structured
tasks such as semantic parsing. One reason for this imminent gap is the
complexity of the logical form, which makes English to multilingual translation
difficult. The process involves alignment of logical forms, intents and slots
with translated unstructured utterance. To address this, we propose an
Inter-bilingual Seq2seq Semantic parsing dataset IE-SEMPARSE for 11 distinct
Indian languages. We highlight the proposed task's practicality, and evaluate
existing multilingual seq2seq models across several train-test strategies. Our
experiment reveals a high correlation across performance of original
multilingual semantic parsing datasets (such as mTOP, multilingual TOP and
multiATIS++) and our proposed IE-SEMPARSE suite.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 9 figures, 15 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tutel: Adaptive Mixture-of-Experts at Scale 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.03382v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.03382v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changho Hwang, Wei Cui, Yifan Xiong, Ziyue Yang, Ze Liu, Han Hu, Zilong Wang, Rafael Salas, Jithin Jose, Prabhat Ram, Joe Chau, Peng Cheng, Fan Yang, Mao Yang, Yongqiang Xiong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparsely-gated mixture-of-experts (MoE) has been widely adopted to scale deep
learning models to trillion-plus parameters with fixed computational cost. The
algorithmic performance of MoE relies on its token routing mechanism that
forwards each input token to the right sub-models or experts. While token
routing dynamically determines the amount of expert workload at runtime,
existing systems suffer inefficient computation due to their static execution,
namely static parallelism and pipelining, which does not adapt to the dynamic
workload. We present Flex, a highly scalable stack design and implementation
for MoE with dynamically adaptive parallelism and pipelining. Flex designs an
identical layout for distributing MoE model parameters and input data, which
can be leveraged by all possible parallelism or pipelining methods without any
mathematical inequivalence or tensor migration overhead. This enables adaptive
parallelism/pipelining optimization at zero cost during runtime. Based on this
key design, Flex also implements various MoE acceleration techniques.
Aggregating all techniques, Flex finally delivers huge speedup at any scale --
4.96x and 5.75x speedup of a single MoE layer over 16 and 2,048 A100 GPUs,
respectively, over the previous state-of-the-art. Our evaluation shows that
Flex efficiently and effectively runs a real-world MoE-based model named
SwinV2-MoE, built upon Swin Transformer V2, a state-of-the-art computer vision
architecture. On efficiency, Flex accelerates SwinV2-MoE, achieving up to 1.55x
and 2.11x speedup in training and inference over Fairseq, respectively. On
effectiveness, the SwinV2-MoE model achieves superior accuracy in both
pre-training and down-stream computer vision tasks such as COCO object
detection than the counterpart dense model, indicating the readiness of Flex
for end-to-end real-world model training and inference.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language
  Representation Learning <span class="chip">SIGIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04183v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04183v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijia Zhao, Longteng Guo, Xingjian He, Shuai Shao, Zehuan Yuan, Jing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal representation learning has shown promising improvements on
various vision-language tasks. Most existing methods excel at building
global-level alignment between vision and language while lacking effective
fine-grained image-text interaction. In this paper, we propose a jointly masked
multimodal modeling method to learn fine-grained multimodal representations.
Our method performs joint masking on image-text input and integrates both
implicit and explicit targets for the masked signals to recover. The implicit
target provides a unified and debiased objective for vision and language, where
the model predicts latent multimodal representations of the unmasked input. The
explicit target further enriches the multimodal representations by recovering
high-level and semantically meaningful information: momentum visual features of
image patches and concepts of word tokens. Through such a masked modeling
process, our model not only learns fine-grained multimodal interaction, but
also avoids the semantic gap between high-level representations and low- or
mid-level prediction targets (e.g. image pixels), thus producing semantically
rich multimodal representations that perform well on both zero-shot and
fine-tuned settings. Our pre-trained model (named MAMO) achieves
state-of-the-art performance on various downstream vision-language tasks,
including image-text retrieval, visual question answering, visual reasoning,
and weakly-supervised visual grounding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR 2023, 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Signs of Language: Embodied Sign Language Fingerspelling Acquisition
  from Demonstrations for Human-Robot Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.05135v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.05135v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Tavella, Aphrodite Galata, Angelo Cangelosi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning fine-grained movements is a challenging topic in robotics,
particularly in the context of robotic hands. One specific instance of this
challenge is the acquisition of fingerspelling sign language in robots. In this
paper, we propose an approach for learning dexterous motor imitation from video
examples without additional information. To achieve this, we first build a URDF
model of a robotic hand with a single actuator for each joint. We then leverage
pre-trained deep vision models to extract the 3D pose of the hand from RGB
videos. Next, using state-of-the-art reinforcement learning algorithms for
motion imitation (namely, proximal policy optimization and soft actor-critic),
we train a policy to reproduce the movement extracted from the demonstrations.
We identify the optimal set of hyperparameters for imitation based on a
reference motion. Finally, we demonstrate the generalizability of our approach
by testing it on six different tasks, corresponding to fingerspelled letters.
Our results show that our approach is able to successfully imitate these
fine-grained movements without additional information, highlighting its
potential for real-world applications in robotics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-train</span>ing for Speech Translation: CTC Meets Optimal Transport <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11716v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11716v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phuong-Hang Le, Hongyu Gong, Changhan Wang, Juan Pino, Benjamin Lecouteux, Didier Schwab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The gap between speech and text modalities is a major challenge in
speech-to-text translation (ST). Different methods have been proposed to reduce
this gap, but most of them require architectural changes in ST training. In
this work, we propose to mitigate this issue at the pre-training stage,
requiring no change in the ST model. First, we show that the connectionist
temporal classification (CTC) loss can reduce the modality gap by design. We
provide a quantitative comparison with the more common cross-entropy loss,
showing that pre-training with CTC consistently achieves better final ST
accuracy. Nevertheless, CTC is only a partial solution and thus, in our second
contribution, we propose a novel pre-training method combining CTC and optimal
transport to further reduce this gap. Our method pre-trains a Siamese-like
model composed of two encoders, one for acoustic inputs and the other for
textual inputs, such that they produce representations that are close to each
other in the Wasserstein space. Extensive experiments on the standard CoVoST-2
and MuST-C datasets show that our pre-training method applied to the vanilla
encoder-decoder Transformer achieves state-of-the-art performance under the
no-external-data setting, and performs on par with recent strong multi-task
learning systems trained with external data. Finally, our method can also be
applied on top of these multi-task systems, leading to further improvements for
these models. Code and pre-trained models are available at
https://github.com/formiel/fairseq.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023 (oral presentation). This version fixed URLs, updated
  affiliations & acknowledgements, and improved formatting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Boosting Radiology Report Generation by Infusing Comparison Prior <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04561v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04561v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sanghwan Kim, Farhad Nooralahzadeh, Morteza Rohanian, Koji Fujimoto, Mizuho Nishio, Ryo Sakamoto, Fabio Rinaldi, Michael Krauthammer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent transformer-based models have made significant strides in generating
radiology reports from chest X-ray images. However, a prominent challenge
remains: these models often lack prior knowledge, resulting in the generation
of synthetic reports that mistakenly reference non-existent prior exams. This
discrepancy can be attributed to a knowledge gap between radiologists and the
generation models. While radiologists possess patient-specific prior
information, the models solely receive X-ray images at a specific time point.
To tackle this issue, we propose a novel approach that leverages a rule-based
labeler to extract comparison prior information from radiology reports. This
extracted comparison prior is then seamlessly integrated into state-of-the-art
transformer-based models, enabling them to produce more realistic and
comprehensive reports. Our method is evaluated on English report datasets, such
as IU X-ray and MIMIC-CXR. The results demonstrate that our approach surpasses
baseline models in terms of natural language generation metrics. Notably, our
model generates reports that are free from false references to non-existent
prior exams, setting it apart from previous models. By addressing this
limitation, our approach represents a significant step towards bridging the gap
between radiologists and generation models in the domain of medical report
generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2023, BioNLP Workshop</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Language Anisotropic Cross-Lingual Model Editing 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.12677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.12677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Xu, Yutai Hou, Wanxiang Che, Min Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multilingual pre-trained language models can learn task-specific abilities or
memorize facts across multiple languages but inevitably make undesired
predictions with specific inputs. Under similar observation, model editing aims
to post-hoc calibrate a model targeted to specific inputs with keeping the
model's raw behavior. However, existing work only studies the monolingual
scenario, which lacks the cross-lingual transferability to perform editing
simultaneously across languages. In this work, we focus on cross-lingual model
editing. Firstly, we define the cross-lingual model editing task and
corresponding metrics, where an edit in one language propagates to the others.
Next, we propose a framework to naturally adapt monolingual model editing
approaches to the cross-lingual scenario using parallel corpus. Further, we
propose language anisotropic editing to improve cross-lingual editing by
amplifying different subsets of parameters for each language. On the newly
defined cross-lingual model editing task, we empirically demonstrate the
failure of monolingual baselines in propagating the edit to multiple languages
and the effectiveness of the proposed language anisotropic model editing. Our
code is publicly available at https://github.com/franklear/LiME.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Assessing the Importance of Frequency versus Compositionality for
  Subword-based Tokenization in NMT 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01393v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01393v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benoist Wolleb, Romain Silvestri, Giorgos Vernikos, Ljiljana Dolamic, Andrei Popescu-Belis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Subword tokenization is the de facto standard for tokenization in neural
language models and machine translation systems. Three advantages are
frequently cited in favor of subwords: shorter encoding of frequent tokens,
compositionality of subwords, and ability to deal with unknown words. As their
relative importance is not entirely clear yet, we propose a tokenization
approach that enables us to separate frequency (the first advantage) from
compositionality. The approach uses Huffman coding to tokenize words, by order
of frequency, using a fixed amount of symbols. Experiments with CS-DE, EN-FR
and EN-DE NMT show that frequency alone accounts for 90%-95% of the scores
reached by BPE, hence compositionality has less importance than previously
thought.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EAMT 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Generative Patent Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.14578v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.14578v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieh-Sheng Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative language models are promising for assisting human writing in
various domains. This manuscript aims to build generative language models in
the patent domain and evaluate model performance from a human-centric
perspective. The perspective is to measure the ratio of keystrokes that can be
saved by autocompletion based on generative patent language models. A higher
ratio means a more effective model which can save more keystrokes. This metric
can be used to benchmark model performance. The metric is different from
conventional machine-centric metrics that are token-based instead of
keystroke-based. In terms of model size, the largest model built in this
manuscript is 6B, which is state-of-the-art in the patent domain. Based on the
metric, it is found that the largest model is not necessarily the best for the
human-centric metric. The finding means that keeping increasing model sizes in
the patent domain might be unnecessary if the purpose is to assist human
writing with autocompletion. Several patent language models are pre-trained
from scratch in this research. The pre-trained models are released for future
researchers. Several visualization tools are also provided. The importance of
building a generative language model in the patent domain is the potential to
facilitate creativity and innovations in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, and 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Unified Knowledge Graph Augmentation Service for Boosting
  Domain-specific NLP Tasks <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.05251v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.05251v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ruiqing Ding, Xiao Han, Leye Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  By focusing the pre-training process on domain-specific corpora, some
domain-specific pre-trained language models (PLMs) have achieved
state-of-the-art results. However, it is under-investigated to design a unified
paradigm to inject domain knowledge in the PLM fine-tuning stage. We propose
KnowledgeDA, a unified domain language model development service to enhance the
task-specific training procedure with domain knowledge graphs. Given
domain-specific task texts input, KnowledgeDA can automatically generate a
domain-specific language model following three steps: (i) localize domain
knowledge entities in texts via an embedding-similarity approach; (ii) generate
augmented samples by retrieving replaceable domain entity pairs from two views
of both knowledge graph and training data; (iii) select high-quality augmented
samples for fine-tuning via confidence-based assessment. We implement a
prototype of KnowledgeDA to learn language models for two domains, healthcare
and software development. Experiments on domain-specific text classification
and QA tasks verify the effectiveness and generalizability of KnowledgeDA.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL Findings 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-Context Analogical Reasoning with <span class="highlight-title">Pre-Train</span>ed Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17626v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17626v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyang Hu, Shane Storks, Richard L. Lewis, Joyce Chai
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Analogical reasoning is a fundamental capacity of human cognition that allows
us to reason abstractly about novel situations by relating them to past
experiences. While it is thought to be essential for robust reasoning in AI
systems, conventional approaches require significant training and/or
hard-coding of domain knowledge to be applied to benchmark tasks. Inspired by
cognitive science research that has found connections between human language
and analogy-making, we explore the use of intuitive language-based abstractions
to support analogy in AI systems. Specifically, we apply large pre-trained
language models (PLMs) to visual Raven's Progressive Matrices (RPM), a common
relational reasoning test. By simply encoding the perceptual features of the
problem into language form, we find that PLMs exhibit a striking capacity for
zero-shot relational reasoning, exceeding human performance and nearing
supervised vision-based methods. We explore different encodings that vary the
level of abstraction over task features, finding that higher-level abstractions
further strengthen PLMs' analogical reasoning. Our detailed analysis reveals
insights on the role of model complexity, in-context learning, and prior
knowledge in solving RPM tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Self-Edit: Fault-Aware Code Editor for Code Generation <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04087v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04087v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kechi Zhang, Zhuo Li, Jia Li, Ge Li, Zhi Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) have demonstrated an impressive ability to
generate codes on competitive programming tasks. However, with limited sample
numbers, LLMs still suffer from poor accuracy. Inspired by the process of human
programming, we propose a generate-and-edit approach named Self-Edit that
utilizes execution results of the generated code from LLMs to improve the code
quality on the competitive programming task. We execute the generated code on
the example test case provided in the question and wrap execution results into
a supplementary comment. Utilizing this comment as guidance, our fault-aware
code editor is employed to correct errors in the generated code. We perform
extensive evaluations across two competitive programming datasets with nine
different LLMs. Compared to directly generating from LLMs, our approach can
improve the average of pass@1 by 89\% on APPS-dev, 31\% on APPS-test, and 48\%
on HumanEval over nine popular code generation LLMs with parameter sizes
ranging from 110M to 175B. Compared to other post-processing methods, our
method demonstrates superior accuracy and efficiency.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Dataless Knowledge Fusion by Merging Weights of Language Models <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09849v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09849v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, Pengxiang Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fine-tuning pre-trained language models has become the prevalent paradigm for
building downstream NLP models. Oftentimes fine-tuned models are readily
available but their training data is not, due to data privacy or intellectual
property concerns. This creates a barrier to fusing knowledge across individual
models to yield a better single model. In this paper, we study the problem of
merging individual models built on different training data sets to obtain a
single model that performs well both across all data set domains and can
generalize on out-of-domain data. We propose a dataless knowledge fusion method
that merges models in their parameter space, guided by weights that minimize
prediction differences between the merged model and the individual models. Over
a battery of evaluation settings, we show that the proposed method
significantly outperforms baselines such as Fisher-weighted averaging or model
ensembling. Further, we find that our method is a promising alternative to
multi-task learning that can preserve or sometimes improve over the individual
models without access to the training data. Finally, model merging is more
efficient than training a multi-task model, thus making it applicable to a
wider set of scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICLR 2023; Updated captions of Table 1. The code is available at
  https://github.com/bloomberg/dataless-model-merging</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Large Language Models can be Guided to Evade AI-Generated Text Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10847v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10847v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ning Lu, Shengcai Liu, Rui He, Qi Wang, Ke Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models (LLMs) have demonstrated exceptional performance in a
variety of tasks, including essay writing and question answering. However, it
is crucial to address the potential misuse of these models, which can lead to
detrimental outcomes such as plagiarism and spamming. Recently, several
detectors have been proposed, including fine-tuned classifiers and various
statistical methods. In this study, we reveal that with the aid of carefully
crafted prompts, LLMs can effectively evade these detection systems. We propose
a novel Substitution-based In-Context example Optimization method (SICO) to
automatically generate such prompts. On three real-world tasks where LLMs can
be misused, SICO successfully enables ChatGPT to evade six existing detectors,
causing a significant 0.54 AUC drop on average. Surprisingly, in most cases
these detectors perform even worse than random classifiers. These results
firmly reveal the vulnerability of existing detectors. Finally, the strong
performance of SICO suggests itself as a reliable evaluation protocol for any
new detector in this field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MGR: Multi-generator Based Rationalization <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04492v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04492v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wei Liu, Haozhao Wang, Jun Wang, Ruixuan Li, Xinyang Li, Yuankai Zhang, Yang Qiu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Rationalization is to employ a generator and a predictor to construct a
self-explaining NLP model in which the generator selects a subset of
human-intelligible pieces of the input text to the following predictor.
However, rationalization suffers from two key challenges, i.e., spurious
correlation and degeneration, where the predictor overfits the spurious or
meaningless pieces solely selected by the not-yet well-trained generator and in
turn deteriorates the generator. Although many studies have been proposed to
address the two challenges, they are usually designed separately and do not
take both of them into account. In this paper, we propose a simple yet
effective method named MGR to simultaneously solve the two problems. The key
idea of MGR is to employ multiple generators such that the occurrence stability
of real pieces is improved and more meaningful pieces are delivered to the
predictor. Empirically, we show that MGR improves the F1 score by up to 20.9%
as compared to state-of-the-art methods. Codes are available at
https://github.com/jugechengzi/Rationalization-MGR .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a main conference paper of ACL 2023. arXiv admin note:
  text overlap with arXiv:2209.08285</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ BUMP: A Benchmark of Unfaithful Minimal Pairs for Meta-Evaluation of
  Faithfulness Metrics <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09955v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09955v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liang Ma, Shuyang Cao, Robert L. Logan IV, Di Lu, Shihao Ran, Ke Zhang, Joel Tetreault, Alejandro Jaimes
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The proliferation of automatic faithfulness metrics for summarization has
produced a need for benchmarks to evaluate them. While existing benchmarks
measure the correlation with human judgements of faithfulness on
model-generated summaries, they are insufficient for diagnosing whether metrics
are: 1) consistent, i.e., indicate lower faithfulness as errors are introduced
into a summary, 2) effective on human-written texts, and 3) sensitive to
different error types (as summaries can contain multiple errors). To address
these needs, we present a benchmark of unfaithful minimal pairs (BUMP), a
dataset of 889 human-written, minimally different summary pairs, where a single
error is introduced to a summary from the CNN/DailyMail dataset to produce an
unfaithful summary. We find BUMP complements existing benchmarks in a number of
ways: 1) the summaries in BUMP are harder to discriminate and less probable
under SOTA summarization models, 2) unlike non-pair-based datasets, BUMP can be
used to measure the consistency of metrics, and reveals that the most
discriminative metrics tend not to be the most consistent, and 3) unlike
datasets containing generated summaries with multiple errors, BUMP enables the
measurement of metrics' performance on individual error types.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted as a long main conference paper at ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Few-shot Domain-Adaptive Visually-fused Event Detection from Text 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03517v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03517v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Farhad Moghimifar, Fatemeh Shiri, Van Nguyen, Reza Haffari, Yuan-Fang Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating auxiliary modalities such as images into event detection models
has attracted increasing interest over the last few years. The complexity of
natural language in describing situations has motivated researchers to leverage
the related visual context to improve event detection performance. However,
current approaches in this area suffer from data scarcity, where a large amount
of labelled text-image pairs are required for model training. Furthermore,
limited access to the visual context at inference time negatively impacts the
performance of such models, which makes them practically ineffective in
real-world scenarios. In this paper, we present a novel domain-adaptive
visually-fused event detection approach that can be trained on a few labelled
image-text paired data points. Specifically, we introduce a visual imaginator
method that synthesises images from text in the absence of visual context.
Moreover, the imaginator can be customised to a specific domain. In doing so,
our model can leverage the capabilities of pre-trained vision-language models
and can be trained in a few-shot setting. This also allows for effective
inference where only single-modality data (i.e. text) is available. The
experimental evaluation on the benchmark M2E2 dataset shows that our model
outperforms existing state-of-the-art models, by up to 11 points.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ APOLLO: A Simple Approach for Adaptive <span class="highlight-title">Pretrain</span>ing of Language Models
  for Logical Reasoning <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soumya Sanyal, Yichong Xu, Shuohang Wang, Ziyi Yang, Reid Pryzant, Wenhao Yu, Chenguang Zhu, Xiang Ren
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Logical reasoning of text is an important ability that requires understanding
the information present in the text, their interconnections, and then reasoning
through them to infer new conclusions. Prior works on improving the logical
reasoning ability of language models require complex processing of training
data (e.g., aligning symbolic knowledge to text), yielding task-specific data
augmentation solutions that restrict the learning of general logical reasoning
skills. In this work, we propose APOLLO, an adaptively pretrained language
model that has improved logical reasoning abilities. We select a subset of
Wikipedia, based on a set of logical inference keywords, for continued
pretraining of a language model. We use two self-supervised loss functions: a
modified masked language modeling loss where only specific parts-of-speech
words, that would likely require more reasoning than basic language
understanding, are masked, and a sentence-level classification loss that
teaches the model to distinguish between entailment and contradiction types of
sentences. The proposed training paradigm is both simple and independent of
task formats. We demonstrate the effectiveness of APOLLO by comparing it with
prior baselines on two logical reasoning datasets. APOLLO performs comparably
on ReClor and outperforms baselines on LogiQA. The code base has been made
publicly available.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ACL 2023, code available at
  https://github.com/INK-USC/APOLLO</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Multi-hop Evidence Retrieval for Cross-document Relation Extraction <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10786v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10786v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Keming Lu, I-Hung Hsu, Wenxuan Zhou, Mingyu Derek Ma, Muhao Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Relation Extraction (RE) has been extended to cross-document scenarios
because many relations are not simply described in a single document. This
inevitably brings the challenge of efficient open-space evidence retrieval to
support the inference of cross-document relations, along with the challenge of
multi-hop reasoning on top of entities and evidence scattered in an open set of
documents. To combat these challenges, we propose MR.COD (Multi-hop evidence
retrieval for Cross-document relation extraction), which is a multi-hop
evidence retrieval method based on evidence path mining and ranking. We explore
multiple variants of retrievers to show evidence retrieval is essential in
cross-document RE. We also propose a contextual dense retriever for this
setting. Experiments on CodRED show that evidence retrieval with MR.COD
effectively acquires crossdocument evidence and boosts end-to-end RE
performance in both closed and open settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 (Findings)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Multilingual Perspective Towards the Evaluation of Attribution Methods
  in Natural Language Inference <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.05428v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.05428v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kerem Zaman, Yonatan Belinkov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most evaluations of attribution methods focus on the English language. In
this work, we present a multilingual approach for evaluating attribution
methods for the Natural Language Inference (NLI) task in terms of faithfulness
and plausibility. First, we introduce a novel cross-lingual strategy to measure
faithfulness based on word alignments, which eliminates the drawbacks of
erasure-based evaluations.We then perform a comprehensive evaluation of
attribution methods, considering different output mechanisms and aggregation
methods. Finally, we augment the XNLI dataset with highlight-based
explanations, providing a multilingual NLI dataset with highlights, to support
future exNLP studies. Our results show that attribution methods performing best
for plausibility and faithfulness are different.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 7 figures. Code and data at
  https://keremzaman.com/explaiNLI/; Published in the Proceedings of EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Narrow<span class="highlight-title">BERT</span>: Accelerating Masked Language Model <span class="highlight-title">Pretrain</span>ing and Inference <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.04761v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.04761v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haoxin Li, Phillip Keung, Daniel Cheng, Jungo Kasai, Noah A. Smith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale language model pretraining is a very successful form of
self-supervised learning in natural language processing, but it is increasingly
expensive to perform as the models and pretraining corpora have become larger
over time. We propose NarrowBERT, a modified transformer encoder that increases
the throughput for masked language model pretraining by more than $2\times$.
NarrowBERT sparsifies the transformer model such that the self-attention
queries and feedforward layers only operate on the masked tokens of each
sentence during pretraining, rather than all of the tokens as with the usual
transformer encoder. We also show that NarrowBERT increases the throughput at
inference time by as much as $3.5\times$ with minimal (or no) performance
degradation on sentence encoding tasks like MNLI. Finally, we examine the
performance of NarrowBERT on the IMDB and Amazon reviews classification and
CoNLL NER tasks and show that it is also comparable to standard BERT
performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in ACL 2023 (main conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Less is More: Task-aware Layer-wise Distillation for Language Model
  Compression <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01351v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01351v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Liang, Simiao Zuo, Qingru Zhang, Pengcheng He, Weizhu Chen, Tuo Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Layer-wise distillation is a powerful tool to compress large models (i.e.
teacher models) into small ones (i.e., student models). The student distills
knowledge from the teacher by mimicking the hidden representations of the
teacher at every intermediate layer. However, layer-wise distillation is
difficult. Since the student has a smaller model capacity than the teacher, it
is often under-fitted. Furthermore, the hidden representations of the teacher
contain redundant information that the student does not necessarily need for
the target task's learning. To address these challenges, we propose a novel
Task-aware layEr-wise Distillation (TED). TED designs task-aware filters to
align the hidden representations of the student and the teacher at each layer.
The filters select the knowledge that is useful for the target task from the
hidden representations. As such, TED reduces the knowledge gap between the two
models and helps the student to fit better on the target task. We evaluate TED
in two scenarios: continual pre-training and fine-tuning. TED demonstrates
significant and consistent improvements over existing distillation methods in
both scenarios. Code is available at
https://github.com/cliang1453/task-aware-distillation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SmoothQuant: Accurate and Efficient Post-Training Quantization for Large
  Language Models <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.10438v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.10438v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, Song Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) show excellent performance but are compute- and
memory-intensive. Quantization can reduce memory and accelerate inference.
However, existing methods cannot maintain accuracy and hardware efficiency at
the same time. We propose SmoothQuant, a training-free, accuracy-preserving,
and general-purpose post-training quantization (PTQ) solution to enable 8-bit
weight, 8-bit activation (W8A8) quantization for LLMs. Based on the fact that
weights are easy to quantize while activations are not, SmoothQuant smooths the
activation outliers by offline migrating the quantization difficulty from
activations to weights with a mathematically equivalent transformation.
SmoothQuant enables an INT8 quantization of both weights and activations for
all the matrix multiplications in LLMs, including OPT, BLOOM, GLM, MT-NLG, and
LLaMA family. We demonstrate up to 1.56x speedup and 2x memory reduction for
LLMs with negligible loss in accuracy. SmoothQuant enables serving 530B LLM
within a single node. Our work offers a turn-key solution that reduces hardware
costs and democratizes LLMs. Code is available at
https://github.com/mit-han-lab/smoothquant.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023. First two authors contributed equally to this work</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Structured Knowledge Grounding for Question Answering 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.08284v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.08284v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yujie Lu, Siqi Ouyang, Kairui Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Can language models (LM) ground question-answering (QA) tasks in the
knowledge base via inherent relational reasoning ability? While previous models
that use only LMs have seen some success on many QA tasks, more recent methods
include knowledge graphs (KG) to complement LMs with their more logic-driven
implicit knowledge. However, effectively extracting information from structured
data, like KGs, empowers LMs to remain an open question, and current models
rely on graph techniques to extract knowledge. In this paper, we propose to
solely leverage the LMs to combine the language and knowledge for knowledge
based question-answering with flexibility, breadth of coverage and structured
reasoning. Specifically, we devise a knowledge construction method that
retrieves the relevant context with a dynamic hop, which expresses more
comprehensivenes than traditional GNN-based techniques. And we devise a deep
fusion mechanism to further bridge the information exchanging bottleneck
between the language and the knowledge. Extensive experiments show that our
model consistently demonstrates its state-of-the-art performance over
CommensenseQA benchmark, showcasing the possibility to leverage LMs solely to
robustly ground QA into the knowledge base.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ DISCO: Distilling Counterfactuals with Large Language Models <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10534v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10534v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeming Chen, Qiyue Gao, Antoine Bosselut, Ashish Sabharwal, Kyle Richardson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Models trained with counterfactually augmented data learn representations of
the causal structure of tasks, enabling robust generalization. However,
high-quality counterfactual data is scarce for most tasks and not easily
generated at scale. When crowdsourced, such data is typically limited in scale
and diversity; when generated using supervised methods, it is computationally
expensive to extend to new counterfactual dimensions. In this work, we
introduce DISCO (DIStilled COunterfactual Data), a new method for automatically
generating high quality counterfactual data at scale. DISCO engineers prompts
to generate phrasal perturbations with a large general language model. Then, a
task-specific teacher model filters these generations to distill high-quality
counterfactual data. While task-agnostic, we apply our pipeline to the task of
natural language inference (NLI) and find that on challenging evaluations such
as the NLI stress test, comparatively smaller student models trained with DISCO
generated counterfactuals are more robust (6% absolute) and generalize better
across distributions (2%) compared to models trained without data augmentation.
Furthermore, DISCO augmented models are 10% more consistent between
counterfactual pairs on three evaluation sets, demonstrating that DISCO
augmentation enables models to more reliably learn causal representations. Our
repository is available at: https://github.com/eric11eca/disco
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 camera ready, final title change</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Joint Repetition Suppression and Content Moderation of Large Language
  Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.10611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.10611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Minghui Zhang, Alex Sokolov, Weixin Cai, Si-Qing Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Natural language generation (NLG) is one of the most impactful fields in NLP,
and recent years have witnessed its evolution brought about by large language
models (LLMs). As the key instrument for writing assistance applications, they
are generally prone to replicating or extending offensive content provided in
the input. In low-resource data regime, they can also lead to repetitive
outputs. Usually, offensive content and repetitions are mitigated with post-hoc
methods, including n-gram level blocklists, top-k and nucleus sampling. In this
paper, we apply non-exact repetition suppression using token and sequence level
unlikelihood loss, and further explore the framework of unlikelihood training
objective in order to jointly endow the model with abilities to avoid
generating offensive words and phrases from the beginning. Finally, with
comprehensive experiments, we demonstrate that our proposed methods work
exceptionally in controlling the repetition and content quality of LLM outputs.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Optimization and Control <span class="chip" style="font-size: 60%">38</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LibAUC: A Deep Learning Library for X-Risk Optimization <span class="chip">KDD2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoning Yuan, Dixian Zhu, Zi-Hao Qiu, Gang Li, Xuanhui Wang, Tianbao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the award-winning deep learning (DL) library called
LibAUC for implementing state-of-the-art algorithms towards optimizing a family
of risk functions named X-risks. X-risks refer to a family of compositional
functions in which the loss function of each data point is defined in a way
that contrasts the data point with a large number of others. They have broad
applications in AI for solving classical and emerging problems, including but
not limited to classification for imbalanced data (CID), learning to rank
(LTR), and contrastive learning of representations (CLR). The motivation of
developing LibAUC is to address the convergence issues of existing libraries
for solving these problems. In particular, existing libraries may not converge
or require very large mini-batch sizes in order to attain good performance for
these problems, due to the usage of the standard mini-batch technique in the
empirical risk minimization (ERM) framework. Our library is for deep X-risk
optimization (DXO) that has achieved great success in solving a variety of
tasks for CID, LTR and CLR. The contributions of this paper include: (1) It
introduces a new mini-batch based pipeline for implementing DXO algorithms,
which differs from existing DL pipeline in the design of controlled data
samplers and dynamic mini-batch losses; (2) It provides extensive benchmarking
experiments for ablation studies and comparison with existing libraries. The
LibAUC library features scalable performance for millions of items to be
contrasted, faster and better convergence than existing libraries for
optimizing X-risks, seamless PyTorch deployment and versatile APIs for various
loss optimization. Our library is available to the open source community at
https://github.com/Optimization-AI/LibAUC, to facilitate further academic
research and industrial applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Entropic mean-field min-max problems via Best Response and Fisher-Rao
  flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03033v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03033v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Razvan-Andrei Lascu, Mateusz B. Majka, Łukasz Szpruch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We investigate convergence properties of two continuous-time optimization
methods, the Mean-Field Best Response and the Fisher-Rao (Mean-Field
Birth-Death) flows, for solving convex-concave min-max games with entropy
regularization. We introduce suitable Lyapunov functions to establish
exponential convergence to the unique mixed Nash equilibrium for both methods,
albeit under slightly different conditions. Additionally, we demonstrate the
convergence of the fictitious play flow as a by-product of our analysis.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>39 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explicit feedback synthesis driven by quasi-interpolation for nonlinear
  robust model predictive control 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03027v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03027v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Siddhartha Ganguly, Debasish Chatterjee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present QuIFS (Quasi-Interpolation driven Feedback Synthesis) -- an
offline feedback synthesis algorithm for explicit nonlinear robust minmax model
predictive control (MPC) problems with guaranteed quality of approximation. The
underlying technique is driven by a particular type of grid-based
quasi-interpolation scheme. The QuIFS algorithm departs drastically from
conventional approximation algorithms that are employed in the MPC industry (in
particular, it is neither based on multi-parametric programming tools nor does
it involve kernel methods), and the essence of their point of departure is
encoded in the following challenge-answer approach: Given an error margin
$\varepsilon>0$, compute a feasible feedback policy that is uniformly
$\varepsilon$-close to the optimal MPC feedback policy for a given nonlinear
system subjected to hard constraints and bounded uncertainties. Conditions for
closed-loop stability and recursive feasibility under the approximate feedback
policy are also established. We provide a library of numerical examples to
illustrate our results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Frequency Regulation with Storage: On Losses and Profits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02987v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02987v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dirk Lauinger, François Vuille, Daniel Kuhn
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Low-carbon societies will need to store vast amounts of electricity to
balance intermittent generation from wind and solar energy, for example,
through frequency regulation. Here, we derive an analytical solution to the
decision-making problem of storage operators who sell frequency regulation
power to grid operators and trade electricity on day-ahead markets.
Mathematically, we treat future frequency deviation trajectories as functional
uncertainties in a receding horizon robust optimization problem. We constrain
the expected terminal state-of-charge to be equal to some target to allow
storage operators to make good decisions not only for the present but also the
future. Thanks to this constraint, the amount of electricity traded on
day-ahead markets is an implicit function of the regulation power sold to grid
operators. The implicit function quantifies the amount of power that needs to
be purchased to cover the expected energy loss that results from providing
frequency regulation. We show how the marginal cost associated with the
expected energy loss decreases with roundtrip efficiency and increases with
frequency deviation dispersion. We find that the profits from frequency
regulation over the lifetime of energy-constrained storage devices are roughly
inversely proportional to the length of time for which regulation power must be
committed.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Curvature and complexity: Better lower bounds for geodesically convex
  optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02959v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02959v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Christopher Criscitiello, Nicolas Boumal
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the query complexity of geodesically convex (g-convex) optimization
on a manifold. To isolate the effect of that manifold's curvature, we primarily
focus on hyperbolic spaces. In a variety of settings (smooth or not; strongly
g-convex or not; high- or low-dimensional), known upper bounds worsen with
curvature. It is natural to ask whether this is warranted, or an artifact.
  For many such settings, we propose a first set of lower bounds which indeed
confirm that (negative) curvature is detrimental to complexity. To do so, we
build on recent lower bounds (Hamilton and Moitra, 2021; Criscitiello and
Boumal, 2022) for the particular case of smooth, strongly g-convex
optimization. Using a number of techniques, we also secure lower bounds which
capture dependence on condition number and optimality gap, which was not
previously the case.
  We suspect these bounds are not optimal. We conjecture optimal ones, and
support them with a matching lower bound for a class of algorithms which
includes subgradient descent, and a lower bound for a related game. Lastly, to
pinpoint the difficulty of proving lower bounds, we study how negative
curvature influences (and sometimes obstructs) interpolation with g-convex
functions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>45 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exact Two-Step Benders Decomposition for Two-Stage Stochastic
  Mixed-Integer Programs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02849v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02849v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sifa Celik, Layla Martin, Albert H. Schrotenboer, Tom Van Woensel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many real-life optimization problems belong to the class of two-stage
stochastic mixed-integer programming problems with continuous recourse. This
paper introduces Two-Step Benders Decomposition with Scenario Clustering (TBDS)
as a general exact solution methodology for solving such stochastic programs to
optimality. The method combines and generalizes Benders dual decomposition,
partial Benders decomposition, and Scenario Clustering techniques and does so
within a novel two-step decomposition along the binary and continuous
first-stage decisions. We use TBDS to provide the first exact solutions for the
so-called Time Window Assignment Traveling Salesperson problem. This is a
canonical optimization problem for service-oriented vehicle routing; it
considers jointly assigning time windows to customers and routing a vehicle
among them while travel times are stochastic. Extensive experiments show that
TBDS is superior to state-of-the-art approaches in the literature. It solves
instances with up to 25 customers to optimality. It provides better lower and
upper bounds that lead to faster convergence than related methods. For example,
Benders dual decomposition cannot solve instances of 10 customers to
optimality. We use TBDS to analyze the structure of the optimal solutions. By
increasing routing costs only slightly, customer service can be improved
tremendously, driven by smartly alternating between high- and low-variance
travel arcs to reduce the impact of delay propagation throughout the executed
vehicle route.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probabilistic Region-of-Attraction Estimation with Scenario Optimization
  and Converse Theorems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02832v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02832v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Torbjørn Cunis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The region of attraction characterizes well-behaved and safe operation of a
nonlinear system and is hence sought after for verification. In this paper, a
framework for probabilistic region of attraction estimation is developed that
combines scenario optimization and converse theorems. With this approach, the
probability of an unstable condition being included in the estimate is
independent of the system's complexity, while convergence in probability to the
true region of attraction is proven. Numerical examples demonstrate the
effectiveness for optimization-based control applications. Combining systems
theory and sampling, the complexity of Monte--Carlo-based verification
techniques can be reduced. The results can be extended to arbitrary level sets
of which the defining function can be sampled, such as finite-horizon
viability. Thus, the proposed approach is applicable and/or adaptable to
verification of a wide range of safety-related properties for nonlinear systems
including feedback laws based on optimization or learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to IEEE Transactions of Automatic Control</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integer Programming Games: A Gentle Computational <span class="highlight-title">Overview</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02817v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02817v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Margarida Carvalho, Gabriele Dragotto, Andrea Lodi, Sriram Sankaranarayan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this tutorial, we present a computational overview on computing Nash
equilibria in Integer Programming Games ($IPG$s), $i.e.$, how to compute
solutions for a class of non-cooperative and nonconvex games where each player
solves a mixed-integer optimization problem. $IPG$s are a broad class of games
extending the modeling power of mixed-integer optimization to multi-agent
settings. This class of games includes, for instance, any finite game and any
multi-agent extension of traditional combinatorial optimization problems. After
providing some background motivation and context of applications, we
systematically review and classify the state-of-the-art algorithms to compute
Nash equilibria. We propose an essential taxonomy of the algorithmic
ingredients needed to compute equilibria, and we describe the theoretical and
practical challenges associated with equilibria computation. Finally, we
quantitatively and qualitatively compare a sequential Stackelberg game with a
simultaneous $IPG$ to highlight the different properties of their solutions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in INFORMS TutORials in Operations Research 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tight Big-Ms for Optimal Transmission Switching 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02784v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02784v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Salvador Pineda, Juan Miguel Morales, Álvaro Porras, Concepción Domínguez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper addresses the Optimal Transmission Switching (OTS) problem in
electricity networks, which aims to find an optimal power grid topology that
minimizes system operation costs while satisfying physical and operational
constraints. Existing methods typically convert the OTS problem into a
Mixed-Integer Linear Program (MILP) using big-M constants. However, the
computational performance of these approaches relies significantly on the
tightness of these big-Ms. In this paper, we propose an iterative tightening
strategy to strengthen the big-Ms by efficiently solving a series of bounding
problems that account for the economics of the OTS objective function through
an upper-bound on the generating cost. We also discuss how the performance of
the proposed tightening strategy is enhanced if reduced line capacities are
considered. Using the 118-bus test system we demonstrate that the proposed
methodology outperforms existing approaches, offering tighter bounds and
significantly reducing the computational burden of the OTS problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Split Closure of the Periodic Timetabling Polytope 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02746v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02746v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Niels Lindner, Berenike Masing
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Periodic Event Scheduling Problem (PESP) is the central mathematical tool
for periodic timetable optimization in public transport. PESP can be formulated
in several ways as a mixed-integer linear program with typically general
integer variables. We investigate the split closure of these formulations and
show that split inequalities are identical with the recently introduced flip
inequalities. While split inequalities are a general mixed-integer programming
technique, flip inequalities are defined in purely combinatorial terms, namely
cycles and arc sets of the digraph underlying the PESP instance. It is known
that flip inequalities can be separated in pseudo-polynomial time. We prove
that this is best possible unless P $=$ NP, but also observe that the
complexity becomes linear-time if the cycle defining the flip inequality is
fixed. Moreover, introducing mixed-integer-compatible maps, we compare the
split closures of different formulations, and show that reformulation or
binarization by subdivision do not lead to stronger split closures. Finally, we
estimate computationally how much of the optimality gap of the instances of the
benchmark library PESPlib can be closed exclusively by split cuts, and provide
better dual bounds for five instances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative analysis of the existence and uniqueness conditions of
  parameter estimation in paired comparison models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02737v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02737v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        László Gyarmati, Éva Orbán-Mihálykó, Csaba Mihálykó
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper paired comparison models with stochastic background are
investigated. We focus on the models which allow three options for choice and
the parameters are estimated by maximum likelihood method. The existence and
uniqueness of the estimator is a key issue of the evaluation. In the case of
two options, a necessary and sufficient condition is given by Ford in the
Bradley-Terry model. We generalize this statement for the set of strictly
log-concave distribution. Although in the case of three options necessary and
sufficient condition is not known, there are two different sufficient
conditions which are formulated in the literature. In this paper we generalize
them, moreover we compare these conditions. Their capacities to indicate the
existence of the maximum are analyzed by a large number of computer
simulations. These simulations support that the new condition indicates the
existence of the maximum much more frequently then the previously known ones,
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the convergence of the $k$-point bound for topological packing graphs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02725v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02725v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bram Bekker, Fernando Mário de Oliveira Filho
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We show that the $k$-point bound of de Laat, Machado, Oliveira, and
Vallentin, a hierarchy of upper bounds for the independence number of a
topological packing graph derived from the Lasserre hierarchy, converges to the
independence number.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving NP-hard Min-max Routing Problems as Sequential Generation with
  Equity Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwoo Son, Minsu Kim, Sanghyeok Choi, Jinkyoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Min-max routing problems aim to minimize the maximum tour length among agents
as they collaboratively visit all cities, i.e., the completion time. These
problems include impactful real-world applications but are known as NP-hard.
Existing methods are facing challenges, particularly in large-scale problems
that require the coordination of numerous agents to cover thousands of cities.
This paper proposes a new deep-learning framework to solve large-scale min-max
routing problems. We model the simultaneous decision-making of multiple agents
as a sequential generation process, allowing the utilization of scalable
deep-learning models for sequential decision-making. In the sequentially
approximated problem, we propose a scalable contextual Transformer model,
Equity-Transformer, which generates sequential actions considering an equitable
workload among other agents. The effectiveness of Equity-Transformer is
demonstrated through its superior performance in two representative min-max
routing tasks: the min-max multiple traveling salesman problem (min-max mTSP)
and the min-max multiple pick-up and delivery problem (min-max mPDP). Notably,
our method achieves significant reductions of runtime, approximately 335 times,
and cost values of about 53% compared to a competitive heuristic (LKH3) in the
case of 100 vehicles with 1,000 cities of mTSP. We provide reproducible source
code: https://github.com/kaist-silab/equity-transformer
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does a sparse ReLU network training problem always admit an optimum? 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02666v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02666v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quoc-Tung Le, Elisa Riccietti, Rémi Gribonval
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Given a training set, a loss function, and a neural network architecture, it
is often taken for granted that optimal network parameters exist, and a common
practice is to apply available optimization algorithms to search for them. In
this work, we show that the existence of an optimal solution is not always
guaranteed, especially in the context of {\em sparse} ReLU neural networks. In
particular, we first show that optimization problems involving deep networks
with certain sparsity patterns do not always have optimal parameters, and that
optimization algorithms may then diverge. Via a new topological relation
between sparse ReLU neural networks and their linear counterparts, we derive --
using existing tools from real algebraic geometry -- an algorithm to verify
that a given sparsity pattern suffers from this issue. Then, the existence of a
global optimum is proved for every concrete optimization problem involving a
shallow sparse ReLU neural network of output dimension one. Overall, the
analysis is based on the investigation of two topological properties of the
space of functions implementable as sparse ReLU neural networks: a best
approximation property, and a closedness property, both in the uniform norm.
This is studied both for (finite) domains corresponding to practical training
on finite training sets, and for more general domains such as the unit cube.
This allows us to provide conditions for the guaranteed existence of an optimum
given a sparsity pattern. The results apply not only to several sparsity
patterns proposed in recent works on network pruning/sparsification, but also
to classical dense neural networks, including architectures not covered by
existing results.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automated Importance Sampling via Optimal Control for Stochastic
  Reaction Networks: A Markovian Projection-based Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02660v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02660v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chiheb Ben Hammouda, Nadhir Ben Rached, Raúl Tempone, Sophia Wiechert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a novel alternative approach to our previous work (Ben Hammouda et
al., 2023) to improve the efficiency of Monte Carlo (MC) estimators for rare
event probabilities for stochastic reaction networks (SRNs). In the same spirit
of (Ben Hammouda et al., 2023), an efficient path-dependent measure change is
derived based on a connection between determining optimal importance sampling
(IS) parameters within a class of probability measures and a stochastic optimal
control formulation, corresponding to solving a variance minimization problem.
In this work, we propose a novel approach to address the encountered curse of
dimensionality by mapping the problem to a significantly lower-dimensional
space via a Markovian projection (MP) idea. The output of this model reduction
technique is a low-dimensional SRN (potentially even one dimensional) that
preserves the marginal distribution of the original high-dimensional SRN
system. The dynamics of the projected process are obtained by solving a related
optimization problem via a discrete $L^2$ regression. By solving the resulting
projected Hamilton-Jacobi-Bellman (HJB) equations for the reduced-dimensional
SRN, we obtain projected IS parameters, which are then mapped back to the
original full-dimensional SRN system, resulting in an efficient IS-MC estimator
for rare events probabilities of the full-dimensional SRN. Our analysis and
numerical experiments reveal that the proposed MP-HJB-IS approach substantially
reduces the MC estimator variance, resulting in a lower computational
complexity in the rare event regime than standard MC estimators.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Aiming towards the minimizers: fast convergence of SGD for
  overparametrized problems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02601v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02601v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chaoyue Liu, Dmitriy Drusvyatskiy, Mikhail Belkin, Damek Davis, Yi-An Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Modern machine learning paradigms, such as deep learning, occur in or close
to the interpolation regime, wherein the number of model parameters is much
larger than the number of data samples. In this work, we propose a regularity
condition within the interpolation regime which endows the stochastic gradient
method with the same worst-case iteration complexity as the deterministic
gradient method, while using only a single sampled gradient (or a minibatch) in
each iteration. In contrast, all existing guarantees require the stochastic
gradient method to take small steps, thereby resulting in a much slower linear
rate of convergence. Finally, we demonstrate that our condition holds when
training sufficiently wide feedforward neural networks with a linear output
layer.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ When Decentralized Optimization Meets Federated Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02570v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02570v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongchang Gao, My T. Thai, Jie Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is a new learning paradigm for extracting knowledge from
distributed data. Due to its favorable properties in preserving privacy and
saving communication costs, it has been extensively studied and widely applied
to numerous data analysis applications. However, most existing federated
learning approaches concentrate on the centralized setting, which is vulnerable
to a single-point failure. An alternative strategy for addressing this issue is
the decentralized communication topology. In this article, we systematically
investigate the challenges and opportunities when renovating decentralized
optimization for federated learning. In particular, we discussed them from the
model, data, and communication sides, respectively, which can deepen our
understanding about decentralized federated learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IEEE Network</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Searching for Optimal Per-Coordinate Step-sizes with Multidimensional
  Backtracking 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02527v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02527v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Frederik Kunstner, Victor S. Portella, Mark Schmidt, Nick Harvey
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The backtracking line-search is an effective technique to automatically tune
the step-size in smooth optimization. It guarantees similar performance to
using the theoretically optimal step-size. Many approaches have been developed
to instead tune per-coordinate step-sizes, also known as diagonal
preconditioners, but none of the existing methods are provably competitive with
the optimal per-coordinate stepsizes. We propose multidimensional backtracking,
an extension of the backtracking line-search to find good diagonal
preconditioners for smooth convex problems. Our key insight is that the
gradient with respect to the step-sizes, also known as hypergradients, yields
separating hyperplanes that let us search for good preconditioners using
cutting-plane methods. As black-box cutting-plane approaches like the ellipsoid
method are computationally prohibitive, we develop an efficient algorithm
tailored to our setting. Multidimensional backtracking is provably competitive
with the best diagonal preconditioner and requires no manual tuning.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Lagrange multipliers of the KKT system in Hil<span class="highlight-title">bert</span> spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03261v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03261v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhiyu Tan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper we develop a new decomposition framework to deal with Lagrange
multipliers of the Karush-Kuhn-Tucker (KKT) system of constrained optimization
problems and variational inequalities in Hilbert spaces. It is different from
existing frameworks based on separation theorems. We introduce the essential
Lagrange multiplier and establish the basic theory of this new multiplier. The
essential Lagrange multiplier poses essentially different existence results in
finite and infinite-dimensional spaces. It can also be used to give an
essential characterization of the convergence of multipliers generated by the
classical augmented Lagrangian method. Our analysis reveals that the essential
Lagrange multiplier is at the core of both theories and applications of
Lagrange multipliers.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Equilibration of Coordinating Imitation and Best-Response Dynamics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03243v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03243v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nazanin Hasheminejad, Pouria Ramazi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision-making individuals are often considered to be either imitators who
copy the action of their most successful neighbors or best-responders who
maximize their benefit against the current actions of their neighbors. In the
context of coordination games, where neighboring individuals earn more if they
take the same action, by means of potential functions, it was shown that
populations of all imitators and populations of all best-responders equilibrate
in finite time when they become active to update their decisions sequentially.
However, for mixed populations of the two, the equilibration was shown only for
specific activation sequences. It is therefore, unknown, whether a potential
function also exists for mixed populations or if there actually exists a
counter example where an activation sequence prevents equilibration. We show
that in a linear graph, the number of ``sections'' (a sequence of consecutive
individuals taking the same action) serves as a potential function, leading to
equilibration, and that this result can be extended to sparse trees. The
existence of a potential function for other types of networks remains an open
problem.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonlinear Distributionally Robust Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03202v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03202v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammed Rayyan Sheriff, Peyman Mohajerin Esfahani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article focuses on a class of distributionally robust optimization (DRO)
problems where, unlike the growing body of the literature, the objective
function is potentially non-linear in the distribution. Existing methods to
optimize nonlinear functions in probability space use the Frechet derivatives,
which present both theoretical and computational challenges. Motivated by this,
we propose an alternative notion for the derivative and corresponding
smoothness based on Gateaux (G)-derivative for generic risk measures. These
concepts are explained via three running risk measure examples of variance,
entropic risk, and risk on finite support sets. We then propose a G-derivative
based Frank-Wolfe~(FW) algorithm for generic non-linear optimization problems
in probability spaces and establish its convergence under the proposed notion
of smoothness in a completely norm-independent manner. We use the set-up of the
FW algorithm to devise a methodology to compute a saddle point of the
non-linear DRO problem. Finally, for the minimum variance portfolio selection
problem we analyze the regularity conditions and compute the FW-oracle in
various settings, and validate the theoretical results numerically.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ambiguous Dynamic Treatment Regimes: A Reinforcement Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.04571v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.04571v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroush Saghafian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A main research goal in various studies is to use an observational data set
and provide a new set of counterfactual guidelines that can yield causal
improvements. Dynamic Treatment Regimes (DTRs) are widely studied to formalize
this process. However, available methods in finding optimal DTRs often rely on
assumptions that are violated in real-world applications (e.g., medical
decision-making or public policy), especially when (a) the existence of
unobserved confounders cannot be ignored, and (b) the unobserved confounders
are time-varying (e.g., affected by previous actions). When such assumptions
are violated, one often faces ambiguity regarding the underlying causal model.
This ambiguity is inevitable, since the dynamics of unobserved confounders and
their causal impact on the observed part of the data cannot be understood from
the observed data. Motivated by a case study of finding superior treatment
regimes for patients who underwent transplantation in our partner hospital and
faced a medical condition known as New Onset Diabetes After Transplantation
(NODAT), we extend DTRs to a new class termed Ambiguous Dynamic Treatment
Regimes (ADTRs), in which the causal impact of treatment regimes is evaluated
based on a "cloud" of causal models. We then connect ADTRs to Ambiguous
Partially Observable Mark Decision Processes (APOMDPs) and develop
Reinforcement Learning methods, which enable using the observed data to
efficiently learn an optimal treatment regime. We establish theoretical results
for these learning methods, including (weak) consistency and asymptotic
normality. We further evaluate the performance of these learning methods both
in our case study and in simulation experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Polyak-Łojasiewicz inequality on the space of measures and convergence
  of mean-field birth-death processes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.02774v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.02774v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Linshan Liu, Mateusz B. Majka, Łukasz Szpruch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Polyak-Lojasiewicz inequality (PLI) in $\mathbb{R}^d$ is a natural
condition for proving convergence of gradient descent algorithms. In the
present paper, we study an analogue of PLI on the space of probability measures
$\mathcal{P}(\mathbb{R}^d)$ and show that it is a natural condition for showing
exponential convergence of a class of birth-death processes related to certain
mean-field optimization problems. We verify PLI for a broad class of such
problems for energy functions regularised by the KL-divergence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, revised version, accepted for publication in Applied
  Mathematics & Optimization. The final manuscript is available at Springer via
  https://doi.org/10.1007/s00245-022-09962-0</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Right-left asymmetry of the eigenvector method: A simulation study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.09523v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.09523v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        László Csató
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The eigenvalue method, suggested by the developer of the extensively used
Analytic Hierarchy Process methodology, exhibits right-left asymmetry: the
priorities derived from the right eigenvector do not necessarily coincide with
the priorities derived from the reciprocal left eigenvector. This paper offers
a comprehensive numerical experiment to compare the two eigenvector-based
weighting procedures and their reasonable alternative of the row geometric mean
with respect to four measures. The underlying pairwise comparison matrices are
constructed randomly with different dimensions and levels of inconsistency. The
disagreement between the two eigenvectors turns out to be not always a
monotonic function of these important characteristics of the matrix. The
ranking contradictions can affect alternatives with relatively distant
priorities. The row geometric mean is found to be almost at the midpoint
between the right and inverse left eigenvectors, making it a straightforward
compromise between them.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Robust Optimisation Perspective on Counterexample-Guided Repair of
  Neural Networks <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11342v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11342v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Boetius, Stefan Leue, Tobias Sutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterexample-guided repair aims at creating neural networks with
mathematical safety guarantees, facilitating the application of neural networks
in safety-critical domains. However, whether counterexample-guided repair is
guaranteed to terminate remains an open question. We approach this question by
showing that counterexample-guided repair can be viewed as a robust
optimisation algorithm. While termination guarantees for neural network repair
itself remain beyond our reach, we prove termination for more restrained
machine learning models and disprove termination in a general setting. We
empirically study the practical implications of our theoretical results,
demonstrating the suitability of common verifiers and falsifiers for repair
despite a disadvantageous theoretical result. Additionally, we use our
theoretical insights to devise a novel algorithm for repairing linear
regression models based on quadratic programming, surpassing existing
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023. 9 pages + 13 pages appendix, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exact solution approaches for the discrete $α$-neighbor $p$-center
  problem 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12908v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12908v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Elisabeth Gaar, Markus Sinnl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The discrete $\alpha$-neighbor $p$-center problem (d-$\alpha$-$p$CP) is an
emerging variant of the classical $p$-center problem which recently got
attention in literature. In this problem, we are given a discrete set of points
and we need to locate $p$ facilities on these points in such a way that the
maximum distance between each point where no facility is located and its
$\alpha$-closest facility is minimized. The only existing algorithms in
literature for solving the d-$\alpha$-$p$CP are approximation algorithms and
two recently proposed heuristics.
  In this work, we present two integer programming formulations for the
d-$\alpha$-$p$CP, together with lifting of inequalities, valid inequalities,
inequalities that do not change the optimal objective function value and
variable fixing procedures. We provide theoretical results on the strength of
the formulations and convergence results for the lower bounds obtained after
applying the lifting procedures or the variable fixing procedures in an
iterative fashion. Based on our formulations and theoretical results, we
develop branch-and-cut (B&C) algorithms, which are further enhanced with a
starting heuristic and a primal heuristic.
  We evaluate the effectiveness of our B&C algorithms using instances from
literature. Our algorithms are able to solve 116 out of 194 instances from
literature to proven optimality, with a runtime of under a minute for most of
them. By doing so, we also provide improved solution values for 116 instances.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Existence and Estimation of Critical Batch Size for Training Generative
  Adversarial Networks with Two Time-Scale Update Rule <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.11989v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.11989v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Sato, Hideaki Iiduka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous results have shown that a two time-scale update rule (TTUR) using
different learning rates, such as different constant rates or different
decaying rates, is useful for training generative adversarial networks (GANs)
in theory and in practice. Moreover, not only the learning rate but also the
batch size is important for training GANs with TTURs and they both affect the
number of steps needed for training. This paper studies the relationship
between batch size and the number of steps needed for training GANs with TTURs
based on constant learning rates. We theoretically show that, for a TTUR with
constant learning rates, the number of steps needed to find stationary points
of the loss functions of both the discriminator and generator decreases as the
batch size increases and that there exists a critical batch size minimizing the
stochastic first-order oracle (SFO) complexity. Then, we use the Fr'echet
inception distance (FID) as the performance measure for training and provide
numerical results indicating that the number of steps needed to achieve a low
FID score decreases as the batch size increases and that the SFO complexity
increases once the batch size exceeds the measured critical batch size.
Moreover, we show that measured critical batch sizes are close to the sizes
estimated from our theoretical results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 40th International Conference on Machine Learning
  (ICML 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Peeling for L0-Regularized Least-Squares with supplementary
  material 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14471v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14471v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Théo Guyard, Gilles Monnoyer, Clément Elvira, Cédric Herzet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new methodology dubbed ``safe peeling'' to accelerate the
resolution of L0-regularized least-squares problems via a Branch-and-Bound
(BnB) algorithm. Our procedure enables to tighten the convex relaxation
considered at each node of the BnB decision tree and therefore potentially
allows for more aggressive pruning. Numerical simulations show that our
proposed methodology leads to significant gains in terms of number of nodes
explored and overall solving time.s show that our proposed methodology leads to
significant gains in terms of number of nodes explored and overall solving
time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An adaptive safety layer with hard constraints for safe reinforcement
  learning in multi-energy management systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Glenn Ceusters, Muhammad Andy Putratama, Rüdiger Franke, Ann Nowé, Maarten Messagie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe reinforcement learning (RL) with hard constraint guarantees is a
promising optimal control direction for multi-energy management systems. It
only requires the environment-specific constraint functions itself a priori and
not a complete model (i.e. plant, disturbance and noise models, and prediction
models for states not included in the plant model - e.g. demand forecasts,
weather forecasts, price forecasts). The project-specific upfront and ongoing
engineering efforts are therefore still reduced, better representations of the
underlying system dynamics can still be learned and modelling bias is kept to a
minimum (no model-based objective function). However, even the constraint
functions alone are not always trivial to accurately provide in advance,
leading to potentially unsafe behaviour. In this paper, we present two novel
advancements: (I) combining the Optlayer and SafeFallback method, named
OptLayerPolicy, to increase the initial utility while keeping a high sample
efficiency. (II) introducing self-improving hard constraints, to increase the
accuracy of the constraint functions as more data becomes available so that
better policies can be learned. Both advancements keep the constraint
formulation decoupled from the RL formulation, so that new (presumably better)
RL algorithms can act as drop-in replacements. We have shown that, in a
simulated multi-energy system case study, the initial utility is increased to
92.4% (OptLayerPolicy) compared to 86.1% (OptLayer) and that the policy after
training is increased to 104.9% (GreyOptLayerPolicy) compared to 103.4%
(OptLayer) - all relative to a vanilla RL benchmark. While introducing
surrogate functions into the optimization problem requires special attention,
we do conclude that the newly presented GreyOptLayerPolicy method is the most
advantageous.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4703 words. arXiv admin note: text overlap with arXiv:2207.03830</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Gauss-Newton for learning over-parameterized models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Arbel, Romain Menegaux, Pierre Wolinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work studies the global convergence and generalization properties of
Gauss Newton's (GN) when optimizing one-hidden layer networks in the
over-parameterized regime. We first establish a global convergence result for
GN in the continuous-time limit exhibiting a faster convergence rate compared
to GD due to improved conditioning. We then perform an empirical study on a
synthetic regression task to investigate the implicit bias of GN's method. We
find that, while GN is consistently faster than GD in finding a global optimum,
the performance of the learned model on a test dataset is heavily influenced by
both the learning rate and the variance of the randomly initialized network's
weights. Specifically, we find that initializing with a smaller variance
results in a better generalization, a behavior also observed for GD. However,
in contrast to GD where larger learning rates lead to the best generalization,
we find that GN achieves an improved generalization when using smaller learning
rates, albeit at the cost of slower convergence. This study emphasizes the
significance of the learning rate in balancing the optimization speed of GN
with the generalization ability of the learned solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Accelerated Stochastic Optimization Methods under Quasar-convexity <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04736v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04736v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qiang Fu, Dongchu Xu, Ashia Wilson
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Non-convex optimization plays a key role in a growing number of machine
learning applications. This motivates the identification of specialized
structure that enables sharper theoretical analysis. One such identified
structure is quasar-convexity, a non-convex generalization of convexity that
subsumes convex functions. Existing algorithms for minimizing quasar-convex
functions in the stochastic setting have either high complexity or slow
convergence, which prompts us to derive a new class of stochastic methods for
optimizing smooth quasar-convex functions. We demonstrate that our algorithms
have fast convergence and outperform existing algorithms on several examples,
including the classical problem of learning linear dynamical systems. We also
present a unified analysis of our newly proposed algorithms and a previously
studied deterministic algorithm.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the main conference of ICML 2023. 30 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Second-order analysis of an optimal control problem in a phase field
  tumor growth model with singular potentials and chemotaxis 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2009.07574v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2009.07574v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pierluigi Colli, Andrea Signori, Jürgen Sprekels
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper concerns a distributed optimal control problem for a tumor growth
model of Cahn-Hilliard type including chemotaxis with possibly singular
potentials, where the control and state variables are nonlinearly coupled.
First, we discuss the weak well-posedness of the system under very general
assumptions for the potentials, which may be singular and nonsmooth. Then, we
establish the strong well-posedness of the system in a reduced setting, which
however admits the logarithmic potential: this analysis will lay the foundation
for the study of the corresponding optimal control problem. Concerning the
optimization problem, we address the existence of minimizers and establish both
first-order necessary and second-order sufficient conditions for optimality.
The mathematically challenging second-order analysis is completely performed
here, after showing that the solution mapping is twice continuously
differentiable between suitable Banach spaces via the implicit function
theorem. Then, we completely identify the second-order Fr\'echet derivative of
the control-to-state operator and carry out a thorough and detailed
investigation about the related properties.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Please note that the formula (6.5), in both the arXiv preprint and in
  the journal article, contains three sign errors: indeed, the term in the
  second line of (6.5) involving P'' should carry a `plus' sign, while the two
  terms in the third line should carry `minus' signs. These sign errors,
  however, do not have an impact on the validity of the results</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust optimality and duality for composite uncertain multiobjective
  optimization in Asplund spaces with its applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.07611v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.07611v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Maryam Saadati, Morteza Oveisiha
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This article is devoted to investigate a nonsmooth/nonconvex uncertain
multiobjective optimization problem with composition fields
($(\hyperlink{CUP}{\mathrm{CUP}})$ for brevity) over arbitrary Asplund spaces.
Employing some advanced techniques of variational analysis and generalized
differentiation, we establish necessary optimality conditions for weakly robust
efficient solutions of $(\hyperlink{CUP}{\mathrm{CUP}})$ in terms of the
limiting subdifferential. Sufficient conditions for the existence of (weakly)
robust efficient solutions to such a problem are also driven under the new
concept of pseudo-quasi convexity for composite functions. We formulate a
Mond-Weir-type robust dual problem to the primal problem
$(\hyperlink{CUP}{\mathrm{CUP}})$, and explore weak, strong, and converse
duality properties. In addition, the obtained results are applied to an
approximate uncertain multiobjective problem and a composite uncertain
multiobjective problem with linear operators.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>arXiv admin note: substantial text overlap with arXiv:2105.14366,
  arXiv:2205.01145</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Frugal and Decentralised Resolvent Splittings Defined by Nonexpansive
  Operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.04594v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.04594v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew K. Tam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Frugal resolvent splittings are a class of fixed point algorithms for finding
a zero in the sum of the sum of finitely many set-valued monotone operators,
where the fixed point operator uses only vector addition, scalar multiplication
and the resolvent of each monotone operator once per iteration. In the
literature, the convergence analyses of these schemes are performed in an
inefficient, algorithm-by-algorithm basis. In this work, we address this by
developing a general framework for frugal resolvent splitting which
simultaneously covers and extends several important schemes in the literature.
The framework also yields a new resolvent splitting algorithm which is suitable
for decentralised implementation on regular networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convergence of Adam under Relaxed Assumptions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.13972v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.13972v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haochuan Li, Alexander Rakhlin, Ali Jadbabaie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we provide a rigorous proof of convergence of the Adaptive
Moment Estimate (Adam) algorithm for a wide class of optimization objectives.
Despite the popularity and efficiency of the Adam algorithm in training deep
neural networks, its theoretical properties are not yet fully understood, and
existing convergence proofs require unrealistically strong assumptions, such as
globally bounded gradients, to show the convergence to stationary points. In
this paper, we show that Adam provably converges to $\epsilon$-stationary
points with $\mathcal{O}(\epsilon^{-4})$ gradient complexity under far more
realistic conditions. The key to our analysis is a new proof of boundedness of
gradients along the optimization trajectory of Adam, under a generalized
smoothness assumption according to which the local smoothness (i.e., Hessian
norm when it exists) is bounded by a sub-quadratic function of the gradient
norm. Moreover, we propose a variance-reduced version of Adam with an
accelerated gradient complexity of $\mathcal{O}(\epsilon^{-3})$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>33 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Approximate Dynamic Programming for a Mean-field Game of Traffic Flow:
  Existence and Uniqueness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.05416v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.05416v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amoolya Tirumalai, John S. Baras
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Highway vehicular traffic is an inherently multi-agent problem. Traffic jams
can appear and disappear mysteriously. We develop a method for traffic flow
control that is applied at the vehicular level via mean-field games. We begin
this work with a microscopic model of vehicles subject to control input,
disturbances, noise, and a speed limit. We formulate a discounted-cost
infinite-horizon robust mean-field game on the vehicles, and obtain the
associated dynamic programming (DP) PDE system. We then perform approximate
dynamic programming (ADP) using these equations to obtain a sub-optimal control
for the traffic density adaptively. The sub-optimal controls are subject to an
ODE-PDE system. We show that the ADP ODE-PDE system has a unique weak solution
in a suitable Hilbert space using semigroup and successive approximation
methods. We additionally give a numerical simulation, and interpret the
results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>42 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Viability and Exponentially Stable Trajectories for Differential
  Inclusions in Wasserstein Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.03640v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.03640v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Benoît Bonnet, Hélène Frankowska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this article, we prove a general viability theorem for continuity
inclusions in Wasserstein spaces, and provide an application thereof to the
existence of exponentially stable trajectories obtained via the second method
of Lyapunov.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Learning MPC with Limited Model Knowledge and Data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2004.00759v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2004.00759v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aaron Kandel, Scott J. Moura
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents an end-to-end framework for safe learning-based control
(LbC) using nonlinear stochastic MPC and distributionally robust optimization
(DRO). This work is motivated by several open challenges in LbC literature. In
particular, many control-theoretic LbC methods require subject matter expertise
in order to translate their own safety guarantees, often manifested as
preexisting data of safe trajectories or structural model knowledge. In this
paper, we focus on LbC where the controller is applied directly to a system of
which it has no or extremely limited direct experience, towards safety during
tabula-rasa or ``blank slate'' model-based learning and control. This explores
the boundary of the status-quo in control theory relating to requirements for
subject matter expertise. We show under basic and limited assumptions on the
underlying problem, we can translate probabilistic guarantees on feasibility to
nonlinear systems using existing results in stochastic MPC and DRO literature.
We also present a coupled and intuitive formulation for persistence of
excitation (PoE), and illustrate the connection between PoE and applicability
of the proposed method. Our case studies of vehicle obstacle avoidance and safe
extreme fast charging of lithium-ion batteries reveal powerful empirical
results supporting the underlying DRO theory. Our method is widely applicable
within the LbC domain to applications including for example airborne wind
energy systems, vehicle obstacle avoidance, energy storage systems management,
and video compression.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Machine Learning <span class="chip" style="font-size: 60%">150</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ DeepGraphDMD: Interpretable Spatio-Temporal Decomposition of Non-linear
  Functional Brain Network Dynamics <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03088v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03088v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Asadullah Turja, Martin Styner, Guorong Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Functional brain dynamics is supported by parallel and overlapping functional
network modes that are associated with specific neural circuits. Decomposing
these network modes from fMRI data and finding their temporal characteristics
is challenging due to their time-varying nature and the non-linearity of the
functional dynamics. Dynamic Mode Decomposition (DMD) algorithms have been
quite popular for solving this decomposition problem in recent years. In this
work, we apply GraphDMD -- an extension of the DMD for network data -- to
extract the dynamic network modes and their temporal characteristics from the
fMRI time series in an interpretable manner. GraphDMD, however, regards the
underlying system as a linear dynamical system that is sub-optimal for
extracting the network modes from non-linear functional data. In this work, we
develop a generalized version of the GraphDMD algorithm -- DeepGraphDMD --
applicable to arbitrary non-linear graph dynamical systems. DeepGraphDMD is an
autoencoder-based deep learning model that learns Koopman eigenfunctions for
graph data and embeds the non-linear graph dynamics into a latent linear space.
We show the effectiveness of our method in both simulated data and the HCP
resting-state fMRI data. In the HCP data, DeepGraphDMD provides novel insights
into cognitive brain functions by discovering two major network modes related
to fluid and crystallized intelligence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>to be published in MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpQR: A Sparse-Quantized Representation for Near-Lossless LLM Weight
  Compression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03078v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03078v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tim Dettmers, Ruslan Svirschevski, Vage Egiazarian, Denis Kuznedelev, Elias Frantar, Saleh Ashkboos, Alexander Borzunov, Torsten Hoefler, Dan Alistarh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in large language model (LLM) pretraining have led to
high-quality LLMs with impressive abilities. By compressing such LLMs via
quantization to 3-4 bits per parameter, they can fit into memory-limited
devices such as laptops and mobile phones, enabling personalized use. However,
quantization down to 3-4 bits per parameter usually leads to moderate-to-high
accuracy losses, especially for smaller models in the 1-10B parameter range,
which are well-suited for edge deployments. To address this accuracy issue, we
introduce the Sparse-Quantized Representation (SpQR), a new compressed format
and quantization technique which enables for the first time near-lossless
compression of LLMs across model scales, while reaching similar compression
levels to previous methods. SpQR works by identifying and isolating outlier
weights, which cause particularly-large quantization errors, and storing them
in higher precision, while compressing all other weights to 3-4 bits, and
achieves relative accuracy losses of less than 1% in perplexity for
highly-accurate LLaMA and Falcon LLMs. This makes it possible to run 33B
parameter LLM on a single 24 GB consumer GPU without any performance
degradation at 15% speedup thus making powerful LLMs available to consumer
without any downsides. SpQR comes with efficient algorithms for both encoding
weights into its format, as well as decoding them efficiently at runtime.
Specifically, we provide an efficient GPU inference algorithm for SpQR which
yields faster inference than 16-bit baselines at similar accuracy, while
enabling memory compression gains of more than 4x.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Extended preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sensitivity-Aware Finetuning for Accuracy Recovery on Deep Learning
  Hardware 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03076v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03076v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lakshmi Nair, Darius Bunandar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Existing methods to recover model accuracy on analog-digital hardware in the
presence of quantization and analog noise include noise-injection training.
However, it can be slow in practice, incurring high computational costs, even
when starting from pretrained models. We introduce the Sensitivity-Aware
Finetuning (SAFT) approach that identifies noise sensitive layers in a model,
and uses the information to freeze specific layers for noise-injection
training. Our results show that SAFT achieves comparable accuracy to
noise-injection training and is 2x to 8x faster.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 2 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A General Perspective on Objectives of Reinforcement Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03074v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03074v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this lecture, we present a general perspective on reinforcement learning
(RL) objectives, where we show three versions of objectives. The first version
is the standard definition of objective in RL literature. Then we extend the
standard definition to the $\lambda$-return version, which unifies the standard
definition of objective. Finally, we propose a general objective that unifies
the previous two versions. The last version provides a high level to understand
of RL's objective, where it shows a fundamental formulation that connects some
widely used RL techniques (e.g., TD$(\lambda)$ and GAE), and this objective can
be potentially applied to extensive RL algorithms.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Explore to Generalize in Zero-Shot RL 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03072v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03072v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ev Zisselman, Itai Lavie, Daniel Soudry, Aviv Tamar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study zero-shot generalization in reinforcement learning - optimizing a
policy on a set of training tasks such that it will perform well on a similar
but unseen test task. To mitigate overfitting, previous work explored different
notions of invariance to the task. However, on problems such as the ProcGen
Maze, an adequate solution that is invariant to the task visualization does not
exist, and therefore invariance-based approaches fail. Our insight is that
learning a policy that $\textit{explores}$ the domain effectively is harder to
memorize than a policy that maximizes reward for a specific task, and therefore
we expect such learned behavior to generalize well; we indeed demonstrate this
empirically on several domains that are difficult for invariance-based
approaches. Our $\textit{Explore to Generalize}$ algorithm (ExpGen) builds on
this insight: We train an additional ensemble of agents that optimize reward.
At test time, either the ensemble agrees on an action, and we generalize well,
or we take exploratory actions, which are guaranteed to generalize and drive us
to a novel part of the state space, where the ensemble may potentially agree
again. We show that our approach is the state-of-the-art on several tasks in
the ProcGen challenge that have so far eluded effective generalization. For
example, we demonstrate a success rate of $82\%$ on the Maze task and $74\%$ on
Heist with $200$ training levels.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Of Mice and Mates: Automated Classification and Modelling of Mouse
  Behaviour in Groups using a Single Model across Cages 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03066v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03066v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael P. J. Camilleri, Rasneer S. Bains, Christopher K. I. Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Behavioural experiments often happen in specialised arenas, but this may
confound the analysis. To address this issue, we provide tools to study mice in
the homecage environment, equipping biologists with the possibility to capture
the temporal aspect of the individual's behaviour and model the interaction and
interdependence between cage-mates with minimal human intervention. We develop
the Activity Labelling Module (ALM) to automatically classify mouse behaviour
from video, and a novel Group Behaviour Model (GBM) for summarising their joint
behaviour across cages, using a permutation matrix to match the mouse
identities in each cage to the model. We also release two datasets, ABODe for
training behaviour classifiers and IMADGE for modelling behaviour.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ LibAUC: A Deep Learning Library for X-Risk Optimization <span class="chip">KDD2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03065v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03065v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhuoning Yuan, Dixian Zhu, Zi-Hao Qiu, Gang Li, Xuanhui Wang, Tianbao Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper introduces the award-winning deep learning (DL) library called
LibAUC for implementing state-of-the-art algorithms towards optimizing a family
of risk functions named X-risks. X-risks refer to a family of compositional
functions in which the loss function of each data point is defined in a way
that contrasts the data point with a large number of others. They have broad
applications in AI for solving classical and emerging problems, including but
not limited to classification for imbalanced data (CID), learning to rank
(LTR), and contrastive learning of representations (CLR). The motivation of
developing LibAUC is to address the convergence issues of existing libraries
for solving these problems. In particular, existing libraries may not converge
or require very large mini-batch sizes in order to attain good performance for
these problems, due to the usage of the standard mini-batch technique in the
empirical risk minimization (ERM) framework. Our library is for deep X-risk
optimization (DXO) that has achieved great success in solving a variety of
tasks for CID, LTR and CLR. The contributions of this paper include: (1) It
introduces a new mini-batch based pipeline for implementing DXO algorithms,
which differs from existing DL pipeline in the design of controlled data
samplers and dynamic mini-batch losses; (2) It provides extensive benchmarking
experiments for ablation studies and comparison with existing libraries. The
LibAUC library features scalable performance for millions of items to be
contrasted, faster and better convergence than existing libraries for
optimizing X-risks, seamless PyTorch deployment and versatile APIs for various
loss optimization. Our library is available to the open source community at
https://github.com/Optimization-AI/LibAUC, to facilitate further academic
research and industrial applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discriminative Adversarial Privacy: Balancing Accuracy and Membership
  Privacy in Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03054v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03054v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eugenio Lomurno, Alberto Archetti, Francesca Ausonio, Matteo Matteucci
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The remarkable proliferation of deep learning across various industries has
underscored the importance of data privacy and security in AI pipelines. As the
evolution of sophisticated Membership Inference Attacks (MIAs) threatens the
secrecy of individual-specific information used for training deep learning
models, Differential Privacy (DP) raises as one of the most utilized techniques
to protect models against malicious attacks. However, despite its proven
theoretical properties, DP can significantly hamper model performance and
increase training time, turning its use impractical in real-world scenarios.
Tackling this issue, we present Discriminative Adversarial Privacy (DAP), a
novel learning technique designed to address the limitations of DP by achieving
a balance between model performance, speed, and privacy. DAP relies on
adversarial training based on a novel loss function able to minimise the
prediction error while maximising the MIA's error. In addition, we introduce a
novel metric named Accuracy Over Privacy (AOP) to capture the
performance-privacy trade-off. Finally, to validate our claims, we compare DAP
with diverse DP scenarios, providing an analysis of the results from
performance, time, and privacy preservation perspectives.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Forecasting Crude Oil Prices Using Reservoir Computing Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03052v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03052v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kaushal Kumar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate crude oil price prediction is crucial for financial decision-making.
We propose a novel reservoir computing model for forecasting crude oil prices.
It outperforms popular deep learning methods in most scenarios, as demonstrated
through rigorous evaluation using daily closing price data from major stock
market indices. Our model's competitive advantage is further validated by
comparing it with recent deep-learning approaches. This study introduces
innovative reservoir computing models for predicting crude oil prices, with
practical implications for financial practitioners. By leveraging advanced
techniques, market participants can enhance decision-making and gain valuable
insights into crude oil market dynamics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SERT: A Transfomer Based Model for Spatio-Temporal Sensor Data with
  Missing Values for Environmental Monitoring 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03042v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03042v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Amin Shoari Nejad, Rocío Alaiz-Rodríguez, Gerard D. McCarthy, Brian Kelleher, Anthony Grey, Andrew Parnell
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Environmental monitoring is crucial to our understanding of climate change,
biodiversity loss and pollution. The availability of large-scale
spatio-temporal data from sources such as sensors and satellites allows us to
develop sophisticated models for forecasting and understanding key drivers.
However, the data collected from sensors often contain missing values due to
faulty equipment or maintenance issues. The missing values rarely occur
simultaneously leading to data that are multivariate misaligned sparse time
series. We propose two models that are capable of performing multivariate
spatio-temporal forecasting while handling missing data naturally without the
need for imputation. The first model is a transformer-based model, which we
name SERT (Spatio-temporal Encoder Representations from Transformers). The
second is a simpler model named SST-ANN (Sparse Spatio-Temporal Artificial
Neural Network) which is capable of providing interpretable results. We conduct
extensive experiments on two different datasets for multivariate
spatio-temporal forecasting and show that our models have competitive or
superior performance to those at the state-of-the-art.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Similarity among Users for Personalized Session-Based
  Recommendation from hierarchical structure of User-Session-Item 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03040v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03040v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jisoo Cha, Haemin Jeong, Wooju Kim
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The task of the session-based recommendation is to predict the next
interaction of the user based on the anonymized user's behavior pattern. And
personalized version of this system is a promising research field due to its
availability to deal with user information. However, there's a problem that the
user's preferences and historical sessions were not considered in the typical
session-based recommendation since it concentrates only on user-item
interaction. In addition, the existing personalized session-based
recommendation model has a limited capability in that it only considers the
preference of the current user without considering those of similar users. It
means there can be the loss of information included within the hierarchical
data structure of the user-session-item. To tackle with this problem, we
propose USP-SBR(abbr. of User Similarity Powered - Session Based Recommender).
To model global historical sessions of users, we propose UserGraph that has two
types of nodes - ItemNode and UserNode. We then connect the nodes with three
types of edges. The first type of edges connects ItemNode as chronological
order, and the second connects ItemNode to UserNode, and the last connects
UserNode to ItemNode. With these user embeddings, we propose additional
contrastive loss, that makes users with similar intention be close to each
other in the vector space. we apply graph neural network on these UserGraph and
update nodes. Experimental results on two real-world datasets demonstrate that
our method outperforms some state-of-the-art approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ AI Techniques for Cone Beam Computed Tomography in Dentistry: Trends and
  Practices 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03025v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03025v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saba Sarwar, Suraiya Jabin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Cone-beam computed tomography (CBCT) is a popular imaging modality in
dentistry for diagnosing and planning treatment for a variety of oral diseases
with the ability to produce detailed, three-dimensional images of the teeth,
jawbones, and surrounding structures. CBCT imaging has emerged as an essential
diagnostic tool in dentistry. CBCT imaging has seen significant improvements in
terms of its diagnostic value, as well as its accuracy and efficiency, with the
most recent development of artificial intelligence (AI) techniques. This paper
reviews recent AI trends and practices in dental CBCT imaging. AI has been used
for lesion detection, malocclusion classification, measurement of buccal bone
thickness, and classification and segmentation of teeth, alveolar bones,
mandibles, landmarks, contours, and pharyngeal airways using CBCT images.
Mainly machine learning algorithms, deep learning algorithms, and
super-resolution techniques are used for these tasks. This review focuses on
the potential of AI techniques to transform CBCT imaging in dentistry, which
would improve both diagnosis and treatment planning. Finally, we discuss the
challenges and limitations of artificial intelligence in dentistry and CBCT
imaging.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Recent Advances in Electrical, Electronics & Digital Healthcare
  Technologies REEDCON 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interpretable Alzheimer's Disease Classification Via a Contrastive
  Diffusion Autoencoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03022v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03022v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ayodeji Ijishakin, Ahmed Abdulaal, Adamos Hadjivasiliou, Sophie Martin, James Cole
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In visual object classification, humans often justify their choices by
comparing objects to prototypical examples within that class. We may therefore
increase the interpretability of deep learning models by imbuing them with a
similar style of reasoning. In this work, we apply this principle by
classifying Alzheimer's Disease based on the similarity of images to training
examples within the latent space. We use a contrastive loss combined with a
diffusion autoencoder backbone, to produce a semantically meaningful latent
space, such that neighbouring latents have similar image-level features. We
achieve a classification accuracy comparable to black box approaches on a
dataset of 2D MRI images, whilst producing human interpretable model
explanations. Therefore, this work stands as a contribution to the pertinent
development of accurate and interpretable deep learning within medical imaging.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Automating Style Analysis and Visualization With Explainable AI -- Case
  Studies on Brand Recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03021v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03021v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-hsuan Chen, Levent Burak Kara, Jonathan Cagan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Incorporating style-related objectives into shape design has been centrally
important to maximize product appeal. However, stylistic features such as
aesthetics and semantic attributes are hard to codify even for experts. As
such, algorithmic style capture and reuse have not fully benefited from
automated data-driven methodologies due to the challenging nature of design
describability. This paper proposes an AI-driven method to fully automate the
discovery of brand-related features. Our approach introduces BIGNet, a two-tier
Brand Identification Graph Neural Network (GNN) to classify and analyze scalar
vector graphics (SVG). First, to tackle the scarcity of vectorized product
images, this research proposes two data acquisition workflows: parametric
modeling from small curve-based datasets, and vectorization from large
pixel-based datasets. Secondly, this study constructs a novel hierarchical GNN
architecture to learn from both SVG's curve-level and chunk-level parameters.
In the first case study, BIGNet not only classifies phone brands but also
captures brand-related features across multiple scales, such as the location of
the lens, the height-width ratio, and the screen-frame gap, as confirmed by AI
evaluation. In the second study, this paper showcases the generalizability of
BIGNet learning from a vectorized car image dataset and validates the
consistency and robustness of its predictions given four scenarios. The results
match the difference commonly observed in luxury vs. economy brands in the
automobile market. Finally, this paper also visualizes the activation maps
generated from a convolutional neural network and shows BIGNet's advantage of
being a more human-friendly, explainable, and explicit style-capturing agent.
Code and dataset can be found on Github:
  1. Phone case study: github.com/parksandrecfan/bignet-phone 2. Car case
study: github.com/parksandrecfan/bignet-car
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Quantification of Uncertainties in Deep Learning-based Environment
  Perception 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03018v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03018v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Marco Braun, Moritz Luszek, Jan Siegemund, Kevin Kollek, Anton Kummert
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we introduce a novel Deep Learning-based method to perceive the
environment of a vehicle based on radar scans while accounting for
uncertainties in its predictions. The environment of the host vehicle is
segmented into equally sized grid cells which are classified individually.
Complementary to the segmentation output, our Deep Learning-based algorithm is
capable of differentiating uncertainties in its predictions as being related to
an inadequate model (epistemic uncertainty) or noisy data (aleatoric
uncertainty). To this end, weights are described as probability distributions
accounting for uncertainties in the model parameters. Distributions are learned
in a supervised fashion using gradient descent. We prove that uncertainties in
the model output correlate with the precision of its predictions. Compared to
previous concepts, we show superior performance of our approach to reliably
perceive the environment of a vehicle.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>2021 IEEE International Conference on Omni-Layer Intelligent Systems
  (COINS), Barcelona, Spain, 2021</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On the Behavior of Intrusive and Non-intrusive Speech Enhancement
  Metrics in Predictive and Generative Settings 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03014v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03014v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Danilo de Oliveira, Julius Richter, Jean-Marie Lemercier, Tal Peer, Timo Gerkmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since its inception, the field of deep speech enhancement has been dominated
by predictive (discriminative) approaches, such as spectral mapping or masking.
Recently, however, novel generative approaches have been applied to speech
enhancement, attaining good denoising performance with high subjective quality
scores. At the same time, advances in deep learning also allowed for the
creation of neural network-based metrics, which have desirable traits such as
being able to work without a reference (non-intrusively). Since generatively
enhanced speech tends to exhibit radically different residual distortions, its
evaluation using instrumental speech metrics may behave differently compared to
predictively enhanced speech. In this paper, we evaluate the performance of the
same speech enhancement backbone trained under predictive and generative
paradigms on a variety of metrics and show that intrusive and non-intrusive
measures correlate differently for each paradigm. This analysis motivates the
search for metrics that can together paint a complete and unbiased picture of
speech enhancement performance, irrespective of the model's training process.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to ITG Conference on Speech Communication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Hiding in Plain Sight: Disguising Data Stealing Attacks in Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03013v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03013v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kostadin Garov, Dimitar I. Dimitrov, Nikola Jovanović, Martin Vechev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Malicious server (MS) attacks have enabled the scaling of data stealing in
federated learning to large batch sizes and secure aggregation, settings
previously considered private. However, many concerns regarding client-side
detectability of MS attacks were raised, questioning their practicality once
they are publicly known. In this work, for the first time, we thoroughly study
the problem of client-side detectability.We demonstrate that most prior MS
attacks, which fundamentally rely on one of two key principles, are detectable
by principled client-side checks. Further, we formulate desiderata for
practical MS attacks and propose SEER, a novel attack framework that satisfies
all desiderata, while stealing user data from gradients of realistic networks,
even for large batch sizes (up to 512 in our experiments) and under secure
aggregation. The key insight of SEER is the use of a secret decoder, which is
jointly trained with the shared model. Our work represents a promising first
step towards more principled treatment of MS attacks, paving the way for
realistic data stealing that can compromise user privacy in real-world
deployments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Interval Load Forecasting for Individual Households in the Presence of
  Electric Vehicle Charging 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03010v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03010v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raiden Skala, Mohamed Ahmed T. A. Elgalhud, Katarina Grolinger, Syed Mir
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The transition to Electric Vehicles (EV) in place of traditional internal
combustion engines is increasing societal demand for electricity. The ability
to integrate the additional demand from EV charging into forecasting
electricity demand is critical for maintaining the reliability of electricity
generation and distribution. Load forecasting studies typically exclude
households with home EV charging, focusing on offices, schools, and public
charging stations. Moreover, they provide point forecasts which do not offer
information about prediction uncertainty. Consequently, this paper proposes the
Long Short-Term Memory Bayesian Neural Networks (LSTM-BNNs) for household load
forecasting in presence of EV charging. The approach takes advantage of the
LSTM model to capture the time dependencies and uses the dropout layer with
Bayesian inference to generate prediction intervals. Results show that the
proposed LSTM-BNNs achieve accuracy similar to point forecasts with the
advantage of prediction intervals. Moreover, the impact of lockdowns related to
the COVID-19 pandemic on the load forecasting model is examined, and the
analysis shows that there is no major change in the model performance as, for
the considered households, the randomness of the EV charging outweighs the
change due to pandemic.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Sequences of Life-events to Predict Human Lives 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03009v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03009v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Germans Savcisens, Tina Eliassi-Rad, Lars Kai Hansen, Laust Mortensen, Lau Lilleholt, Anna Rogers, Ingo Zettler, Sune Lehmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Over the past decade, machine learning has revolutionized computers' ability
to analyze text through flexible computational models. Due to their structural
similarity to written language, transformer-based architectures have also shown
promise as tools to make sense of a range of multi-variate sequences from
protein-structures, music, electronic health records to weather-forecasts. We
can also represent human lives in a way that shares this structural similarity
to language. From one perspective, lives are simply sequences of events: People
are born, visit the pediatrician, start school, move to a new location, get
married, and so on. Here, we exploit this similarity to adapt innovations from
natural language processing to examine the evolution and predictability of
human lives based on detailed event sequences. We do this by drawing on
arguably the most comprehensive registry data in existence, available for an
entire nation of more than six million individuals across decades. Our data
include information about life-events related to health, education, occupation,
income, address, and working hours, recorded with day-to-day resolution. We
create embeddings of life-events in a single vector space showing that this
embedding space is robust and highly structured. Our models allow us to predict
diverse outcomes ranging from early mortality to personality nuances,
outperforming state-of-the-art models by a wide margin. Using methods for
interpreting deep learning models, we probe the algorithm to understand the
factors that enable our predictions. Our framework allows researchers to
identify new potential mechanisms that impact life outcomes and associated
possibilities for personalized interventions.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Nonparametric Iterative Machine Teaching <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03007v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03007v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chen Zhang, Xiaofeng Cao, Weiyang Liu, Ivor Tsang, James Kwok
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the problem of Iterative Machine Teaching (IMT),
where the teacher provides examples to the learner iteratively such that the
learner can achieve fast convergence to a target model. However, existing IMT
algorithms are solely based on parameterized families of target models. They
mainly focus on convergence in the parameter space, resulting in difficulty
when the target models are defined to be functions without dependency on
parameters. To address such a limitation, we study a more general task --
Nonparametric Iterative Machine Teaching (NIMT), which aims to teach
nonparametric target models to learners in an iterative fashion. Unlike
parametric IMT that merely operates in the parameter space, we cast NIMT as a
functional optimization problem in the function space. To solve it, we propose
both random and greedy functional teaching algorithms. We obtain the iterative
teaching dimension (ITD) of the random teaching algorithm under proper
assumptions, which serves as a uniform upper bound of ITD in NIMT. Further, the
greedy teaching algorithm has a significantly lower ITD, which reaches a
tighter upper bound of ITD in NIMT. Finally, we verify the correctness of our
theoretical findings with extensive experiments in nonparametric scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023 (20 pages, 10 figures)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unveiling the Two-Faced Truth: Disentangling Morphed Identities for Face
  Morphing Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03002v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03002v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eduarda Caldeira, Pedro C. Neto, Tiago Gonçalves, Naser Damer, Ana F. Sequeira, Jaime S. Cardoso
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Morphing attacks keep threatening biometric systems, especially face
recognition systems. Over time they have become simpler to perform and more
realistic, as such, the usage of deep learning systems to detect these attacks
has grown. At the same time, there is a constant concern regarding the lack of
interpretability of deep learning models. Balancing performance and
interpretability has been a difficult task for scientists. However, by
leveraging domain information and proving some constraints, we have been able
to develop IDistill, an interpretable method with state-of-the-art performance
that provides information on both the identity separation on morph samples and
their contribution to the final prediction. The domain information is learnt by
an autoencoder and distilled to a classifier system in order to teach it to
separate identity information. When compared to other methods in the literature
it outperforms them in three out of five databases and is competitive in the
remaining.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at EUSIPCO 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Over-the-Air Federated Learning in Satellite systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02996v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02996v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edward Akito Carlos, Raphael Pinard, Mitra Hassani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning in satellites offers several advantages. Firstly, it
ensures data privacy and security, as sensitive data remains on the satellites
and is not transmitted to a central location. This is particularly important
when dealing with sensitive or classified information. Secondly, federated
learning allows satellites to collectively learn from a diverse set of data
sources, benefiting from the distributed knowledge across the satellite
network. Lastly, the use of federated learning reduces the communication
bandwidth requirements between satellites and the central server, as only model
updates are exchanged instead of raw data. By leveraging federated learning,
satellites can collaborate and continuously improve their machine learning
models while preserving data privacy and minimizing communication overhead.
This enables the development of more intelligent and efficient satellite
systems for various applications, such as Earth observation, weather
forecasting, and space exploration.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Integrated Sensing, Computation, and Communication for UAV-assisted
  Federated Edge Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02990v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02990v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yao Tang, Guangxu Zhu, Wei Xu, Man Hon Cheung, Tat-Ming Lok, Shuguang Cui
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated edge learning (FEEL) enables privacy-preserving model training
through periodic communication between edge devices and the server. Unmanned
Aerial Vehicle (UAV)-mounted edge devices are particularly advantageous for
FEEL due to their flexibility and mobility in efficient data collection. In
UAV-assisted FEEL, sensing, computation, and communication are coupled and
compete for limited onboard resources, and UAV deployment also affects sensing
and communication performance. Therefore, the joint design of UAV deployment
and resource allocation is crucial to achieving the optimal training
performance. In this paper, we address the problem of joint UAV deployment
design and resource allocation for FEEL via a concrete case study of human
motion recognition based on wireless sensing. We first analyze the impact of
UAV deployment on the sensing quality and identify a threshold value for the
sensing elevation angle that guarantees a satisfactory quality of data samples.
Due to the non-ideal sensing channels, we consider the probabilistic sensing
model, where the successful sensing probability of each UAV is determined by
its position. Then, we derive the upper bound of the FEEL training loss as a
function of the sensing probability. Theoretical results suggest that the
convergence rate can be improved if UAVs have a uniform successful sensing
probability. Based on this analysis, we formulate a training time minimization
problem by jointly optimizing UAV deployment, integrated sensing, computation,
and communication (ISCC) resources under a desirable optimality gap constraint.
To solve this challenging mixed-integer non-convex problem, we apply the
alternating optimization technique, and propose the bandwidth, batch size, and
position optimization (BBPO) scheme to optimize these three decision variables
alternately.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Brain tumor segmentation using synthetic MR images -- A comparison of
  GANs and diffusion models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02986v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02986v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Muhammad Usman Akbar, Måns Larsson, Anders Eklund
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large annotated datasets are required for training deep learning models, but
in medical imaging data sharing is often complicated due to ethics,
anonymization and data protection legislation (e.g. the general data protection
regulation (GDPR)). Generative AI models, such as generative adversarial
networks (GANs) and diffusion models, can today produce very realistic
synthetic images, and can potentially facilitate data sharing as GDPR should
not apply for medical images which do not belong to a specific person. However,
in order to share synthetic images it must first be demonstrated that they can
be used for training different networks with acceptable performance. Here, we
therefore comprehensively evaluate four GANs (progressive GAN, StyleGAN 1-3)
and a diffusion model for the task of brain tumor segmentation. Our results
show that segmentation networks trained on synthetic images reach Dice scores
that are 80\% - 90\% of Dice scores when training with real images, but that
memorization of the training images can be a problem for diffusion models if
the original dataset is too small. Furthermore, we demonstrate that common
metrics for evaluating synthetic images, Fr\'echet inception distance (FID) and
inception score (IS), do not correlate well with the obtained performance when
using the synthetic images for training segmentation networks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 Pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Deep Learning Approach Utilizing Covariance Matrix Analysis for the
  ISBI Edited MRS Reconstruction Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02984v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02984v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Julian P. Merkofer, Dennis M. J. van de Sande, Sina Amirrajab, Gerhard S. Drenthen, Mitko Veta, Jacobus F. A. Jansen, Marcel Breeuwer, Ruud J. G. van Sloun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work proposes a method to accelerate the acquisition of high-quality
edited magnetic resonance spectroscopy (MRS) scans using machine learning
models taking the sample covariance matrix as input. The method is invariant to
the number of transients and robust to noisy input data for both synthetic as
well as in-vivo scenarios.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Simultaneous or Sequential Training? How Speech Representations
  Cooperate in a Multi-Task <span class="highlight-title">Self-Supervised</span> Learning System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02972v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02972v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Khazar Khorrami, María Andrea Cruz Blandón, Tuomas Virtanen, Okko Räsänen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Speech representation learning with self-supervised algorithms has resulted
in notable performance boosts in many downstream tasks. Recent work combined
self-supervised learning (SSL) and visually grounded speech (VGS) processing
mechanisms for representation learning. The joint training with SSL and VGS
mechanisms provides the opportunity to utilize both unlabeled speech and
speech-related visual information based on data availability. This has shown to
enhance the quality of learned representations, especially at encoding
semantic- and lexical-level knowledge. In this work, we further study the joint
optimization of wav2vec 2.0-based SSL and transformer-based VGS as a multi-task
learning system. We explore a set of training scenarios to understand how
speech representations are shared or transferred between the two tasks, and
what is the optimal training strategy for cross-modal semantic retrieval and
phoneme discrimination performance. As a result, we find that sequential
training with wav2vec 2.0 first and VGS next provides higher performance on
audio-visual retrieval compared to simultaneous optimization of both learning
mechanisms. However, the parallel SSL-VGS training reduces the effects of
catastrophic forgetting when switching between optimization criteria. Moreover,
the results suggest that phonemic representations learned through the VGS
mechanism may generalize better across datasets compared to those learned with
SSL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>5 pages, accepted by EUSIPCO 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Online Learning with Feedback Graphs: The True Shape of Regret 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02971v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02971v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomáš Kocák, Alexandra Carpentier
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sequential learning with feedback graphs is a natural extension of the
multi-armed bandit problem where the problem is equipped with an underlying
graph structure that provides additional information - playing an action
reveals the losses of all the neighbors of the action. This problem was
introduced by \citet{mannor2011} and received considerable attention in recent
years. It is generally stated in the literature that the minimax regret rate
for this problem is of order $\sqrt{\alpha T}$, where $\alpha$ is the
independence number of the graph, and $T$ is the time horizon. However, this is
proven only when the number of rounds $T$ is larger than $\alpha^3$, which
poses a significant restriction for the usability of this result in large
graphs. In this paper, we define a new quantity $R^*$, called the \emph{problem
complexity}, and prove that the minimax regret is proportional to $R^*$ for any
graph and time horizon $T$. Introducing an intricate exploration strategy, we
define the \mainAlgorithm algorithm that achieves the minimax optimal regret
bound and becomes the first provably optimal algorithm for this setting, even
if $T$ is smaller than $\alpha^3$.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Time Interpret: a Unified Model Interpretability Library for Time Series 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02968v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02968v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Joseph Enguehard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce $\texttt{time_interpret}$, a library designed as an extension of
Captum, with a specific focus on temporal data. As such, this library
implements several feature attribution methods that can be used to explain
predictions made by any Pytorch model. $\texttt{time_interpret}$ also provides
several synthetic and real world time series datasets, various PyTorch models,
as well as a set of methods to evaluate feature attributions. Moreover, while
being primarily developed to explain predictions based on temporal data, some
of its components have a different application, including for instance methods
explaining predictions made by language models. In this paper, we give a
general introduction of this library. We also present several previously
unpublished feature attribution methods, which have been developed along with
$\texttt{time_interpret}$.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 1 figure. Code available at
  https://github.com/josephenguehard/time_interpret</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Best of Both Worlds: Hybrid SNN-ANN Architecture for Event-based Optical
  Flow Estimation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02960v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02960v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shubham Negi, Deepika Sharma, Adarsh Kumar Kosta, Kaushik Roy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Event-based cameras offer a low-power alternative to frame-based cameras for
capturing high-speed motion and high dynamic range scenes. They provide
asynchronous streams of sparse events. Spiking Neural Networks (SNNs) with
their asynchronous event-driven compute, show great potential for extracting
the spatio-temporal features from these event streams. In contrast, the
standard Analog Neural Networks (ANNs1) fail to process event data effectively.
However, training SNNs is difficult due to additional trainable parameters
(thresholds and leaks), vanishing spikes at deeper layers, non-differentiable
binary activation function etc. Moreover, an additional data structure
"membrane potential" responsible for keeping track of temporal information,
must be fetched and updated at every timestep in SNNs. To overcome these, we
propose a novel SNN-ANN hybrid architecture that combines the strengths of
both. Specifically, we leverage the asynchronous compute capabilities of SNN
layers to effectively extract the input temporal information. While the ANN
layers offer trouble-free training and implementation on standard machine
learning hardware such as GPUs. We provide extensive experimental analysis for
assigning each layer to be spiking or analog in nature, leading to a network
configuration optimized for performance and ease of training. We evaluate our
hybrid architectures for optical flow estimation using event-data on DSEC-flow
and Mutli-Vehicle Stereo Event-Camera (MVSEC) datasets. The results indicate
that our configured hybrid architectures outperform the state-of-the-art
ANN-only, SNN-only and past hybrid architectures both in terms of accuracy and
efficiency. Specifically, our hybrid architecture exhibit a 31% and 24.8% lower
average endpoint error (AEE) at 2.1x and 3.1x lower energy, compared to an
SNN-only architecture on DSEC and MVSEC datasets, respectively.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complex Preferences for Different Convergent Priors in Discrete Graph
  Diffusion 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02957v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02957v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alex M. Tseng, Nathaniel Diamant, Tommaso Biancalani, Gabriele Scalia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Diffusion models have achieved state-of-the-art performance in generating
many different kinds of data, including images, text, and videos. Despite their
success, there has been limited research on how the underlying diffusion
process and the final convergent prior can affect generative performance; this
research has also been limited to continuous data types and a score-based
diffusion framework. To fill this gap, we explore how different discrete
diffusion kernels (which converge to different prior distributions) affect the
performance of diffusion models for graphs. To this end, we developed a novel
formulation of a family of discrete diffusion kernels which are easily
adjustable to converge to different Bernoulli priors, and we study the effect
of these different kernels on generative performance. We show that the quality
of generated graphs is sensitive to the prior used, and that the optimal choice
cannot be explained by obvious statistics or metrics, which challenges the
intuitions which previous works have suggested.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Simple and Flexible Modeling for Mental Disorder Detection by Learning
  from Clinical Questionnaires <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02955v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02955v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hoyun Song, Jisu Shin, Huije Lee, Jong C. Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Social media is one of the most highly sought resources for analyzing
characteristics of the language by its users. In particular, many researchers
utilized various linguistic features of mental health problems from social
media. However, existing approaches to detecting mental disorders face critical
challenges, such as the scarcity of high-quality data or the trade-off between
addressing the complexity of models and presenting interpretable results
grounded in expert domain knowledge. To address these challenges, we design a
simple but flexible model that preserves domain-based interpretability. We
propose a novel approach that captures the semantic meanings directly from the
text and compares them to symptom-related descriptions. Experimental results
demonstrate that our model outperforms relevant baselines on various mental
disorder detection tasks. Our detailed analysis shows that the proposed model
is effective at leveraging domain knowledge, transferable to other mental
disorders, and providing interpretable detection results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023, 15 pages, 11 tables, 4 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Random Distribution Shift in Refugee Placement: Strategies for Building
  Robust Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02948v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02948v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kirk Bansak, Elisabeth Paulson, Dominik Rothenhäusler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Algorithmic assignment of refugees and asylum seekers to locations within
host countries has gained attention in recent years, with implementations in
the US and Switzerland. These approaches use data on past arrivals to generate
machine learning models that can be used (along with assignment algorithms) to
match families to locations, with the goal of maximizing a policy-relevant
integration outcome such as employment status after a certain duration.
Existing implementations and research train models to predict the policy
outcome directly, and use these predictions in the assignment procedure.
However, the merits of this approach, particularly in non-stationary settings,
has not been previously explored. This study proposes and compares three
different modeling strategies: the standard approach described above, an
approach that uses newer data and proxy outcomes, and a hybrid approach. We
show that the hybrid approach is robust to both distribution shift and weak
proxy relationships -- the failure points of the other two methods,
respectively. We compare these approaches empirically using data on asylum
seekers in the Netherlands. Surprisingly, we find that both the proxy and
hybrid approaches out-perform the standard approach in practice. These insights
support the development of a real-world recommendation tool currently used by
NGOs and government agencies.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Continual Learning with <span class="highlight-title">Pretrain</span>ed Backbones by Tuning in the Input
  Space 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02947v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02947v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simone Marullo, Matteo Tiezzi, Marco Gori, Stefano Melacci, Tinne Tuytelaars
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The intrinsic difficulty in adapting deep learning models to non-stationary
environments limits the applicability of neural networks to real-world tasks.
This issue is critical in practical supervised learning settings, such as the
ones in which a pre-trained model computes projections toward a latent space
where different task predictors are sequentially learned over time. As a matter
of fact, incrementally fine-tuning the whole model to better adapt to new tasks
usually results in catastrophic forgetting, with decreasing performance over
the past experiences and losing valuable knowledge from the pre-training stage.
In this paper, we propose a novel strategy to make the fine-tuning procedure
more effective, by avoiding to update the pre-trained part of the network and
learning not only the usual classification head, but also a set of
newly-introduced learnable parameters that are responsible for transforming the
input data. This process allows the network to effectively leverage the
pre-training knowledge and find a good trade-off between plasticity and
stability with modest computational efforts, thus especially suitable for
on-the-edge settings. Our experiments on four image classification problems in
a continual learning setting confirm the quality of the proposed approach when
compared to several fine-tuning procedures and to popular continual learning
methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Improved Stability and Generalization Analysis of the Decentralized SGD
  Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02939v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02939v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Batiste Le Bars, Aurélien Bellet, Marc Tommasi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper presents a new generalization error analysis for the Decentralized
Stochastic Gradient Descent (D-SGD) algorithm based on algorithmic stability.
The obtained results largely improve upon state-of-the-art results, and even
invalidate their claims that the communication graph has a detrimental effect
on generalization. For instance, we show that in convex settings, D-SGD has the
same generalization bounds as the classical SGD algorithm, no matter the choice
of graph. We exhibit that this counter-intuitive result comes from considering
the average of local parameters, which hides a final global averaging step
incompatible with the decentralized scenario. In light of this observation, we
advocate to analyze the supremum over local parameters and show that in this
case, the graph does have an impact on the generalization. Unlike prior
results, our analysis yields non-vacuous bounds even for non-connected graphs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Causal Discovery using Bayesian Model Selection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02931v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02931v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anish Dhir, Mark van der Wilk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  With only observational data on two variables, and without other assumptions,
it is not possible to infer which one causes the other. Much of the causal
literature has focused on guaranteeing identifiability of causal direction in
statistical models for datasets where strong assumptions hold, such as additive
noise or restrictions on parameter count. These methods are then subsequently
tested on realistic datasets, most of which violate their assumptions. Building
on previous attempts, we show how to use causal assumptions within the Bayesian
framework. This allows us to specify models with realistic assumptions, while
also encoding independent causal mechanisms, leading to an asymmetry between
the causal directions. Identifying causal direction then becomes a Bayesian
model selection problem. We analyse why Bayesian model selection works for
known identifiable cases and flexible model classes, while also providing
correctness guarantees about its behaviour. To demonstrate our approach, we
construct a Bayesian non-parametric model that can flexibly model the joint. We
then outperform previous methods on a wide range of benchmark datasets with
varying data generating assumptions showing the usefulness of our method.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Decentralized SGD and Average-direction SAM are Asymptotically
  Equivalent <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02913v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02913v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tongtian Zhu, Fengxiang He, Kaixuan Chen, Mingli Song, Dacheng Tao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decentralized stochastic gradient descent (D-SGD) allows collaborative
learning on massive devices simultaneously without the control of a central
server. However, existing theories claim that decentralization invariably
undermines generalization. In this paper, we challenge the conventional belief
and present a completely new perspective for understanding decentralized
learning. We prove that D-SGD implicitly minimizes the loss function of an
average-direction Sharpness-aware minimization (SAM) algorithm under general
non-convex non-$\beta$-smooth settings. This surprising asymptotic equivalence
reveals an intrinsic regularization-optimization trade-off and three advantages
of decentralization: (1) there exists a free uncertainty evaluation mechanism
in D-SGD to improve posterior estimation; (2) D-SGD exhibits a gradient
smoothing effect; and (3) the sharpness regularization effect of D-SGD does not
decrease as total batch size increases, which justifies the potential
generalization benefit of D-SGD over centralized SGD (C-SGD) in large-batch
scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication in the 40th International Conference on
  Machine Learning (ICML 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning nonparametric latent causal graphs with unknown interventions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02899v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02899v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibo Jiang, Bryon Aragam
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We establish conditions under which latent causal graphs are
nonparametrically identifiable and can be reconstructed from unknown
interventions in the latent space. Our primary focus is the identification of
the latent structure in a measurement model, i.e. causal graphical models where
dependence between observed variables is insignificant compared to dependence
between latent representations, without making parametric assumptions such as
linearity or Gaussianity. Moreover, we do not assume the number of hidden
variables is known, and we show that at most one unknown intervention per
hidden variable is needed. This extends a recent line of work on learning
causal representations from observations and interventions. The proofs are
constructive and introduce two new graphical concepts -- imaginary subsets and
isolated edges -- that may be useful in their own right. As a matter of
independent interest, the proofs also involve a novel characterization of the
limits of edge orientations within the equivalence class of DAGs induced by
unknown interventions. Experiments confirm that the latent graph can be
recovered from data using our theoretical results. These are the first results
to characterize the conditions under which causal representations are
identifiable without making any parametric assumptions in a general setting
with unknown interventions and without faithfulness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Representational Strengths and Limitations of <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02896v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02896v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clayton Sanford, Daniel Hsu, Matus Telgarsky
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Attention layers, as commonly used in transformers, form the backbone of
modern deep learning, yet there is no mathematical description of their
benefits and deficiencies as compared with other architectures. In this work we
establish both positive and negative results on the representation power of
attention layers, with a focus on intrinsic complexity parameters such as
width, depth, and embedding dimension. On the positive side, we present a
sparse averaging task, where recurrent networks and feedforward networks all
have complexity scaling polynomially in the input size, whereas transformers
scale merely logarithmically in the input size; furthermore, we use the same
construction to show the necessity and role of a large embedding dimension in a
transformer. On the negative side, we present a triple detection task, where
attention layers in turn have complexity scaling linearly in the input size; as
this scenario seems rare in practice, we also present natural variants that can
be efficiently solved by attention layers. The proof techniques emphasize the
value of communication complexity in the analysis of transformers and related
models, and the role of sparse averaging as a prototypical attention task,
which even finds use in the analysis of triple detection.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evading Black-box Classifiers Without Breaking Eggs 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02895v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02895v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Edoardo Debenedetti, Nicholas Carlini, Florian Tramèr
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Decision-based evasion attacks repeatedly query a black-box classifier to
generate adversarial examples. Prior work measures the cost of such attacks by
the total number of queries made to the classifier. We argue this metric is
flawed. Most security-critical machine learning systems aim to weed out "bad"
data (e.g., malware, harmful content, etc). Queries to such systems carry a
fundamentally asymmetric cost: queries detected as "bad" come at a higher cost
because they trigger additional security filters, e.g., usage throttling or
account suspension. Yet, we find that existing decision-based attacks issue a
large number of "bad" queries, which likely renders them ineffective against
security-critical systems. We then design new attacks that reduce the number of
bad queries by $1.5$-$7.3\times$, but often at a significant increase in total
(non-bad) queries. We thus pose it as an open problem to build black-box
attacks that are more effective under realistic cost metrics.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code at https://github.com/ethz-privsec/realistic-adv-examples</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Image Reconstruction for Accelerated MR Scan with Faster Fourier
  Convolutional Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02886v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02886v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaohan Liu, Yanwei Pang, Xuebin Sun, Yiming Liu, Yonghong Hou, Zhenchang Wang, Xuelong Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Partial scan is a common approach to accelerate Magnetic Resonance Imaging
(MRI) data acquisition in both 2D and 3D settings. However, accurately
reconstructing images from partial scan data (i.e., incomplete k-space
matrices) remains challenging due to lack of an effectively global receptive
field in both spatial and k-space domains. To address this problem, we propose
the following: (1) a novel convolutional operator called Faster Fourier
Convolution (FasterFC) to replace the two consecutive convolution operations
typically used in convolutional neural networks (e.g., U-Net, ResNet). Based on
the spectral convolution theorem in Fourier theory, FasterFC employs
alternating kernels of size 1 in 3D case) in different domains to extend the
dual-domain receptive field to the global and achieves faster calculation speed
than traditional Fast Fourier Convolution (FFC). (2) A 2D accelerated MRI
method, FasterFC-End-to-End-VarNet, which uses FasterFC to improve the
sensitivity maps and reconstruction quality. (3) A multi-stage 3D accelerated
MRI method called FasterFC-based Single-to-group Network (FAS-Net) that
utilizes a single-to-group algorithm to guide k-space domain reconstruction,
followed by FasterFC-based cascaded convolutional neural networks to expand the
effective receptive field in the dual-domain. Experimental results on the
fastMRI and Stanford MRI Data datasets demonstrate that FasterFC improves the
quality of both 2D and 3D reconstruction. Moreover, FAS-Net, as a 3D
high-resolution multi-coil (eight) accelerated MRI method, achieves superior
reconstruction performance in both qualitative and quantitative results
compared with state-of-the-art 2D and 3D methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Neuron Activation Coverage: Rethinking Out-of-distribution Detection and
  Generalization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02879v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02879v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yibing Liu, Chris Xing Tian, Haoliang Li, Lei Ma, Shiqi Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The out-of-distribution (OOD) problem generally arises when neural networks
encounter data that significantly deviates from the training data distribution,
\ie, in-distribution (InD). In this paper, we study the OOD problem from a
neuron activation view. We first formulate neuron activation states by
considering both the neuron output and its influence on model decisions. Then,
we propose the concept of \textit{neuron activation coverage} (NAC), which
characterizes the neuron behaviors under InD and OOD data. Leveraging our NAC,
we show that 1) InD and OOD inputs can be naturally separated based on the
neuron behavior, which significantly eases the OOD detection problem and
achieves a record-breaking performance of 0.03% FPR95 on ResNet-50,
outperforming the previous best method by 20.67%; 2) a positive correlation
between NAC and model generalization ability consistently holds across
architectures and datasets, which enables a NAC-based criterion for evaluating
model robustness. By comparison with the traditional validation criterion, we
show that NAC-based criterion not only can select more robust models, but also
has a stronger correlation with OOD test performance.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Data-Driven Regret Balancing for Online Model Selection in Bandits 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02869v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02869v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aldo Pacchiano, Christoph Dann, Claudio Gentile
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider model selection for sequential decision making in stochastic
environments with bandit feedback, where a meta-learner has at its disposal a
pool of base learners, and decides on the fly which action to take based on the
policies recommended by each base learner. Model selection is performed by
regret balancing but, unlike the recent literature on this subject, we do not
assume any prior knowledge about the base learners like candidate regret
guarantees; instead, we uncover these quantities in a data-driven manner. The
meta-learner is therefore able to leverage the realized regret incurred by each
base learner for the learning environment at hand (as opposed to the expected
regret), and single out the best such regret. We design two model selection
algorithms operating with this more ambitious notion of regret and, besides
proving model selection guarantees via regret balancing, we experimentally
demonstrate the compelling practical benefits of dealing with actual regrets
instead of candidate regret bounds.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning Probabilistic Symmetrization for Architecture Agnostic
  Equivariance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02866v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02866v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinwoo Kim, Tien Dat Nguyen, Ayhan Suleymanzade, Hyeokjun An, Seunghoon Hong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a novel framework to overcome the limitations of equivariant
architectures in learning functions with group symmetries. In contrary to
equivariant architectures, we use an arbitrary base model (such as an MLP or a
transformer) and symmetrize it to be equivariant to the given group by
employing a small equivariant network that parameterizes the probabilistic
distribution underlying the symmetrization. The distribution is end-to-end
trained with the base model which can maximize performance while reducing
sample complexity of symmetrization. We show that this approach ensures not
only equivariance to given group but also universal approximation capability in
expectation. We implement our method on a simple patch-based transformer that
can be initialized from pretrained vision transformers, and test it for a wide
range of symmetry groups including permutation and Euclidean groups and their
combinations. Empirical tests show competitive results against tailored
equivariant architectures, suggesting the potential for learning equivariant
functions for diverse groups using a non-equivariant universal base
architecture. We further show evidence of enhanced learning in symmetric
modalities, like graphs, when pretrained from non-symmetric modalities, like
vision. Our implementation will be open-sourced at
https://github.com/jw9730/lps.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>25 pages, 3 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Seizing Serendipity: Exploiting the Value of Past Success in Off-Policy
  Actor-Critic 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02865v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02865v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianying Ji, Yu Luo, Fuchun Sun, Xianyuan Zhan, Jianwei Zhang, Huazhe Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning high-quality Q-value functions plays a key role in the success of
many modern off-policy deep reinforcement learning (RL) algorithms. Previous
works focus on addressing the value overestimation issue, an outcome of
adopting function approximators and off-policy learning. Deviating from the
common viewpoint, we observe that Q-values are indeed underestimated in the
latter stage of the RL training process, primarily related to the use of
inferior actions from the current policy in Bellman updates as compared to the
more optimal action samples in the replay buffer. We hypothesize that this
long-neglected phenomenon potentially hinders policy learning and reduces
sample efficiency. Our insight to address this issue is to incorporate
sufficient exploitation of past successes while maintaining exploration
optimism. We propose the Blended Exploitation and Exploration (BEE) operator, a
simple yet effective approach that updates Q-value using both historical
best-performing actions and the current policy. The instantiations of our
method in both model-free and model-based settings outperform state-of-the-art
methods in various continuous control tasks and achieve strong performance in
failure-prone scenarios and real-world robot tasks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Local Boosting for Weakly-Supervised Learning <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02859v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02859v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rongzhi Zhang, Yue Yu, Jiaming Shen, Xiquan Cui, Chao Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Boosting is a commonly used technique to enhance the performance of a set of
base models by combining them into a strong ensemble model. Though widely
adopted, boosting is typically used in supervised learning where the data is
labeled accurately. However, in weakly supervised learning, where most of the
data is labeled through weak and noisy sources, it remains nontrivial to design
effective boosting approaches. In this work, we show that the standard
implementation of the convex combination of base learners can hardly work due
to the presence of noisy labels. Instead, we propose $\textit{LocalBoost}$, a
novel framework for weakly-supervised boosting. LocalBoost iteratively boosts
the ensemble model from two dimensions, i.e., intra-source and inter-source.
The intra-source boosting introduces locality to the base learners and enables
each base learner to focus on a particular feature regime by training new base
learners on granularity-varying error regions. For the inter-source boosting,
we leverage a conditional function to indicate the weak source where the sample
is more likely to appear. To account for the weak labels, we further design an
estimate-then-modify approach to compute the model weights. Experiments on
seven datasets show that our method significantly outperforms vanilla boosting
methods and other weakly-supervised methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2023 Research Track</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ HireVAE: An Online and Adaptive Factor Model Based on Hierarchical and
  Regime-Switch VAE <span class="chip">IJCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02848v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02848v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zikai Wei, Anyi Rao, Bo Dai, Dahua Lin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Factor model is a fundamental investment tool in quantitative investment,
which can be empowered by deep learning to become more flexible and efficient
in practical complicated investing situations. However, it is still an open
question to build a factor model that can conduct stock prediction in an online
and adaptive setting, where the model can adapt itself to match the current
market regime identified based on only point-in-time market information. To
tackle this problem, we propose the first deep learning based online and
adaptive factor model, HireVAE, at the core of which is a hierarchical latent
space that embeds the underlying relationship between the market situation and
stock-wise latent factors, so that HireVAE can effectively estimate useful
latent factors given only historical market information and subsequently
predict accurate stock returns. Across four commonly used real stock market
benchmarks, the proposed HireVAE demonstrate superior performance in terms of
active returns over previous methods, verifying the potential of such online
and adaptive factor model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to IJCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Learning to Substitute Spans towards Improving Compositional
  Generalization <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02840v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02840v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhaoyi Li, Ying Wei, Defu Lian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the rising prevalence of neural sequence models, recent empirical
evidences suggest their deficiency in compositional generalization. One of the
current de-facto solutions to this problem is compositional data augmentation,
aiming to incur additional compositional inductive bias. Nonetheless, the
improvement offered by existing handcrafted augmentation strategies is limited
when successful systematic generalization of neural sequence models requires
multi-grained compositional bias (i.e., not limited to either lexical or
structural biases only) or differentiation of training sequences in an
imbalanced difficulty distribution. To address the two challenges, we first
propose a novel compositional augmentation strategy dubbed \textbf{Span}
\textbf{Sub}stitution (SpanSub) that enables multi-grained composition of
substantial substructures in the whole training set. Over and above that, we
introduce the \textbf{L}earning \textbf{to} \textbf{S}ubstitute \textbf{S}pan
(L2S2) framework which empowers the learning of span substitution probabilities
in SpanSub in an end-to-end manner by maximizing the loss of neural sequence
models, so as to outweigh those challenging compositions with elusive concepts
and novel surroundings. Our empirical results on three standard compositional
generalization benchmarks, including SCAN, COGS and GeoQuery (with an
improvement of at most 66.5\%, 10.3\%, 1.2\%, respectively), demonstrate the
superiority of SpanSub, %the learning framework L2S2 and their combination.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted by ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Computational Complexity of Detecting Proximity to Losslessly
  Compressible Neural Network Parameters 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02834v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02834v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthew Farrugia-Roberts
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  To better understand complexity in neural networks, we theoretically
investigate the idealised phenomenon of lossless network compressibility,
whereby an identical function can be implemented with a smaller network. We
give an efficient formal algorithm for optimal lossless compression in the
setting of single-hidden-layer hyperbolic tangent networks. To measure lossless
compressibility, we define the rank of a parameter as the minimum number of
hidden units required to implement the same function. Losslessly compressible
parameters are atypical, but their existence has implications for nearby
parameters. We define the proximate rank of a parameter as the rank of the most
compressible parameter within a small $L^\infty$ neighbourhood. Unfortunately,
detecting nearby losslessly compressible parameters is not so easy: we show
that bounding the proximate rank is an NP-complete problem, using a reduction
from Boolean satisfiability via a geometric problem involving covering points
in the plane with small squares. These results underscore the computational
complexity of measuring neural network complexity, laying a foundation for
future theoretical and empirical work in this direction.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages paper, 31 pages total, 9 figures, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ The $L^\infty$ Learnability of Reproducing Kernel Hil<span class="highlight-title">bert</span> Spaces 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02833v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02833v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hongrui Chen, Jihao Long, Lei Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, we analyze the learnability of reproducing kernel Hilbert
spaces (RKHS) under the $L^\infty$ norm, which is critical for understanding
the performance of kernel methods and random feature models in safety- and
security-critical applications. Specifically, we relate the $L^\infty$
learnability of a RKHS to the spectrum decay of the associate kernel and both
lower bounds and upper bounds of the sample complexity are established. In
particular, for dot-product kernels on the sphere, we identify conditions when
the $L^\infty$ learning can be achieved with polynomial samples. Let $d$ denote
the input dimension and assume the kernel spectrum roughly decays as
$\lambda_k\sim k^{-1-\beta}$ with $\beta>0$. We prove that if $\beta$ is
independent of the input dimension $d$, then functions in the RKHS can be
learned efficiently under the $L^\infty$ norm, i.e., the sample complexity
depends polynomially on $d$. In contrast, if $\beta=1/\mathrm{poly}(d)$, then
the $L^\infty$ learning requires exponentially many samples.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>20 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MM-DAG: Multi-task DAG Learning for Multi-modal Data -- with Application
  for Traffic Congestion Analysis <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02831v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02831v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tian Lan, Ziyue Li, Zhishuai Li, Lei Bai, Man Li, Fugee Tsung, Wolfgang Ketter, Rui Zhao, Chen Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes to learn Multi-task, Multi-modal Direct Acyclic Graphs
(MM-DAGs), which are commonly observed in complex systems, e.g., traffic,
manufacturing, and weather systems, whose variables are multi-modal with
scalars, vectors, and functions. This paper takes the traffic congestion
analysis as a concrete case, where a traffic intersection is usually regarded
as a DAG. In a road network of multiple intersections, different intersections
can only have some overlapping and distinct variables observed. For example, a
signalized intersection has traffic light-related variables, whereas
unsignalized ones do not. This encourages the multi-task design: with each DAG
as a task, the MM-DAG tries to learn the multiple DAGs jointly so that their
consensus and consistency are maximized. To this end, we innovatively propose a
multi-modal regression for linear causal relationship description of different
variables. Then we develop a novel Causality Difference (CD) measure and its
differentiable approximator. Compared with existing SOTA measures, CD can
penalize the causal structural difference among DAGs with distinct nodes and
can better consider the uncertainty of causal orders. We rigidly prove our
design's topological interpretation and consistency properties. We conduct
thorough simulations and one case study to show the effectiveness of our
MM-DAG. The code is available under https://github.com/Lantian72/MM-DAG
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in SIGKDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Near-Optimal Quantum Coreset Construction Algorithms for Clustering <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02826v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02826v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yecheng Xue, Xiaoyu Chen, Tongyang Li, Shaofeng H. -C. Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  $k$-Clustering in $\mathbb{R}^d$ (e.g., $k$-median and $k$-means) is a
fundamental machine learning problem. While near-linear time approximation
algorithms were known in the classical setting for a dataset with cardinality
$n$, it remains open to find sublinear-time quantum algorithms. We give quantum
algorithms that find coresets for $k$-clustering in $\mathbb{R}^d$ with
$\tilde{O}(\sqrt{nk}d^{3/2})$ query complexity. Our coreset reduces the input
size from $n$ to $\mathrm{poly}(k\epsilon^{-1}d)$, so that existing
$\alpha$-approximation algorithms for clustering can run on top of it and yield
$(1 + \epsilon)\alpha$-approximation. This eventually yields a quadratic
speedup for various $k$-clustering approximation algorithms. We complement our
algorithm with a nearly matching lower bound, that any quantum algorithm must
make $\Omega(\sqrt{nk})$ queries in order to achieve even $O(1)$-approximation
for $k$-clustering.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Comments: 32 pages, 0 figures, 1 table. To appear in the Fortieth
  International Conference on Machine Learning (ICML 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ COMET: Learning Cardinality Constrained Mixture of Experts with Trees
  and Local Search <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02824v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02824v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shibal Ibrahim, Wenyu Chen, Hussein Hazimeh, Natalia Ponomareva, Zhe Zhao, Rahul Mazumder
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The sparse Mixture-of-Experts (Sparse-MoE) framework efficiently scales up
model capacity in various domains, such as natural language processing and
vision. Sparse-MoEs select a subset of the "experts" (thus, only a portion of
the overall network) for each input sample using a sparse, trainable gate.
Existing sparse gates are prone to convergence and performance issues when
training with first-order optimization methods. In this paper, we introduce two
improvements to current MoE approaches. First, we propose a new sparse gate:
COMET, which relies on a novel tree-based mechanism. COMET is differentiable,
can exploit sparsity to speed up computation, and outperforms state-of-the-art
gates. Second, due to the challenging combinatorial nature of sparse expert
selection, first-order methods are typically prone to low-quality solutions. To
deal with this challenge, we propose a novel, permutation-based local search
method that can complement first-order methods in training any sparse gate,
e.g., Hash routing, Top-k, DSelect-k, and COMET. We show that local search can
help networks escape bad initializations or solutions. We performed large-scale
experiments on various domains, including recommender systems, vision, and
natural language processing. On standard vision and recommender systems
benchmarks, COMET+ (COMET with local search) achieves up to 13% improvement in
ROC AUC over popular gates, e.g., Hash routing and Top-k, and up to 9% over
prior differentiable gates e.g., DSelect-k. When Top-k and Hash gates are
combined with local search, we see up to $100\times$ reduction in the budget
needed for hyperparameter tuning. Moreover, for language modeling, our approach
improves over the state-of-the-art MoEBERT model for distilling BERT on 5/7
GLUE benchmarks as well as SQuAD dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in KDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Discovering Dynamic Causal Space for DAG Structure Learning <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02822v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02822v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fangfu Liu, Wenchang Ma, An Zhang, Xiang Wang, Yueqi Duan, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Discovering causal structure from purely observational data (i.e., causal
discovery), aiming to identify causal relationships among variables, is a
fundamental task in machine learning. The recent invention of differentiable
score-based DAG learners is a crucial enabler, which reframes the combinatorial
optimization problem into a differentiable optimization with a DAG constraint
over directed graph space. Despite their great success, these cutting-edge DAG
learners incorporate DAG-ness independent score functions to evaluate the
directed graph candidates, lacking in considering graph structure. As a result,
measuring the data fitness alone regardless of DAG-ness inevitably leads to
discovering suboptimal DAGs and model vulnerabilities. Towards this end, we
propose a dynamic causal space for DAG structure learning, coined CASPER, that
integrates the graph structure into the score function as a new measure in the
causal space to faithfully reflect the causal distance between estimated and
ground truth DAG. CASPER revises the learning process as well as enhances the
DAG structure learning via adaptive attention to DAG-ness. Grounded by
empirical visualization, CASPER, as a space, satisfies a series of desired
properties, such as structure awareness and noise robustness. Extensive
experiments on both synthetic and real-world datasets clearly validate the
superiority of our CASPER over the state-of-the-art causal discovery methods in
terms of accuracy and robustness.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by KDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ MultiAdam: Parameter-wise Scale-invariant Optimizer for Multiscale
  Training of Physics-informed Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02816v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02816v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiachen Yao, Chang Su, Zhongkai Hao, Songming Liu, Hang Su, Jun Zhu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Physics-informed Neural Networks (PINNs) have recently achieved remarkable
progress in solving Partial Differential Equations (PDEs) in various fields by
minimizing a weighted sum of PDE loss and boundary loss. However, there are
several critical challenges in the training of PINNs, including the lack of
theoretical frameworks and the imbalance between PDE loss and boundary loss. In
this paper, we present an analysis of second-order non-homogeneous PDEs, which
are classified into three categories and applicable to various common problems.
We also characterize the connections between the training loss and actual
error, guaranteeing convergence under mild conditions. The theoretical analysis
inspires us to further propose MultiAdam, a scale-invariant optimizer that
leverages gradient momentum to parameter-wisely balance the loss terms.
Extensive experiment results on multiple problems from different physical
domains demonstrate that our MultiAdam solver can improve the predictive
accuracy by 1-2 orders of magnitude compared with strong baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ <span class="highlight-title">Transformer</span>-Based UNet with Multi-Headed Cross-Attention Skip
  Connections to Eliminate Artifacts in Scanned Documents 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02815v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02815v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Kreuzer, Michael Munz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The extraction of text in high quality is essential for text-based document
analysis tasks like Document Classification or Named Entity Recognition.
Unfortunately, this is not always ensured, as poor scan quality and the
resulting artifacts lead to errors in the Optical Character Recognition (OCR)
process. Current approaches using Convolutional Neural Networks show promising
results for background removal tasks but fail correcting artifacts like
pixelation or compression errors. For general images, Transformer backbones are
getting integrated more frequently in well-known neural network structures for
denoising tasks. In this work, a modified UNet structure using a Swin
Transformer backbone is presented to remove typical artifacts in scanned
documents. Multi-headed cross-attention skip connections are used to more
selectively learn features in respective levels of abstraction. The performance
of this approach is examined regarding compression errors, pixelation and
random noise. An improvement in text extraction quality with a reduced error
rate of up to 53.9% on the synthetic data is archived. The pretrained
base-model can be easily adapted to new artifacts. The cross-attention skip
connections allow to integrate textual information extracted from the encoder
or in form of commands to more selectively control the models outcome. The
latter is shown by means of an example application.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Deep Active Learning with Structured Neural Depth Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02808v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02808v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xiaoyun Zhang, Xieyi Ping, Jianwei Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous work optimizes traditional active learning (AL) processes with
incremental neural network architecture search (Active-iNAS) based on data
complexity change, which improves the accuracy and learning efficiency.
However, Active-iNAS trains several models and selects the model with the best
generalization performance for querying the subsequent samples after each
active learning cycle. The independent training processes lead to an
insufferable computational budget, which is significantly inefficient and
limits search flexibility and final performance. To address this issue, we
propose a novel active strategy with the method called structured variational
inference (SVI) or structured neural depth search (SNDS) whereby we could use
the gradient descent method in neural network depth search during AL processes.
At the same time, we theoretically demonstrate that the current VI-based
methods based on the mean-field assumption could lead to poor performance. We
apply our strategy using three querying techniques and three datasets and show
that our strategy outperforms current methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages, 8 figures, prepare for TNNLS</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ On Tail Decay Rate Estimation of Loss Function Distributions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02807v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02807v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Etrit Haxholli, Marco Lorenzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The study of loss function distributions is critical to characterize a
model's behaviour on a given machine learning problem. For example, while the
quality of a model is commonly determined by the average loss assessed on a
testing set, this quantity does not reflect the existence of the true mean of
the loss distribution. Indeed, the finiteness of the statistical moments of the
loss distribution is related to the thickness of its tails, which are generally
unknown. Since typical cross-validation schemes determine a family of testing
loss distributions conditioned on the training samples, the total loss
distribution must be recovered by marginalizing over the space of training
sets. As we show in this work, the finiteness of the sampling procedure
negatively affects the reliability and efficiency of classical tail estimation
methods from the Extreme Value Theory, such as the Peaks-Over-Threshold
approach. In this work we tackle this issue by developing a novel general
theory for estimating the tails of marginal distributions, when there exists a
large variability between locations of the individual conditional distributions
underlying the marginal. To this end, we demonstrate that under some regularity
conditions, the shape parameter of the marginal distribution is the maximum
tail shape parameter of the family of conditional distributions. We term this
estimation approach as Cross Tail Estimation (CTE). We test cross-tail
estimation in a series of experiments on simulated and real data, showing the
improved robustness and quality of tail estimation as compared to classical
approaches, and providing evidence for the relationship between overfitting and
loss distribution tail thickness.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Data-driven Region Generation Framework for Spatiotemporal
  Transportation Service Management 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02806v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02806v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liyue Chen, Jiangyi Fang, Zhe Yu, Yongxin Tong, Shaosheng Cao, Leye Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  MAUP (modifiable areal unit problem) is a fundamental problem for spatial
data management and analysis. As an instantiation of MAUP in online
transportation platforms, region generation (i.e., specifying the areal unit
for service operations) is the first and vital step for supporting
spatiotemporal transportation services such as ride-sharing and freight
transport. Most existing region generation methods are manually specified
(e.g., fixed-size grids), suffering from poor spatial semantic meaning and
inflexibility to meet service operation requirements. In this paper, we propose
RegionGen, a data-driven region generation framework that can specify regions
with key characteristics (e.g., good spatial semantic meaning and
predictability) by modeling region generation as a multi-objective optimization
problem. First, to obtain good spatial semantic meaning, RegionGen segments the
whole city into atomic spatial elements based on road networks and obstacles
(e.g., rivers). Then, it clusters the atomic spatial elements into regions by
maximizing various operation characteristics, which is formulated as a
multi-objective optimization problem. For this optimization problem, we propose
a multi-objective co-optimization algorithm. Extensive experiments verify that
RegionGen can generate more suitable regions than traditional methods for
spatiotemporal service management.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Using Multiple Dermoscopic Photographs of One Lesion Improves Melanoma
  Classification via Deep Learning: A Prognostic Diagnostic Accuracy Study 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02800v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02800v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Achim Hekler, Roman C. Maron, Sarah Haggenmüller, Max Schmitt, Christoph Wies, Jochen S. Utikal, Friedegund Meier, Sarah Hobelsberger, Frank F. Gellrich, Mildred Sergon, Axel Hauschild, Lars E. French, Lucie Heinzerling, Justin G. Schlager, Kamran Ghoreschi, Max Schlaak, Franz J. Hilke, Gabriela Poch, Sören Korsing, Carola Berking, Markus V. Heppt, Michael Erdmann, Sebastian Haferkamp, Konstantin Drexler, Dirk Schadendorf, Wiebke Sondermann, Matthias Goebeler, Bastian Schilling, Jakob N. Kather, Eva Krieghoff-Henning, Titus J. Brinker
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Background: Convolutional neural network (CNN)-based melanoma classifiers
face several challenges that limit their usefulness in clinical practice.
Objective: To investigate the impact of multiple real-world dermoscopic views
of a single lesion of interest on a CNN-based melanoma classifier.
  Methods: This study evaluated 656 suspected melanoma lesions. Classifier
performance was measured using area under the receiver operating characteristic
curve (AUROC), expected calibration error (ECE) and maximum confidence change
(MCC) for (I) a single-view scenario, (II) a multiview scenario using multiple
artificially modified images per lesion and (III) a multiview scenario with
multiple real-world images per lesion.
  Results: The multiview approach with real-world images significantly
increased the AUROC from 0.905 (95% CI, 0.879-0.929) in the single-view
approach to 0.930 (95% CI, 0.909-0.951). ECE and MCC also improved
significantly from 0.131 (95% CI, 0.105-0.159) to 0.072 (95% CI: 0.052-0.093)
and from 0.149 (95% CI, 0.125-0.171) to 0.115 (95% CI: 0.099-0.131),
respectively. Comparing multiview real-world to artificially modified images
showed comparable diagnostic accuracy and uncertainty estimation, but
significantly worse robustness for the latter.
  Conclusion: Using multiple real-world images is an inexpensive method to
positively impact the performance of a CNN-based melanoma classifier.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhancing naive classifier for positive unlabeled data based on logistic
  regression approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02798v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02798v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mateusz Płatek, Jan Mielniczuk
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We argue that for analysis of Positive Unlabeled (PU) data under Selected
Completely At Random (SCAR) assumption it is fruitful to view the problem as
fitting of misspecified model to the data. Namely, we show that the results on
misspecified fit imply that in the case when posterior probability of the
response is modelled by logistic regression, fitting the logistic regression to
the observable PU data which {\it does not} follow this model, still yields the
vector of estimated parameters approximately colinear with the true vector of
parameters. This observation together with choosing the intercept of the
classifier based on optimisation of analogue of F1 measure yields a classifier
which performs on par or better than its competitors on several real data sets
considered.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Human-like Concept Learning with Bayesian Inference over
  Natural Language 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02797v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02797v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kevin Ellis
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We model learning of abstract symbolic concepts by performing Bayesian
inference over utterances in natural language. For efficient inference, we use
a large language model as a proposal distribution. We fit a prior to human data
to better model human learners, and evaluate on both generative and logical
concepts.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Navigating Explanatory Multiverse Through Counterfactual Path Geometry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02786v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02786v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kacper Sokol, Edward Small, Yueqing Xuan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterfactual explanations are the de facto standard when tasked with
interpreting decisions of (opaque) predictive models. Their generation is often
subject to algorithmic and domain-specific constraints -- such as density-based
feasibility for the former and attribute (im)mutability or directionality of
change for the latter -- that aim to maximise their real-life utility. In
addition to desiderata with respect to the counterfactual instance itself, the
existence of a viable path connecting it with the factual data point, known as
algorithmic recourse, has become an important technical consideration. While
both of these requirements ensure that the steps of the journey as well as its
destination are admissible, current literature neglects the multiplicity of
such counterfactual paths. To address this shortcoming we introduce the novel
concept of explanatory multiverse that encompasses all the possible
counterfactual journeys and shows how to navigate, reason about and compare the
geometry of these paths -- their affinity, branching, divergence and possible
future convergence -- with two methods: vector spaces and graphs. Implementing
this (interactive) explanatory process grants explainees more agency by
allowing them to select counterfactuals based on the properties of the journey
leading to them in addition to their absolute differences.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A <span class="highlight-title">survey</span> of Generative AI Applications 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02781v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02781v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto Gozalo-Brizuela, Eduardo C. Garrido-Merchán
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative AI has experienced remarkable growth in recent years, leading to a
wide array of applications across diverse domains. In this paper, we present a
comprehensive survey of more than 350 generative AI applications, providing a
structured taxonomy and concise descriptions of various unimodal and even
multimodal generative AIs. The survey is organized into sections, covering a
wide range of unimodal generative AI applications such as text, images, video,
gaming and brain information. Our survey aims to serve as a valuable resource
for researchers and practitioners to navigate the rapidly expanding landscape
of generative AI, facilitating a better understanding of the current
state-of-the-art and fostering further innovation in the field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Input gradient diversity for neural network ensembles 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02775v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02775v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Trung Trinh, Markus Heinonen, Luigi Acerbi, Samuel Kaski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep Ensembles (DEs) demonstrate improved accuracy, calibration and
robustness to perturbations over single neural networks partly due to their
functional diversity. Particle-based variational inference (ParVI) methods
enhance diversity by formalizing a repulsion term based on a network similarity
kernel. However, weight-space repulsion is inefficient due to
over-parameterization, while direct function-space repulsion has been found to
produce little improvement over DEs. To sidestep these difficulties, we propose
First-order Repulsive Deep Ensemble (FoRDE), an ensemble learning method based
on ParVI, which performs repulsion in the space of first-order input gradients.
As input gradients uniquely characterize a function up to translation and are
much smaller in dimension than the weights, this method guarantees that
ensemble members are functionally different. Intuitively, diversifying the
input gradients encourages each network to learn different features, which is
expected to improve the robustness of an ensemble. Experiments on image
classification datasets show that FoRDE significantly outperforms the
gold-standard DEs and other ensemble methods in accuracy and calibration under
covariate shift due to input perturbations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Identifying the style by a qualified reader on a short fragment of
  generated poetry 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02771v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02771v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Boris Orekhov
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Style is an important concept in today's challenges in natural language
generating. After the success in the field of image style transfer, the task of
text style transfer became actual and attractive. Researchers are also
interested in the tasks of style reproducing in generation of the poetic text.
Evaluation of style reproducing in natural poetry generation remains a problem.
I used 3 character-based LSTM-models to work with style reproducing assessment.
All three models were trained on the corpus of texts by famous Russian-speaking
poets. Samples were shown to the assessors and 4 answer options were offered,
the style of which poet this sample reproduces. In addition, the assessors were
asked how well they were familiar with the work of the poet they had named.
Students studying history of literature were the assessors, 94 answers were
received. It has appeared that accuracy of definition of style increases if the
assessor can quote the poet by heart. Each model showed at least 0.7
macro-average accuracy. The experiment showed that it is better to involve a
professional rather than a naive reader in the evaluation of style in the tasks
of poetry generation, while lstm models are good at reproducing the style of
Russian poets even on a limited training corpus.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>6 pages, 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Networked Communication for Decentralised Agents in Mean-Field Games 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02766v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02766v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Patrick Benjamin, Alessandro Abate
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce networked communication to the mean-field game framework. In
particular, we look at oracle-free settings where $N$ decentralised agents
learn along a single, non-episodic evolution path of the empirical system, such
as we may encounter for a large range of many-agent cooperation problems in the
real-world. We provide theoretical evidence that by spreading improved policies
through the network in a decentralised fashion, our sample guarantees are
upper-bounded by those of the purely independent-learning case. Moreover, we
show empirically that our networked method can give faster convergence in
practice, while removing the reliance on a centralised controller. We also
demonstrate that our decentralised communication architecture brings
significant benefits over both the centralised and independent alternatives in
terms of robustness and flexibility to unexpected learning failures and changes
in population size. For comparison purposes with our new architecture, we
modify recent algorithms for the centralised and independent cases to make
their practical convergence feasible: while contributing the first empirical
demonstrations of these algorithms in our setting of $N$ agents learning along
a single system evolution with only local state observability, we additionally
display the empirical benefits of our new, networked approach.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Tackling Non-Stationarity in Reinforcement Learning via Causal-Origin
  Representation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02747v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02747v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wanpeng Zhang, Yilin Li, Boyu Yang, Zongqing Lu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In real-world scenarios, the application of reinforcement learning is
significantly challenged by complex non-stationarity. Most existing methods
attempt to model the changes of the environment explicitly, often requiring
impractical prior knowledge. In this paper, we propose a new perspective,
positing that non-stationarity can propagate and accumulate through complex
causal relationships during state transitions, thereby compounding its
sophistication and affecting policy learning. We believe that this challenge
can be more effectively addressed by tracing the causal origin of
non-stationarity. To this end, we introduce the Causal-Origin REPresentation
(COREP) algorithm. COREP primarily employs a guided updating mechanism to learn
a stable graph representation for states termed as causal-origin
representation. By leveraging this representation, the learned policy exhibits
impressive resilience to non-stationarity. We supplement our approach with a
theoretical analysis grounded in the causal interpretation for non-stationary
reinforcement learning, advocating for the validity of the causal-origin
representation. Experimental results further demonstrate the superior
performance of COREP over existing methods in tackling non-stationarity.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Towards Better Explanations for Object Detection 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02744v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02744v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Van Binh Truong, Truong Thanh Hung Nguyen, Vo Thanh Khang Nguyen, Quoc Khanh Nguyen, Quoc Hung Cao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent advances in Artificial Intelligence (AI) technology have promoted
their use in almost every field. The growing complexity of deep neural networks
(DNNs) makes it increasingly difficult and important to explain the inner
workings and decisions of the network. However, most current techniques for
explaining DNNs focus mainly on interpreting classification tasks. This paper
proposes a method to explain the decision for any object detection model called
D-CLOSE. To closely track the model's behavior, we used multiple levels of
segmentation on the image and a process to combine them. We performed tests on
the MS-COCO dataset with the YOLOX model, which shows that our method
outperforms D-RISE and can give a better quality and less noise explanation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Large-Scale Study of Probabilistic Calibration in Neural Network
  Regression 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02738v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02738v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Victor Dheur, Souhaib Ben Taieb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Accurate probabilistic predictions are essential for optimal decision making.
While neural network miscalibration has been studied primarily in
classification, we investigate this in the less-explored domain of regression.
We conduct the largest empirical study to date to assess the probabilistic
calibration of neural networks. We also analyze the performance of
recalibration, conformal, and regularization methods to enhance probabilistic
calibration. Additionally, we introduce novel differentiable recalibration and
regularization methods, uncovering new insights into their effectiveness. Our
findings reveal that regularization methods offer a favorable tradeoff between
calibration and sharpness. Post-hoc methods exhibit superior probabilistic
calibration, which we attribute to the finite-sample coverage guarantee of
conformal prediction. Furthermore, we demonstrate that quantile recalibration
can be considered as a specific case of conformal prediction. Our study is
fully reproducible and implemented in a common code base for fair comparisons.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Realising Synthetic Active Inference Agents, Part II: Variational
  Message Updates 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02733v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02733v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thijs van de Laar, Magnus Koudahl, Bert de Vries
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The Free Energy Principle (FEP) describes (biological) agents as minimising a
variational Free Energy (FE) with respect to a generative model of their
environment. Active Inference (AIF) is a corollary of the FEP that describes
how agents explore and exploit their environment by minimising an expected FE
objective. In two related papers, we describe a scalable, epistemic approach to
synthetic AIF agents, by message passing on free-form Forney-style Factor
Graphs (FFGs). A companion paper (part I) introduces a Constrained FFG (CFFG)
notation that visually represents (generalised) FE objectives for AIF. The
current paper (part II) derives message passing algorithms that minimise
(generalised) FE objectives on a CFFG by variational calculus. A comparison
between simulated Bethe and generalised FE agents illustrates how synthetic AIF
induces epistemic behaviour on a T-maze navigation task. With a full message
passing account of synthetic AIF agents, it becomes possible to derive and
reuse message updates across models and move closer to industrial applications
of synthetic AIF.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Conformal Prediction with Missing Values 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02732v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02732v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Margaux Zaffran, Aymeric Dieuleveut, Julie Josse, Yaniv Romano
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Conformal prediction is a theoretically grounded framework for constructing
predictive intervals. We study conformal prediction with missing values in the
covariates -- a setting that brings new challenges to uncertainty
quantification. We first show that the marginal coverage guarantee of conformal
prediction holds on imputed data for any missingness distribution and almost
all imputation functions. However, we emphasize that the average coverage
varies depending on the pattern of missing values: conformal methods tend to
construct prediction intervals that under-cover the response conditionally to
some missing patterns. This motivates our novel generalized conformalized
quantile regression framework, missing data augmentation, which yields
prediction intervals that are valid conditionally to the patterns of missing
values, despite their exponential number. We then show that a universally
consistent quantile regression algorithm trained on the imputed data is Bayes
optimal for the pinball risk, thus achieving valid coverage conditionally to
any given data point. Moreover, we examine the case of a linear model, which
demonstrates the importance of our proposal in overcoming the
heteroskedasticity induced by missing values. Using synthetic and data from
critical care, we corroborate our theory and report improved performance of our
methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Code for our experiments can be found at
  https://github.com/mzaffran/ConformalPredictionMissingValues . To be
  published in the proceedings of the 40th International Conference on Machine
  Learning, Honolulu, Hawaii, USA</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Enhanced Distribution Modelling via Augmented Architectures For Neural
  ODE Flows 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02731v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02731v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Etrit Haxholli, Marco Lorenzi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While the neural ODE formulation of normalizing flows such as in FFJORD
enables us to calculate the determinants of free form Jacobians in O(D) time,
the flexibility of the transformation underlying neural ODEs has been shown to
be suboptimal. In this paper, we present AFFJORD, a neural ODE-based
normalizing flow which enhances the representation power of FFJORD by defining
the neural ODE through special augmented transformation dynamics which preserve
the topology of the space. Furthermore, we derive the Jacobian determinant of
the general augmented form by generalizing the chain rule in the continuous
sense into the Cable Rule, which expresses the forward sensitivity of ODEs with
respect to their initial conditions. The cable rule gives an explicit
expression for the Jacobian of a neural ODE transformation, and provides an
elegant proof of the instantaneous change of variable. Our experimental results
on density estimation in synthetic and high dimensional data, such as MNIST,
CIFAR-10 and CelebA 32x32, show that AFFJORD outperforms the baseline FFJORD
through the improved flexibility of the underlying vector field.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Gibbs Sampling the Posterior of Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02729v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02729v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Giovanni Piccioli, Emanuele Troiani, Lenka Zdeborová
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study sampling from a posterior derived from a neural
network. We propose a new probabilistic model consisting of adding noise at
every pre- and post-activation in the network, arguing that the resulting
posterior can be sampled using an efficient Gibbs sampler. The Gibbs sampler
attains similar performances as the state-of-the-art Monte Carlo Markov chain
methods, such as the Hamiltonian Monte Carlo or the Metropolis adjusted
Langevin algorithm, both on real and synthetic data. By framing our analysis in
the teacher-student setting, we introduce a thermalization criterion that
allows us to detect when an algorithm, when run on data with synthetic labels,
fails to sample from the posterior. The criterion is based on the fact that in
the teacher-student setting we can initialize an algorithm directly at
equilibrium.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Multiple output samples for each input in a single-output Gaussian
  process 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02719v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02719v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jeremy H. M. Wong, Huayun Zhang, Nancy F. Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The standard Gaussian Process (GP) only considers a single output sample per
input in the training set. Datasets for subjective tasks, such as spoken
language assessment, may be annotated with output labels from multiple human
raters per input. This paper proposes to generalise the GP to allow for these
multiple output samples in the training set, and thus make use of available
output uncertainty information. This differs from a multi-output GP, as all
output samples are from the same task here. The output density function is
formulated to be the joint likelihood of observing all output samples, and
latent variables are not repeated to reduce computation cost. The test set
predictions are inferred similarly to a standard GP, with a difference being in
the optimised hyper-parameters. This is evaluated on speechocean762, showing
that it allows the GP to compute a test set output distribution that is more
similar to the collection of reference outputs from the multiple human raters.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Federated Intrusion Detection System based on Deep Belief Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02715v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02715v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Othmane Belarbi, Theodoros Spyridopoulos, Eirini Anthi, Ioannis Mavromatis, Pietro Carnelli, Aftab Khan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The vast increase of IoT technologies and the ever-evolving attack vectors
and threat actors have increased cyber-security risks dramatically. Novel
attacks can compromise IoT devices to gain access to sensitive data or control
them to deploy further malicious activities. The detection of novel attacks
often relies upon AI solutions. A common approach to implementing AI-based IDS
in distributed IoT systems is in a centralised manner. However, this approach
may violate data privacy and secrecy. In addition, centralised data collection
prohibits the scale-up of IDSs. Therefore, intrusion detection solutions in IoT
ecosystems need to move towards a decentralised direction. FL has attracted
significant interest in recent years due to its ability to perform
collaborative learning while preserving data confidentiality and locality.
Nevertheless, most FL-based IDS for IoT systems are designed under unrealistic
data distribution conditions. To that end, we design an experiment
representative of the real world and evaluate the performance of two FL IDS
implementations, one based on DNNs and another on our previous work on DBNs.
For our experiments, we rely on TON-IoT, a realistic IoT network traffic
dataset, associating each IP address with a single FL client. Additionally, we
explore pre-training and investigate various aggregation methods to mitigate
the impact of data heterogeneity. Lastly, we benchmark our approach against a
centralised solution. The comparison shows that the heterogeneous nature of the
data has a considerable negative impact on the model performance when trained
in a distributed manner. However, in the case of a pre-trained initial global
FL model, we demonstrate a performance improvement of over 20% (F1-score) when
compared against a randomly initiated global model.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 5 figues, 3 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Comparative Study on Semi-supervised Learning Applied for Anomaly
  Detection in Hydraulic Condition Monitoring System 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02709v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02709v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yongqi Dong, Kejia Chen, Zhiyuan Ma
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Condition-based maintenance is becoming increasingly important in hydraulic
systems. However, anomaly detection for these systems remains challenging,
especially since that anomalous data is scarce and labeling such data is
tedious and even dangerous. Therefore, it is advisable to make use of
unsupervised or semi-supervised methods, especially for semi-supervised
learning which utilizes unsupervised learning as a feature extraction mechanism
to aid the supervised part when only a small number of labels are available.
This study systematically compares semi-supervised learning methods applied for
anomaly detection in hydraulic condition monitoring systems. Firstly, thorough
data analysis and feature learning were carried out to understand the
open-sourced hydraulic condition monitoring dataset. Then, various methods were
implemented and evaluated including traditional stand-alone semi-supervised
learning models (e.g., one-class SVM, Robust Covariance), ensemble models
(e.g., Isolation Forest), and deep neural network based models (e.g.,
autoencoder, Hierarchical Extreme Learning Machine (HELM)). Typically, this
study customized and implemented an extreme learning machine based
semi-supervised HELM model and verified its superiority over other
semi-supervised methods. Extensive experiments show that the customized HELM
model obtained state-of-the-art performance with the highest accuracy (99.5%),
the lowest false positive rate (0.015), and the best F1-score (0.985) beating
other semi-supervised methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>7 pages, 8 figures, accepted by 2023 IEEE International Conference on
  Systems, Man, and Cybernetics (SMC 2023) https://ieeesmc2023.org/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Orca: Progressive Learning from Complex Explanation Traces of <span class="highlight-title">GPT</span>-4 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02707v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02707v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Subhabrata Mukherjee, Arindam Mitra, Ganesh Jawahar, Sahaj Agarwal, Hamid Palangi, Ahmed Awadallah
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent research has focused on enhancing the capability of smaller models
through imitation learning, drawing on the outputs generated by large
foundation models (LFMs). A number of issues impact the quality of these
models, ranging from limited imitation signals from shallow LFM outputs; small
scale homogeneous training data; and most notably a lack of rigorous evaluation
resulting in overestimating the small model's capability as they tend to learn
to imitate the style, but not the reasoning process of LFMs. To address these
challenges, we develop Orca (We are working with our legal team to publicly
release a diff of the model weights in accordance with LLaMA's release policy
to be published at https://aka.ms/orca-lm), a 13-billion parameter model that
learns to imitate the reasoning process of LFMs. Orca learns from rich signals
from GPT-4 including explanation traces; step-by-step thought processes; and
other complex instructions, guided by teacher assistance from ChatGPT. To
promote this progressive learning, we tap into large-scale and diverse
imitation data with judicious sampling and selection. Orca surpasses
conventional state-of-the-art instruction-tuned models such as Vicuna-13B by
more than 100% in complex zero-shot reasoning benchmarks like Big-Bench Hard
(BBH) and 42% on AGIEval. Moreover, Orca reaches parity with ChatGPT on the BBH
benchmark and shows competitive performance (4 pts gap with optimized system
message) in professional and academic examinations like the SAT, LSAT, GRE, and
GMAT, both in zero-shot settings without CoT; while trailing behind GPT-4. Our
research indicates that learning from step-by-step explanations, whether these
are generated by humans or more advanced AI models, is a promising direction to
improve model capabilities and skills.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Unlocking the Potential of Federated Learning for Deeper Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02701v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02701v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Haolin Wang, Xuefeng Liu, Jianwei Niu, Shaojie Tang, Jiaxing Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning (FL) is a new paradigm for distributed machine learning
that allows a global model to be trained across multiple clients without
compromising their privacy. Although FL has demonstrated remarkable success in
various scenarios, recent studies mainly utilize shallow and small neural
networks. In our research, we discover a significant performance decline when
applying the existing FL framework to deeper neural networks, even when client
data are independently and identically distributed (i.i.d.). Our further
investigation shows that the decline is due to the continuous accumulation of
dissimilarities among client models during the layer-by-layer back-propagation
process, which we refer to as "divergence accumulation." As deeper models
involve a longer chain of divergence accumulation, they tend to manifest
greater divergence, subsequently leading to performance decline. Both
theoretical derivations and empirical evidence are proposed to support the
existence of divergence accumulation and its amplified effects in deeper
models. To address this issue, we propose several technical guidelines based on
reducing divergence, such as using wider models and reducing the receptive
field. These approaches can greatly improve the accuracy of FL on deeper
models. For example, the application of these guidelines can boost the
ResNet101 model's performance by as much as 43\% on the Tiny-ImageNet dataset.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>16 pages, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Solving NP-hard Min-max Routing Problems as Sequential Generation with
  Equity Context 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02689v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02689v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwoo Son, Minsu Kim, Sanghyeok Choi, Jinkyoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Min-max routing problems aim to minimize the maximum tour length among agents
as they collaboratively visit all cities, i.e., the completion time. These
problems include impactful real-world applications but are known as NP-hard.
Existing methods are facing challenges, particularly in large-scale problems
that require the coordination of numerous agents to cover thousands of cities.
This paper proposes a new deep-learning framework to solve large-scale min-max
routing problems. We model the simultaneous decision-making of multiple agents
as a sequential generation process, allowing the utilization of scalable
deep-learning models for sequential decision-making. In the sequentially
approximated problem, we propose a scalable contextual Transformer model,
Equity-Transformer, which generates sequential actions considering an equitable
workload among other agents. The effectiveness of Equity-Transformer is
demonstrated through its superior performance in two representative min-max
routing tasks: the min-max multiple traveling salesman problem (min-max mTSP)
and the min-max multiple pick-up and delivery problem (min-max mPDP). Notably,
our method achieves significant reductions of runtime, approximately 335 times,
and cost values of about 53% compared to a competitive heuristic (LKH3) in the
case of 100 vehicles with 1,000 cities of mTSP. We provide reproducible source
code: https://github.com/kaist-silab/equity-transformer
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 7 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Meta-SAGE: Scale Meta-Learning Scheduled Adaptation with Guided
  Exploration for Mitigating Scale Shift on Combinatorial Optimization <span class="chip">ICML</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02688v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02688v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jiwoo Son, Minsu Kim, Hyeonah Kim, Jinkyoo Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes Meta-SAGE, a novel approach for improving the scalability
of deep reinforcement learning models for combinatorial optimization (CO)
tasks. Our method adapts pre-trained models to larger-scale problems in test
time by suggesting two components: a scale meta-learner (SML) and scheduled
adaptation with guided exploration (SAGE). First, SML transforms the context
embedding for subsequent adaptation of SAGE based on scale information. Then,
SAGE adjusts the model parameters dedicated to the context embedding for a
specific instance. SAGE introduces locality bias, which encourages selecting
nearby locations to determine the next location. The locality bias gradually
decays as the model is adapted to the target instance. Results show that
Meta-SAGE outperforms previous adaptation methods and significantly improves
scalability in representative CO tasks. Our source code is available at
https://github.com/kaist-silab/meta-sage
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>18 pages, 9 figures, International Conference on Machine Learning
  (ICML) 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Predicting malaria dynamics in Burundi using deep Learning Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02685v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02685v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daxelle Sakubu, Kelly Joelle Gatore Sinigirira, David Niyukuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Malaria continues to be a major public health problem on the African
continent, particularly in Sub-Saharan Africa. Nonetheless, efforts are
ongoing, and significant progress has been made. In Burundi, malaria is among
the main public health concerns. In the literature, there are limited
prediction models for Burundi. We know that such tools are much needed for
interventions design. In our study, we built machine-learning based models to
estimates malaria cases in Burundi. The forecast was carried out at province
level, allowing us to estimate malaria cases on a national scale as well. Long
short term memory (LSTM) model, a type of deep learning model has been used to
achieve best results using climate-change related factors such as temperature,
rainfal, and relative humidity, together with malaria historical data and human
population. The results showed that at country level different tuning of
parameters can be used in order to determine the minimum and maximum expected
malaria
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ STEVE-1: A Generative Model for Text-to-Behavior in Minecraft 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.00937v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.00937v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, Sheila McIlraith
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Constructing AI models that respond to text instructions is challenging,
especially for sequential decision-making tasks. This work introduces an
instruction-tuned Video Pretraining (VPT) model for Minecraft called STEVE-1,
demonstrating that the unCLIP approach, utilized in DALL-E 2, is also effective
for creating instruction-following sequential decision-making agents. STEVE-1
is trained in two steps: adapting the pretrained VPT model to follow commands
in MineCLIP's latent space, then training a prior to predict latent codes from
text. This allows us to finetune VPT through self-supervised behavioral cloning
and hindsight relabeling, bypassing the need for costly human text annotations.
By leveraging pretrained models like VPT and MineCLIP and employing best
practices from text-conditioned image generation, STEVE-1 costs just $60 to
train and can follow a wide range of short-horizon open-ended text and visual
instructions in Minecraft. STEVE-1 sets a new bar for open-ended instruction
following in Minecraft with low-level controls (mouse and keyboard) and raw
pixel inputs, far outperforming previous baselines. We provide experimental
evidence highlighting key factors for downstream performance, including
pretraining, classifier-free guidance, and data scaling. All resources,
including our model weights, training scripts, and evaluation tools are made
available for further research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ FilFL: Client Filtering for Optimized Client Participation in Federated
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.06599v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.06599v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fares Fourati, Salma Kharrat, Vaneet Aggarwal, Mohamed-Slim Alouini, Marco Canini
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Federated learning is an emerging machine learning paradigm that enables
clients to train collaboratively without exchanging local data. The clients
participating in the training process have a crucial impact on the convergence
rate, learning efficiency, and model generalization. In this work, we propose
FilFL, a new approach to optimizing client participation and training by
introducing client filtering. FilFL periodically filters the available clients
to identify a subset that maximizes a combinatorial objective function using an
efficient greedy filtering algorithm. From this filtered-in subset, clients are
then selected for the training process. We provide a thorough analysis of FilFL
convergence in a heterogeneous setting and evaluate its performance across
diverse vision and language tasks and realistic federated scenarios with
time-varying client availability. Our empirical results demonstrate several
benefits of our approach, including improved learning efficiency, faster
convergence, and up to 10 percentage points higher test accuracy compared to
scenarios where client filtering is not utilized.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ In-context Example Selection with Influences 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11042v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11042v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tai Nguyen, Eric Wong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In-context learning (ICL) is a powerful paradigm emerged from large language
models (LLMs). Despite its promises, ICL performance is known to be highly
sensitive to input examples. In this work, we use $\textit{in-context
influences}$ to analyze few-shot ICL performance directly from the in-context
examples. Our proposed influence-based example selection method can identify
both positive and negative examples, outperforming several baselines when
evaluated on 9 SuperGLUE tasks. Our analysis uncovers up to a $16.3\%$
performance gap between using the most negative in-context examples compared to
the most positive. In a case study, we apply our influence-based framework to
quantify the phenomena of recency bias in example ordering for few-shot ICL.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ CLIP-Dissect: Automatic Description of Neuron Representations in Deep
  Vision Networks <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.10965v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.10965v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuomas Oikarinen, Tsui-Wei Weng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we propose CLIP-Dissect, a new technique to automatically
describe the function of individual hidden neurons inside vision networks.
CLIP-Dissect leverages recent advances in multimodal vision/language models to
label internal neurons with open-ended concepts without the need for any
labeled data or human examples. We show that CLIP-Dissect provides more
accurate descriptions than existing methods for last layer neurons where the
ground-truth is available as well as qualitatively good descriptions for hidden
layer neurons. In addition, our method is very flexible: it is model agnostic,
can easily handle new concepts and can be extended to take advantage of better
multimodal models in the future. Finally CLIP-Dissect is computationally
efficient and can label all neurons from five layers of ResNet-50 in just 4
minutes, which is more than 10 times faster than existing methods. Our code is
available at https://github.com/Trustworthy-ML-Lab/CLIP-dissect. Finally,
crowdsourced user study results are available at Appendix B to further support
the effectiveness of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in ICLR 2023 Conference (Spotlight). New v5(5 June 2023) -
  Added crowdsourced user study in Appendix B, not included in ICLR publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Centralized Training with Hybrid Execution in Multi-Agent Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06274v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06274v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Pedro P. Santos, Diogo S. Carvalho, Miguel Vasco, Alberto Sardinha, Pedro A. Santos, Ana Paiva, Francisco S. Melo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce hybrid execution in multi-agent reinforcement learning (MARL), a
new paradigm in which agents aim to successfully complete cooperative tasks
with arbitrary communication levels at execution time by taking advantage of
information-sharing among the agents. Under hybrid execution, the communication
level can range from a setting in which no communication is allowed between
agents (fully decentralized), to a setting featuring full communication (fully
centralized), but the agents do not know beforehand which communication level
they will encounter at execution time. To formalize our setting, we define a
new class of multi-agent partially observable Markov decision processes
(POMDPs) that we name hybrid-POMDPs, which explicitly model a communication
process between the agents. We contribute MARO, an approach that makes use of
an auto-regressive predictive model, trained in a centralized manner, to
estimate missing agents' observations at execution time. We evaluate MARO on
standard scenarios and extensions of previous benchmarks tailored to emphasize
the negative impact of partial observability in MARL. Experimental results show
that our method consistently outperforms relevant baselines, allowing agents to
act with faulty communication while successfully exploiting shared information.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Label-Free Concept Bottleneck Models <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.06129v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.06129v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tuomas Oikarinen, Subhro Das, Lam M. Nguyen, Tsui-Wei Weng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Concept bottleneck models (CBM) are a popular way of creating more
interpretable neural networks by having hidden layer neurons correspond to
human-understandable concepts. However, existing CBMs and their variants have
two crucial limitations: first, they need to collect labeled data for each of
the predefined concepts, which is time consuming and labor intensive; second,
the accuracy of a CBM is often significantly lower than that of a standard
neural network, especially on more complex datasets. This poor performance
creates a barrier for adopting CBMs in practical real world applications.
Motivated by these challenges, we propose Label-free CBM which is a novel
framework to transform any neural network into an interpretable CBM without
labeled concept data, while retaining a high accuracy. Our Label-free CBM has
many advantages, it is: scalable - we present the first CBM scaled to ImageNet,
efficient - creating a CBM takes only a few hours even for very large datasets,
and automated - training it for a new dataset requires minimal human effort.
Our code is available at https://github.com/Trustworthy-ML-Lab/Label-free-CBM.
Finally, in Appendix B we conduct a large scale user evaluation of the
interpretability of our method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at ICLR 2023. New v2(5 June 2023): added crowdsourced human
  study in Appendix B</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ PFNs4BO: In-Context Learning for Bayesian Optimization <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17535v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17535v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Samuel Müller, Matthias Feurer, Noah Hollmann, Frank Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we use Prior-data Fitted Networks (PFNs) as a flexible
surrogate for Bayesian Optimization (BO). PFNs are neural processes that are
trained to approximate the posterior predictive distribution (PPD) through
in-context learning on any prior distribution that can be efficiently sampled
from. We describe how this flexibility can be exploited for surrogate modeling
in BO. We use PFNs to mimic a naive Gaussian process (GP), an advanced GP, and
a Bayesian Neural Network (BNN). In addition, we show how to incorporate
further information into the prior, such as allowing hints about the position
of optima (user priors), ignoring irrelevant dimensions, and performing
non-myopic BO by learning the acquisition function. The flexibility underlying
these extensions opens up vast possibilities for using PFNs for BO. We
demonstrate the usefulness of PFNs for BO in a large-scale evaluation on
artificial GP samples and three different hyperparameter optimization testbeds:
HPO-B, Bayesmark, and PD1. We publish code alongside trained models at
https://github.com/automl/PFNs4BO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Cooperative Open-ended Learning Framework for Zero-shot Coordination 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.04831v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.04831v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yang Li, Shao Zhang, Jichen Sun, Yali Du, Ying Wen, Xinbing Wang, Wei Pan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Zero-shot coordination in cooperative artificial intelligence (AI) remains a
significant challenge, which means effectively coordinating with a wide range
of unseen partners. Previous algorithms have attempted to address this
challenge by optimizing fixed objectives within a population to improve
strategy or behaviour diversity. However, these approaches can result in a loss
of learning and an inability to cooperate with certain strategies within the
population, known as cooperative incompatibility. To address this issue, we
propose the Cooperative Open-ended LEarning (COLE) framework, which constructs
open-ended objectives in cooperative games with two players from the
perspective of graph theory to assess and identify the cooperative ability of
each strategy. We further specify the framework and propose a practical
algorithm that leverages knowledge from game theory and graph theory.
Furthermore, an analysis of the learning process of the algorithm shows that it
can efficiently overcome cooperative incompatibility. The experimental results
in the Overcooked game environment demonstrate that our method outperforms
current state-of-the-art methods when coordinating with different-level
partners. Our demo is available at https://sites.google.com/view/cole-2023.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>15 pages with 9 pages main body</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Transformer</span> and Snowball Graph Convolution Learning for Brain functional
  network Classification 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.16132v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.16132v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jinlong Hu, Yangmin Huang, Shoubin Dong
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Advanced deep learning methods, especially graph neural networks (GNNs), are
increasingly expected to learn from brain functional network data and identify
the functional connections between brain disorder and health. In this paper, we
proposed a novel Transformer and snowball encoding networks (TSEN) for brain
functional network classification, which introduced Transformer architecture
with graph snowball connection into GNNs for learning whole-graph
representation. TSEN combined graph snowball connection with graph Transformer
by snowball encoding layers, which enhanced the power to capture multi-scale
information and global patterns of brain functional networks. TSEN also
introduced snowball graph convolution as position embedding in Transformer
structure, which was a simple yet effective method for capturing local patterns
naturally. We evaluated the proposed model by two large-scale brain functional
network datasets, and the results demonstrated that TSEN outperformed the
state-of-the-art GNN models and the graph-transformer based GNN models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Prepared for submitting to HBP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Ambiguous Dynamic Treatment Regimes: A Reinforcement Learning Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2112.04571v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2112.04571v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Soroush Saghafian
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  A main research goal in various studies is to use an observational data set
and provide a new set of counterfactual guidelines that can yield causal
improvements. Dynamic Treatment Regimes (DTRs) are widely studied to formalize
this process. However, available methods in finding optimal DTRs often rely on
assumptions that are violated in real-world applications (e.g., medical
decision-making or public policy), especially when (a) the existence of
unobserved confounders cannot be ignored, and (b) the unobserved confounders
are time-varying (e.g., affected by previous actions). When such assumptions
are violated, one often faces ambiguity regarding the underlying causal model.
This ambiguity is inevitable, since the dynamics of unobserved confounders and
their causal impact on the observed part of the data cannot be understood from
the observed data. Motivated by a case study of finding superior treatment
regimes for patients who underwent transplantation in our partner hospital and
faced a medical condition known as New Onset Diabetes After Transplantation
(NODAT), we extend DTRs to a new class termed Ambiguous Dynamic Treatment
Regimes (ADTRs), in which the causal impact of treatment regimes is evaluated
based on a "cloud" of causal models. We then connect ADTRs to Ambiguous
Partially Observable Mark Decision Processes (APOMDPs) and develop
Reinforcement Learning methods, which enable using the observed data to
efficiently learn an optimal treatment regime. We establish theoretical results
for these learning methods, including (weak) consistency and asymptotic
normality. We further evaluate the performance of these learning methods both
in our case study and in simulation experiments.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Action Noise in Off-Policy Deep Reinforcement Learning: Impact on
  Exploration and Performance 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.03787v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.03787v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jakob Hollenstein, Sayantan Auddy, Matteo Saveriano, Erwan Renaudo, Justus Piater
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Many Deep Reinforcement Learning (D-RL) algorithms rely on simple forms of
exploration such as the additive action noise often used in continuous control
domains. Typically, the scaling factor of this action noise is chosen as a
hyper-parameter and is kept constant during training. In this paper, we focus
on action noise in off-policy deep reinforcement learning for continuous
control. We analyze how the learned policy is impacted by the noise type, noise
scale, and impact scaling factor reduction schedule. We consider the two most
prominent types of action noise, Gaussian and Ornstein-Uhlenbeck noise, and
perform a vast experimental campaign by systematically varying the noise type
and scale parameter, and by measuring variables of interest like the expected
return of the policy and the state-space coverage during exploration. For the
latter, we propose a novel state-space coverage measure
$\operatorname{X}_{\mathcal{U}\text{rel}}$ that is more robust to estimation
artifacts caused by points close to the state-space boundary than
previously-proposed measures. Larger noise scales generally increase
state-space coverage. However, we found that increasing the space coverage
using a larger noise scale is often not beneficial. On the contrary, reducing
the noise scale over the training process reduces the variance and generally
improves the learning performance. We conclude that the best noise type and
scale are environment dependent, and based on our observations derive heuristic
rules for guiding the choice of the action noise as a starting point for
further optimization.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published in Transactions on Machine Learning Research (11/2022)
  https://openreview.net/forum?id=NljBlZ6hmG</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Systematic Study and Comprehensive Evaluation of Chat<span class="highlight-title">GPT</span> on Benchmark
  <span class="highlight-title">Dataset</span>s <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18486v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18486v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Md Tahmid Rahman Laskar, M Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, Jimmy Xiangji Huang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The development of large language models (LLMs) such as ChatGPT has brought a
lot of attention recently. However, their evaluation in the benchmark academic
datasets remains under-explored due to the difficulty of evaluating the
generative outputs produced by this model against the ground truth. In this
paper, we aim to present a thorough evaluation of ChatGPT's performance on
diverse academic datasets, covering tasks like question-answering, text
summarization, code generation, commonsense reasoning, mathematical
problem-solving, machine translation, bias detection, and ethical
considerations. Specifically, we evaluate ChatGPT across 140 tasks and analyze
255K responses it generates in these datasets. This makes our work the largest
evaluation of ChatGPT in NLP benchmarks. In short, our study aims to validate
the strengths and weaknesses of ChatGPT in various tasks and provide insights
for future research using LLMs. We also report a new emergent ability to follow
multi-query instructions that we mostly found in ChatGPT and other
instruction-tuned models. Our extensive evaluation shows that even though
ChatGPT is capable of performing a wide variety of tasks, and may obtain
impressive performance in several benchmark datasets, it is still far from
achieving the ability to reliably solve many challenging tasks. By providing a
thorough assessment of ChatGPT's performance across diverse NLP tasks, this
paper sets the stage for a targeted deployment of ChatGPT-like LLMs in
real-world applications.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2023 Findings. The first three authors contributed
  equally</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Reduce, Reuse, Recycle: Compositional Generation with Energy-Based
  Diffusion Models and MCMC <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.11552v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.11552v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilun Du, Conor Durkan, Robin Strudel, Joshua B. Tenenbaum, Sander Dieleman, Rob Fergus, Jascha Sohl-Dickstein, Arnaud Doucet, Will Grathwohl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Since their introduction, diffusion models have quickly become the prevailing
approach to generative modeling in many domains. They can be interpreted as
learning the gradients of a time-varying sequence of log-probability density
functions. This interpretation has motivated classifier-based and
classifier-free guidance as methods for post-hoc control of diffusion models.
In this work, we build upon these ideas using the score-based interpretation of
diffusion models, and explore alternative ways to condition, modify, and reuse
diffusion models for tasks involving compositional generation and guidance. In
particular, we investigate why certain types of composition fail using current
techniques and present a number of solutions. We conclude that the sampler (not
the model) is responsible for this failure and propose new samplers, inspired
by MCMC, which enable successful compositional generation. Further, we propose
an energy-based parameterization of diffusion models which enables the use of
new compositional operators and more sophisticated, Metropolis-corrected
samplers. Intriguingly we find these samplers lead to notable improvements in
compositional generation across a wide set of problems such as
classifier-guided ImageNet modeling and compositional text-to-image generation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023, Project Webpage:
  https://energy-based-model.github.io/reduce-reuse-recycle/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ XAudit : A Theoretical Look at Auditing with Explanations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.04740v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.04740v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chhavi Yadav, Michal Moshkovitz, Kamalika Chaudhuri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Responsible use of machine learning requires models to be audited for
undesirable properties. While a body of work has proposed using explanations
for auditing, how to do so and why has remained relatively ill-understood. This
work formalizes the role of explanations in auditing and investigates if and
how model explanations can help audits. Specifically, we propose
explanation-based algorithms for auditing linear classifiers and decision trees
for feature sensitivity. Our results illustrate that Counterfactual
explanations are extremely helpful for auditing. While Anchors and decision
paths may not be as beneficial in the worst-case, in the average-case they do
aid a lot.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust incremental learning pipelines for temporal tabular <span class="highlight-title">dataset</span>s with
  distribution shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.07925v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.07925v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Thomas Wong, Mauricio Barahona
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we present a robust incremental learning model for regression
tasks on temporal tabular datasets. Using commonly available tabular and
time-series prediction models as building blocks, a machine-learning model is
built incrementally to adapt to distributional shifts in data. Using the
concept of self-similarity, the model uses only two basic building blocks of
machine learning models, gradient boosting decision trees and neural networks
to build models for any required complexity. The model is efficient as no
specialised neural architectures are used and each model building block can be
independently trained in parallel. The model is demonstrated to have robust
performances under adverse situations such as regime changes, fat-tailed
distributions and low signal-to-noise ratios. Model robustness are studied
under different hyper-parameters and complexities.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ \textit{spred}: Solving $L_1$ Penalty with SGD <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.01212v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.01212v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Liu Ziyin, Zihao Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose to minimize a generic differentiable objective with $L_1$
constraint using a simple reparametrization and straightforward stochastic
gradient descent. Our proposal is the direct generalization of previous ideas
that the $L_1$ penalty may be equivalent to a differentiable reparametrization
with weight decay. We prove that the proposed method, \textit{spred}, is an
exact differentiable solver of $L_1$ and that the reparametrization trick is
completely ``benign" for a generic nonconvex function. Practically, we
demonstrate the usefulness of the method in (1) training sparse neural networks
to perform gene selection tasks, which involves finding relevant features in a
very high dimensional space, and (2) neural network compression task, to which
previous attempts at applying the $L_1$-penalty have been unsuccessful.
Conceptually, our result bridges the gap between the sparsity in deep learning
and conventional statistical learning.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023, 16 pages, 10 figures, and 2 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MiDi: Mixed Graph and 3D Denoising Diffusion for Molecule Generation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.09048v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.09048v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Clement Vignac, Nagham Osman, Laura Toni, Pascal Frossard
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work introduces MiDi, a novel diffusion model for jointly generating
molecular graphs and their corresponding 3D arrangement of atoms. Unlike
existing methods that rely on predefined rules to determine molecular bonds
based on the 3D conformation, MiDi offers an end-to-end differentiable approach
that streamlines the molecule generation process. Our experimental results
demonstrate the effectiveness of this approach. On the challenging GEOM-DRUGS
dataset, MiDi generates 92% of stable molecules, against 6% for the previous
EDM model that uses interatomic distances for bond prediction, and 40% using
EDM followed by an algorithm that directly optimize bond orders for validity.
Our code is available at github.com/cvignac/MiDi.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages. Under review</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploring the Connection between Robust and Generative Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.04033v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.04033v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Senad Beadini, Iacopo Masi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We offer a study that connects robust discriminative classifiers trained with
adversarial training (AT) with generative modeling in the form of Energy-based
Models (EBM). We do so by decomposing the loss of a discriminative classifier
and showing that the discriminative model is also aware of the input data
density. Though a common assumption is that adversarial points leave the
manifold of the input data, our study finds out that, surprisingly, untargeted
adversarial points in the input space are very likely under the generative
model hidden inside the discriminative classifier -- have low energy in the
EBM. We present two evidence: untargeted attacks are even more likely than the
natural data and their likelihood increases as the attack strength increases.
This allows us to easily detect them and craft a novel attack called
High-Energy PGD that fools the classifier yet has energy similar to the data
set. The code is available at github.com/senad96/Robust-Generative
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Italian Conference on AI - AI per Cybersecurity, 6 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Adaptive Identification of Populations with Treatment Benefit in
  Clinical Trials: Machine Learning Challenges and Solutions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.05844v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.05844v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alicia Curth, Alihan Hüyük, Mihaela van der Schaar
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the problem of adaptively identifying patient subpopulations that
benefit from a given treatment during a confirmatory clinical trial. This type
of adaptive clinical trial has been thoroughly studied in biostatistics, but
has been allowed only limited adaptivity so far. Here, we aim to relax
classical restrictions on such designs and investigate how to incorporate ideas
from the recent machine learning literature on adaptive and online
experimentation to make trials more flexible and efficient. We find that the
unique characteristics of the subpopulation selection problem -- most
importantly that (i) one is usually interested in finding subpopulations with
any treatment benefit (and not necessarily the single subgroup with largest
effect) given a limited budget and that (ii) effectiveness only has to be
demonstrated across the subpopulation on average -- give rise to interesting
challenges and new desiderata when designing algorithmic solutions. Building on
these findings, we propose AdaGGI and AdaGCPI, two meta-algorithms for
subpopulation construction. We empirically investigate their performance across
a range of simulation scenarios and derive insights into their (dis)advantages
across different settings.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear in the Proceedings of the 40th International Conference on
  Machine Learning, Honolulu, Hawaii, USA. PMLR 202, 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linking generative semi-supervised learning and generative open-set
  recognition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.11702v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.11702v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Emile Reyn Engelbrecht, Johan du Preez
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study investigates the relationship between semi-supervised learning
(SSL) and open-set recognition (OSR) in the context of generative adversarial
networks (GANs). Although no previous study has formally linked SSL and OSR,
their respective methods share striking similarities. Specifically, SSL-GANs
and OSR-GANs require their generators to produce samples in the complementary
space. Subsequently, by regularising networks with generated samples, both SSL
and OSR classifiers generalize the open space. To demonstrate the connection
between SSL and OSR, we theoretically and experimentally compare
state-of-the-art SSL-GAN methods with state-of-the-art OSR-GAN methods. Our
results indicate that the SSL optimised margin-GANs, which have a stronger
foundation in literature, set the new standard for the combined SSL-OSR task
and achieves new state-of-other art results in certain general OSR experiments.
However, the OSR optimised adversarial reciprocal point (ARP)-GANs still
slightly out-performed margin-GANs at other OSR experiments. This result
indicates unique insights for the combined optimisation task of SSL-OSR.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Weakly-supervised Anomaly Detection <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/1910.13601v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/1910.13601v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guansong Pang, Chunhua Shen, Huidong Jin, Anton van den Hengel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent semi-supervised anomaly detection methods that are trained using small
labeled anomaly examples and large unlabeled data (mostly normal data) have
shown largely improved performance over unsupervised methods. However, these
methods often focus on fitting abnormalities illustrated by the given anomaly
examples only (i.e.,, seen anomalies), and consequently they fail to generalize
to those that are not, i.e., new types/classes of anomaly unseen during
training. To detect both seen and unseen anomalies, we introduce a novel deep
weakly-supervised approach, namely Pairwise Relation prediction Network
(PReNet), that learns pairwise relation features and anomaly scores by
predicting the relation of any two randomly sampled training instances, in
which the pairwise relation can be anomaly-anomaly, anomaly-unlabeled, or
unlabeled-unlabeled. Since unlabeled instances are mostly normal, the relation
prediction enforces a joint learning of anomaly-anomaly, anomaly-normal, and
normal-normal pairwise discriminative patterns, respectively. PReNet can then
detect any seen/unseen abnormalities that fit the learned pairwise abnormal
patterns, or deviate from the normal patterns. Further, this pairwise approach
also seamlessly and significantly augments the training anomaly data. Empirical
results on 12 real-world datasets show that PReNet significantly outperforms
nine competing methods in detecting seen and unseen anomalies. We also
theoretically and empirically justify the robustness of our model w.r.t.
anomaly contamination in the unlabeled data. The code is available at
https://github.com/mala-lab/PReNet.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to KDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Topology Optimization via Machine Learning and Deep Learning: A <span class="highlight-title">Review</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.10782v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.10782v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Seungyeon Shin, Dongju Shin, Namwoo Kang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Topology optimization (TO) is a method of deriving an optimal design that
satisfies a given load and boundary conditions within a design domain. This
method enables effective design without initial design, but has been limited in
use due to high computational costs. At the same time, machine learning (ML)
methodology including deep learning has made great progress in the 21st
century, and accordingly, many studies have been conducted to enable effective
and rapid optimization by applying ML to TO. Therefore, this study reviews and
analyzes previous research on ML-based TO (MLTO). Two different perspectives of
MLTO are used to review studies: (1) TO and (2) ML perspectives. The TO
perspective addresses "why" to use ML for TO, while the ML perspective
addresses "how" to apply ML to TO. In addition, the limitations of current MLTO
research and future research directions are examined.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep surrogate accelerated delayed-acceptance HMC for Bayesian inference
  of spatio-temporal heat fluxes in rotating disc systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.02272v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.02272v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Teo Deveney, Eike Mueller, Tony Shardlow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a deep learning accelerated methodology to solve PDE-based
Bayesian inverse problems with guaranteed accuracy. This is motivated by the
ill-posed problem of inferring a spatio-temporal heat-flux parameter known as
the Biot number given temperature data, however the methodology is
generalisable to other settings. To accelerate Bayesian inference, we develop a
novel training scheme that uses data to adaptively train a neural-network
surrogate simulating the parametric forward model. By simultaneously
identifying an approximate posterior distribution over the Biot number, and
weighting a physics-informed training loss according to this, our approach
approximates forward and inverse solution together without any need for
external solves. Using a random Chebyshev series, we outline how to approximate
a Gaussian process prior, and using the surrogate we apply Hamiltonian Monte
Carlo (HMC) to sample from the posterior distribution. We derive convergence of
the surrogate posterior to the true posterior distribution in the Hellinger
metric as our adaptive loss approaches zero. Additionally, we describe how this
surrogate-accelerated HMC approach can be combined with traditional PDE solvers
in a delayed-acceptance scheme to a-priori control the posterior accuracy. This
overcomes a major limitation of deep learning-based surrogate approaches, which
do not achieve guaranteed accuracy a-priori due to their non-convex training.
Biot number calculations are involved in turbo-machinery design, which is
safety critical and highly regulated, therefore it is important that our
results have such mathematical guarantees. Our approach achieves fast mixing in
high dimensions whilst retaining the convergence guarantees of a traditional
PDE solver, and without the burden of evaluating this solver for proposals that
are likely to be rejected. Numerical results are given using real and simulated
data.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Linkless Link Prediction via Relational Distillation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.05801v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.05801v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zhichun Guo, William Shiao, Shichang Zhang, Yozen Liu, Nitesh V. Chawla, Neil Shah, Tong Zhao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph Neural Networks (GNNs) have shown exceptional performance in the task
of link prediction. Despite their effectiveness, the high latency brought by
non-trivial neighborhood data dependency limits GNNs in practical deployments.
Conversely, the known efficient MLPs are much less effective than GNNs due to
the lack of relational knowledge. In this work, to combine the advantages of
GNNs and MLPs, we start with exploring direct knowledge distillation (KD)
methods for link prediction, i.e., predicted logit-based matching and node
representation-based matching. Upon observing direct KD analogs do not perform
well for link prediction, we propose a relational KD framework, Linkless Link
Prediction (LLP), to distill knowledge for link prediction with MLPs. Unlike
simple KD methods that match independent link logits or node representations,
LLP distills relational knowledge that is centered around each (anchor) node to
the student MLP. Specifically, we propose rank-based matching and
distribution-based matching strategies that complement each other. Extensive
experiments demonstrate that LLP boosts the link prediction performance of MLPs
with significant margins, and even outperforms the teacher GNNs on 7 out of 8
benchmarks. LLP also achieves a 70.68x speedup in link prediction inference
compared to GNNs on the large-scale OGB dataset.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ MAMO: Masked Multimodal Modeling for Fine-Grained Vision-Language
  Representation Learning <span class="chip">SIGIR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04183v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04183v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zijia Zhao, Longteng Guo, Xingjian He, Shuai Shao, Zehuan Yuan, Jing Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal representation learning has shown promising improvements on
various vision-language tasks. Most existing methods excel at building
global-level alignment between vision and language while lacking effective
fine-grained image-text interaction. In this paper, we propose a jointly masked
multimodal modeling method to learn fine-grained multimodal representations.
Our method performs joint masking on image-text input and integrates both
implicit and explicit targets for the masked signals to recover. The implicit
target provides a unified and debiased objective for vision and language, where
the model predicts latent multimodal representations of the unmasked input. The
explicit target further enriches the multimodal representations by recovering
high-level and semantically meaningful information: momentum visual features of
image patches and concepts of word tokens. Through such a masked modeling
process, our model not only learns fine-grained multimodal interaction, but
also avoids the semantic gap between high-level representations and low- or
mid-level prediction targets (e.g. image pixels), thus producing semantically
rich multimodal representations that perform well on both zero-shot and
fine-tuned settings. Our pre-trained model (named MAMO) achieves
state-of-the-art performance on various downstream vision-language tasks,
including image-text retrieval, visual question answering, visual reasoning,
and weakly-supervised visual grounding.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>SIGIR 2023, 10 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Robust and Generalisable Segmentation of Subtle Epilepsy-causing
  Lesions: a Graph Convolutional Approach <span class="chip">MICCAI 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01375v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01375v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hannah Spitzer, Mathilde Ripart, Abdulah Fawaz, Logan Z. J. Williams, MELD project, Emma Robinson, Juan Eugenio Iglesias, Sophie Adler, Konrad Wagstyl
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Focal cortical dysplasia (FCD) is a leading cause of drug-resistant focal
epilepsy, which can be cured by surgery. These lesions are extremely subtle and
often missed even by expert neuroradiologists. "Ground truth" manual lesion
masks are therefore expensive, limited and have large inter-rater variability.
Existing FCD detection methods are limited by high numbers of false positive
predictions, primarily due to vertex- or patch-based approaches that lack
whole-brain context. Here, we propose to approach the problem as semantic
segmentation using graph convolutional networks (GCN), which allows our model
to learn spatial relationships between brain regions. To address the specific
challenges of FCD identification, our proposed model includes an auxiliary loss
to predict distance from the lesion to reduce false positives and a weak
supervision classification loss to facilitate learning from uncertain lesion
masks. On a multi-centre dataset of 1015 participants with surface-based
features and manual lesion masks from structural MRI data, the proposed GCN
achieved an AUC of 0.74, a significant improvement against a previously used
vertex-wise multi-layer perceptron (MLP) classifier (AUC 0.64). With
sensitivity thresholded at 67%, the GCN had a specificity of 71% in comparison
to 49% when using the MLP. This improvement in specificity is vital for
clinical integration of lesion-detection tools into the radiological workflow,
through increasing clinical confidence in the use of AI radiological adjuncts
and reducing the number of areas requiring expert review.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>accepted at MICCAI 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A Robust Optimisation Perspective on Counterexample-Guided Repair of
  Neural Networks <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11342v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11342v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Boetius, Stefan Leue, Tobias Sutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Counterexample-guided repair aims at creating neural networks with
mathematical safety guarantees, facilitating the application of neural networks
in safety-critical domains. However, whether counterexample-guided repair is
guaranteed to terminate remains an open question. We approach this question by
showing that counterexample-guided repair can be viewed as a robust
optimisation algorithm. While termination guarantees for neural network repair
itself remain beyond our reach, we prove termination for more restrained
machine learning models and disprove termination in a general setting. We
empirically study the practical implications of our theoretical results,
demonstrating the suitability of common verifiers and falsifiers for repair
despite a disadvantageous theoretical result. Additionally, we use our
theoretical insights to devise a novel algorithm for repairing linear
regression models based on quadratic programming, surpassing existing
approaches.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023. 9 pages + 13 pages appendix, 8 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sequoia: A Software Framework to Unify Continual Learning Research 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.01005v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.01005v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Fabrice Normandin, Florian Golemo, Oleksiy Ostapenko, Pau Rodriguez, Matthew D Riemer, Julio Hurtado, Khimya Khetarpal, Ryan Lindeborg, Lucas Cecchi, Timothée Lesort, Laurent Charlin, Irina Rish, Massimo Caccia
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The field of Continual Learning (CL) seeks to develop algorithms that
accumulate knowledge and skills over time through interaction with
non-stationary environments. In practice, a plethora of evaluation procedures
(settings) and algorithmic solutions (methods) exist, each with their own
potentially disjoint set of assumptions. This variety makes measuring progress
in CL difficult. We propose a taxonomy of settings, where each setting is
described as a set of assumptions. A tree-shaped hierarchy emerges from this
view, where more general settings become the parents of those with more
restrictive assumptions. This makes it possible to use inheritance to share and
reuse research, as developing a method for a given setting also makes it
directly applicable onto any of its children. We instantiate this idea as a
publicly available software framework called Sequoia, which features a wide
variety of settings from both the Continual Supervised Learning (CSL) and
Continual Reinforcement Learning (CRL) domains. Sequoia also includes a growing
suite of methods which are easy to extend and customize, in addition to more
specialized methods from external libraries. We hope that this new paradigm and
its first implementation can help unify and accelerate research in CL. You can
help us grow the tree by visiting www.github.com/lebrice/Sequoia.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Deep Reinforcement Learning with Swin <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.15269v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.15269v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Li Meng, Morten Goodwin, Anis Yazidi, Paal Engelstad
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformers are neural network models that utilize multiple layers of
self-attention heads and have exhibited enormous potential in natural language
processing tasks. Meanwhile, there have been efforts to adapt transformers to
visual tasks of machine learning, including Vision Transformers and Swin
Transformers. Although some researchers use Vision Transformers for
reinforcement learning tasks, their experiments remain at a small scale due to
the high computational cost. Experiments conducted at a large scale, on the
other hand, have to rely on techniques to cut the costs of Vision Transformers,
which also yield inferior results.
  To address this challenge, this article presents the first online
reinforcement learning scheme that is based on Swin Transformers: Swin DQN.
Swin Transformers are promising as a backbone in neural networks by splitting
groups of image pixels into small patches and applying local self-attention
operations inside the (shifted) windows of fixed sizes. They have demonstrated
state-of-the-art performances in benchmarks. In contrast to existing research,
our novel approach is reducing the computational costs, as well as
significantly improving the performance. We demonstrate the superior
performance with experiments on 49 games in the Arcade Learning Environment.
The results show that our approach, using Swin Transformers with Double DQN,
achieves significantly higher maximal evaluation scores than the baseline
method in 45 of all the 49 games ~92%, and higher mean evaluation scores than
the baseline method in 40 of all the 49 games ~82%.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Prototype Classifiers for Long-Tailed Recognition <span class="chip">IJCAI-23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.00491v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.00491v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Sharma, Yongqin Xian, Ning Yu, Ambuj Singh
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The problem of long-tailed recognition (LTR) has received attention in recent
years due to the fundamental power-law distribution of objects in the
real-world. Most recent works in LTR use softmax classifiers that have a
tendency to correlate classifier norm with the amount of training data for a
given class. On the other hand, Prototype classifiers do not suffer from this
shortcoming and can deliver promising results simply using Nearest-Class-Mean
(NCM), a special case where prototypes are empirical centroids. However, the
potential of Prototype classifiers as an alternative to softmax in LTR is
relatively underexplored. In this work, we propose Prototype classifiers, which
jointly learn prototypes that minimize average cross-entropy loss based on
probability scores from distances to prototypes. We theoretically analyze the
properties of Euclidean distance based prototype classifiers that leads to
stable gradient-based optimization which is robust to outliers. We further
enhance Prototype classifiers by learning channel-dependent temperature
parameters to enable independent distance scales along each channel. Our
analysis shows that prototypes learned by Prototype classifiers are better
separated than empirical centroids. Results on four long-tailed recognition
benchmarks show that Prototype classifier outperforms or is comparable to the
state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at IJCAI-23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tensor Completion with Provable Consistency and Fairness Guarantees for
  Recommender Systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.01815v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.01815v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tung Nguyen, Jeffrey Uhlmann
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new consistency-based approach for defining and solving
nonnegative/positive matrix and tensor completion problems. The novelty of the
framework is that instead of artificially making the problem well-posed in the
form of an application-arbitrary optimization problem, e.g., minimizing a bulk
structural measure such as rank or norm, we show that a single
property/constraint: preserving unit-scale consistency, guarantees the
existence of both a solution and, under relatively weak support assumptions,
uniqueness. The framework and solution algorithms also generalize directly to
tensors of arbitrary dimensions while maintaining computational complexity that
is linear in problem size for fixed dimension d. In the context of recommender
system (RS) applications, we prove that two reasonable properties that should
be expected to hold for any solution to the RS problem are sufficient to permit
uniqueness guarantees to be established within our framework. Key theoretical
contributions include a general unit-consistent tensor-completion framework
with proofs of its properties, e.g., consensus-order and fairness, and
algorithms with optimal runtime and space complexities, e.g., O(1)
term-completion with preprocessing complexity that is linear in the number of
known terms of the matrix/tensor. From a practical perspective, the seamless
ability of the framework to generalize to exploit high-dimensional structural
relationships among key state variables, e.g., user and product attributes,
offers a means for extracting significantly more information than is possible
for alternative methods that cannot generalize beyond direct user-product
relationships. Finally, we propose our consensus ordering property as an
admissibility criterion for any proposed RS method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Final revision after acceptance by journal</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ RLSbench: Domain Adaptation Under Relaxed Label Shift <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.03020v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.03020v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Saurabh Garg, Nick Erickson, James Sharpnack, Alex Smola, Sivaraman Balakrishnan, Zachary C. Lipton
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite the emergence of principled methods for domain adaptation under label
shift, their sensitivity to shifts in class conditional distributions is
precariously under explored. Meanwhile, popular deep domain adaptation
heuristics tend to falter when faced with label proportions shifts. While
several papers modify these heuristics in attempts to handle label proportions
shifts, inconsistencies in evaluation standards, datasets, and baselines make
it difficult to gauge the current best practices. In this paper, we introduce
RLSbench, a large-scale benchmark for relaxed label shift, consisting of $>$500
distribution shift pairs spanning vision, tabular, and language modalities,
with varying label proportions. Unlike existing benchmarks, which primarily
focus on shifts in class-conditional $p(x|y)$, our benchmark also focuses on
label marginal shifts. First, we assess 13 popular domain adaptation methods,
demonstrating more widespread failures under label proportion shifts than were
previously known. Next, we develop an effective two-step meta-algorithm that is
compatible with most domain adaptation heuristics: (i) pseudo-balance the data
at each epoch; and (ii) adjust the final classifier with target label
distribution estimate. The meta-algorithm improves existing domain adaptation
heuristics under large label proportion shifts, often by 2--10\% accuracy
points, while conferring minimal effect ($<$0.5\%) when label proportions do
not shift. We hope that these findings and the availability of RLSbench will
encourage researchers to rigorously evaluate proposed methods in relaxed label
shift settings. Code is publicly available at
https://github.com/acmi-lab/RLSbench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023. Paper website:
  https://sites.google.com/view/rlsbench/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Data Poisoning Attacks Against Multimodal Encoders 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.15266v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.15266v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ziqing Yang, Xinlei He, Zheng Li, Michael Backes, Mathias Humbert, Pascal Berrang, Yang Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recently, the newly emerged multimodal models, which leverage both visual and
linguistic modalities to train powerful encoders, have gained increasing
attention. However, learning from a large-scale unlabeled dataset also exposes
the model to the risk of potential poisoning attacks, whereby the adversary
aims to perturb the model's training data to trigger malicious behaviors in it.
In contrast to previous work, only poisoning visual modality, in this work, we
take the first step to studying poisoning attacks against multimodal models in
both visual and linguistic modalities. Specially, we focus on answering two
questions: (1) Is the linguistic modality also vulnerable to poisoning attacks?
and (2) Which modality is most vulnerable? To answer the two questions, we
propose three types of poisoning attacks against multimodal models. Extensive
evaluations on different datasets and model architectures show that all three
attacks can achieve significant attack performance while maintaining model
utility in both visual and linguistic modalities. Furthermore, we observe that
the poisoning effect differs between different modalities. To mitigate the
attacks, we propose both pre-training and post-training defenses. We
empirically show that both defenses can significantly reduce the attack
performance while preserving the model's utility.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To Appear in the 40th International Conference on Machine Learning,
  July 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Differentially Private Episodic Reinforcement Learning with Heavy-tailed
  Rewards <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.01121v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.01121v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yulian Wu, Xingyu Zhou, Sayak Ray Chowdhury, Di Wang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we study the problem of (finite horizon tabular) Markov
decision processes (MDPs) with heavy-tailed rewards under the constraint of
differential privacy (DP). Compared with the previous studies for private
reinforcement learning that typically assume rewards are sampled from some
bounded or sub-Gaussian distributions to ensure DP, we consider the setting
where reward distributions have only finite $(1+v)$-th moments with some $v \in
(0,1]$. By resorting to robust mean estimators for rewards, we first propose
two frameworks for heavy-tailed MDPs, i.e., one is for value iteration and
another is for policy optimization. Under each framework, we consider both
joint differential privacy (JDP) and local differential privacy (LDP) models.
Based on our frameworks, we provide regret upper bounds for both JDP and LDP
cases and show that the moment of distribution and privacy budget both have
significant impacts on regrets. Finally, we establish a lower bound of regret
minimization for heavy-tailed MDPs in JDP model by reducing it to the
instance-independent lower bound of heavy-tailed multi-armed bandits in DP
model. We also show the lower bound for the problem in LDP by adopting some
private minimax methods. Our results reveal that there are fundamental
differences between the problem of private RL with sub-Gaussian and that with
heavy-tailed rewards.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Tactile-Filter: Interactive Tactile Perception for Part Mating 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.06034v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.06034v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kei Ota, Devesh K. Jha, Hsiao-Yu Tung, Joshua B. Tenenbaum
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Humans rely on touch and tactile sensing for a lot of dexterous manipulation
tasks. Our tactile sensing provides us with a lot of information regarding
contact formations as well as geometric information about objects during any
interaction. With this motivation, vision-based tactile sensors are being
widely used for various robotic perception and control tasks. In this paper, we
present a method for interactive perception using vision-based tactile sensors
for a part mating task, where a robot can use tactile sensors and a feedback
mechanism using a particle filter to incrementally improve its estimate of
objects (pegs and holes) that fit together. To do this, we first train a deep
neural network that makes use of tactile images to predict the probabilistic
correspondence between arbitrarily shaped objects that fit together. The
trained model is used to design a particle filter which is used twofold. First,
given one partial (or non-unique) observation of the hole, it incrementally
improves the estimate of the correct peg by sampling more tactile observations.
Second, it selects the next action for the robot to sample the next touch (and
thus image) which results in maximum uncertainty reduction to minimize the
number of interactions during the perception task. We evaluate our method on
several part-mating tasks with novel objects using a robot equipped with a
vision-based tactile sensor. We also show the efficiency of the proposed action
selection method against a naive method. See supplementary video at
https://www.youtube.com/watch?v=jMVBg_e3gLw .
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at RSS2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Mitigating Off-Policy Bias in Actor-Critic Methods with One-Step
  Q-learning: A Novel Correction Approach 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2208.00755v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2208.00755v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Baturay Saglam, Dogan C. Cicek, Furkan B. Mutlu, Suleyman S. Kozat
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Compared to on-policy counterparts, off-policy model-free deep reinforcement
learning can improve data efficiency by repeatedly using the previously
gathered data. However, off-policy learning becomes challenging when the
discrepancy between the underlying distributions of the agent's policy and
collected data increases. Although the well-studied importance sampling and
off-policy policy gradient techniques were proposed to compensate for this
discrepancy, they usually require a collection of long trajectories and induce
additional problems such as vanishing/exploding gradients or discarding many
useful experiences, which eventually increases the computational complexity.
Moreover, their generalization to either continuous action domains or policies
approximated by deterministic deep neural networks is strictly limited. To
overcome these limitations, we introduce a novel policy similarity measure to
mitigate the effects of such discrepancy in continuous control. Our method
offers an adequate single-step off-policy correction that is applicable to
deterministic policy networks. Theoretical and empirical studies demonstrate
that it can achieve a "safe" off-policy learning and substantially improve the
state-of-the-art by attaining higher returns in fewer steps than the competing
methods through an effective schedule of the learning rate in Q-learning and
policy optimization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Lightweight and Flexible Deep Equilibrium Learning for CSI Feedback in
  FDD Massive MIMO 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.15079v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.15079v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yifan Ma, Wentao Yu, Xianghao Yu, Jun Zhang, Shenghui Song, Khaled B. Letaief
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In frequency-division duplexing (FDD) massive multiple-input multiple-output
(MIMO) systems, downlink channel state information (CSI) needs to be sent back
to the base station (BS) by the users, which causes prohibitive feedback
overhead. In this paper, we propose a lightweight and flexible deep
learning-based CSI feedback approach by capitalizing on deep equilibrium
models. Different from existing deep learning-based methods that stack multiple
explicit layers, we propose an implicit equilibrium block to mimic the behavior
of an infinite-depth neural network. In particular, the implicit equilibrium
block is defined by a fixed-point iteration and the trainable parameters in
different iterations are shared, which results in a lightweight model.
Furthermore, the number of forward iterations can be adjusted according to
users' computation capability, enabling a flexible accuracy-efficiency
trade-off. Simulation results will show that the proposed design obtains a
comparable performance as the benchmarks but with much-reduced complexity and
permits an accuracy-efficiency trade-off at runtime.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>submitted to IEEE for possible publication</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ OpenFE: Automated Feature Generation with Expert-level Performance <span class="chip">ICML2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.12507v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.12507v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianping Zhang, Zheyu Zhang, Zhiyuan Fan, Haoyan Luo, Fengyuan Liu, Qian Liu, Wei Cao, Jian Li
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The goal of automated feature generation is to liberate machine learning
experts from the laborious task of manual feature generation, which is crucial
for improving the learning performance of tabular data. The major challenge in
automated feature generation is to efficiently and accurately identify
effective features from a vast pool of candidate features. In this paper, we
present OpenFE, an automated feature generation tool that provides competitive
results against machine learning experts. OpenFE achieves high efficiency and
accuracy with two components: 1) a novel feature boosting method for accurately
evaluating the incremental performance of candidate features and 2) a two-stage
pruning algorithm that performs feature pruning in a coarse-to-fine manner.
Extensive experiments on ten benchmark datasets show that OpenFE outperforms
existing baseline methods by a large margin. We further evaluate OpenFE in two
Kaggle competitions with thousands of data science teams participating. In the
two competitions, features generated by OpenFE with a simple baseline model can
beat 99.3% and 99.6% data science teams respectively. In addition to the
empirical results, we provide a theoretical perspective to show that feature
generation can be beneficial in a simple yet representative setting. The code
is available at https://github.com/ZhangTP1996/OpenFE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>22 pages, 3 figures, accepted by ICML2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exploiting Noise as a Resource for Computation and Learning in Spiking
  Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16044v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16044v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gehua Ma, Rui Yan, Huajin Tang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Networks of spiking neurons underpin the extraordinary information-processing
capabilities of the brain and have emerged as pillar models in neuromorphic
intelligence. Despite extensive research on spiking neural networks (SNNs),
most are established on deterministic models. Integrating noise into SNNs leads
to biophysically more realistic neural dynamics and may benefit model
performance. This work presents the noisy spiking neural network (NSNN) and the
noise-driven learning rule (NDL) by introducing a spiking neuron model
incorporating noisy neuronal dynamics. Our approach shows how noise may act as
a resource for computation and learning and theoretically provides a framework
for general SNNs. Moreover, NDL provides an insightful biological rationale for
surrogate gradients. By incorporating various SNN architectures and algorithms,
we show that our approach exhibits competitive performance and improved
robustness against challenging perturbations than deterministic SNNs.
Additionally, we demonstrate the utility of the NSNN model for neural coding
studies. Overall, NSNN offers a powerful, flexible, and easy-to-use tool for
machine learning practitioners and computational neuroscience researchers.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated the code link; fixed the bug in the BBL file generated with
  bibliography management program</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Existence and Estimation of Critical Batch Size for Training Generative
  Adversarial Networks with Two Time-Scale Update Rule <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2201.11989v6">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2201.11989v6.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Naoki Sato, Hideaki Iiduka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Previous results have shown that a two time-scale update rule (TTUR) using
different learning rates, such as different constant rates or different
decaying rates, is useful for training generative adversarial networks (GANs)
in theory and in practice. Moreover, not only the learning rate but also the
batch size is important for training GANs with TTURs and they both affect the
number of steps needed for training. This paper studies the relationship
between batch size and the number of steps needed for training GANs with TTURs
based on constant learning rates. We theoretically show that, for a TTUR with
constant learning rates, the number of steps needed to find stationary points
of the loss functions of both the discriminator and generator decreases as the
batch size increases and that there exists a critical batch size minimizing the
stochastic first-order oracle (SFO) complexity. Then, we use the Fr'echet
inception distance (FID) as the performance measure for training and provide
numerical results indicating that the number of steps needed to achieve a low
FID score decreases as the batch size increases and that the SFO complexity
increases once the batch size exceeds the measured critical batch size.
Moreover, we show that measured critical batch sizes are close to the sizes
estimated from our theoretical results.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at the 40th International Conference on Machine Learning
  (ICML 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Canonical foliations of neural networks: application to robustness 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.00922v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.00922v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Eliot Tron, Nicolas Couellan, Stéphane Puechmorel
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep learning models are known to be vulnerable to adversarial attacks.
Adversarial learning is therefore becoming a crucial task. We propose a new
vision on neural network robustness using Riemannian geometry and foliation
theory. The idea is illustrated by creating a new adversarial attack that takes
into account the curvature of the data space. This new adversarial attack
called the two-step spectral attack is a piece-wise linear approximation of a
geodesic in the data space. The data space is treated as a (degenerate)
Riemannian manifold equipped with the pullback of the Fisher Information Metric
(FIM) of the neural network. In most cases, this metric is only semi-definite
and its kernel becomes a central object to study. A canonical foliation is
derived from this kernel. The curvature of transverse leaves gives the
appropriate correction to get a two-step approximation of the geodesic and
hence a new efficient adversarial attack. The method is first illustrated on a
2D toy example in order to visualize the neural network foliation and the
corresponding attacks. Next, experiments on the MNIST dataset with the proposed
technique and a state of the art attack presented in Zhao et al. (2019) are
reported. The result show that the proposed attack is more efficient at all
levels of available budget for the attack (norm of the attack), confirming that
the curvature of the transverse neural network FIM foliation plays an important
role in the robustness of neural networks.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scalable Optimal Margin Distribution Machine 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04837v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04837v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yilin Wang, Nan Cao, Teng Zhang, Xuanhua Shi, Hai Jin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Optimal margin Distribution Machine (ODM) is a newly proposed statistical
learning framework rooting in the novel margin theory, which demonstrates
better generalization performance than the traditional large margin based
counterparts. Nonetheless, it suffers from the ubiquitous scalability problem
regarding both computation time and memory as other kernel methods. This paper
proposes a scalable ODM, which can achieve nearly ten times speedup compared to
the original ODM training method. For nonlinear kernels, we propose a novel
distribution-aware partition method to make the local ODM trained on each
partition be close and converge fast to the global one. When linear kernel is
applied, we extend a communication efficient SVRG method to accelerate the
training further. Extensive empirical studies validate that our proposed method
is highly computational efficient and almost never worsen the generalization.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluation of Interpretability Methods and Perturbation Artifacts in
  Deep Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2203.02928v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2203.02928v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lennart Brocki, Neo Christopher Chung
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite excellent performance of deep neural networks (DNNs) in image
classification, detection, and prediction, characterizing how DNNs make a given
decision remains an open problem, resulting in a number of interpretability
methods. Post-hoc interpretability methods primarily aim to quantify the
importance of input features with respect to the class probabilities. However,
due to the lack of ground truth and the existence of interpretability methods
with diverse operating characteristics, evaluating these methods is a crucial
challenge. A popular approach to evaluate interpretability methods is to
perturb input features deemed important for a given prediction and observe the
decrease in accuracy. However, perturbation itself may introduce artifacts. We
propose a method for estimating the impact of such artifacts on the fidelity
estimation by utilizing model accuracy curves from perturbing input features
according to the Most Import First (MIF) and Least Import First (LIF) orders.
Using the ResNet-50 trained on the ImageNet, we demonstrate the proposed
fidelity estimation of four popular post-hoc interpretability methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages, 5 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Exponential Smoothing for Off-Policy Learning <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.15877v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.15877v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Imad Aouali, Victor-Emmanuel Brunel, David Rohde, Anna Korba
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Off-policy learning (OPL) aims at finding improved policies from logged
bandit data, often by minimizing the inverse propensity scoring (IPS) estimator
of the risk. In this work, we investigate a smooth regularization for IPS, for
which we derive a two-sided PAC-Bayes generalization bound. The bound is
tractable, scalable, interpretable and provides learning certificates. In
particular, it is also valid for standard IPS without making the assumption
that the importance weights are bounded. We demonstrate the relevance of our
approach and its favorable performance through a set of learning tasks. Since
our bound holds for standard IPS, we are able to provide insight into when
regularizing IPS is useful. Namely, we identify cases where regularization
might not be needed. This goes against the belief that, in practice, clipped
IPS often enjoys favorable performance than standard IPS in OPL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023 (Oral and Poster)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Signs of Language: Embodied Sign Language Fingerspelling Acquisition
  from Demonstrations for Human-Robot Interaction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2209.05135v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2209.05135v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Federico Tavella, Aphrodite Galata, Angelo Cangelosi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Learning fine-grained movements is a challenging topic in robotics,
particularly in the context of robotic hands. One specific instance of this
challenge is the acquisition of fingerspelling sign language in robots. In this
paper, we propose an approach for learning dexterous motor imitation from video
examples without additional information. To achieve this, we first build a URDF
model of a robotic hand with a single actuator for each joint. We then leverage
pre-trained deep vision models to extract the 3D pose of the hand from RGB
videos. Next, using state-of-the-art reinforcement learning algorithms for
motion imitation (namely, proximal policy optimization and soft actor-critic),
we train a policy to reproduce the movement extracted from the demonstrations.
We identify the optimal set of hyperparameters for imitation based on a
reference motion. Finally, we demonstrate the generalizability of our approach
by testing it on six different tasks, corresponding to fingerspelled letters.
Our results show that our approach is able to successfully imitate these
fine-grained movements without additional information, highlighting its
potential for real-world applications in robotics.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scaling Multi-Objective Security Games Provably via Space Discretization
  Based Evolutionary Search 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.15821v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.15821v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yu-Peng Wu, Hong Qian, Rong-Jun Qin, Yi Chen, Aimin Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In the field of security, multi-objective security games (MOSGs) allow
defenders to simultaneously protect targets from multiple heterogeneous
attackers. MOSGs aim to simultaneously maximize all the heterogeneous payoffs,
e.g., life, money, and crime rate, without merging heterogeneous attackers. In
real-world scenarios, the number of heterogeneous attackers and targets to be
protected may exceed the capability of most existing state-of-the-art methods,
i.e., MOSGs are limited by the issue of scalability. To this end, this paper
proposes a general framework called SDES based on many-objective evolutionary
search to scale up MOSGs to large-scale targets and heterogeneous attackers.
SDES consists of four consecutive key components, i.e., discretization,
optimization, evaluation, and refinement. Specifically, SDES first discretizes
the originally high-dimensional continuous solution space to the
low-dimensional discrete one by the maximal indifference property in game
theory. This property helps evolutionary algorithms (EAs) bypass the
high-dimensional step function and ensure a well-convergent Pareto front. Then,
a many-objective EA is used for optimization in the low-dimensional discrete
solution space to obtain a well-spaced Pareto front. To evaluate solutions,
SDES restores solutions back to the original space via greedily optimizing a
novel divergence measurement. Finally, the refinement in SDES boosts the
optimization performance with acceptable cost. Theoretically, we prove the
optimization consistency and convergence of SDES. Experiment results show that
SDES is the first linear-time MOSG algorithm for both large-scale attackers and
targets. SDES is able to solve up to 20 attackers and 100 targets MOSG
problems, while the state-of-the-art (SOTA) methods can only solve up to 8
attackers and 25 targets ones. Ablation study verifies the necessity of all
components in SDES.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Construction of Hierarchical Neural Architecture Search Spaces based on
  Context-free Grammars 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.01842v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.01842v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Simon Schrodi, Danny Stoll, Binxin Ru, Rhea Sukthanker, Thomas Brox, Frank Hutter
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The discovery of neural architectures from simple building blocks is a
long-standing goal of Neural Architecture Search (NAS). Hierarchical search
spaces are a promising step towards this goal but lack a unifying search space
design framework and typically only search over some limited aspect of
architectures. In this work, we introduce a unifying search space design
framework based on context-free grammars that can naturally and compactly
generate expressive hierarchical search spaces that are 100s of orders of
magnitude larger than common spaces from the literature. By enhancing and using
their properties, we effectively enable search over the complete architecture
and can foster regularity. Further, we propose an efficient hierarchical kernel
design for a Bayesian Optimization search strategy to efficiently search over
such huge spaces. We demonstrate the versatility of our search space design
framework and show that our search strategy can be superior to existing NAS
approaches. Code is available at
https://github.com/automl/hierarchical_nas_construction.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Graph Neural Networks are Inherently Good Generalizers: Insights by
  Bridging GNNs and MLPs <span class="chip">ICLR 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.09034v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.09034v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Chenxiao Yang, Qitian Wu, Jiahua Wang, Junchi Yan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Graph neural networks (GNNs), as the de-facto model class for representation
learning on graphs, are built upon the multi-layer perceptrons (MLP)
architecture with additional message passing layers to allow features to flow
across nodes. While conventional wisdom commonly attributes the success of GNNs
to their advanced expressivity, we conjecture that this is not the main cause
of GNNs' superiority in node-level prediction tasks. This paper pinpoints the
major source of GNNs' performance gain to their intrinsic generalization
capability, by introducing an intermediate model class dubbed as
P(ropagational)MLP, which is identical to standard MLP in training, but then
adopts GNN's architecture in testing. Intriguingly, we observe that PMLPs
consistently perform on par with (or even exceed) their GNN counterparts, while
being much more efficient in training. This finding sheds new insights into
understanding the learning behavior of GNNs, and can be used as an analytic
tool for dissecting various GNN-related research problems. As an initial step
to analyze the inherent generalizability of GNNs, we show the essential
difference between MLP and PMLP at infinite-width limit lies in the NTK feature
map in the post-training stage. Moreover, by examining their extrapolation
behavior, we find that though many GNNs and their PMLP counterparts cannot
extrapolate non-linear functions for extremely out-of-distribution samples,
they have greater potential to generalize to testing samples near the training
data range as natural advantages of GNN architectures.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICLR 2023. Codes in https://github.com/chr26195/PMLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Safe Peeling for L0-Regularized Least-Squares with supplementary
  material 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.14471v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.14471v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Théo Guyard, Gilles Monnoyer, Clément Elvira, Cédric Herzet
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We introduce a new methodology dubbed ``safe peeling'' to accelerate the
resolution of L0-regularized least-squares problems via a Branch-and-Bound
(BnB) algorithm. Our procedure enables to tighten the convex relaxation
considered at each node of the BnB decision tree and therefore potentially
allows for more aggressive pruning. Numerical simulations show that our
proposed methodology leads to significant gains in terms of number of nodes
explored and overall solving time.s show that our proposed methodology leads to
significant gains in terms of number of nodes explored and overall solving
time.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning the Relation between Similarity Loss and Clustering Loss in
  <span class="highlight-title">Self-Supervised</span> Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.03041v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.03041v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jidong Ge, Yuxiang Liu, Jie Gui, Lanting Fang, Ming Lin, James Tin-Yau Kwok, LiGuo Huang, Bin Luo
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised learning enables networks to learn discriminative features
from massive data itself. Most state-of-the-art methods maximize the similarity
between two augmentations of one image based on contrastive learning. By
utilizing the consistency of two augmentations, the burden of manual
annotations can be freed. Contrastive learning exploits instance-level
information to learn robust features. However, the learned information is
probably confined to different views of the same instance. In this paper, we
attempt to leverage the similarity between two distinct images to boost
representation in self-supervised learning. In contrast to instance-level
information, the similarity between two distinct images may provide more useful
information. Besides, we analyze the relation between similarity loss and
feature-level cross-entropy loss. These two losses are essential for most deep
learning methods. However, the relation between these two losses is not clear.
Similarity loss helps obtain instance-level representation, while feature-level
cross-entropy loss helps mine the similarity between two distinct images. We
provide theoretical analyses and experiments to show that a suitable
combination of these two losses can get state-of-the-art results. Code is
available at https://github.com/guijiejie/ICCL.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>This paper is accepted by IEEE Transactions on Image Processing</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Failure-Inducing Models for Testing Software-Defined Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.15469v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.15469v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Raphaël Ollando, Seung Yeob Shin, Lionel C. Briand
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Software-defined networks (SDN) enable flexible and effective communication
systems that are managed by centralized software controllers. However, such a
controller can undermine the underlying communication network of an SDN-based
system and thus must be carefully tested. When an SDN-based system fails, in
order to address such a failure, engineers need to precisely understand the
conditions under which it occurs. In this article, we introduce a machine
learning-guided fuzzing method, named FuzzSDN, aiming at both (1) generating
effective test data leading to failures in SDN-based systems and (2) learning
accurate failure-inducing models that characterize conditions under which such
system fails. To our knowledge, FuzzSDN is the first attempt to simultaneously
address these two objectives for SDNs. We evaluate FuzzSDN by applying it to
systems controlled by two open-source SDN controllers. Further, we compare
FuzzSDN with two state-of-the-art methods for fuzzing SDNs and two baselines
for learning failure-inducing models. Our results show that (1) compared to the
state-of-the-art methods, FuzzSDN generates at least 12 times more failures,
within the same time budget, with a controller that is fairly robust to fuzzing
and (2) our failure-inducing models have, on average, a precision of 98% and a
recall of 86%, significantly outperforming the baselines.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An adaptive safety layer with hard constraints for safe reinforcement
  learning in multi-energy management systems 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.08897v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.08897v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Glenn Ceusters, Muhammad Andy Putratama, Rüdiger Franke, Ann Nowé, Maarten Messagie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Safe reinforcement learning (RL) with hard constraint guarantees is a
promising optimal control direction for multi-energy management systems. It
only requires the environment-specific constraint functions itself a priori and
not a complete model (i.e. plant, disturbance and noise models, and prediction
models for states not included in the plant model - e.g. demand forecasts,
weather forecasts, price forecasts). The project-specific upfront and ongoing
engineering efforts are therefore still reduced, better representations of the
underlying system dynamics can still be learned and modelling bias is kept to a
minimum (no model-based objective function). However, even the constraint
functions alone are not always trivial to accurately provide in advance,
leading to potentially unsafe behaviour. In this paper, we present two novel
advancements: (I) combining the Optlayer and SafeFallback method, named
OptLayerPolicy, to increase the initial utility while keeping a high sample
efficiency. (II) introducing self-improving hard constraints, to increase the
accuracy of the constraint functions as more data becomes available so that
better policies can be learned. Both advancements keep the constraint
formulation decoupled from the RL formulation, so that new (presumably better)
RL algorithms can act as drop-in replacements. We have shown that, in a
simulated multi-energy system case study, the initial utility is increased to
92.4% (OptLayerPolicy) compared to 86.1% (OptLayer) and that the policy after
training is increased to 104.9% (GreyOptLayerPolicy) compared to 103.4%
(OptLayer) - all relative to a vanilla RL benchmark. While introducing
surrogate functions into the optimization problem requires special attention,
we do conclude that the newly presented GreyOptLayerPolicy method is the most
advantageous.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>4703 words. arXiv admin note: text overlap with arXiv:2207.03830</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ACTC: Active Threshold Calibration for Cold-Start Knowledge Graph
  Completion <span class="chip">ACL'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06395v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06395v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anastasiia Sedova, Benjamin Roth
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised knowledge-graph completion (KGC) relies on estimating a
scoring model over (entity, relation, entity)-tuples, for example, by embedding
an initial knowledge graph. Prediction quality can be improved by calibrating
the scoring model, typically by adjusting the prediction thresholds using
manually annotated examples. In this paper, we attempt for the first time
cold-start calibration for KGC, where no annotated examples exist initially for
calibration, and only a limited number of tuples can be selected for
annotation. Our new method ACTC finds good per-relation thresholds efficiently
based on a limited set of annotated tuples. Additionally to a few annotated
tuples, ACTC also leverages unlabeled tuples by estimating their correctness
with Logistic Regression or Gaussian Process classifiers. We also experiment
with different methods for selecting candidate tuples for annotation:
density-based and random selection. Experiments with five scoring models and an
oracle annotator show an improvement of 7% points when using ACTC in the
challenging setting with an annotation budget of only 10 tuples, and an average
improvement of 4% points over different budgets.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL'23</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ <span class="highlight-title">Pre-train</span>ing for Speech Translation: CTC Meets Optimal Transport <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.11716v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.11716v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Phuong-Hang Le, Hongyu Gong, Changhan Wang, Juan Pino, Benjamin Lecouteux, Didier Schwab
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The gap between speech and text modalities is a major challenge in
speech-to-text translation (ST). Different methods have been proposed to reduce
this gap, but most of them require architectural changes in ST training. In
this work, we propose to mitigate this issue at the pre-training stage,
requiring no change in the ST model. First, we show that the connectionist
temporal classification (CTC) loss can reduce the modality gap by design. We
provide a quantitative comparison with the more common cross-entropy loss,
showing that pre-training with CTC consistently achieves better final ST
accuracy. Nevertheless, CTC is only a partial solution and thus, in our second
contribution, we propose a novel pre-training method combining CTC and optimal
transport to further reduce this gap. Our method pre-trains a Siamese-like
model composed of two encoders, one for acoustic inputs and the other for
textual inputs, such that they produce representations that are close to each
other in the Wasserstein space. Extensive experiments on the standard CoVoST-2
and MuST-C datasets show that our pre-training method applied to the vanilla
encoder-decoder Transformer achieves state-of-the-art performance under the
no-external-data setting, and performs on par with recent strong multi-task
learning systems trained with external data. Finally, our method can also be
applied on top of these multi-task systems, leading to further improvements for
these models. Code and pre-trained models are available at
https://github.com/formiel/fairseq.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023 (oral presentation). This version fixed URLs, updated
  affiliations & acknowledgements, and improved formatting</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Candidate Set Re-ranking for Composed Image Retrieval with Dual
  Multi-modal Encoder 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16304v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16304v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zheyuan Liu, Weixuan Sun, Damien Teney, Stephen Gould
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Composed image retrieval aims to find an image that best matches a given
multi-modal user query consisting of a reference image and text pair. Existing
methods commonly pre-compute image embeddings over the entire corpus and
compare these to a reference image embedding modified by the query text at test
time. Such a pipeline is very efficient at test time since fast vector
distances can be used to evaluate candidates, but modifying the reference image
embedding guided only by a short textual description can be difficult,
especially independent of potential candidates. An alternative approach is to
allow interactions between the query and every possible candidate, i.e.,
reference-text-candidate triplets, and pick the best from the entire set.
Though this approach is more discriminative, for large-scale datasets the
computational cost is prohibitive since pre-computation of candidate embeddings
is no longer possible. We propose to combine the merits of both schemes using a
two-stage model. Our first stage adopts the conventional vector distancing
metric and performs a fast pruning among candidates. Meanwhile, our second
stage employs a dual-encoder architecture, which effectively attends to the
input triplet of reference-text-candidate and re-ranks the candidates. Both
stages utilize a vision-and-language pre-trained network, which has proven
beneficial for various downstream tasks. Our method consistently outperforms
state-of-the-art approaches on standard benchmarks for the task.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Position tracking of a varying number of sound sources with sliding
  permutation invariant training 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.14536v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.14536v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        David Diaz-Guerra, Archontis Politis, Tuomas Virtanen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Recent data- and learning-based sound source localization (SSL) methods have
shown strong performance in challenging acoustic scenarios. However, little
work has been done on adapting such methods to track consistently multiple
sources appearing and disappearing, as would occur in reality. In this paper,
we present a new training strategy for deep learning SSL models with a
straightforward implementation based on the mean squared error of the optimal
association between estimated and reference positions in the preceding time
frames. It optimizes the desired properties of a tracking system: handling a
time-varying number of sources and ordering localization estimates according to
their trajectories, minimizing identity switches (IDSs). Evaluation on
simulated data of multiple reverberant moving sources and on two model
architectures proves its effectiveness on reducing identity switches without
compromising frame-wise localization accuracy.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for publication at the 31st European Signal Processing
  Conference (EUSIPCO 2023)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Centralised rehearsal of decentralised cooperation: Multi-agent
  reinforcement learning for the scalable coordination of residential energy
  flexibility 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18875v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18875v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Flora Charbonnier, Bei Peng, Thomas Morstyn, Malcolm McCulloch
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper investigates how deep multi-agent reinforcement learning can
enable the scalable and privacy-preserving coordination of residential energy
flexibility. The coordination of distributed resources such as electric
vehicles and heating will be critical to the successful integration of large
shares of renewable energy in our electricity grid and, thus, to help mitigate
climate change. The pre-learning of individual reinforcement learning policies
can enable distributed control with no sharing of personal data required during
execution. However, previous approaches for multi-agent reinforcement
learning-based distributed energy resources coordination impose an ever greater
training computational burden as the size of the system increases. We therefore
adopt a deep multi-agent actor-critic method which uses a \emph{centralised but
factored critic} to rehearse coordination ahead of execution. Results show that
coordination is achieved at scale, with minimal information and communication
infrastructure requirements, no interference with daily activities, and privacy
protection. Significant savings are obtained for energy users, the distribution
network and greenhouse gas emissions. Moreover, training times are nearly 40
times shorter than with a previous state-of-the-art reinforcement learning
approach without the factored critic for 30 homes.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Robustness and Uncertainty of Graph Models Under Structural
  Distributional Shifts 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.13875v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.13875v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Gleb Bazhenov, Denis Kuznedelev, Andrey Malinin, Artem Babenko, Liudmila Prokhorenkova
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In reliable decision-making systems based on machine learning, models have to
be robust to distributional shifts or provide the uncertainty of their
predictions. In node-level problems of graph learning, distributional shifts
can be especially complex since the samples are interdependent. To evaluate the
performance of graph models, it is important to test them on diverse and
meaningful distributional shifts. However, most graph benchmarks considering
distributional shifts for node-level problems focus mainly on node features,
while structural properties are also essential for graph problems. In this
work, we propose a general approach for inducing diverse distributional shifts
based on graph structure. We use this approach to create data splits according
to several structural node properties: popularity, locality, and density. In
our experiments, we thoroughly evaluate the proposed distributional shifts and
show that they can be quite challenging for existing graph models. We also
reveal that simple models often outperform more sophisticated methods on these
challenging shifts. Finally, our experiments provide evidence that there is a
trade-off between the quality of learned representations for the base
classification task under structural distributional shift and the ability to
separate the nodes from different distributions using these representations.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Vandermonde Neural Operators 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.19663v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.19663v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Levi Lingsch, Mike Michelis, Sirani M. Perera, Robert K. Katzschmann, Siddartha Mishra
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Fourier Neural Operators (FNOs) have emerged as very popular machine learning
architectures for learning operators, particularly those arising in PDEs.
However, as FNOs rely on the fast Fourier transform for computational
efficiency, the architecture can be limited to input data on equispaced
Cartesian grids. Here, we generalize FNOs to handle input data on
non-equispaced point distributions. Our proposed model, termed as Vandermonde
Neural Operator (VNO), utilizes Vandermonde-structured matrices to efficiently
compute forward and inverse Fourier transforms, even on arbitrarily distributed
points. We present numerical experiments to demonstrate that VNOs can be
significantly faster than FNOs, while retaining comparable accuracy, and
improve upon accuracy of comparable non-equispaced methods such as the Geo-FNO.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>21 pages, 10 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Rethinking Gauss-Newton for learning over-parameterized models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.02904v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.02904v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Michael Arbel, Romain Menegaux, Pierre Wolinski
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This work studies the global convergence and generalization properties of
Gauss Newton's (GN) when optimizing one-hidden layer networks in the
over-parameterized regime. We first establish a global convergence result for
GN in the continuous-time limit exhibiting a faster convergence rate compared
to GD due to improved conditioning. We then perform an empirical study on a
synthetic regression task to investigate the implicit bias of GN's method. We
find that, while GN is consistently faster than GD in finding a global optimum,
the performance of the learned model on a test dataset is heavily influenced by
both the learning rate and the variance of the randomly initialized network's
weights. Specifically, we find that initializing with a smaller variance
results in a better generalization, a behavior also observed for GD. However,
in contrast to GD where larger learning rates lead to the best generalization,
we find that GN achieves an improved generalization when using smaller learning
rates, albeit at the cost of slower convergence. This study emphasizes the
significance of the learning rate in balancing the optimization speed of GN
with the generalization ability of the learned solution.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Activation Functions for Sparse Neural Networks 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.10964v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.10964v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mohammad Loni, Aditya Mohan, Mehdi Asadi, Marius Lindauer
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sparse Neural Networks (SNNs) can potentially demonstrate similar performance
to their dense counterparts while saving significant energy and memory at
inference. However, the accuracy drop incurred by SNNs, especially at high
pruning ratios, can be an issue in critical deployment conditions. While recent
works mitigate this issue through sophisticated pruning techniques, we shift
our focus to an overlooked factor: hyperparameters and activation functions.
Our analyses have shown that the accuracy drop can additionally be attributed
to (i) Using ReLU as the default choice for activation functions unanimously,
and (ii) Fine-tuning SNNs with the same hyperparameters as dense counterparts.
Thus, we focus on learning a novel way to tune activation functions for sparse
networks and combining these with a separate hyperparameter optimization (HPO)
regime for sparse networks. By conducting experiments on popular DNN models
(LeNet-5, VGG-16, ResNet-18, and EfficientNet-B0) trained on MNIST, CIFAR-10,
and ImageNet-16 datasets, we show that the novel combination of these two
approaches, dubbed Sparse Activation Function Search, short: SAFS, results in
up to 15.53%, 8.88%, and 6.33% absolute improvement in the accuracy for
LeNet-5, VGG-16, and ResNet-18 over the default training protocols, especially
at high pruning ratios. Our code can be found at https://github.com/automl/SAFS
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Causal Counterfactuals for Improving the Robustness of Reinforcement
  Learning <span class="chip">AAMAS 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2211.05551v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2211.05551v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tom He, Jasmina Gajcin, Ivana Dusparic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning (RL) is used in various robotic applications. RL
enables agents to learn tasks autonomously by interacting with the environment.
The more critical the tasks are, the higher the demand for the robustness of
the RL systems. Causal RL combines RL and causal inference to make RL more
robust. Causal RL agents use a causal representation to capture the invariant
causal mechanisms that can be transferred from one task to another. Currently,
there is limited research in Causal RL, and existing solutions are usually not
complete or feasible for real-world applications. In this work, we propose
CausalCF, the first complete Causal RL solution incorporating ideas from Causal
Curiosity and CoPhy. Causal Curiosity provides an approach for using
interventions, and CoPhy is modified to enable the RL agent to perform
counterfactuals. Causal Curiosity has been applied to robotic grasping and
manipulation tasks in CausalWorld. CausalWorld provides a realistic simulation
environment based on the TriFinger robot. We apply CausalCF to complex robotic
tasks and show that it improves the RL agent's robustness using CausalWorld.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ARMS-2023 (ARMS-2023: AAMAS 2023 Workshop on Autonomous
  Robots and Multirobot Systems)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning Meta Representations for Agents in Multi-Agent Reinforcement
  Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2108.12988v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2108.12988v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shenao Zhang, Li Shen, Lei Han, Li Shen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In multi-agent reinforcement learning, the behaviors that agents learn in a
single Markov Game (MG) are typically confined to the given agent number. Every
single MG induced by varying the population may possess distinct optimal joint
strategies and game-specific knowledge, which are modeled independently in
modern multi-agent reinforcement learning algorithms. In this work, our focus
is on creating agents that can generalize across population-varying MGs.
Instead of learning a unimodal policy, each agent learns a policy set
comprising effective strategies across a variety of games. To achieve this, we
propose Meta Representations for Agents (MRA) that explicitly models the
game-common and game-specific strategic knowledge. By representing the policy
sets with multi-modal latent policies, the game-common strategic knowledge and
diverse strategic modes are discovered through an iterative optimization
procedure. We prove that by approximately maximizing the resulting constrained
mutual information objective, the policies can reach Nash Equilibrium in every
evaluation MG when the latent space is sufficiently large. When deploying MRA
in practical settings with limited latent space sizes, fast adaptation can be
achieved by leveraging the first-order gradient information. Extensive
experiments demonstrate the effectiveness of MRA in improving training
performance and generalization ability in challenging evaluation games.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published at CoLLAs</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Prediction intervals for neural network models using weighted asymmetric
  loss functions 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.04318v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.04318v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Milo Grillo, Yunpeng Han, Agnieszka Werpachowska
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a simple and efficient approach to generate prediction intervals
(PIs) for approximated and forecasted trends. Our method leverages a weighted
asymmetric loss function to estimate the lower and upper bounds of the PIs,
with the weights determined by the interval width. We provide a concise
mathematical proof of the method, show how it can be extended to derive PIs for
parametrised functions and argue why the method works for predicting PIs of
dependent variables. The presented tests of the method on a real-world
forecasting task using a neural network-based model show that it can produce
reliable PIs in complex machine learning scenarios.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>14 pages, 3 figures, not submitted anywhere yet</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Discover and Cure: Concept-aware Mitigation of Spurious Correlation <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.00650v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.00650v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Shirley Wu, Mert Yuksekgonul, Linjun Zhang, James Zou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Deep neural networks often rely on spurious correlations to make predictions,
which hinders generalization beyond training environments. For instance, models
that associate cats with bed backgrounds can fail to predict the existence of
cats in other environments without beds. Mitigating spurious correlations is
crucial in building trustworthy models. However, the existing works lack
transparency to offer insights into the mitigation process. In this work, we
propose an interpretable framework, Discover and Cure (DISC), to tackle the
issue. With human-interpretable concepts, DISC iteratively 1) discovers
unstable concepts across different environments as spurious attributes, then 2)
intervenes on the training data using the discovered concepts to reduce
spurious correlation. Across systematic experiments, DISC provides superior
generalization ability and interpretability than the existing approaches.
Specifically, it outperforms the state-of-the-art methods on an object
recognition task and a skin-lesion classification task by 7.5% and 9.6%,
respectively. Additionally, we offer theoretical analysis and guarantees to
understand the benefits of models trained by DISC. Code and data are available
at https://github.com/Wuyxin/DISC.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating Generative Patent Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2206.14578v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2206.14578v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jieh-Sheng Lee
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generative language models are promising for assisting human writing in
various domains. This manuscript aims to build generative language models in
the patent domain and evaluate model performance from a human-centric
perspective. The perspective is to measure the ratio of keystrokes that can be
saved by autocompletion based on generative patent language models. A higher
ratio means a more effective model which can save more keystrokes. This metric
can be used to benchmark model performance. The metric is different from
conventional machine-centric metrics that are token-based instead of
keystroke-based. In terms of model size, the largest model built in this
manuscript is 6B, which is state-of-the-art in the patent domain. Based on the
metric, it is found that the largest model is not necessarily the best for the
human-centric metric. The finding means that keeping increasing model sizes in
the patent domain might be unnecessary if the purpose is to assist human
writing with autocompletion. Several patent language models are pre-trained
from scratch in this research. The pre-trained models are released for future
researchers. Several visualization tools are also provided. The importance of
building a generative language model in the patent domain is the potential to
facilitate creativity and innovations in the future.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>12 pages, 7 figures, and 5 tables</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ ClusterFuG: Clustering Fully connected Graphs by Multicut <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.12159v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.12159v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ahmed Abbas, Paul Swoboda
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a graph clustering formulation based on multicut (a.k.a. weighted
correlation clustering) on the complete graph. Our formulation does not need
specification of the graph topology as in the original sparse formulation of
multicut, making our approach simpler and potentially better performing. In
contrast to unweighted correlation clustering we allow for a more expressive
weighted cost structure. In dense multicut, the clustering objective is given
in a factorized form as inner products of node feature vectors. This allows for
an efficient formulation and inference in contrast to multicut/weighted
correlation clustering, which has at least quadratic representation and
computation complexity when working on the complete graph. We show how to
rewrite classical greedy algorithms for multicut in our dense setting and how
to modify them for greater efficiency and solution quality. In particular, our
algorithms scale to graphs with tens of thousands of nodes. Empirical evidence
on instance segmentation on Cityscapes and clustering of ImageNet datasets
shows the merits of our approach.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ICML 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Revisiting Personalized Federated Learning: Robustness Against Backdoor
  Attacks <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2302.01677v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2302.01677v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zeyu Qin, Liuyi Yao, Daoyuan Chen, Yaliang Li, Bolin Ding, Minhao Cheng
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this work, besides improving prediction accuracy, we study whether
personalization could bring robustness benefits to backdoor attacks. We conduct
the first study of backdoor attacks in the pFL framework, testing 4 widely used
backdoor attacks against 6 pFL methods on benchmark datasets FEMNIST and
CIFAR-10, a total of 600 experiments. The study shows that pFL methods with
partial model-sharing can significantly boost robustness against backdoor
attacks. In contrast, pFL methods with full model-sharing do not show
robustness. To analyze the reasons for varying robustness performances, we
provide comprehensive ablation studies on different pFL methods. Based on our
findings, we further propose a lightweight defense method, Simple-Tuning, which
empirically improves defense performance against backdoor attacks. We believe
that our work could provide both guidance for pFL application in terms of its
robustness and offer valuable insights to design more robust FL methods in the
future. We open-source our code to establish the first benchmark for black-box
backdoor attacks in pFL:
https://github.com/alibaba/FederatedScope/tree/backdoor-bench.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond the Meta: Leveraging Game Design Parameters for Patch-Agnostic
  Esport Analytics 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18477v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18477v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alan Pedrassoli Chitayat, Florian Block, James Walker, Anders Drachen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Esport games comprise a sizeable fraction of the global games market, and is
the fastest growing segment in games. This has given rise to the domain of
esports analytics, which uses telemetry data from games to inform players,
coaches, broadcasters and other stakeholders. Compared to traditional sports,
esport titles change rapidly, in terms of mechanics as well as rules. Due to
these frequent changes to the parameters of the game, esport analytics models
can have a short life-spam, a problem which is largely ignored within the
literature. This paper extracts information from game design (i.e. patch notes)
and utilises clustering techniques to propose a new form of character
representation. As a case study, a neural network model is trained to predict
the number of kills in a Dota 2 match utilising this novel character
representation technique. The performance of this model is then evaluated
against two distinct baselines, including conventional techniques. Not only did
the model significantly outperform the baselines in terms of accuracy (85%
AUC), but the model also maintains the accuracy in two newer iterations of the
game that introduced one new character and a brand new character type. These
changes introduced to the design of the game would typically break conventional
techniques that are commonly used within the literature. Therefore, the
proposed methodology for representing characters can increase the life-spam of
machine learning models as well as contribute to a higher performance when
compared to traditional techniques typically employed within the literature.
</span>
                                    </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>
    <section class="day-container">
        <div class="date">
            <time datetime="2023-06-04T00:00:00Z">2023-06-04</time>
        </div>
            <article>
                <details>
                    <Summary>
                        Computation and Language <span class="chip" style="font-size: 60%">47</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ RadLing: Towards Efficient Radiology Report Understanding <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02492v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02492v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Rikhiya Ghosh, Sanjeev Kumar Karn, Manuela Daniela Danu, Larisa Micu, Ramya Vunikili, Oladimeji Farri
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Most natural language tasks in the radiology domain use language models
pre-trained on biomedical corpus. There are few pretrained language models
trained specifically for radiology, and fewer still that have been trained in a
low data setting and gone on to produce comparable results in fine-tuning
tasks. We present RadLing, a continuously pretrained language model using
Electra-small (Clark et al., 2020) architecture, trained using over 500K
radiology reports, that can compete with state-of-the-art results for fine
tuning tasks in radiology domain. Our main contribution in this paper is
knowledge-aware masking which is a taxonomic knowledge-assisted pretraining
task that dynamically masks tokens to inject knowledge during pretraining. In
addition, we also introduce an knowledge base-aided vocabulary extension to
adapt the general tokenization vocabulary to radiology domain.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Association for Computational Linguistics (ACL), 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modeling Cross-Cultural Pragmatic Inference with Codenames Duet <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02475v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02475v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Shaikh, Caleb Ziems, William Held, Aryan J. Pariani, Fred Morstatter, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pragmatic reference enables efficient interpersonal communication. Prior work
uses simple reference games to test models of pragmatic reasoning, often with
unidentified speakers and listeners. In practice, however, speakers'
sociocultural background shapes their pragmatic assumptions. For example,
readers of this paper assume NLP refers to "Natural Language Processing," and
not "Neuro-linguistic Programming." This work introduces the Cultural Codes
dataset, which operationalizes sociocultural pragmatic inference in a simple
word reference game.
  Cultural Codes is based on the multi-turn collaborative two-player game,
Codenames Duet. Our dataset consists of 794 games with 7,703 turns, distributed
across 153 unique players. Alongside gameplay, we collect information about
players' personalities, values, and demographics. Utilizing theories of
communication and pragmatics, we predict each player's actions via joint
modeling of their sociocultural priors and the game context. Our experiments
show that accounting for background characteristics significantly improves
model performance for tasks related to both clue giving and guessing,
indicating that sociocultural priors play a vital role in gameplay decisions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Adaptive and Personalized Exercise Generation for Online Language
  Learning <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02457v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02457v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Peng Cui, Mrinmaya Sachan
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive learning aims to provide customized educational activities (e.g.,
exercises) to address individual learning needs. However, manual construction
and delivery of such activities is a laborious process. Thus, in this paper, we
study a novel task of adaptive and personalized exercise generation for online
language learning. To this end, we combine a knowledge tracing model that
estimates each student's evolving knowledge states from their learning history
and a controlled text generation model that generates exercise sentences based
on the student's current estimated knowledge state and instructor requirements
of desired properties (e.g., domain knowledge and difficulty). We train and
evaluate our model on real-world learner interaction data from Duolingo and
demonstrate that LMs guided by student states can generate superior exercises.
Then, we discuss the potential use of our model in educational applications
using various simulations. These simulations show that our model can adapt to
students' individual abilities and can facilitate their learning efficiency by
personalizing learning sequences.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Taught by the Internet, Exploring Bias in OpenAIs <span class="highlight-title">GPT</span>3 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02428v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02428v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ali Ayaz, Aditya Nawalgaria, Ruilian Yin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This research delves into the current literature on bias in Natural Language
Processing Models and the techniques proposed to mitigate the problem of bias,
including why it is important to tackle bias in the first place. Additionally,
these techniques are further analysed in the light of newly developed models
that tower in size over past editions. To achieve those aims, the authors of
this paper conducted their research on GPT3 by OpenAI, the largest NLP model
available to consumers today. With 175 billion parameters in contrast to BERTs
340 million, GPT3 is the perfect model to test the common pitfalls of NLP
models. Tests were conducted through the development of an Applicant Tracking
System using GPT3. For the sake of feasibility and time constraints, the tests
primarily focused on gender bias, rather than all or multiple types of bias.
Finally, current mitigation techniques are considered and tested to measure
their degree of functionality.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evaluating and Improving Tool-Augmented Computation-Intensive Math
  Reasoning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02408v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02408v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Beichen Zhang, Kun Zhou, Xilin Wei, Wayne Xin Zhao, Jing Sha, Shijin Wang, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Chain-of-thought prompting~(CoT) and tool augmentation have been validated in
recent work as effective practices for improving large language models~(LLMs)
to perform step-by-step reasoning on complex math-related tasks. However, most
existing math reasoning datasets may be not able to fully evaluate and analyze
the ability of LLMs in manipulating tools and performing reasoning, as they may
only require very few invocations of tools or miss annotations for evaluating
intermediate reasoning steps. To address the issue, we construct \textbf{CARP},
a new Chinese dataset consisting of 4,886 computation-intensive algebra
problems with formulated annotations on intermediate steps. In CARP, we test
four LLMs with CoT prompting, and find that they are all prone to make mistakes
at the early steps of the solution, leading to wrong answers. Based on this
finding, we propose a new approach that can deliberate the reasoning steps with
tool interfaces, namely \textbf{DELI}. In DELI, we first initialize a
step-by-step solution based on retrieved exemplars, then iterate two
deliberation procedures that check and refine the intermediate steps of the
generated solution, from the perspectives of tool manipulation and natural
language reasoning, until obtaining converged solutions or reaching the maximum
turn. Experimental results on CARP and six other datasets show that the
proposed DELI mostly outperforms competitive baselines, and can further boost
the performance of existing CoT methods. Our data and code are available in
\url{https://github.com/RUCAIBox/CARP}.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>17 pages, working in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Information-Theoretic Analysis of <span class="highlight-title">Self-supervised</span> Discrete
  Representations of Speech 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02405v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02405v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Badr M. Abdullah, Mohammed Maqsood Shaik, Bernd Möbius, Dietrich Klakow
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Self-supervised representation learning for speech often involves a
quantization step that transforms the acoustic input into discrete units.
However, it remains unclear how to characterize the relationship between these
discrete units and abstract phonetic categories such as phonemes. In this
paper, we develop an information-theoretic framework whereby we represent each
phonetic category as a distribution over discrete units. We then apply our
framework to two different self-supervised models (namely wav2vec 2.0 and XLSR)
and use American English speech as a case study. Our study demonstrates that
the entropy of phonetic distributions reflects the variability of the
underlying speech sounds, with phonetically similar sounds exhibiting similar
distributions. While our study confirms the lack of direct, one-to-one
correspondence, we find an intriguing, indirect relationship between phonetic
categories and discrete units.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Commonsense Knowledge Transfer for <span class="highlight-title">Pre-train</span>ed Language Models <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02388v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02388v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangchunshu Zhou, Ronan Le Bras, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Despite serving as the foundation models for a wide range of NLP benchmarks,
pre-trained language models have shown limited capabilities of acquiring
implicit commonsense knowledge from self-supervision alone, compared to
learning linguistic and factual knowledge that appear more explicitly in the
surface patterns in text. In this work, we introduce commonsense knowledge
transfer, a framework to transfer the commonsense knowledge stored in a neural
commonsense knowledge model to a general-purpose pre-trained language model. It
first exploits general texts to form queries for extracting commonsense
knowledge from the neural commonsense knowledge model and then refines the
language model with two self-supervised objectives: commonsense mask infilling
and commonsense relation prediction, which align human language with the
underlying commonsense knowledge. Empirical results show that our approach
consistently improves the model's performance on downstream tasks that require
commonsense reasoning. Moreover, we find that the improvement is more
significant in the few-shot setting. This suggests that our approach helps
language models better transfer to downstream tasks without extensive
supervision by injecting commonsense knowledge into their parameters.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Evolution of Efficient Symbolic Communication Codes 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02383v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02383v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Anton Kolonin
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The paper explores how the human natural language structure can be seen as a
product of evolution of inter-personal communication code, targeting
maximisation of such culture-agnostic and cross-lingual metrics such as
anti-entropy, compression factor and cross-split F1 score. The exploration is
done as part of a larger unsupervised language learning effort, the attempt is
made to perform meta-learning in a space of hyper-parameters maximising F1
score based on the "ground truth" language structure, by means of maximising
the metrics mentioned above. The paper presents preliminary results of
cross-lingual word-level segmentation tokenisation study for Russian, Chinese
and English as well as subword segmentation or morphological parsing study for
English. It is found that language structure form the word-level segmentation
or tokenisation can be found as driven by all of these metrics, anti-entropy
being more relevant to English and Russian while compression factor more
specific for Chinese. The study for subword segmentation or morphological
parsing on English lexicon has revealed straight connection between the
compression been found to be associated with compression factor, while,
surprising, the same connection with anti-entropy has turned to be the inverse.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>9 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Modular <span class="highlight-title">Transformer</span>s: Compressing <span class="highlight-title">Transformer</span>s into Modularized Layers
  for Flexible Efficient Inference <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02379v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02379v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Wangchunshu Zhou, Ronan Le Bras, Yejin Choi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Pre-trained Transformer models like T5 and BART have advanced the state of
the art on a wide range of text generation tasks. Compressing these models into
smaller ones has become critically important for practical use. Common neural
network compression techniques such as knowledge distillation or quantization
are limited to static compression where the compression ratio is fixed. In this
paper, we introduce Modular Transformers, a modularized encoder-decoder
framework for flexible sequence-to-sequence model compression. Modular
Transformers train modularized layers that have the same function of two or
more consecutive layers in the original model via module replacing and
knowledge distillation. After training, the modularized layers can be flexibly
assembled into sequence-to-sequence models that meet different
performance-efficiency trade-offs. Experimental results show that after a
single training phase, by simply varying the assembling strategy, Modular
Transformers can achieve flexible compression ratios from 1.1x to 6x with
little to moderate relative performance drop.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 Findings</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ "Are you telling me to put glasses on the dog?'' Content-Grounded
  Annotation of Instruction Clarification Requests in the CoDraw <span class="highlight-title">Dataset</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02377v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02377v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Brielen Madureira, David Schlangen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Instruction Clarification Requests are a mechanism to solve communication
problems, which is very functional in instruction-following interactions.
Recent work has argued that the CoDraw dataset is a valuable source of
naturally occurring iCRs. Beyond identifying when iCRs should be made, dialogue
models should also be able to generate them with suitable form and content. In
this work, we introduce CoDraw-iCR (v2), which extends the existing iCR
identifiers fine-grained information grounded in the underlying dialogue game
items and possible actions. Our annotation can serve to model and evaluate
repair capabilities of dialogue agents.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Work in progress</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ bgGLUE: A Bulgarian General Language Understanding Evaluation Benchmark <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02349v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02349v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Momchil Hardalov, Pepa Atanasova, Todor Mihaylov, Galia Angelova, Kiril Simov, Petya Osenova, Ves Stoyanov, Ivan Koychev, Preslav Nakov, Dragomir Radev
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present bgGLUE (Bulgarian General Language Understanding Evaluation), a
benchmark for evaluating language models on Natural Language Understanding
(NLU) tasks in Bulgarian. Our benchmark includes NLU tasks targeting a variety
of NLP problems (e.g., natural language inference, fact-checking, named entity
recognition, sentiment analysis, question answering, etc.) and machine learning
tasks (sequence labeling, document-level classification, and regression). We
run the first systematic evaluation of pre-trained language models for
Bulgarian, comparing and contrasting results across the nine tasks in the
benchmark. The evaluation results show strong performance on sequence labeling
tasks, but there is a lot of room for improvement for tasks that require more
complex reasoning. We make bgGLUE publicly available together with the
fine-tuning and the evaluation code, as well as a public leaderboard at
https://bgglue.github.io/, and we hope that it will enable further advancements
in developing NLU models for Bulgarian.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2023 (Main Conference)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Leverage Points in Modality Shifts: Comparing Language-only and
  Multimodal Word Representations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02348v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02348v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Aleksey Tikhonov, Lisa Bylinina, Denis Paperno
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Multimodal embeddings aim to enrich the semantic information in neural
representations of language compared to text-only models. While different
embeddings exhibit different applicability and performance on downstream tasks,
little is known about the systematic representation differences attributed to
the visual modality. Our paper compares word embeddings from three
vision-and-language models (CLIP, OpenCLIP and Multilingual CLIP) and three
text-only models, with static (FastText) as well as contextual representations
(multilingual BERT; XLM-RoBERTa). This is the first large-scale study of the
effect of visual grounding on language representations, including 46 semantic
parameters. We identify meaning properties and relations that characterize
words whose embeddings are most affected by the inclusion of visual modality in
the training data; that is, points where visual grounding turns out most
important. We find that the effect of visual modality correlates most with
denotational semantic properties related to concreteness, but is also detected
for several specific semantic classes, as well as for valence, a
sentiment-related connotational property of linguistic expressions.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted for StarSEM 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Long Text Generation Challenge 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02334v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02334v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nikolay Mikhaylovskiy
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a shared task of human-like long text generation, LTG Challenge,
that asks models to output a consistent human-like long text (a Harry Potter
generic audience fanfic in English), given a prompt of about 1000 tokens. We
suggest a novel statistical metric of the text structuredness, GloVe
Autocorrelations Power/ Exponential Law Mean Absolute Percentage Error Ratio
(GAPELMAPER) and a human evaluation protocol. We hope that LTG can open new
avenues for researchers to investigate sampling approaches, prompting
strategies, autoregressive and non-autoregressive text generation architectures
and break the barrier to generate consistent long (40K+ token) texts.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Submitted to INLG 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Arbitrary Few Parameters are Good Enough for Adapting Large-scale
  <span class="highlight-title">Pre-train</span>ed Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02320v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02320v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yusheng Su, Chi-Min Chan, Jiali Cheng, Yujia Qin, Yankai Lin, Shengding Hu, Zonghan Yang, Ning Ding, Zhiyuan Liu, Maosong Sun
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Parameter-efficient tuning (PET) methods can effectively drive extremely
large pre-trained language models (PLMs) by only training minimal parameters.
Different PET methods utilize different manually designed modules. In a small
PLM, there are usually noticeable performance differences among PET methods.
Nevertheless, when a PLM's scale grows up to tens of billions of parameters,
all PET methods achieve almost the same performance and even perform on par
with the full-parameter fine-tuning method. Hence, we hypothesize that model
scaling can mitigate the design differences (the module structures and the
number of trainable parameters) among PET methods. To study this hypothesis, we
introduce a more flexible PET method - arbitrary PET (APET) method - to be
compatible with arbitrary module structures and any number of trainable
parameters. Then, we experiment on $11$ NLP tasks of $5$ types and $2$
representative PLMs. From our investigations, we find that the model scaling
(1) mitigates the effects of the arbitrary module structure on the performance
of tuning methods, and (2) enables the tuning methods to optimize fewer
parameters to achieve the full-parameter fine-tuning performance. Intriguingly,
we also observe that all tuning methods require almost the same number of
trainable parameters to drive PLMs. We discuss this phenomenon and the above
two findings collectively from optimization perspectives to fathom the
mechanisms behind them. These conclusions not only demonstrate the positive
impact of model scaling on tuning methods but disclose its mechanisms, which
help us design more effective and efficient tuning methods on larger-scale
PLMs.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ SpellMapper: A non-autoregressive neural spellchecker for ASR
  customization with candidate retrieval based on n-gram mappings <span class="chip">INTERSPEECH 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02317v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02317v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Alexandra Antonova, Evelina Bakhturina, Boris Ginsburg
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contextual spelling correction models are an alternative to shallow fusion to
improve automatic speech recognition (ASR) quality given user vocabulary. To
deal with large user vocabularies, most of these models include candidate
retrieval mechanisms, usually based on minimum edit distance between fragments
of ASR hypothesis and user phrases. However, the edit-distance approach is
slow, non-trainable, and may have low recall as it relies only on common
letters. We propose: 1) a novel algorithm for candidate retrieval, based on
misspelled n-gram mappings, which gives up to 90% recall with just the top 10
candidates on Spoken Wikipedia; 2) a non-autoregressive neural model based on
BERT architecture, where the initial transcript and ten candidates are combined
into one input. The experiments on Spoken Wikipedia show 21.4% word error rate
improvement compared to a baseline ASR system.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by INTERSPEECH 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Finding the SWEET Spot: Analysis and Improvement of Adaptive Inference
  in Low Resource Settings <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02307v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02307v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Daniel Rotem, Michael Hassid, Jonathan Mamou, Roy Schwartz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Adaptive inference is a simple method for reducing inference costs. The
method works by maintaining multiple classifiers of different capacities, and
allocating resources to each test instance according to its difficulty. In this
work, we compare the two main approaches for adaptive inference, Early-Exit and
Multi-Model, when training data is limited. First, we observe that for models
with the same architecture and size, individual Multi-Model classifiers
outperform their Early-Exit counterparts by an average of 2.3%. We show that
this gap is caused by Early-Exit classifiers sharing model parameters during
training, resulting in conflicting gradient updates of model weights. We find
that despite this gap, Early-Exit still provides a better speed-accuracy
trade-off due to the overhead of the Multi-Model approach. To address these
issues, we propose SWEET (Separating Weights in Early Exit Transformers), an
Early-Exit fine-tuning method that assigns each classifier its own set of
unique model weights, not updated by other classifiers. We compare SWEET's
speed-accuracy curve to standard Early-Exit and Multi-Model baselines and find
that it outperforms both methods at fast speeds while maintaining comparable
scores to Early-Exit at slow speeds. Moreover, SWEET individual classifiers
outperform Early-Exit ones by 1.1% on average. SWEET enjoys the benefits of
both methods, paving the way for further reduction of inference costs in NLP.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Does Character-level Information Always Improve DRS-based Semantic
  Parsing? <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02302v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02302v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tomoya Kurosawa, Hitomi Yanaka
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Even in the era of massive language models, it has been suggested that
character-level representations improve the performance of neural models. The
state-of-the-art neural semantic parser for Discourse Representation Structures
uses character-level representations, improving performance in the four
languages (i.e., English, German, Dutch, and Italian) in the Parallel Meaning
Bank dataset. However, how and why character-level information improves the
parser's performance remains unclear. This study provides an in-depth analysis
of performance changes by order of character sequences. In the experiments, we
compare F1-scores by shuffling the order and randomizing character sequences
after testing the performance of character-level information. Our results
indicate that incorporating character-level information does not improve the
performance in English and German. In addition, we find that the parser is not
sensitive to correct character order in Dutch. Nevertheless, performance
improvements are observed when using character-level information.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>10 pages. To appear in the 12th Joint Conference on Lexical and
  Computational Semantics (*SEM 2023) with ACL2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Mathematical Abstraction for Balancing the Trade-off Between
  Creativity and Reality in Large Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02295v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02295v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ritwik Sinha, Zhao Song, Tianyi Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large Language Models have become popular for their remarkable capabilities
in human-oriented tasks and traditional natural language processing tasks. Its
efficient functioning is attributed to the attention mechanism in the
Transformer architecture, enabling it to concentrate on particular aspects of
the input.
  LLMs are increasingly being used in domains such as generating prose, poetry
or art, which require the model to be creative (e.g. Adobe firefly). LLMs
possess advanced language generation abilities that enable them to generate
distinctive and captivating content. This utilization of LLMs in generating
narratives shows their flexibility and potential for use in domains that extend
beyond conventional natural language processing duties.
  In different contexts, we may expect the LLM to generate factually correct
answers, that match reality; e.g., question-answering systems or online
assistants. In such situations, being correct is critical to LLMs being trusted
in practice. The Bing Chatbot provides its users with the flexibility to select
one of the three output modes: creative, balanced, and precise. Each mode
emphasizes creativity and factual accuracy differently.
  In this work, we provide a mathematical abstraction to describe creativity
and reality based on certain losses. A model trained on these losses balances
the trade-off between the creativity and reality of the model.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exposing Bias in Online Communities through Large-Scale Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02294v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02294v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Celine Wald, Lukas Pfahler
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Progress in natural language generation research has been shaped by the
ever-growing size of language models. While large language models pre-trained
on web data can generate human-sounding text, they also reproduce social biases
and contribute to the propagation of harmful stereotypes. This work utilises
the flaw of bias in language models to explore the biases of six different
online communities. In order to get an insight into the communities'
viewpoints, we fine-tune GPT-Neo 1.3B with six social media datasets. The bias
of the resulting models is evaluated by prompting the models with different
demographics and comparing the sentiment and toxicity values of these
generations. Together, these methods reveal that bias differs in type and
intensity for the various models. This work not only affirms how easily bias is
absorbed from training data but also presents a scalable method to identify and
compare the bias of different datasets or communities. Additionally, the
examples generated for this work demonstrate the limitations of using automated
sentiment and toxicity classifiers in bias research.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Exploring and Verbalizing Academic Ideas by Concept Co-occurrence <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02282v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02282v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yi Xu, Shuqian Sheng, Bo Xue, Luoyi Fu, Xinbing Wang, Chenghu Zhou
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Researchers usually come up with new ideas only after thoroughly
comprehending vast quantities of literature. The difficulty of this procedure
is exacerbated by the fact that the number of academic publications is growing
exponentially. In this study, we devise a framework based on concept
co-occurrence for academic idea inspiration, which has been integrated into a
research assistant system. From our perspective, the fusion of two concepts
that co-occur in an academic paper can be regarded as an important way of the
emergence of a new idea. We construct evolving concept graphs according to the
co-occurrence relationship of concepts from 20 disciplines or topics. Then we
design a temporal link prediction method based on masked language model to
explore potential connections between different concepts. To verbalize the
newly discovered connections, we also utilize the pretrained language model to
generate a description of an idea based on a new data structure called
co-occurrence citation quintuple. We evaluate our proposed system using both
automatic metrics and human assessment. The results demonstrate that our system
has broad prospects and can assist researchers in expediting the process of
discovering new ideas.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ End-to-End Joint Target and Non-Target Speakers ASR 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02273v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02273v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ryo Masumura, Naoki Makishima, Taiga Yamane, Yoshihiko Yamazaki, Saki Mizuno, Mana Ihori, Mihiro Uchida, Keita Suzuki, Hiroshi Sato, Tomohiro Tanaka, Akihiko Takashima, Satoshi Suzuki, Takafumi Moriya, Nobukatsu Hojo, Atsushi Ando
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This paper proposes a novel automatic speech recognition (ASR) system that
can transcribe individual speaker's speech while identifying whether they are
target or non-target speakers from multi-talker overlapped speech.
Target-speaker ASR systems are a promising way to only transcribe a target
speaker's speech by enrolling the target speaker's information. However, in
conversational ASR applications, transcribing both the target speaker's speech
and non-target speakers' ones is often required to understand interactive
information. To naturally consider both target and non-target speakers in a
single ASR model, our idea is to extend autoregressive modeling-based
multi-talker ASR systems to utilize the enrollment speech of the target
speaker. Our proposed ASR is performed by recursively generating both textual
tokens and tokens that represent target or non-target speakers. Our experiments
demonstrate the effectiveness of our proposed method.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at Interspeech 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ OWQ: Lessons learned from activation outliers for weight quantization in
  large language models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02272v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02272v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Changhun Lee, Jungyu Jin, Taesu Kim, Hyungjun Kim, Eunhyeok Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large language models (LLMs) with hundreds of billions of parameters show
impressive results across various language tasks using simple prompt tuning and
few-shot examples, without the need for task-specific fine-tuning. However,
their enormous size requires multiple server-grade GPUs even for inference,
creating a significant cost barrier. To address this limitation, we introduce a
novel post-training quantization method for weights with minimal quality
degradation. While activation outliers are known to be problematic in
activation quantization, our theoretical analysis suggests that we can identify
factors contributing to weight quantization errors by considering activation
outliers. We propose an innovative PTQ scheme called outlier-aware weight
quantization (OWQ), which identifies vulnerable weights and allocates
high-precision to them. Our extensive experiments demonstrate that the 3.01-bit
models produced by OWQ exhibit comparable quality to the 4-bit models generated
by OPTQ.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Probing Physical Reasoning with Counter-Commonsense Context <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02258v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02258v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kazushi Kondo, Saku Sugawara, Akiko Aizawa
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this study, we create a CConS (Counter-commonsense Contextual Size
comparison) dataset to investigate how physical commonsense affects the
contextualized size comparison task; the proposed dataset consists of both
contexts that fit physical commonsense and those that do not. This dataset
tests the ability of language models to predict the size relationship between
objects under various contexts generated from our curated noun list and
templates. We measure the ability of several masked language models and
generative models. The results show that while large language models can use
prepositions such as ``in'' and ``into'' in the provided context to infer size
relationships, they fail to use verbs and thus make incorrect judgments led by
their prior physical commonsense.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2023(Short Paper)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Technical Report for Polyglot-Ko: Open-Source Large-Scale Korean
  Language Models 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02254v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02254v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hyunwoong Ko, Kichang Yang, Minho Ryu, Taekyoon Choi, Seungmu Yang, jiwung Hyun, Sungho Park
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Polyglot is a pioneering project aimed at enhancing the non-English language
performance of multilingual language models. Despite the availability of
various multilingual models such as mBERT (Devlin et al., 2019), XGLM (Lin et
al., 2022), and BLOOM (Scao et al., 2022), researchers and developers often
resort to building monolingual models in their respective languages due to the
dissatisfaction with the current multilingual models non-English language
capabilities. Addressing this gap, we seek to develop advanced multilingual
language models that offer improved performance in non-English languages. In
this paper, we introduce the Polyglot Korean models, which represent a specific
focus rather than being multilingual in nature. In collaboration with TUNiB,
our team collected 1.2TB of Korean data meticulously curated for our research
journey. We made a deliberate decision to prioritize the development of Korean
models before venturing into multilingual models. This choice was motivated by
multiple factors: firstly, the Korean models facilitated performance
comparisons with existing multilingual models; and finally, they catered to the
specific needs of Korean companies and researchers. This paper presents our
work in developing the Polyglot Korean models, which propose some steps towards
addressing the non-English language performance gap in multilingual language
models.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Large Language Model Augmented Narrative Driven Recommendations 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02250v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02250v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Sheshera Mysore, Andrew McCallum, Hamed Zamani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Narrative-driven recommendation (NDR) presents an information access problem
where users solicit recommendations with verbose descriptions of their
preferences and context, for example, travelers soliciting recommendations for
points of interest while describing their likes/dislikes and travel
circumstances. These requests are increasingly important with the rise of
natural language-based conversational interfaces for search and recommendation
systems. However, NDR lacks abundant training data for models, and current
platforms commonly do not support these requests. Fortunately, classical
user-item interaction datasets contain rich textual data, e.g., reviews, which
often describe user preferences and context - this may be used to bootstrap
training for NDR models. In this work, we explore using large language models
(LLMs) for data augmentation to train NDR models. We use LLMs for authoring
synthetic narrative queries from user-item interactions with few-shot prompting
and train retrieval models for NDR on synthetic queries and user-item
interaction data. Our experiments demonstrate that this is an effective
strategy for training small-parameter retrieval models that outperform other
retrieval and LLM baselines for narrative-driven recommendation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Pre-print</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Sen2Pro: A Probabilistic Perspective to Sentence Embedding from
  <span class="highlight-title">Pre-train</span>ed Language Model <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02247v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02247v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lingfeng Shen, Haiyun Jiang, Lemao Liu, Shuming Shi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Sentence embedding is one of the most fundamental tasks in Natural Language
Processing and plays an important role in various tasks. The recent
breakthrough in sentence embedding is achieved by pre-trained language models
(PLMs). Despite its success, an embedded vector (Sen2Vec) representing a point
estimate does not naturally express uncertainty in a taskagnostic way. This
paper thereby proposes an efficient framework on probabilistic sentence
embedding (Sen2Pro) from PLMs, and it represents a sentence as a probability
density distribution in an embedding space to reflect both model uncertainty
and data uncertainty (i.e., many-to-one nature) in the sentence representation.
The proposed framework performs in a plug-and-play way without retraining PLMs
anymore, and it is easy to implement and generally applied on top of any PLM.
The superiority of Sen2Pro over Sen2Vec has been theoretically verified and
practically illustrated on different NLP tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL2023 workshop Rep4NLP</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Extract and Attend: Improving Entity Translation in Neural Machine
  Translation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02242v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02242v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Zixin Zeng, Rui Wang, Yichong Leng, Junliang Guo, Xu Tan, Tao Qin, Tie-yan Liu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While Neural Machine Translation(NMT) has achieved great progress in recent
years, it still suffers from inaccurate translation of entities (e.g.,
person/organization name, location), due to the lack of entity training
instances. When we humans encounter an unknown entity during translation, we
usually first look up in a dictionary and then organize the entity translation
together with the translations of other parts to form a smooth target sentence.
Inspired by this translation process, we propose an Extract-and-Attend approach
to enhance entity translation in NMT, where the translation candidates of
source entities are first extracted from a dictionary and then attended to by
the NMT model to generate the target sentence. Specifically, the translation
candidates are extracted by first detecting the entities in a source sentence
and then translating the entities through looking up in a dictionary. Then, the
extracted candidates are added as a prefix of the decoder input to be attended
to by the decoder when generating the target sentence through self-attention.
Experiments conducted on En-Zh and En-Ru demonstrate that the proposed method
is effective on improving both the translation accuracy of entities and the
overall translation quality, with up to 35% reduction on entity error rate and
0.85 gain on BLEU and 13.8 gain on COMET.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Fine-Tuning Language Models with Advantage-Induced Policy Alignment 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02231v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02231v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Banghua Zhu, Hiteshi Sharma, Felipe Vieira Frujeri, Shi Dong, Chenguang Zhu, Michael I. Jordan, Jiantao Jiao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Reinforcement learning from human feedback (RLHF) has emerged as a reliable
approach to aligning large language models (LLMs) to human preferences. Among
the plethora of RLHF techniques, proximal policy optimization (PPO) is of the
most widely used methods. Despite its popularity, however, PPO may suffer from
mode collapse, instability, and poor sample efficiency. We show that these
issues can be alleviated by a novel algorithm that we refer to as
Advantage-Induced Policy Alignment (APA), which leverages a squared error loss
function based on the estimated advantages. We demonstrate empirically that APA
consistently outperforms PPO in language tasks by a large margin, when a
separate reward model is employed as the evaluator. In addition, compared with
PPO, APA offers a more stable form of control over the deviation from the
model's initial policy, ensuring that the model improves its performance
without collapsing to deterministic output. In addition to empirical results,
we also provide a theoretical justification supporting the design of our loss
function.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Paraphrastic Representations at Scale <span class="chip">EMNLP 2022</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.15114v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.15114v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Wieting, Kevin Gimpel, Graham Neubig, Taylor Berg-Kirkpatrick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We present a system that allows users to train their own state-of-the-art
paraphrastic sentence representations in a variety of languages. We also
release trained models for English, Arabic, German, French, Spanish, Russian,
Turkish, and Chinese. We train these models on large amounts of data, achieving
significantly improved performance from the original papers proposing the
methods on a suite of monolingual semantic similarity, cross-lingual semantic
similarity, and bitext mining tasks. Moreover, the resulting models surpass all
prior work on unsupervised semantic textual similarity, significantly
outperforming even BERT-based models like Sentence-BERT (Reimers and Gurevych,
2019). Additionally, our models are orders of magnitude faster than prior work
and can be used on CPU with little difference in inference speed (even improved
speed over GPU when using more CPU cores), making these models an attractive
choice for users without access to GPUs or for use on embedded devices.
Finally, we add significantly increased functionality to the code bases for
training paraphrastic sentence models, easing their use for both inference and
for training them for any desired language with parallel data. We also include
code to automatically download and preprocess training data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a demo paper at EMNLP 2022</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Beyond Contrastive Learning: A Variational Generative Model for
  Multilingual Retrieval <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.10726v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.10726v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        John Wieting, Jonathan H. Clark, William W. Cohen, Graham Neubig, Taylor Berg-Kirkpatrick
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Contrastive learning has been successfully used for retrieval of semantically
aligned sentences, but it often requires large batch sizes or careful
engineering to work well. In this paper, we instead propose a generative model
for learning multilingual text embeddings which can be used to retrieve or
score sentence pairs. Our model operates on parallel data in $N$ languages and,
through an approximation we introduce, efficiently encourages source separation
in this multilingual setting, separating semantic information that is shared
between translations from stylistic or language-specific variation. We show
careful large-scale comparisons between contrastive and generation-based
approaches for learning multilingual text embeddings, a comparison that has not
been done to the best of our knowledge despite the popularity of these
approaches. We evaluate this method on a suite of tasks including semantic
similarity, bitext mining, and cross-lingual question retrieval -- the last of
which we introduce in this paper. Overall, our Variational Multilingual
Source-Separation Transformer (VMSST) model outperforms both a strong
contrastive and generative baseline on these tasks.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Published as a long paper at ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Evaluating natural language processing models with generalization
  metrics that do not need access to any training or testing data 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2202.02842v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2202.02842v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yaoqing Yang, Ryan Theisen, Liam Hodgkinson, Joseph E. Gonzalez, Kannan Ramchandran, Charles H. Martin, Michael W. Mahoney
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Selecting suitable architecture parameters and training hyperparameters is
essential for enhancing machine learning (ML) model performance. Several recent
empirical studies conduct large-scale correlational analysis on neural networks
(NNs) to search for effective \emph{generalization metrics} that can guide this
type of model selection. Effective metrics are typically expected to correlate
strongly with test performance. In this paper, we expand on prior analyses by
examining generalization-metric-based model selection with the following
objectives: (i) focusing on natural language processing (NLP) tasks, as prior
work primarily concentrates on computer vision (CV) tasks; (ii) considering
metrics that directly predict \emph{test error} instead of the
\emph{generalization gap}; (iii) exploring metrics that do not need access to
data to compute. From these objectives, we are able to provide the first model
selection results on large pretrained Transformers from Huggingface using
generalization metrics. Our analyses consider (I) hundreds of Transformers
trained in different settings, in which we systematically vary the amount of
data, the model size and the optimization hyperparameters, (II) a total of 51
pretrained Transformers from eight families of Huggingface NLP models,
including GPT2, BERT, etc., and (III) a total of 28 existing and novel
generalization metrics. Despite their niche status, we find that metrics
derived from the heavy-tail (HT) perspective are particularly useful in NLP
tasks, exhibiting stronger correlations than other, more popular metrics. To
further examine these metrics, we extend prior formulations relying on power
law (PL) spectral distributions to exponential (EXP) and
exponentially-truncated power law (E-TPL) families.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Heterformer: <span class="highlight-title">Transformer</span>-based Deep Node Representation Learning on
  Heterogeneous Text-Rich Networks <span class="chip">KDD 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.10282v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.10282v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Bowen Jin, Yu Zhang, Qi Zhu, Jiawei Han
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Representation learning on networks aims to derive a meaningful vector
representation for each node, thereby facilitating downstream tasks such as
link prediction, node classification, and node clustering. In heterogeneous
text-rich networks, this task is more challenging due to (1) presence or
absence of text: Some nodes are associated with rich textual information, while
others are not; (2) diversity of types: Nodes and edges of multiple types form
a heterogeneous network structure. As pretrained language models (PLMs) have
demonstrated their effectiveness in obtaining widely generalizable text
representations, a substantial amount of effort has been made to incorporate
PLMs into representation learning on text-rich networks. However, few of them
can jointly consider heterogeneous structure (network) information as well as
rich textual semantic information of each node effectively. In this paper, we
propose Heterformer, a Heterogeneous Network-Empowered Transformer that
performs contextualized text encoding and heterogeneous structure encoding in a
unified model. Specifically, we inject heterogeneous structure information into
each Transformer layer when encoding node texts. Meanwhile, Heterformer is
capable of characterizing node/edge type heterogeneity and encoding nodes with
or without texts. We conduct comprehensive experiments on three tasks (i.e.,
link prediction, node classification, and node clustering) on three large-scale
datasets from different domains, where Heterformer outperforms competitive
baselines significantly and consistently.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>KDD 2023. (Code: https://github.com/PeterGriffinJin/Heterformer)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Second Thought, Let's Not Think Step by Step! Bias and Toxicity in
  Zero-Shot Reasoning <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2212.08061v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2212.08061v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Omar Shaikh, Hongxin Zhang, William Held, Michael Bernstein, Diyi Yang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Generating a Chain of Thought (CoT) has been shown to consistently improve
large language model (LLM) performance on a wide range of NLP tasks. However,
prior work has mainly focused on logical reasoning tasks (e.g. arithmetic,
commonsense QA); it remains unclear whether improvements hold for more diverse
types of reasoning, especially in socially situated contexts. Concretely, we
perform a controlled evaluation of zero-shot CoT across two socially sensitive
domains: harmful questions and stereotype benchmarks. We find that zero-shot
CoT reasoning in sensitive domains significantly increases a model's likelihood
to produce harmful or undesirable output, with trends holding across different
prompt formats and model variants. Furthermore, we show that harmful CoTs
increase with model size, but decrease with improved instruction following. Our
work suggests that zero-shot CoT should be used with caution on socially
important tasks, especially when marginalized groups or sensitive topics are
involved.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>ACL 2023 Main Conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Simulate Natural Language Feedback for Interactive Semantic
  Parsing <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.08195v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.08195v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Yan, Saurabh Srivastava, Yintao Tai, Sida I. Wang, Wen-tau Yih, Ziyu Yao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Interactive semantic parsing based on natural language (NL) feedback, where
users provide feedback to correct the parser mistakes, has emerged as a more
practical scenario than the traditional one-shot semantic parsing. However,
prior work has heavily relied on human-annotated feedback data to train the
interactive semantic parser, which is prohibitively expensive and not scalable.
In this work, we propose a new task of simulating NL feedback for interactive
semantic parsing. We accompany the task with a novel feedback evaluator. The
evaluator is specifically designed to assess the quality of the simulated
feedback, based on which we decide the best feedback simulator from our
proposed variants. On a text-to-SQL dataset, we show that our feedback
simulator can generate high-quality NL feedback to boost the error correction
ability of a specific parser. In low-data settings, our feedback simulator can
help achieve comparable error correction performance as trained using the
costly, full set of human annotations.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ACL 2023. 18 pages, 6 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Investigating Massive Multilingual <span class="highlight-title">Pre-Train</span>ed Machine Translation
  Models for Clinical Domain via Transfer Learning <span class="chip">ACL-2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2210.06068v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2210.06068v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lifeng Han, Gleb Erofeev, Irina Sorokina, Serge Gladkoff, Goran Nenadic
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Massively multilingual pre-trained language models (MMPLMs) are developed in
recent years demonstrating superpowers and the pre-knowledge they acquire for
downstream tasks. This work investigates whether MMPLMs can be applied to
clinical domain machine translation (MT) towards entirely unseen languages via
transfer learning. We carry out an experimental investigation using Meta-AI's
MMPLMs ``wmt21-dense-24-wide-en-X and X-en (WMT21fb)'' which were pre-trained
on 7 language pairs and 14 translation directions including English to Czech,
German, Hausa, Icelandic, Japanese, Russian, and Chinese, and the opposite
direction. We fine-tune these MMPLMs towards English-\textit{Spanish} language
pair which \textit{did not exist at all} in their original pre-trained corpora
both implicitly and explicitly. We prepare carefully aligned \textit{clinical}
domain data for this fine-tuning, which is different from their original mixed
domain knowledge. Our experimental result shows that the fine-tuning is very
successful using just 250k well-aligned in-domain EN-ES segments for three
sub-task translation testings: clinical cases, clinical terms, and ontology
concepts. It achieves very close evaluation scores to another MMPLM NLLB from
Meta-AI, which included Spanish as a high-resource setting in the pre-training.
To the best of our knowledge, this is the first work on using MMPLMs towards
\textit{clinical domain transfer-learning NMT} successfully for totally unseen
languages during pre-training.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ClinicalNLP-2023 WS@ACL-2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Are We Really Making Much Progress in Text Classification? A Comparative
  <span class="highlight-title">Review</span> <span class="chip">ACL</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2204.03954v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2204.03954v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Galke, Andor Diera, Bao Xin Lin, Bhakti Khera, Tim Meuser, Tushar Singhal, Fabian Karl, Ansgar Scherp
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  This study reviews and compares methods for single-label and multi-label text
classification, categorized into bag-of-words, sequence-based, graph-based, and
hierarchical methods. The comparison aggregates results from the literature
over five single-label and seven multi-label datasets and complements them with
new experiments. The findings reveal that all recently proposed graph-based and
hierarchy-based methods fail to outperform pre-trained language models and
sometimes perform worse than standard machine learning methods like a
multilayer perceptron on a bag-of-words. To assess the true scientific progress
in text classification, future work should thoroughly test against strong
bag-of-words baselines and state-of-the-art pre-trained language models.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Update: revised text and new included methods. This work is an
  extension of "Bag-of-Words vs. Graph vs. Sequence in Text Classification:
  Questioning the Necessity of Text-Graphs and the Surprising Strength of a
  Wide MLP. ACL (1) 2022: 4038-4051", URL:
  https://aclanthology.org/2022.acl-long.279/</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Improving Self-training for Cross-lingual Named Entity Recognition with
  Contrastive and Prototype Learning <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13628v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13628v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ran Zhou, Xin Li, Lidong Bing, Erik Cambria, Chunyan Miao
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In cross-lingual named entity recognition (NER), self-training is commonly
used to bridge the linguistic gap by training on pseudo-labeled target-language
data. However, due to sub-optimal performance on target languages, the pseudo
labels are often noisy and limit the overall performance. In this work, we aim
to improve self-training for cross-lingual NER by combining representation
learning and pseudo label refinement in one coherent framework. Our proposed
method, namely ContProto mainly comprises two components: (1) contrastive
self-training and (2) prototype-based pseudo-labeling. Our contrastive
self-training facilitates span classification by separating clusters of
different classes, and enhances cross-lingual transferability by producing
closely-aligned representations between the source and target language.
Meanwhile, prototype-based pseudo-labeling effectively improves the accuracy of
pseudo labels during training. We evaluate ContProto on multiple transfer
pairs, and experimental results show our method brings in substantial
improvements over current state-of-the-art methods.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Modular and On-demand Bias Mitigation with Attribute-Removal Subnetworks <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.15171v5">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.15171v5.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Lukas Hauzenberger, Shahed Masoudian, Deepak Kumar, Markus Schedl, Navid Rekabsaz
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Societal biases are reflected in large pre-trained language models and their
fine-tuned versions on downstream tasks. Common in-processing bias mitigation
approaches, such as adversarial training and mutual information removal,
introduce additional optimization criteria, and update the model to reach a new
debiased state. However, in practice, end-users and practitioners might prefer
to switch back to the original model, or apply debiasing only on a specific
subset of protected attributes. To enable this, we propose a novel modular bias
mitigation approach, consisting of stand-alone highly sparse debiasing
subnetworks, where each debiasing module can be integrated into the core model
on-demand at inference time. Our approach draws from the concept of \emph{diff}
pruning, and proposes a novel training regime adaptable to various
representation disentanglement optimizations. We conduct experiments on three
classification tasks with gender, race, and age as protected attributes. The
results show that our modular approach, while maintaining task performance,
improves (or at least remains on-par with) the effectiveness of bias mitigation
in comparison with baseline finetuning. Particularly on a two-attribute
dataset, our approach with separately learned debiasing subnetworks shows
effective utilization of either or both the subnetworks for selective bias
mitigation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted in Findings of ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Learning to Imagine: Visually-Augmented Natural Language Generation <span class="chip">ACL 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16944v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16944v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tianyi Tang, Yushuo Chen, Yifan Du, Junyi Li, Wayne Xin Zhao, Ji-Rong Wen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  People often imagine relevant scenes to aid in the writing process. In this
work, we aim to utilize visual information for composition in the same manner
as humans. We propose a method, LIVE, that makes pre-trained language models
(PLMs) Learn to Imagine for Visuallyaugmented natural language gEneration.
First, we imagine the scene based on the text: we use a diffusion model to
synthesize high-quality images conditioned on the input texts. Second, we use
CLIP to determine whether the text can evoke the imagination in a posterior
way. Finally, our imagination is dynamic, and we conduct synthesis for each
sentence rather than generate only one image for an entire paragraph.
Technically, we propose a novel plug-and-play fusion layer to obtain
visually-augmented representations for each text. Our vision-text fusion layer
is compatible with Transformerbased architecture. We have conducted extensive
experiments on four generation tasks using BART and T5, and the automatic
results and human evaluation demonstrate the effectiveness of our proposed
method. We will release the code, model, and data at the link:
https://github.com/RUCAIBox/LIVE.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Constructing Code-mixed Universal Dependency Forest for Unbiased
  Cross-lingual Relation Extraction 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.12258v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.12258v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Hao Fei, Meishan Zhang, Min Zhang, Tat-Seng Chua
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Latest efforts on cross-lingual relation extraction (XRE) aggressively
leverage the language-consistent structural features from the universal
dependency (UD) resource, while they may largely suffer from biased transfer
(e.g., either target-biased or source-biased) due to the inevitable linguistic
disparity between languages. In this work, we investigate an unbiased UD-based
XRE transfer by constructing a type of code-mixed UD forest. We first translate
the sentence of the source language to the parallel target-side language, for
both of which we parse the UD tree respectively. Then, we merge the
source-/target-side UD structures as a unified code-mixed UD forest. With such
forest features, the gaps of UD-based XRE between the training and predicting
phases can be effectively closed. We conduct experiments on the ACE XRE
benchmark datasets, where the results demonstrate that the proposed code-mixed
UD forests help unbiased UD-based XRE transfer, with which we achieve
significant XRE performance gains.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ UnifieR: A Unified Retriever for Large-Scale Retrieval <span class="chip">KDD</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2205.11194v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2205.11194v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Tao Shen, Xiubo Geng, Chongyang Tao, Can Xu, Guodong Long, Kai Zhang, Daxin Jiang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Large-scale retrieval is to recall relevant documents from a huge collection
given a query. It relies on representation learning to embed documents and
queries into a common semantic encoding space. According to the encoding space,
recent retrieval methods based on pre-trained language models (PLM) can be
coarsely categorized into either dense-vector or lexicon-based paradigms. These
two paradigms unveil the PLMs' representation capability in different
granularities, i.e., global sequence-level compression and local word-level
contexts, respectively. Inspired by their complementary global-local
contextualization and distinct representing views, we propose a new learning
framework, UnifieR which unifies dense-vector and lexicon-based retrieval in
one model with a dual-representing capability. Experiments on passage retrieval
benchmarks verify its effectiveness in both paradigms. A uni-retrieval scheme
is further presented with even better retrieval quality. We lastly evaluate the
model on BEIR benchmark to verify its transferability.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear at KDD ADS 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Target-Side Augmentation for Document-Level Machine Translation <span class="chip">ACL2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.04505v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.04505v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Guangsheng Bao, Zhiyang Teng, Yue Zhang
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Document-level machine translation faces the challenge of data sparsity due
to its long input length and a small amount of training data, increasing the
risk of learning spurious patterns. To address this challenge, we propose a
target-side augmentation method, introducing a data augmentation (DA) model to
generate many potential translations for each source document. Learning on
these wider range translations, an MT model can learn a smoothed distribution,
thereby reducing the risk of data sparsity. We demonstrate that the DA model,
which estimates the posterior distribution, largely improves the MT
performance, outperforming the previous best system by 2.30 s-BLEU on News and
achieving new state-of-the-art on News and Europarl benchmarks. Our code is
available at https://github.com/baoguangsheng/target-side-augmentation.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by ACL2023 main conference</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On the Copying Problem of Unsupervised NMT: A Training Schedule with a
  Language Discriminator Loss 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17182v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17182v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yihong Liu, Alexandra Chronopoulou, Hinrich Schütze, Alexander Fraser
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Although unsupervised neural machine translation (UNMT) has achieved success
in many language pairs, the copying problem, i.e., directly copying some parts
of the input sentence as the translation, is common among distant language
pairs, especially when low-resource languages are involved. We find this issue
is closely related to an unexpected copying behavior during online
back-translation (BT). In this work, we propose a simple but effective training
schedule that incorporates a language discriminator loss. The loss imposes
constraints on the intermediate translation so that the translation is in the
desired language. By conducting extensive experiments on different language
pairs, including similar and distant, high and low-resource languages, we find
that our method alleviates the copying problem, thus improving the translation
performance on low-resource languages.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>IWSLT 2023</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Technical Report on Token Position Bias in <span class="highlight-title">Transformer</span>s 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2304.13567v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2304.13567v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Mehdi Ben Amor, Michael Granitzer, Jelena Mitrović
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Language Models (LMs) have shown state-of-the-art performance in Natural
Language Processing (NLP) tasks. Downstream tasks such as Named Entity
Recognition (NER) or Part-of-Speech (POS) tagging are known to suffer from data
imbalance issues, specifically in terms of the ratio of positive to negative
examples, and class imbalance. In this paper, we investigate an additional
specific issue for language models, namely the position bias of positive
examples in token classification tasks. Therefore, we conduct an in-depth
evaluation of the impact of position bias on the performance of LMs when
fine-tuned on Token Classification benchmarks. Our study includes CoNLL03 and
OntoNote5.0 for NER, English Tree Bank UD_en and TweeBank for POS tagging. We
propose an evaluation approach to investigate position bias in Transformer
models. We show that encoders like BERT, ERNIE, ELECTRA, and decoders such as
GPT2 and BLOOM can suffer from this bias with an average drop of 3\% and 9\% in
their performance. To mitigate this effect, we propose two methods: Random
Position Shifting and Context Perturbation, that we apply on batches during the
training process. The results show an improvement of $\approx$ 2\% in the
performance of the model on CoNLL03, UD_en, and TweeBank.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Updated title of the preprint</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Scan and Snap: Understanding Training Dynamics and Token Composition in
  1-layer <span class="highlight-title">Transformer</span> 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.16380v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.16380v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Yuandong Tian, Yiping Wang, Beidi Chen, Simon Du
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Transformer architecture has shown impressive performance in multiple
research domains and has become the backbone of many neural network models.
However, there is limited understanding on how it works. In particular, with a
simple predictive loss, how the representation emerges from the gradient
\emph{training dynamics} remains a mystery. In this paper, for 1-layer
transformer with one self-attention layer plus one decoder layer, we analyze
its SGD training dynamics for the task of next token prediction in a
mathematically rigorous manner. We open the black box of the dynamic process of
how the self-attention layer combines input tokens, and reveal the nature of
underlying inductive bias. More specifically, with the assumption (a) no
positional encoding, (b) long input sequence, and (c) the decoder layer learns
faster than the self-attention layer, we prove that self-attention acts as a
\emph{discriminative scanning algorithm}: starting from uniform attention, it
gradually attends more to distinct key tokens for a specific next token to be
predicted, and pays less attention to common key tokens that occur across
different next tokens. Among distinct tokens, it progressively drops attention
weights, following the order of low to high co-occurrence between the key and
the query token in the training set. Interestingly, this procedure does not
lead to winner-takes-all, but decelerates due to a \emph{phase transition} that
is controllable by the learning rates of the two layers, leaving (almost) fixed
token combination. We verify this \textbf{\emph{scan and snap}} dynamics on
synthetic and real-world data (WikiText).
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Fix minor issues in the proofs and figures. Update figures to reflect
  the main conclusions more accurately</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ SI-LSTM: Speaker Hybrid Long-short Term Memory and Cross Modal Attention
  for Emotion Recognition in Conversation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.03506v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.03506v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Xingwei Liang, You Zou, Ruifeng Xu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Emotion Recognition in Conversation~(ERC) across modalities is of vital
importance for a variety of applications, including intelligent healthcare,
artificial intelligence for conversation, and opinion mining over chat history.
The crux of ERC is to model both cross-modality and cross-time interactions
throughout the conversation. Previous methods have made progress in learning
the time series information of conversation while lacking the ability to trace
down the different emotional states of each speaker in a conversation. In this
paper, we propose a recurrent structure called Speaker Information Enhanced
Long-Short Term Memory (SI-LSTM) for the ERC task, where the emotional states
of the distinct speaker can be tracked in a sequential way to enhance the
learning of the emotion in conversation. Further, to improve the learning of
multimodal features in ERC, we utilize a cross-modal attention component to
fuse the features between different modalities and model the interaction of the
important information from different modalities. Experimental results on two
benchmark datasets demonstrate the superiority of the proposed SI-LSTM against
the state-of-the-art baseline methods in the ERC task on multimodal data.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Modification needed</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ A First Look at LLM-Powered Generative News Recommendation 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.06566v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.06566v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Qijiong Liu, Nuo Chen, Tetsuya Sakai, Xiao-Ming Wu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Personalized news recommendation systems have become essential tools for
users to navigate the vast amount of online news content, yet existing news
recommenders face significant challenges such as the cold-start problem, user
profile modeling, and news content understanding. Previous works have typically
followed an inflexible routine to address a particular challenge through model
design, but are limited in their ability to understand news content and capture
user interests. In this paper, we introduce GENRE, an LLM-powered generative
news recommendation framework, which leverages pretrained semantic knowledge
from large language models to enrich news data. Our aim is to provide a
flexible and unified solution for news recommendation by moving from model
design to prompt design. We showcase the use of GENRE for personalized news
generation, user profiling, and news summarization. Extensive experiments with
various popular recommendation models demonstrate the effectiveness of GENRE.
We will publish our code and data for other researchers to reproduce our work.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>11 pages</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
            <article>
                <details>
                    <Summary>
                        Optimization and Control <span class="chip" style="font-size: 60%">13</span>
                    </Summary>
                    <div class="details-content">
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ An Inexact Conditional Gradient Method for Constrained Bilevel
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02429v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02429v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Nazanin Abolfazli, Ruichen Jiang, Aryan Mokhtari, Erfan Yazdandoost Hamedani
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilevel optimization is an important class of optimization problems where one
optimization problem is nested within another. This framework is widely used in
machine learning problems, including meta-learning, data hyper-cleaning, and
matrix completion with denoising. In this paper, we focus on a bilevel
optimization problem with a strongly convex lower-level problem and a smooth
upper-level objective function over a compact and convex constraint set.
Several methods have been developed for tackling unconstrained bilevel
optimization problems, but there is limited work on methods for the constrained
setting. In fact, for those methods that can handle constrained problems,
either the convergence rate is slow or the computational cost per iteration is
expensive. To address this issue, in this paper, we introduce a novel
single-loop projection-free method using a nested approximation technique. Our
proposed method has an improved per-iteration complexity, surpassing existing
methods, and achieves optimal convergence rate guarantees matching the
best-known complexity of projection-free algorithms for solving convex
constrained single-level optimization problems. In particular, when the
upper-level objective function is convex, our method requires
$\tilde{\mathcal{O}}(\epsilon^{-1})$ iterations to find an $\epsilon$-optimal
solution. Moreover, when the upper-level objective function is non-convex the
complexity of our method is $\mathcal{O}(\epsilon^{-2})$ to find an
$\epsilon$-stationary point. We also present numerical experiments to showcase
the superior performance of our method compared with state-of-the-art methods.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ A Generalized Alternating Method for Bilevel Optimization under the
  Polyak-Łojasiewicz Condition 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02422v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02422v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Quan Xiao, Songtao Lu, Tianyi Chen
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Bilevel optimization has recently regained interest owing to its applications
in emerging machine learning fields such as hyperparameter optimization,
meta-learning, and reinforcement learning. Recent results have shown that
simple alternating (implicit) gradient-based algorithms can achieve the same
convergence rate of single-level gradient descent (GD) for bilevel problems
with a strongly convex lower-level objective. However, it remains unclear
whether this result can be generalized to bilevel problems beyond this basic
setting. In this paper, we propose a Generalized ALternating mEthod for bilevel
opTimization (GALET) with a nonconvex lower-level objective that satisfies the
Polyak-{\L}ojasiewicz (PL) condition. We first introduce a stationary metric
for the considered bilevel problems, which generalizes the existing metric. We
then establish that GALET achieves an $\epsilon$-stationary metric for the
considered problem within $\tilde{\cal O}(\epsilon^{-1})$ iterations, which
matches the iteration complexity of GD for smooth nonconvex problems.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Complexity of Block Coordinate Descent with Proximal Regularization and
  Applications to Wasserstein CP-dictionary Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02420v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02420v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Dohyun Kwon, Hanbaek Lyu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We consider the block coordinate descent methods of Gauss-Seidel type with
proximal regularization (BCD-PR), which is a classical method of minimizing
general nonconvex objectives under constraints that has a wide range of
practical applications. We theoretically establish the worst-case complexity
bound for this algorithm. Namely, we show that for general nonconvex smooth
objectives with block-wise constraints, the classical BCD-PR algorithm
converges to an epsilon-stationary point within O(1/epsilon) iterations. Under
a mild condition, this result still holds even if the algorithm is executed
inexactly in each step. As an application, we propose a provable and efficient
algorithm for `Wasserstein CP-dictionary learning', which seeks a set of
elementary probability distributions that can well-approximate a given set of
d-dimensional joint probability distributions. Our algorithm is a version of
BCD-PR that operates in the dual space, where the primal problem is regularized
both entropically and proximally.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Proceedings of the 40th International Conference on Machine Learning</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Onsite Job Scheduling by Adaptive Genetic Algorithm 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02296v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02296v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Avijit Basak, Subhas Acharya
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Onsite Job Scheduling is a specialized variant of Vehicle Routing Problem
(VRP) with multiple depots. The objective of this problem is to execute jobs
requested by customers, belonging to different geographic locations by a
limited number of technicians, with minimum travel and overtime of technicians.
Each job is expected to be completed within a specified time limit according to
the service level agreement with customers. Each technician is assumed to start
from a base location, serve several customers and return to the starting place.
Technicians are allotted jobs based on their skill sets, expertise levels of
each skill and availability slots. Although there are considerable number of
literatures on VRP we do not see any explicit work related to Onsite Job
Scheduling. In this paper we have proposed an Adaptive Genetic Algorithm to
solve the scheduling problem. We found an optimized travel route for a
substantial number of jobs and technicians, minimizing travel distance,
overtime duration as well as meeting constraints related to SLA.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Prescriptive PCA: Dimensionality Reduction for Two-stage Stochastic
  Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.02223v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.02223v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Long He, Ho-Yin Mak
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we consider the alignment between an upstream dimensionality
reduction task of learning a low-dimensional representation of a set of
high-dimensional data and a downstream optimization task of solving a
stochastic program parameterized by said representation. In this case, standard
dimensionality reduction methods (e.g., principal component analysis) may not
perform well, as they aim to maximize the amount of information retained in the
representation and do not generally reflect the importance of such information
in the downstream optimization problem. To address this problem, we develop a
prescriptive dimensionality reduction framework that aims to minimize the
degree of suboptimality in the optimization phase. For the case where the
downstream stochastic optimization problem has an expected value objective, we
show that prescriptive dimensionality reduction can be performed via solving a
distributionally-robust optimization problem, which admits a semidefinite
programming relaxation. Computational experiments based on a warehouse
transshipment problem and a vehicle repositioning problem show that our
approach significantly outperforms principal component analysis with real and
synthetic data sets.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                         ☆ Provable convergence guarantees for black-box variational inference 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2306.03638v1">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2306.03638v1.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Justin Domke, Guillaume Garrigos, Robert Gower
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  While black-box variational inference is widely used, there is no proof that
its stochastic optimization succeeds. We suggest this is due to a theoretical
gap in existing stochastic optimization proofs-namely the challenge of gradient
estimators with unusual noise bounds, and a composite non-smooth objective. For
dense Gaussian variational families, we observe that existing gradient
estimators based on reparameterization satisfy a quadratic noise bound and give
novel convergence guarantees for proximal and projected stochastic gradient
descent using this bound. This provides the first rigorous guarantee that
black-box variational inference converges for realistic inference problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>32 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ An SDE for Modeling SAM: Theory and Insights <span class="chip">ICML 2023</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2301.08203v3">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2301.08203v3.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Enea Monzio Compagnoni, Luca Biggio, Antonio Orvieto, Frank Norbert Proske, Hans Kersting, Aurelien Lucchi
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study the SAM (Sharpness-Aware Minimization) optimizer which has recently
attracted a lot of interest due to its increased performance over more
classical variants of stochastic gradient descent. Our main contribution is the
derivation of continuous-time models (in the form of SDEs) for SAM and two of
its variants, both for the full-batch and mini-batch settings. We demonstrate
that these SDEs are rigorous approximations of the real discrete-time
algorithms (in a weak sense, scaling linearly with the learning rate). Using
these models, we then offer an explanation of why SAM prefers flat minima over
sharp ones~--~by showing that it minimizes an implicitly regularized loss with
a Hessian-dependent noise structure. Finally, we prove that SAM is attracted to
saddle points under some realistic conditions. Our theoretical results are
supported by detailed experiments.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted at ICML 2023 (Poster)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Control results for a model of resonant interaction between short and
  long capillary-gravity waves 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2104.12489v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2104.12489v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Roberto de A. Capistrano-Filho, Ademir Pampu
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  The purpose of this article is the investigation of the global control
properties of a coupled nonlinear dispersive system posed in the periodic
domain $\mathbb{T}$, a system with the structure of a nonlinear Schr\"odinger
equation and a nonlinear Korteweg-de Vries equation. Combining estimates
derived from Bourgain spaces and using microlocal analysis we show that this
system has global control properties. The main novelty of this work is twofold.
One is that the global results for the nonlinear system are presented for the
first time thanks to the propagation of singularities. The second one is that
these propagation results are shown to a coupled dispersive system with two
equations defined by differential operators with principal symbols of different
orders.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>To appear on Nonlinear Differential Equations and Applications
  (NoDEA) - 28 pages</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ On Optimal Regularization Parameters via Bilevel Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.18394v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.18394v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Matthias J. Ehrhardt, Silvia Gazzola, Sebastian J. Scott
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Variational regularization is commonly used to solve linear inverse problems,
and involves augmenting a data fidelity by a regularizer. The regularizer is
used to promote a priori information, and is weighted by a regularization
parameter. Selection of an appropriate regularization parameter is critical,
with various choices leading to very different reconstructions. Existing
strategies such as the discrepancy principle and L-curve can be used to
determine a suitable parameter value, but in recent years a supervised machine
learning approach called bilevel learning has been employed. Bilevel learning
is a powerful framework to determine optimal parameters, and involves solving a
nested optimisation problem. While previous strategies enjoy various
theoretical results, the well-posedness of bilevel learning in this setting is
still a developing field. One necessary property is positivity of the
determined regularization parameter. In this work, we provide a new condition
that better characterises positivity of optimal regularization parameters than
the existing theory. Numerical results verify and explore this new condition
for both small and large dimensional problems.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>26 pages, 6 figures. Fixed typos in the header and Lemma 3</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Convex Risk Bounded Continuous-Time Trajectory Planning and Tube Design
  in Uncertain Nonconvex Environments 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.17291v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.17291v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Ashkan Jasour, Weiqiao Han, Brian Williams
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  In this paper, we address the trajectory planning problem in uncertain
nonconvex static and dynamic environments that contain obstacles with
probabilistic location, size, and geometry. To address this problem, we provide
a risk bounded trajectory planning method that looks for continuous-time
trajectories with guaranteed bounded risk over the planning time horizon. Risk
is defined as the probability of collision with uncertain obstacles. Existing
approaches to address risk bounded trajectory planning problems either are
limited to Gaussian uncertainties and convex obstacles or rely on
sampling-based methods that need uncertainty samples and time discretization.
To address the risk bounded trajectory planning problem, we leverage the notion
of risk contours to transform the risk bounded planning problem into a
deterministic optimization problem. Risk contours are the set of all points in
the uncertain environment with guaranteed bounded risk. The obtained
deterministic optimization is, in general, nonlinear and nonconvex time-varying
optimization. We provide convex methods based on sum-of-squares optimization to
efficiently solve the obtained nonconvex time-varying optimization problem and
obtain the continuous-time risk bounded trajectories without time
discretization. The provided approach deals with arbitrary (and known)
probabilistic uncertainties, nonconvex and nonlinear, static and dynamic
obstacles, and is suitable for online trajectory planning problems. In
addition, we provide convex methods based on sum-of-squares optimization to
build the max-sized tube with respect to its parameterization along the
trajectory so that any state inside the tube is guaranteed to have bounded
risk.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted by IJRR (extension of RSS 2021 paper arXiv:2106.05489
  invited to IJRR)</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Sinkhorn Distributionally Robust Optimization 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2109.11926v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2109.11926v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Jie Wang, Rui Gao, Yao Xie
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We study distributionally robust optimization (DRO) with Sinkhorn distance --
a variant of Wasserstein distance based on entropic regularization. We derive
convex programming dual reformulation for general nominal distributions,
transport costs, and loss functions. Compared with Wasserstein DRO, our
proposed approach offers enhanced computational tractability for a broader
class of loss functions, and the worst-case distribution exhibits greater
plausibility in practical scenarios. To solve the dual reformulation, we
develop a stochastic mirror descent algorithm with biased gradient oracles.
Remarkably, this algorithm achieves near-optimal sample complexity for both
smooth and nonsmooth loss functions, nearly matching the sample complexity of
the Empirical Risk Minimization counterpart. Finally, we provide numerical
examples using synthetic and real data to demonstrate its superior performance.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>57 pages, 9 figures</span>
                                        </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Layer-wise Adaptive Step-Sizes for Stochastic First-Order Methods for
  Deep Learning 
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2305.13664v2">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2305.13664v2.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Achraf Bahamou, Donald Goldfarb
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  We propose a new per-layer adaptive step-size procedure for stochastic
first-order optimization methods for minimizing empirical loss functions in
deep learning, eliminating the need for the user to tune the learning rate
(LR). The proposed approach exploits the layer-wise stochastic curvature
information contained in the diagonal blocks of the Hessian in deep neural
networks (DNNs) to compute adaptive step-sizes (i.e., LRs) for each layer. The
method has memory requirements that are comparable to those of first-order
methods, while its per-iteration time complexity is only increased by an amount
that is roughly equivalent to an additional gradient computation. Numerical
experiments show that SGD with momentum and AdamW combined with the proposed
per-layer step-sizes are able to choose effective LR schedules and outperform
fine-tuned LR versions of these methods as well as popular first-order and
second-order algorithms for training DNNs on Autoencoder, Convolutional Neural
Network (CNN) and Graph Convolutional Network (GCN) models. Finally, it is
proved that an idealized version of SGD with the layer-wise step sizes
converges linearly when using full-batch gradients.
</span>
                                    </div>
                                </details>
                            </article>
                            <article>
                                <details class="article-expander">
                                    <summary class="article-expander-title">
                                        ♻ ☆ Practical and Matching Gradient Variance Bounds for Black-Box
  Variational Bayesian Inference <span class="chip">ICML'23</span>
                                    </summary>
                                    <div class="article-authors">
                                        <a href="http://arxiv.org/abs/2303.10472v4">
                                            <i class="ri-links-line"></i>
                                        </a>
                                        <a href="https://arxiv.org/pdf/2303.10472v4.pdf">
                                            <i class="ri-file-paper-2-line"></i>
                                        </a>
                                        Kyurae Kim, Kaiwen Wu, Jisu Oh, Jacob R. Gardner
                                    </div>
                                    <div class="article-summary-box-inner">
                                        <span>  Understanding the gradient variance of black-box variational inference (BBVI)
is a crucial step for establishing its convergence and developing algorithmic
improvements. However, existing studies have yet to show that the gradient
variance of BBVI satisfies the conditions used to study the convergence of
stochastic gradient descent (SGD), the workhorse of BBVI. In this work, we show
that BBVI satisfies a matching bound corresponding to the $ABC$ condition used
in the SGD literature when applied to smooth and quadratically-growing
log-likelihoods. Our results generalize to nonlinear covariance
parameterizations widely used in the practice of BBVI. Furthermore, we show
that the variance of the mean-field parameterization has provably superior
dimensional dependence.
</span>
                                    </div>
                                        <div class="article-summary-box-inner">
                                            <span class="chip">comment</span>: <span>Accepted to ICML'23 for live oral presentation</span>
                                        </div>
                                </details>
                            </article>
                    </div>
                </details>
            </article>
    </section>

</body>

<footer>
    <div>
        <time id="build-timestamp" datetime="2023-06-12T05:24:00.433077894Z">
            2023-06-12 05:24:00 UTC
        </time>
    </div>
</footer>
<script src="index.js"></script>
</html>
